\chapter{Parser Architecture}

\label{parser architecture}

XXX WHEN FINISHED, CHECK ALL REFERENCES TO ENSURE THEY POINT AT THE CORRECT
CHAPTER\@.

XXX INTEGRATE THE NEXT PARAGRAPH INTO THIS SECTION\@.

The solution developed is conceptually simple: provide a few generic
functions (\textit{actions\/}), each capable of dealing with an entire
category of inputs (e.g.\ rejecting a mail delivery attempt), accompanied
by a multitude of precise patterns (\textit{rules\/}), each of which
matches all inputs of a specific type and only that type (e.g.\ rejection
by a specific \acronym{DNSBL}).  It is an accepted standard to separate the
parsing procedure from the declarative grammar it operates with; part of
the novelty here is in the way that the grammar is itself partially
procedural (each action is a separate procedure).  This architecture is
ideally suited to parsing inputs where the input is not fully understood or
does not conform to a fixed grammar: the architecture warns about unparsed
inputs and other errors, but continues parsing as best it can, allowing the
developer of a new parser to decide which deficiencies are most important
and require attention first, rather than being forced to fix the first
error that arises.


To avoid cluttering the explanation of the parser architecture with the
details involved in implementing a parser for Postfix log files, the two
topics have been separated.  This chapter presents the architecture
developed for this project, beginning with the overall architecture and
design, followed by the three components of the architecture: Framework,
Actions, and Rules.  This chapter centers on the theoretical,
implementation-independent aspects of the architecture; the practical
difficulties of writing a parser for Postfix log files are covered in
detail in \sectionref{Postfix Parser Implementation}.

\section{Parser Architecture and Design}

\label{parser design}

It should be clear from the earlier Postfix background (\sectionref{postfix
background}) that log files produced by Postfix may vary widely from host
to host, depending on the set of restrictions chosen by the administrator.
With this in mind, one of the architecture's design aims was to make adding
new rules to parse new inputs as easy as possible, to enable administrators
to properly parse their own log files.  To enable this the architecture is
divided into three parts: Framework, Actions, and Rules.  Each will be
discussed separately, but first an overview:

\begin{eqlist}

    \item [Framework]  The framework is the structure that actions and
        rules plug into.  It manages the parsing process, providing shared
        data storage, loading and validation of rules, storage of results,
        and other support functions.

    \item [Actions] Each action performs the work required to deal with a
        single category of inputs, e.g.\ rejecting a delivery attempt.
        Actions are invoked to process an input once it has been classified
        by a rule.

    \item [Rules]  The rules are responsible for classifying inputs; each
        rule recognises one input variant, and specifies the action that
        will be invoked to process the input. Rules provide an easily
        extensible method of associating inputs with actions.

\end{eqlist}

For each input the framework tries each rule in turn until it finds a rule
that recognises the input, then invokes the action specified by that rule.
If the input is not recognised by any of the rules, the framework issues a
warning; the framework can continue parsing, although for some inputs
stopping immediately may be preferable.

\label{why separate rules, actions, and framework?}

Decoupling the parsing rules from their associated actions allows new rules
to be written and tested without requiring modifications to the parser
source code, significantly lowering the barrier to entry for casual users
who need to parse new inputs, e.g.\ part-time systems administrators
attempting to combat and reduce spam; it also allows companies to develop
user-extensible parsers without divulging their source code.  Decoupling
the framework, actions, and rules simplifies all three and creates a clear
separation of functionality: the framework manages the parsing process and
provides services to the actions, but does not need to perform anything
specific to the input being parsed; actions benefit from having services
provided by the framework, freeing them to concentrate on the task of
accurately and correctly processing the inputs and the information provided
by rules; rules are responsible for classifying inputs, and extracting data
from those inputs for processing by actions.

Separating the rules from the actions and framework makes it possible to
parse new inputs without modifying the core parsing algorithm.  Adding a
new rule with the action to invoke and a regex to recognise the inputs is
trivial in comparison to understanding an entire parser, identifying the
correct location to change, and making the appropriate changes.  Bear in
mind that changes to a parser must be made without adversely affecting
existing parsing, particularly as there may be edge cases that are not
immediately obvious --- \sectionref{yet-more-aborted-delivery-attempts}
describes a complication that occurs only four times in \numberOFlogFILES{}
log files.  Requiring changes to a parser's source code also complicates
upgrades of the parser, because the changes must be preserved during the
upgrade, and they may clash with changes made by the developer.  This
architecture allows the user to add new rules without having to edit a
parser, unless the new inputs cannot be processed by the existing actions.
If the new inputs do require new functionality, new actions can be added to
the parser without having to modify existing actions; only in the rare
event that the new actions need to cooperate with existing actions will
more extensive changes be required.

There is some similarity between this architecture and William Wood's
\acronym{ATN}~\cite{atns,nlpip}, used in Computational Linguistics for
creating grammars to parse or generate sentences.  The resemblance between
the two (shown in \tableref{Similarities between ATN and this
architecture}) is accidental, but it is clear that the two different
approaches share a similar division of responsibilities, despite having
different semantics.

% Do Not Reformat!

\begin{table}[ht]
    \caption{Similarities between ATN and this architecture}
    \empty{}\label{Similarities between ATN and this architecture}
    \begin{tabular}[]{lll}
        \tabletopline{}%
        \acronym{ATN}   & Architecture  & Similarity                  \\
        \tablemiddleline{}%
        Networks        & Parser        & Determines the sequence
                                          of transitions              \\
                        &               & or actions that
                                          constitutes a valid input.  \\
        Transitions     & Actions       & Assemble data and
                                          impose conditions the       \\
                        &               & input must meet to be
                                          accepted as valid.          \\
        Abbreviations   & Rules         & Responsible for
                                          classifying input.          \\
        \tablebottomline{}%
    \end{tabular}
\end{table}

The architecture can be thought of as implementing transduction: it takes
data in one form and transforms it to another form; \parsername{}
transforms log files to a database.  XXX NEED A REFERENCE FOR
TRANSDUCTION\@.

Unlike traditional parsers such as those used when compiling a programming
language, this architecture does not require a fixed grammar specification
that inputs must adhere to.  The architecture is capable of dealing with
interleaved inputs, out of order inputs, and ambiguous inputs where
heuristics must be applied --- all have arisen and been successfully
accommodated in the Postfix log file parser.


\section{Framework}

XXX EXTEND FRAMEWORK SECTION\@: EXPAND ON ALL OF THE POINTS IN THE LIST IN
THE NEXT PARAGRAPH\@.

\label{framework in architecture}

The framework manages the parsing process and provides support functions,
freeing the programmers writing actions to concentrate on writing
productive code.  It links actions and rules, allowing either to be
improved independently of the other, and allows new rules to be written
without needing changes to the source code of a parser.  The framework is
the core of the architecture and is deliberately quite simple: the rules
deal with the variation in inputs, and the actions deal with the
intricacies and complications encountered during parsing.

The function that finds the rule matching the input and invokes the
requested action can be expressed in pseudo-code as:

\begin{verbatim}
INPUT:
for each input {
    for each rule defined by the user {
        if this rule matches the input {
            perform the action specified by the rule
            next INPUT
        }
    }
    warn the user that the input was not parsed
}
\end{verbatim}

Most parsers will require the same basic functionality from the framework,
plus some specialised support functions.  XXX EXPAND THIS\@.

XXX TURN THIS INTO A LIST AND EXPAND THE ITEMS\@.  It provides shared
storage to pass data between actions, saves and loads state, loads and
validates rules, manages parsing, invokes actions, stores results of
parsing, and miscellaneous other tasks.

\section{Actions}

\label{actions in architecture}

Each action is a separate procedure written to process a particular
category of input, e.g.\ rejecting a delivery attempt.  There may be many
input variants within one input category; in general each action will
handle one category of inputs, with each rule recognising one input
variant.  XXX EXPAND AND CLARIFY\@; THE MESSAGE IS HERE, IT JUST NEEDS TO
BE CLEARER\@.  It is anticipated that parsers based on this architecture
will have a high ratio of rules to actions, with the aim of having simple
rules and clear distinctions between the inputs parsed by different rules.
An action may process different input variants in slightly different ways,
but large changes in processing indicate the need for a new action and a
new category of input; if an action becomes unnecessarily complicated it
starts to turn into a monolithic parser, with too much logic contained in a
single procedure.

The ability to easily add special purpose actions to deal with difficulties
and new requirements that are discovered during parser development is one
of the strengths of this architecture.  XXX REWRITE, EXPAND, AND CLARIFY UP
TO THE NEXT XXX\@.  Instead of writing a single monolithic function that
must be modified to support new behaviour, with all the attendant risks of
adversely affecting the existing parser, when a new requirement arises an
independent action can be written to satisfy it.  Sometimes the new action
will require the cooperation of other actions, e.g.\ to set or check a
flag, or save some extra data.  There is a possibility of introducing
failure when modifying existing actions in this way, but the modifications
will be smaller and occur less frequently than with a monolithic
architecture, thus failures will be less likely to occur and will be easier
to test for and diagnose.  XXX FINISH REWRITING\@.  The architecture can be
implemented in an object oriented style, allowing sub-classes to extend or
override actions in addition to adding new actions; because each action is
an independent procedure, the sub-class need only modify the action it is
overriding, and can inherit the others rather than reproducing large chunks
of functionality.

During development of the Postfix log parser it became apparent that in
addition to the obvious variety in log~lines there were
many complications to be overcome.  Some were the result of deficiencies in
Postfix's logging (some of which were rectified by later versions of
Postfix, e.g.\ identifying bounce
notifications~\sectionref{identifying-bounce-notifications}); others were
due to the vagaries of process scheduling, client behaviour, and
administrative actions.  All were successfully accommodated in the Postfix
log parser: adding new actions was enough to overcome several of the
complications; others required modifications to a single existing action to
work around the difficulties; the remainder were resolved by adapting
existing actions to cooperate and exchange extra data (via the framework),
changing their behaviour as appropriate based on that extra data.  Every
architecture should aim to make the hard things possible as well as making
the easy things easy; the successful implementation of the Postfix log
parser demonstrates that this architecture achieves that aim.

Actions are not context-sensitive, but they can consult the results (or
lack of results) of previous actions during execution, providing some
context sensitivity.  XXX EXPAND\@.

Actions may return a modified input line that will be parsed as if read
from the input stream, allowing for a simplified version of cascaded
parsing~\cite{cascaded-parsing}.  This powerful facility allows several
rules and actions to parse a single input, potentially simplifying both
rules and actions.  XXX EXPAND WITH CHEMICAL FORMULAE EXAMPLE\@.

Glucose: $C_{6}H_{12}O_{6}$
Hydrocarbon: $CH_{3}{(CH_{2})}_{50}CH_{3}$

\section{Rules}

\label{rules in architecture}

Rules are responsible for categorising inputs: each rule should recognise
one and only input variant.  An input category with multiple input variants
should have multiple rules, one for each variant.  Rules will typically use
a regex when classifying inputs, but other approaches may prove useful for
some applications, e.g.\ comparing fixed strings to the input, or checking
the length of the input; for the remainder of this thesis it will be
assumed that a regex is used.  Each rule must specify at least the regex to
recognise inputs and the action to invoke when classification is
successful, but implementations are free to add any other attributes they
require; \sectionref{rule attributes} describes the attributes used in
\parsername{}, and some potential attributes will be discussed later in
this section (\sectionref{attaching conditions to rules} and
\sectionref{prioritisation and rule ordering}).  Classifying inputs using
the rules is simple: the first rule to recognise the input determines the
action that will be invoked: there is no backtracking to try alternate
rules; no attempt is made to pick a \textit{best\/} rule.

\subsection{Adding New Rules}

The framework issues a warning for each unparsed input, so it is evident
when the ruleset needs to be augmented.  Parsing new inputs is achieved in
one of three ways:

\begin{enumerate}

    \item Modify an existing rule's regex, because the new input is part of
        an existing variant.

    \item Write a new rule that pairs an existing action with a new regex;
        this adds a new variant to an existing category.

    \item Create a new category of inputs and a new action to process
        inputs from the new category; write a new rule pairing the new
        action with a new regex.

\end{enumerate}

Decoupling the rules from the actions and framework enables other rule
management approaches to be used, e.g.\ instead of manually editing
existing rules or adding new rules, machine learning techniques could be
used to automate the process.  If this approach was taken the choice of
machine learning technique would be constrained by the size of typical data
sets (see \sectionref{parser efficiency}).  Techniques requiring the full
data set when training would be impractical; Instance Based
Learning~\cite{instance-based-learning} techniques that automatically
determine which inputs from the training set are valuable and which inputs
can be discarded might reduce the data required to a manageable size.  A
parser could also dynamically create new rules in response to certain
inputs, e.g.\ XXX GIVE A GOOD EXAMPLE\@.  These avenues of research and
development have not been pursued by the author, but the architecture
allows them to easily be undertaken independently.

\subsection{Attaching Conditions To Rules}

\label{attaching conditions to rules}

XXX ATTACHING CONDITIONS TO RULES\@; GIVE PLP EXAMPLE AND REFER TO
IMPLEMENTATION AND RESULTS SECTIONS\@?

XXX STATEFUL PARSING ENABLED BY CONDITIONS, E.G\@.  PARSING C COMMENTS\@.

\subsection{Prioritisation and Rule Ordering}

\label{prioritisation and rule ordering}

XXX PRIORITY ATTRIBUTE AND RULE ORDERING\@.

The first matching rule wins: no further rules are tried against that
input, but there is a mechanism for prioritising the rules so that more
specific rules can be tried first.

\subsection{Overlapping Rules}

\label{overlapping rules in architecture}

XXX IS THERE ANY PREVIOUS RESEARCH IN THIS AREA\@?

When adding new rules, the rule author must be aware that the new rule may
overlap with one or more existing rules, i.e.\ some inputs could be parsed
by more than one rule.  The architecture does not try to detect overlapping
rules: that responsibility is left to the author of the rules.
Unintentionally overlapping rules lead to inconsistent parsing and data
extraction because the first matching rule wins, and the order in which
rules are tried against each input might change between parser invocations.
Overlapping rules are frequently a requirement, allowing a more specific
rule to match some inputs and a more general rule to match the remainder,
e.g.\ separating \acronym{SMTP} delivery to specific sites from
\acronym{SMTP} delivery to the rest of the world.  Allowing overlapping
rules simplifies both the general rule and the more specific rule;
additionally rules from different sources can be combined with a minimum of
prior cooperation or modification required.  Overlapping rules should have
a priority attribute to specify their relative ordering; negative
priorities may be useful for catchall rules.

XXX CHECK WITH CARL WHETHER I SHOULD SAY CFL OR CFG IN THE NEXT
PARAGRAPH\@.  XXX DO I NEED TO MENTION CFG/CFL AT ALL\@?

Decoupling the rules from the actions allows external tools to be written
to detect overlapping rules.  Traditional regexes are equivalent in
computational power to \acronym{FA} and can be converted to \acronym{FA},
so regex overlap can be detected by finding a non-empty intersection of two
\acronym{FA}\@.  The standard equation for \acronym{FA} intersection (given
for example in~\cite{intersection-of-NFA-using-Z}) uses DeMorgan's law:
$FA1~\cap{}~FA2~=~\overline{(\overline{FA1}~\cup{}~\overline{FA2})}$, which
has considerable computation complexity.  XXX COMMENT ON THE COMPUTATIONAL
COMPLEXITY OF INVERSION AND UNION\@; ADD A REFERENCE TO THE SPINNING WHEEL
BOOK\@.  Perl 5.10 regexes~\cite{perlre} are more powerful than traditional
regexes: it is possible to match correctly balanced brackets nested to an
arbitrary depth, e.g.\ \verb!/^[^<>]*(<(?:(?>[^<>]+)|(?1))*>)[^<>]*$/!
matches \verb!z<123<pq<>rs>j<r>ml>s!.  Matching balanced brackets requires
the regex engine to maintain state on a stack, so Perl 5.10 regexes are
equivalent in computational power to \acronym{PDA} or a \acronym{CFL};
detecting overlap may require calculating the intersection of two
\acronym{PDA} or \acronyms{CFL} instead of two \acronym{FA}.  The
intersection of two \acronyms{CFL} is not closed, i.e.\ the resulting
language cannot always be parsed by a \acronym{CFL}, so intersection may be
intractable in some cases, e.g.:
$a^{*}b^{n}c^{n}~\cap~a^{n}b^{n}c^{*}~\rightarrow~a^{n}b^{n}c^{n}$.
Detecting overlap amongst $n$ regexes requires calculating $n(n-1)/2$
intersections, resulting in $O(n^2x)$ complexity, where $O(x)$ is the
complexity of calculating \acronym{FA} or \acronym{PDA} intersection.  This
is certainly not a task to be performed every time the parser runs: naive
detection of overlap amongst the Postfix log parser's \numberOFrules{}
rules would require calculating \numberOFruleINTERSECTIONS{} intersections.
When detecting overlap any conditions attached to rules must be taken into
account, because two rules whose regexes overlap may have conditions
attached which prevent the overlap in practice.  A less naive approach to
overlap detection would first check for overlapping conditions amongst
rules, and then check for overlapping regexes amongst the rules in each set
of rules with overlapping conditions.  Depending on the conditions employed
a rule may fall into more than one group, e.g.\ given these three
conditions:

\begin{enumerate}

    % Reduce inter-item spacing because each item is a single line.
    \setlength{\itemsep}{0pt}

    \item $total < 10$

    \item $total > 20$

    \item $total < 30$

\end{enumerate}

\noindent{}The first and second conditions do not overlap, but the third
condition does overlap with both the first and second conditions.  If the
rules are divided into sets based on how their conditions overlap, the
complexity of detecting overlap amongst $n$ rules is:

$$O(n^{2}y)~+~\sum{O(|s|^{2}x)~\forall{}~s~\in{}~overlaps}$$

where:

\begin{tabular}[]{rcl}

            $y$ & = & cost of checking for overlap between two conditions \\
            $x$ & = & cost of checking for overlap between two regexes    \\
     $overlaps$ & = & set of sets of rules with overlapping conditions    \\

\end{tabular}

\subsection{Pathological Rules}

It is possible to define pathological regexes which fall into two main
categories: regexes that match inputs they should not, and regexes that
consume excessive amounts of CPU time during matching.  Defining a regex
that matches inputs it should not is trivial: \verb!/^/! matches the start
of every input.  This regex would be highlighted by a tool that detects
overlapping rules, and would easily be noticed by visual inspection, but
more complex regexes would be harder to spot.  Usually excessive CPU time
is consumed when a regex with a lot of alteration and variable quantifiers
fails to match, but successful matching is generally quite fast
(see~\cite{mastering-regular-expressions} for in-depth discussion).  XXX
THIS NEEDS TO BE EXPANDED, AND GIVE AN EXAMPLE\@.

XXX PATHOLOGICAL CONDITIONS\@.


\subsection{XXX TO BE DEALT WITH}

XXX DUNNO WHERE TO PUT THIS\@; IT SHOULD PROBABLY GO IN THE RULES SECTION,
BUT WHERE\@?

Rules are self-contained and can be understood in isolation, without
reference to other rules.  This is similar to a \acronym{CFG}; in
\acronym{CFG} terms the parser rules could be described as:

$\text{\textless{}input\textgreater{}} \mapsto \text{rule-1} |
\text{rule-2} | \text{rule-3} | \dots | \text{rule-n}$

Rules can not take into account past or future inputs when classifying the
current input, though conditions can affect which rules will be used to
classify each input.  The architecture is line oriented at present: there
is no facility for rules to consume more input or push unused input back
onto the input stream.  The current input can be manipulated by actions,
using the cascaded parsing feature.  This was not a deliberate design
decision, rather a consequence of the line oriented nature of Postfix log
files; more flexible approaches could be pursued.

\section{Conclusion}

XXX TO BE WRITTEN\@.
