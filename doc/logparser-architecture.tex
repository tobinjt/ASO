\chapter{Parser Architecture}

\label{parser architecture}

To avoid cluttering the explanation of the parser architecture with the
details involved in implementing a parser for Postfix log files, the two
topics have been separated.  This chapter presents the architecture
developed for this project, beginning with the overall architecture and
design, followed by the three components of the architecture: Framework,
Actions, and Rules.  This chapter centers on the theoretical,
implementation-independent aspects of the architecture; the practical
difficulties of writing a parser for Postfix log files are covered in
detail in \sectionref{Postfix Parser Implementation}.

\section{Parser Architecture and Design}

\label{parser design}

It should be clear from the earlier Postfix background (\sectionref{postfix
background}) that log files produced by Postfix may vary widely from host
to host, depending on the set of restrictions chosen by the administrator.
With this in mind, one of the architecture's design aims was to make adding
new rules to parse new inputs as easy as possible, to enable administrators
to properly parse their own log files.  The solution developed is to divide
the architecture into three parts: Framework, Actions, and Rules.  Each
will be discussed separately, but first an overview:

\begin{eqlist}[\def\makelabel#1{\bfseries#1}]

    \item [Framework]  The framework is the structure that actions and
        rules plug into.  It manages the parsing process, providing shared
        data storage, loading and validation of rules, storage of results,
        and other support functions.

    \item [Actions] Each action performs the work required to deal with a
        single \textit{category\/} of inputs, e.g.\ rejecting a delivery
        attempt.  Actions are invoked to process an input once it has been
        classified by a rule.

    \item [Rules]  The rules are responsible for classifying inputs; each
        rule recognises one input \textit{variant}, and specifies the
        action that will be invoked to process the input.  Each input
        category may have many input variants.  Rules provide an easily
        extensible method of associating inputs with actions.

\end{eqlist}

For each input the framework tries each rule in turn until it finds a rule
that recognises the input, then invokes the action specified by that rule.
If the input is not recognised by any of the rules, the framework issues a
warning; the framework can continue parsing, although for some inputs
stopping immediately may be preferable.

\label{why separate rules, actions, and framework?}

Decoupling the parsing rules from their associated actions allows new rules
to be written and tested without requiring modifications to the parser
source code, significantly lowering the barrier to entry for casual users
who need to parse new inputs, e.g.\ part-time systems administrators
attempting to combat and reduce spam; it also allows companies to develop
user-extensible parsers without divulging their source code.  Decoupling
the framework, actions, and rules simplifies all three and creates a clear
separation of functionality: the framework manages the parsing process and
provides services to the actions, but does not need to perform anything
specific to the input being parsed; actions benefit from having services
provided by the framework, freeing them to concentrate on the task of
accurately and correctly processing the inputs and the information provided
by rules; rules are responsible for classifying inputs, and extracting data
from those inputs for processing by actions.

Separating the rules from the actions and framework makes it possible to
parse new inputs without modifying the core parsing algorithm.  Adding a
new rule with the action to invoke and a regex to recognise the inputs is
trivial in comparison to understanding an entire parser, identifying the
correct location to change, and making the appropriate changes.  Bear in
mind that changes to a parser must be made without adversely affecting
existing parsing, particularly as there may be edge cases that are not
immediately obvious --- \sectionref{yet-more-aborted-delivery-attempts}
describes a complication that occurs only four times in \numberOFlogFILES{}
log files.  Requiring changes to a parser's source code also complicates
upgrades of the parser, because the changes must be preserved during the
upgrade, and they may clash with changes made by the developer.  This
architecture allows the user to add new rules without having to edit a
parser, unless the new inputs cannot be processed by the existing actions.
If the new inputs do require new functionality, new actions can be added to
the parser without having to modify existing actions; only in the rare
event that the new actions need to cooperate with existing actions will
more extensive changes be required.

There is some similarity between this architecture and William Wood's
\acronym{ATN}~\cite{atns,nlpip}, used in Computational Linguistics for
creating grammars to parse or generate sentences.  The resemblance between
the two (shown in \tableref{Similarities between ATN and this
architecture}) is accidental, but it is clear that the two different
approaches share a similar division of responsibilities, despite having
different semantics.

% Do Not Reformat!

\begin{table}[ht]
    \caption{Similarities between ATN and this architecture}
    \empty{}\label{Similarities between ATN and this architecture}
    \begin{tabular}[]{lll}
        \tabletopline{}%
        \acronym{ATN}   & Architecture  & Similarity                  \\
        \tablemiddleline{}%
        Networks        & Parser        & Determines the sequence
                                          of transitions              \\
                        &               & or actions that
                                          constitutes a valid input.  \\
        Transitions     & Actions       & Assemble data and
                                          impose conditions the       \\
                        &               & input must meet to be
                                          accepted as valid.          \\
        Abbreviations   & Rules         & Responsible for
                                          classifying input.          \\
        \tablebottomline{}%
    \end{tabular}
\end{table}

The architecture can be thought of as implementing transduction: it takes
data in one form and transforms it to another form; \parsername{}
transforms log files to a database.  XXX NEED A REFERENCE FOR
TRANSDUCTION\@.

Unlike traditional parsers such as those used when compiling a programming
language, this architecture does not require a fixed grammar specification
that inputs must adhere to.  The architecture is capable of dealing with
interleaved inputs, out of order inputs, and ambiguous inputs where
heuristics must be applied --- all have arisen and been successfully
accommodated in the Postfix log file parser.  This architecture is ideally
suited to parsing inputs where the input is not fully understood or does
not conform to a fixed grammar: the architecture warns about unparsed
inputs and other errors, but continues parsing as best it can, allowing the
developer of a new parser to decide which deficiencies are most important
and the order to address them in, rather than being forced to fix the first
error that arises.

\section{Framework}

\label{framework in architecture}

The framework manages the parsing process and provides support functions,
freeing the programmers writing actions to concentrate on writing
productive code.  It links actions and rules, allowing either to be
improved independently of the other, and allows new rules to be written
without needing changes to the source code of a parser.  The framework is
the core of the architecture and is deliberately quite simple: the rules
deal with the variation in inputs, and the actions deal with the
intricacies and complications encountered during parsing.  The function
that finds the rule matching the input and invokes the requested action can
be expressed in pseudo-code as:

\begin{verbatim}
INPUT:
for each input {
    for each rule defined by the user {
        if this rule recognises the input {
            perform the action specified by the rule
            next INPUT
        }
    }
    warn the user that the input was not parsed
}
\end{verbatim}

Most parsers will require the same basic functionality from the framework;
it is responsible for managing the parsing process from start to finish,
which generally will involve the following steps:

\begin{description}

    \item [Register actions]  Each action needs to be registered with the
        framework so that the framework knows about it; the list of
        registered actions will alsobe used when validating rules.

    \item [Load and validate rules]  The framework loads the rules from
        wherever they are stored: a simple file, a database, or possibly
        even a web server or other network service --- though that would
        have serious security implications.  It validates each rule to
        hopefully catch problems as early in the parsing process as
        possible; the checks will be implementation-specific to some
        extent, but will generally include some of the following:

        \begin{itemize}

            \squeezeitems{}

            \item Ensuring the specified action has been registered.

            \item Checking for conflict in the data to be extracted, e.g.\
                setting the same variable twice.

            \item Checking that the regex used in the rule is valid.

            \item Some optimisation steps may also be performed during
                loading of rules, as discussed in \sectionref{parser
                efficiency}.

        \end{itemize}

    \item [Classify the input]  The pseudo-code above shows how rules are
        successively tried until one is found that recognises the current
        input.  That pseudo-code is very simple: there may be efficiency
        concerns, (\sectionref{parser efficiency}), rule conditions
        (\sectionref{attaching conditions to rules}), or prioritisations
        (\sectionref{rules in architecture}) that complicate the process.

    \item [Invoke actions]  Once a rule has been found that recognises the
        current input, the specified action must be invoked.  The framework
        marshals the data extracted by the rule, invokes the action, and
        repeats the parsing process if cascaded parsing
        (\sectionref{cascaded parsing}) is used.

    \item [Shared storage]  Parsers generally need to save some state
        information about the input being parsed, e.g.\ a compiler tracking
        which variables are in lexical scope as it moves from one lexical
        block to another.  The framework provides shared storage to deal
        with this and any other storage needs the actions have.  Actions
        may need to exchange data to correctly parse the input, e.g.\
        setting or clearing flags, maintaining a list of previously used
        identifiers, or ensuring at a higher level that the input being
        parsed meets a list of requirements.

    \item [Save and load state]  The architecture can save current state,
        viz the shared storage it provides for actions, and reload it
        later, so that information is not lost between parsing runs.

    \item [Specialised support functions]  Actions may need support or
        utility functions, and in general those functions should be
        available to all action.  Depending on the implementation the
        framework may be a good location for support functions; if there is
        another way to make those functions available to all actions it may
        be preferable to use it, maintaining a clear separation of
        concerns.

\end{description}

\section{Actions}

\label{actions in architecture}

Each action is a separate procedure written to process a particular
category of input, e.g.\ rejecting a delivery attempt.  There may be many
input variants within one input category; in general each action will
handle one category of inputs, with each rule recognising one input
variant.  It is anticipated that parsers based on this architecture will
have a high ratio of rules to actions, with the aim of having simple rules
and tightly focused actions.  An action may need to process different input
variants in slightly different ways, but large changes in processing
indicate the need for a new action and a new category of input; if an
action becomes overly complicated it starts to turn into a monolithic
parser, with too much logic contained in a single procedure.

The ability to easily add special purpose actions to deal with difficulties
and new requirements that are discovered during parser development is one
of the strengths of this architecture.  Instead of writing a single
monolithic function that processes every input and must be modified to
support new behaviour, with all the attendant risks of adversely affecting
the existing parsing, when a new requirement arises an independent action
can be written to satisfy it.  Sometimes the new action will require the
cooperation of other actions, e.g.\ to set or check a flag, so actions are
not always self-contained, but there will still be a far lower degree of
coupling and interdependency than in a monolithic parser.

During development of the Postfix log parser it became apparent that in
addition to the obvious variety in log~lines there were many complications
to be overcome.  Some were the result of deficiencies in Postfix's logging,
and some of those were rectified by later versions of Postfix, e.g.\
identifying bounce notifications
(\sectionref{identifying-bounce-notifications}); others were due to the
vagaries of process scheduling, client behaviour, and administrative
actions.  All were successfully accommodated in the Postfix log parser:
adding new actions was enough to overcome several of the complications;
others required modifications to a single existing action to work around a
difficulty; the remainder were resolved by adapting existing actions to
cooperate and exchange extra data (via the framework), changing their
behaviour as appropriate based on that extra data.  Every architecture
should aim to make the hard things possible as well as making the easy
things easy; the successful implementation of the Postfix log parser
demonstrates that this architecture achieves that aim.

\subsection{Cascaded Parsing}

\label{cascaded parsing}

Actions may return a modified input line to the framework that will be
parsed as if read from the input stream, allowing for a simplified version
of cascaded parsing~\cite{cascaded-parsing}.  This powerful facility allows
several rules and actions to parse a single input, potentially simplifying
both rules and actions.

A simple example is to have one rule and action removing comments from
input lines, so that other rules and actions do not have to handle comments
at all; obviously if comment characters can be escaped or embedded in
quoted strings the implementation needs to be more complex, but for some
inputs this can greatly simplify subsequent parsing.

XXX GET GREG TO PROOFREAD THIS SECTION\@.

A more complicated example is determining the atomic weight of a compound
given its chemical formula.  A chemical formula is a sequence of one or two
character chemical element abbreviations, each optionally followed by a
number indicating how many of that element are present in the formula (if
omitted the number is assumed to be one).  Some parts of a formula may be
repeated: they are enclosed in parentheses, and the closing parenthesis is
followed by a number indicating how many are present in the formula.  A
monomer is a simple compound, i.e.\ it does not have any repeated portions;
compounds with repeated portions are called polymers.  E.g.\ glucose (a
monomer) has the chemical formula $C_{6}H_{12}O_{6}$ and hydrocarbon (a
polymer) is $CH_{3}{(CH_{2})}_{50}CH_{3}$.  The atomic weight of an
individual element is given in periodic tables, but one element in
particular will be important in the later explanation: Hydrogen,
abbreviated H, has an atomic weight of 1.  The atomic weight of a given
compound is the sum of the atomic weights of each of the elements
multiplied by the number of times they occur; this is simple to calculate
for a monomer, but it is harder for a polymer because the atomic weight of
the repeated monomer must be multiplied by the number of times the monomer
is present in the compound (e.g.\ for hydrocarbon the atomic weight of
$CH_{2}$ must be multiplied by $50$).

Calculating the atomic weight of a monomer requires a single rule and
action: the action separates each element in the monomer, looks up its
atomic weight, multiplies it by the number of times it is present (one if
unspecified), and adds up the results.  The rule's regex is
\verb!^([A-Z]([a-z])?\d*)+$!, which matches:

\begin{enumerate}

    \squeezeitems{}

    \item The start of the input.

    \item One or more of: an uppercase character, optionally followed by a
        lowercase character, followed by zero or more digits.

    \item The end of the input.

\end{enumerate}

The regex does not guarantee that the chemical element abbreviations in the
formula are correct; the regex could be modified to check that by replacing
\verb![A-Z]([a-z])?! with all the valid abbreviations, slightly simplifying
the action at the cost of complicating the regex (which could be
automatically generated).

Parsing a polymer is more difficult because of the repeated monomers: each
monomer's atomic weight needs to be calculated, multiplied by the number of
times it is present in the compound, and that figure added to the sum.
Without cascaded parsing the action would need to maintain state while
recursively parsing the monomers embedded in the polymer.  With cascaded
parsing available three rules and actions could be used as shown in
\tableref{Regexes and Actions to calculate the atomic weight of a chemical
compound}:

% These figures come from experimentation; set the length to 0pt, see how
% big a hbox latex complains about, and then set the length.  Stupid latex
% cannot do \settowidth{\verb}.  Poo.
\newlength{\chemicalactionwidth}
\setlength{\chemicalactionwidth}{\textwidth}
% This is the width of the number column, roughly.
\addtolength{\chemicalactionwidth}{-30pt}
% The width of the regex column.
\addtolength{\chemicalactionwidth}{-130pt}
% This is the overhead of space around columns, on either side, etc.
\addtolength{\chemicalactionwidth}{-24pt}

\begin{table}[ht]
    \caption{Regexes and Actions to calculate the atomic weight of a
    chemical compound}
    \empty{}\label{Regexes and Actions to calculate the atomic weight of a
    chemical compound}
    \begin{tabular}{llp{\chemicalactionwidth}}

        \tabletopline{}%
        No.   & Regex                       & Action \\
        \tablemiddleline{}%
        1     & \verb'((?!H\d+([()]|$))'    & Parse a monomer as described above,  \\
              & \verb' ([A-Z]([a-z])?\d*)+)'& returning \verb!Hnnn!, where \verb!nnn! is
                                              the atomic weight of the monomer.  The first
                                              part of the regex prevents it from parsing
                                              the return value of this action
                                              (\verb!Hnnn!), ensuring the parsing process
                                              does not get stuck in a loop.  \\
        2     & \verb!\(H(\d+)\)(\d+)!      & Parse a repeated monomer that has been
                                              rewritten by the first rule and action,
                                              returning \verb!Hnnn!, where \verb!nnn! is
                                              the product of the first and second numbers
                                              captured by the regex.  \\
        3     & \verb!^H(\d+)+$!            & Repeated applications of rules and
                                              actions one and two will have rewritten the
                                              input to \verb!Hnnn!, where \verb!nnn! is
                                              the atomic weight of the compound.  This
                                              action should do whatever is required with
                                              the calculated atomic weight.  \\
        \tablebottomline{}%

    \end{tabular}
\end{table}

Applying the rules and actions above to the hydrocarbon formula results in
this sequence:

\begin{tabular}{lrl}

    1 & Start with              & $CH_{3}{(CH_{2})}_{50}CH_{3}$ \\
    2 & Apply rule and action 1 & $H_{15}{(CH_{2})}_{50}CH_{3}$ \\
    3 & Apply rule and action 1 & $H_{15}{(H_{14})}_{50}CH_{3}$ \\
    3 & Apply rule and action 1 & $H_{15}{(H_{14})}_{50}H_{15}$ \\
    5 & Apply rule and action 2 & $H_{15}H_{700}H_{15}$ \\
    6 & Apply rule and action 1 & $H_{730}$ \\
    7 & Apply rule and action 3 & 730 \\

\end{tabular}

\section{Rules}

\label{rules in architecture}

Rules are responsible for categorising inputs: each rule should recognise
one and only input variant.  An input category with multiple input variants
should have multiple rules, one for each variant.  Rules will typically use
a regex when classifying inputs, but other approaches may prove useful for
some applications, e.g.\ comparing fixed strings to the input, or checking
the length of the input; for the remainder of this thesis it will be
assumed that a regex is used.  Each rule must specify at least the regex to
recognise inputs and the action to invoke when classification is
successful, but implementations are free to add any other attributes they
require; \sectionref{rule attributes} describes the attributes used in
\parsername{}, and some potential attributes will be discussed later in
this section.

The architecture is line oriented at present: there is no facility for
rules to consume more input or push unused input back onto the input
stream.  This was not a deliberate design decision, rather a consequence of
the line oriented nature of Postfix log files; more flexible approaches
could be pursued.

Classifying inputs using the rules is simple: the first rule to recognise
the input determines the action that will be invoked; there is no
backtracking to try alternate rules; no attempt is made to pick a
\textit{best\/} rule.  \sectionref{attaching conditions to rules} contains
an example which requires that the rules are used in a specific order to
correctly parse the input, so a mechanism is needed to allow the author of
the rules to specify that ordering.  The framework should be written so
that it sorts the rules according to a priority attribute specified by the
rule author, allowing fine-grained control over the order that rules are
used in.  The priority attribute may be implemented as a number, or as a
range of values, e.g.\ low, medium, and high, or in a different fashion
entirely if it suits the implementation.  Rule ordering for efficiency is a
separate topic that is covered in \sectionref{rule ordering for
efficiency}; overlapping rules are discussed in \sectionref{overlapping
rules in architecture}.

In \acronym{CFG} terms the rules could be described as:

$\text{\textless{}input\textgreater{}}~\mapsto{}~\text{rule-1}~|~\text{rule-2}~|~\text{rule-3}~|~\dots~|~\text{rule-n}$

This is not entirely correct because the rules are not truly context free:
rule conditions (described in \sectionref{attaching conditions to rules})
restrict which rules will be used to recognise each input, imposing a
context of sorts.

\subsection{Adding New Rules}

The framework issues a warning for each unparsed input, so it is evident
when the ruleset needs to be augmented.  Parsing new inputs is achieved in
one of three ways:

\begin{enumerate}

    \item Modify an existing rule's regex, because the new input is part of
        an existing variant.

    \item Write a new rule that pairs an existing action with a new regex;
        this adds a new variant to an existing category.

    \item Create a new category of inputs and a new action to process
        inputs from the new category; write a new rule pairing the new
        action with a new regex.

\end{enumerate}

Decoupling the rules from the actions and framework enables other rule
management approaches to be used, e.g.\ instead of manually editing
existing rules or adding new rules, machine learning techniques could be
used to automate the process.  If this approach was taken the choice of
machine learning technique would be constrained by the size of typical data
sets (see \sectionref{parser efficiency}).  Techniques requiring the full
data set when training would be impractical; Instance Based
Learning~\cite{instance-based-learning} techniques that automatically
determine which inputs from the training set are valuable and which inputs
can be discarded might reduce the data required to a manageable size.  A
parser could also dynamically create new rules in response to certain
inputs, e.g.\ parsing a subroutine declaration could cause a rule to be
created that parses calls to that subroutine, checking that the arguments
used agree with the subroutine's signature.

These avenues of research and development have not been pursued by the
author, but the architecture allows them to easily be undertaken
independently.

\subsection{Attaching Conditions To Rules}

\label{attaching conditions to rules}

Rules can specify conditions that will be evaluated by the framework before
attempting to use a rule to recognise an input: if the condition evaluates
to true the rule will be used, if not the rule will be skipped.  Conditions
can be as simple or complex as the parser requires, though naturally as the
complexity rises so does the difficulty in understanding how everything
interacts.  The framework has to evaluate each condition, so as the
complexity of conditions increases so does the complexity of the code
required to evaluate them; beyond a certain level of complexity conditions
should probably be written in a proper programming language, e.g.\ taking
advantage of dynamic languages' support for evaluating code at run-time, or
embedding a language like Lua
(\urlLastCheckedNoParens{http://ww.lua.org/}{2009/03/03}).  If an
implementation is going to use conditions so complex that they will require
a full programming language to implement them, the design may need to be
re-thought, including the choice to use this architecture --- there may be
other architectures more suitable.

Conditions that examine the input will be the easiest to understand,
because they can be understood in isolation --- they do not depend on
variables set by actions or other rules.  Conditions that examine the input
may be complex, but simple conditions can be quite useful too, e.g.\ every
Postfix log \empty{}line contains the name of the Postfix component that
produced it, so every rule used in \parsername{} specifies the component
whose log \empty{}lines it recognises as a condition, reducing the number
of rules that will be used when classifying an input (see \sectionref{rules
in implementation} for details).

An example of how rule conditions can be useful is parsing C-style
comments, which start with \texttt{/*} and end with \texttt{*/}; the start
and end tokens can be on one line, or may have many lines between them.
\Tableref{Rules to parse C-style comments} shows the regexes, conditions,
and state changes of the four rules required to parse C-style comments.
These rules are somewhat simplified, e.g.\ rules one and two will recognise
the comment start token even if it is within a quoted string, which is
incorrect.  Rules one and two will only be used when the parser is parsing
code, not comments: rule one recognises a comment that is contained within
one line and leaves the parser's state unchanged; rule two recognises the
start of a command and changes the parser's state to parsing comments.
Rules three and four will be used when the parser is parsing a comment:
rule three recognises the end of a comment and switches parsing state back
to parsing code; rule four recognises a comment line without an end token
and keeps the parser's state unchanged.  It is important that the rules are
applied in the order listed in \tableref{Rules to parse C-style comments};
\sectionref{rules in architecture} has explained how this is achieved, and
\sectionref{overlapping rules in architecture} will discuss overlapping
rules.

\begin{table}[ht]
    \caption{Rules to parse C-style comments}
    \empty{}\label{Rules to parse C-style comments}
    \begin{tabular}{llll}
        \tabletopline{}%
        No.   & Regex             & Condition         & New State         \\
        \tablemiddleline{}%
        1     & \verb!/\*.*?\*/!  & state == code     & state = code      \\
        2     & \verb!/\*.*!      & state == code     & state = comment   \\
        3     & \verb!.*\*/!      & state == comment  & state = code      \\
        4     & \verb!.*!         & state == comment  & state = comment   \\
        \tablebottomline{}%
    \end{tabular}
\end{table}

Conditions can also check the value of variables that have been set by
either actions or rules; it will be easier to understand how a variable's
value will be used and changed if it is set by rules only rather than by
actions, because the chain of checking and setting can be followed from
rule to rule.  The downside to actions setting variables used in rule
conditions is action at a distance: to understand when a rule's condition
will be true requires understanding not just every rule but also every
action.  The framework probably does not need to support the same level of
complexity and flexibility when setting variables from rules (actions can
obviously use complex code if they are setting variables); however, if the
framework allows complex conditions, that code can probably be easily
extended to support complex variable assignments too.

\subsection{Overlapping Rules}

\label{overlapping rules in architecture}

XXX IS THERE ANY PREVIOUS RESEARCH IN THIS AREA\@?

When adding new rules, the rule author must be aware that the new rule may
overlap with one or more existing rules, i.e.\ some inputs could be parsed
by more than one rule.  The architecture does not try to detect overlapping
rules: that responsibility is left to the author of the rules.
Unintentionally overlapping rules lead to inconsistent parsing and data
extraction because the first matching rule wins, and the order in which
rules are tried against each input might change between parser invocations.
Overlapping rules are frequently a requirement, allowing a more specific
rule to match some inputs and a more general rule to match the remainder,
e.g.\ separating \acronym{SMTP} delivery to specific sites from
\acronym{SMTP} delivery to the rest of the world.  Allowing overlapping
rules simplifies both the general rule and the more specific rule;
additionally rules from different sources can be combined with a minimum of
prior cooperation or modification required.  Overlapping rules should have
a priority attribute to specify their relative ordering; negative
priorities may be useful for catchall rules.

Decoupling the rules from the actions allows external tools to be written
to detect overlapping rules.  Traditional regexes are equivalent in
computational power to \acronym{FA} and can be converted to \acronym{FA},
so regex overlap can be detected by finding a non-empty intersection of two
\acronym{FA}\@.  The standard equation for \acronym{FA} intersection (given
for example in~\cite{intersection-of-NFA-using-Z}) uses DeMorgan's law:
$FA1~\cap{}~FA2~=~\overline{(\overline{FA1}~\cup{}~\overline{FA2})}$, which
has considerable computation complexity.  XXX COMMENT ON THE COMPUTATIONAL
COMPLEXITY OF INVERSION AND UNION\@; ADD A REFERENCE TO THE SPINNING WHEEL
BOOK\@.  Perl 5.10 regexes~\cite{perlre} are more powerful than traditional
regexes: it is possible to match correctly balanced brackets nested to an
arbitrary depth, e.g.\ \verb!/^[^<>]*(<(?:(?>[^<>]+)|(?1))*>)[^<>]*$/!
matches \verb!z<123<pq<>rs>j<r>ml>s!.  Matching balanced brackets requires
the regex engine to maintain state on a stack, so Perl 5.10 regexes are
equivalent in computational power to \acronym{PDA}; detecting overlap may
require calculating the intersection of two \acronym{PDA} instead of two
\acronym{FA}.  The intersection of two \acronym{PDA} is not closed, i.e.\
the result cannot always be implemented using a \acronym{PDA}, so
intersection may be intractable in some cases, e.g.:
$a^{*}b^{n}c^{n}~\cap~a^{n}b^{n}c^{*}~\rightarrow~a^{n}b^{n}c^{n}$.
Detecting overlap amongst $n$ regexes requires calculating $n(n-1)/2$
intersections, resulting in $O(n^2x)$ complexity, where $O(x)$ is the
complexity of calculating \acronym{FA} or \acronym{PDA} intersection.  This
is certainly not a task to be performed every time the parser runs: naive
detection of overlap amongst \parsernames{} \numberOFrules{} rules would
require calculating \numberOFruleINTERSECTIONS{} intersections.  When
detecting overlap any conditions attached to rules must be taken into
account, because two rules whose regexes overlap may have conditions
attached which prevent the overlap in practice.  A less naive approach to
overlap detection would first check for overlapping conditions amongst
rules, and then check for overlapping regexes amongst the rules in each set
of rules with overlapping conditions.  Depending on the conditions employed
a rule may fall into more than one group, e.g.\ given these three
conditions:

\begin{enumerate}

    % Reduce inter-item spacing because each item is a single line.
    \squeezeitems{}

    \item $total < 10$

    \item $total > 20$

    \item $total < 30$

\end{enumerate}

\noindent{}The first and second conditions do not overlap, but the third
condition does overlap with both the first and second conditions.  If the
rules are divided into sets based on how their conditions overlap, the
complexity of detecting overlap amongst $n$ rules is:

$$O(n^{2}y)~+~\sum{O(|s|^{2}x)~\forall{}~s~\in{}~overlaps}$$

where:

\begin{tabular}[]{rcl}

            $y$ & = & cost of checking for overlap between two conditions \\
            $x$ & = & cost of checking for overlap between two regexes    \\
     $overlaps$ & = & set of sets of rules with overlapping conditions    \\

\end{tabular}

Once conditions pass a certain level of complexity determining whether two
conditions overlap becomes intractable, because it requires so much
knowledge of other rules and possibly even actions.  E.g.\ given two rules
with conditions \verb!verbose == true! and \verb!silent == true!, it
appears that these rules cannot overlap, yet there is nothing to stop both
variables being set to true by one or more other rules or actions.  If
variables used in conditions can be set by actions, determining whether two
conditions overlap is impossible, because understanding how an action will
change the variables is impossible; the Church-Turing Thesis XXX NEED A BIB
ENTRY FOR CHURCH-TURING explains why.

\subsection{Pathological Rules}

It is possible to define pathological regexes which fall into two main
categories: regexes that match inputs they should not, and regexes that
consume excessive amounts of CPU time during matching.  Defining a regex
that matches inputs it should not is trivial: \verb!/^/! matches the start
of every input.  This regex would be found by a tool that detects
overlapping rules, and would easily be noticed by visual inspection, but
more complex regexes would be harder to spot.  Regexes that match more than
they should are a problem not because of excessive resource usage, but
because they may prevent the correct rule from recognising the input.  If
an adaptive ordering system is used (see \sectionref{rule ordering for
efficiency}) then a rule with a regex that matches too much may be promoted
up through the list, displacing an increasing number of correct rules as it
moves.

Usually excessive CPU time is consumed when a regex with a lot of
backtracking, caused by alteration and/or nested quantifiers, fails to
match; successful matching is generally quite fast with such regexes, so
problematic regexes may go unnoticed for some time.  E.g.\ with most regex
engines matching double quoted strings with \verb!"([^"\\]+|\\.)*"! is very
fast when there is a match, but when the match fails its computational
complexity is $O(2^{n})$; see~\cite{mastering-regular-expressions} for
in-depth discussion of nested quantifiers, backtracking, alteration, and
capturing groups.  Pathological regexes which consume excessive CPU time
can be difficult to detect, both by visual inspection and by machine
inspection, but if a regex is converted to a \acronym{FA} or the internal
representation used by the regex engine, it may be possible to determine if
there are nested quantifiers or other troublesome constructs present.
Modern regex engines have addressed many of these problems, e.g.\ the regex
to match double quoted strings given above fails immediately with Perl
5.10, regardless of the input length, because the regex engine looks for
the required double quotes first.  Similarly Perl 5.10's regex engine
optimises alterations starting with literal text into a trie, which has
constant matching time, whereas the matching time for alteration is
proportional to the number of alternatives.  Perl 5.10 regexes can use
\verb!(?>pattern)!, which matches \verb!pattern! the first time the regex
engine passes over it, and does not change what it originally matched if
the regex engine backtracks over it, so troublesome regexes can be modified
to use this to reduce the impact of backtracking; Prolog
(\urlLastCheckedNoParens{http://en.wikipedia.org/wiki/Prolog}{2009/03/03})
users will notice a similarity to the \verb'!' (cut) operator.  A
presentation showing some of Perl 5.10's regex features is available at
\urlLastChecked{http://www.regex-engineer.org/slides/perl510_regex.html}{2009/03/03}.

Conditions can vary in complexity from simple equality through to a
Turing-equivalent language, so enumerating pathological conditions is
difficult if not pointless.  Conditions that examine the input using
regexes can suffer from the same problems as rule regexes; conditions that
check variables in uncomplicated ways may exhibit unexpected or incorrect
behaviour, but are unlikely to exhibit pathological behaviour.  Deciding
whether a more complex condition's behaviour is pathological or simply a
bug is difficult and to some extent is a matter of opinion.  The author has
not experimented with more complex conditions, so this thesis will not
address this point any further.

\section{Summary}

XXX TO BE WRITTEN\@.
