\chapter{Parser Architecture}

\label{parser architecture}

To avoid cluttering the explanation of the parser architecture with the
details involved in implementing a parser for Postfix log files, the two
topics have been separated.  This chapter presents the architecture
developed for this project, beginning with the overall architecture and
design, followed by the three components of the architecture: Framework,
Actions, and Rules.  This chapter centers on the theoretical,
implementation-independent aspects of the architecture; the practical
difficulties of writing a parser for Postfix log files are covered in
detail in \sectionref{Postfix Parser Implementation}.

\section{Parser Architecture and Design}

\label{parser design}

It should be clear from the earlier Postfix background (\sectionref{postfix
background}) that log files produced by Postfix may vary widely from host
to host, depending on the set of restrictions chosen by the administrator.
With this in mind, one of the architecture's design aims was to make adding
new rules to parse new inputs as easy as possible, to enable administrators
to properly parse their own log files.  The solution developed is to divide
the architecture into three parts: Framework, Actions, and Rules.  Each
will be discussed separately, but first an overview:

\begin{eqlist}[\def\makelabel#1{\bfseries#1}]

    \item [Framework]  The framework is the structure that actions and
        rules plug into.  It manages the parsing process, providing shared
        data storage, loading and validation of rules, storage of results,
        and other support functions.

    \item [Actions] Each action performs the work required to deal with a
        single \textit{category\/} of inputs, e.g.\ rejecting a delivery
        attempt.  Actions are invoked to process an input once it has been
        recognised by a rule.

    \item [Rules]  Rules are responsible for classifying inputs: each rule
        recognises one input \textit{variant\/} (a single input category
        may have many input variants).  Each rule also specifies the action
        to be invoked when an input has been recognised; rules thus provide
        an extensible method of associating inputs with actions.

\end{eqlist}

For each input the framework tries each rule in turn until it finds a rule
that recognises the input, then invokes the action specified by that rule.
If the input is not recognised by any of the rules, the framework issues a
warning; the framework can continue parsing after this, although for some
inputs stopping immediately may be preferable.

\label{why separate rules, actions, and framework?}

Decoupling the parsing rules from their associated actions allows new rules
to be written and tested without requiring modifications to the parser
source code, significantly lowering the barrier to entry for casual users
who need to parse new inputs, e.g.\ part-time systems administrators
attempting to combat and reduce spam; it also allows companies to develop
user-extensible parsers without divulging their source code.  Decoupling
the framework, actions, and rules simplifies all three and creates a clear
separation of functionality: the framework manages the parsing process and
provides services to the actions; actions benefit from having services
provided by the framework, freeing them to concentrate on the task of
accurately and correctly processing the inputs and the information provided
by rules; rules are responsible for classifying inputs, and extracting data
from those inputs for processing by actions.

Separating the rules from the actions and framework makes it possible to
parse new inputs without modifying the core parsing algorithm.  Adding a
new rule with the action to invoke and a regex to recognise the inputs is
trivial in comparison to understanding an entire parser, identifying the
correct location to change, and making the appropriate changes.  Bear in
mind that changes to a parser must be made without adversely affecting
existing parsing, including any edge cases that are not immediately obvious
--- \sectionref{yet-more-aborted-delivery-attempts} describes a
complication that occurs only four times in \numberOFlogFILES{} log files.
Requiring changes to a parser's source code also complicates upgrades of
the parser, because the changes must be preserved during the upgrade, and
they may clash with changes made by the developer.  This architecture
allows the user to add new rules without having to edit a parser, unless
the new inputs cannot be processed by the existing actions.  If the new
inputs do require new functionality, new actions can be added to the parser
without having to modify existing actions; only in the rare event that the
new actions need to cooperate with existing actions will more extensive
changes be required.

There is some similarity between this architecture and William Wood's
\acronym{ATN}~\cite{atns,nlpip}, used in Computational Linguistics for
creating grammars to parse or generate sentences.  The resemblance between
the two (shown in \tableref{Similarities between ATN and this
architecture}) is accidental, but it is clear that the two different
approaches share a similar division of responsibilities, despite having
different semantics.

% Do Not Reformat!

\begin{table}[ht]
    \caption{Similarities between ATN and this architecture}
    \empty{}\label{Similarities between ATN and this architecture}
    \begin{tabular}[]{lll}
        \tabletopline{}%
        \acronym{ATN}   & Architecture  & Similarity                  \\
        \tablemiddleline{}%
        Networks        & Parser        & Determines the sequence
                                          of transitions              \\
                        &               & or actions that
                                          constitutes a valid input.  \\
        Transitions     & Actions       & Assemble data and
                                          impose conditions the       \\
                        &               & input must meet to be
                                          accepted as valid.          \\
        Abbreviations   & Rules         & Responsible for
                                          classifying input.          \\
        \tablebottomline{}%
    \end{tabular}
\end{table}

The architecture can be thought of as implementing transduction: it takes
data in one form and transforms it to another form; \parsername{}
transforms log files to a database.  XXX NEED A REFERENCE FOR
TRANSDUCTION\@.

Unlike traditional parsers such as those used when compiling a programming
language, this architecture does not require a fixed grammar specification
that inputs must adhere to.  The architecture is capable of dealing with
interleaved inputs, out of order inputs, and ambiguous inputs where
heuristics must be applied --- all have arisen and been successfully
accommodated in \parsername{}.  This architecture is ideally suited to
parsing inputs where the input is not fully understood or does not conform
to a fixed grammar: the architecture warns about unparsed inputs and other
errors, but continues parsing as best it can, allowing the developer of a
new parser to decide which deficiencies are most important and the order to
address them in, rather than being forced to fix the first error that
arises.

\section{Framework}

\label{framework in architecture}

The framework manages the parsing process and provides support functions,
freeing the programmers writing actions to concentrate on writing
productive code.  It links actions and rules, allowing either to be
improved independently of the other, and allows new rules to be written
without needing changes to the source code of a parser.  The framework is
the core of the architecture and is deliberately quite simple: the rules
deal with the variation in inputs, and the actions deal with the
intricacies and complications encountered during parsing.  The function
that finds the rule matching the input and invokes the requested action can
be expressed in pseudo-code as:

\begin{verbatim}
INPUT:
for each input {
    for each rule defined by the user {
        if this rule recognises the input {
            perform the action specified by the rule
            next INPUT
        }
    }
    warn the user that the input was not parsed
}
\end{verbatim}

Most parsers will require the same basic functionality from the framework;
it is responsible for managing the parsing process from start to finish,
which will generally involve the following:

\begin{description}

    \item [Register actions]  Each action needs to be registered with the
        framework so that the framework knows about it; the list of
        registered actions will also be used when validating rules.

    \item [Load and validate rules]  The framework loads the rules from
        wherever they are stored: a simple file, a database, or possibly
        even a web server or other network service --- though that would
        have serious security implications.  It validates each rule to
        hopefully catch problems as early in the parsing process as
        possible; the checks will be implementation-specific to some
        extent, but will generally include some of the following:

        \begin{itemize}

            \squeezeitems{}

            \item Ensuring the action specified by the rule has been
                registered with the framework.

            \item Checking for conflicts in the data to be extracted, e.g.\
                setting the same variable twice.

            \item Checking that the regex in the rule is valid.

            \item Some optimisation steps may also be performed during
                loading of rules, as discussed in \sectionref{parser
                efficiency}.

        \end{itemize}

    \item [Classify the input]  The pseudo-code above shows how rules are
        successively tried until one is found that recognises the current
        input.  That pseudo-code is very simple: there may be efficiency
        concerns (\sectionref{parser efficiency}), rule conditions
        (\sectionref{attaching conditions to rules}), or prioritisations
        (\sectionref{rules in architecture}) that complicate the process.

    \item [Invoke actions]  Once a rule has been found that recognises the
        current input, the specified action must be invoked.  The framework
        marshals the data extracted by the rule, invokes the action, and
        repeats the parsing process if cascaded parsing
        (\sectionref{cascaded parsing}) is used.

    \item [Shared storage]  Parsers commonly need to save some state
        information about the input being parsed, e.g.\ a compiler tracking
        which variables are in lexical scope as it moves from one lexical
        block to another.  The framework provides shared storage to deal
        with this and any other storage needs the actions have.  Actions
        may need to exchange data to correctly parse the input, e.g.\
        setting or clearing flags, maintaining a list of previously used
        identifiers, or ensuring at a higher level that the input being
        parsed meets requirements.

    \item [Save and load state]  The architecture can save its current
        state, i.e.\ the shared storage it provides for actions, and reload
        it later, so that information is not lost between parsing runs.

    \item [Specialised support functions]  Actions may need support or
        utility functions; the framework may be a good location for support
        functions, but if there is another way to make those functions
        available to all actions it may be preferable to use it instead,
        maintaining a clear separation of concerns.

\end{description}

\section{Actions}

\label{actions in architecture}

Each action is a separate procedure written to process a particular
category of input, e.g.\ rejecting a delivery attempt.  There may be many
input variants within one input category; in general each action will
handle one input category, with each rule recognising one input variant.
It is anticipated that parsers based on this architecture will have a high
ratio of rules to actions, with the aim of having simple rules and tightly
focused actions.  An action may need to process different input variants in
slightly different ways, but large changes in processing indicate the need
for a new action and a new category of input; if an action becomes overly
complicated it starts to turn into a monolithic parser, with too much logic
contained in a single procedure.

The ability to easily add special purpose actions to deal with difficulties
and new requirements that are discovered during parser development is one
of the strengths of this architecture.  Instead of writing a single
monolithic function that processes every input and must be modified to
support any new behaviour, with all the attendant risks of adversely
affecting the existing parsing, when a new requirement arises an
independent action can be written to satisfy it.  Sometimes the new action
will require the cooperation of other actions, e.g.\ to set or check a
flag, so actions are not always self-contained, but there will still be a
far lower degree of coupling and interdependency than in a monolithic
parser.

During development of \parsername{} it became apparent that in addition to
the obvious variety in log~lines there were many complications to be
overcome.  Some were the result of deficiencies in Postfix's logging, and
some of those deficiencies were rectified by later versions of Postfix,
e.g.\ identifying bounce notifications
(\sectionref{identifying-bounce-notifications}); others were due to the
vagaries of process scheduling, client behaviour, and administrative
actions.  All were successfully accommodated in \parsername{}: adding new
actions was enough to overcome several of the complications; others
required modifications to a single existing action to work around a
difficulty; the remainder were resolved by adapting existing actions to
cooperate and exchange extra data (via the framework), changing their
behaviour as appropriate based on that extra data.  Every architecture
should aim to make the easy things easy and the hard things possible; the
successful implementation of \parsername{} demonstrates that this
architecture achieves that aim.

\subsection{Cascaded Parsing}

\label{cascaded parsing}

Actions may return a modified input line to the framework that will be
parsed as if read from the input stream, allowing for a simplified version
of cascaded parsing~\cite{cascaded-parsing}.  This powerful facility allows
several rules and actions to parse a single input, potentially simplifying
both rules and actions.  A simple example is to have one rule and action
removing comments from input lines, so that other rules and actions do not
have to handle comments at all; obviously if comment characters can be
escaped or embedded in quoted strings the implementation needs to be more
complex, but for some inputs this can greatly simplify subsequent parsing.
Actions do not need to be specially registered with the framework or be
declared in a particular way to use cascaded parsing: actions that do not
use cascaded parsing will return nothing, those that do will simply return
a string to be parsed.

XXX GET GREG TO PROOFREAD THIS SECTION\@.

A more complicated example is determining the atomic weight of a compound
given its chemical formula.  A chemical formula is a sequence of chemical
element abbreviations, each optionally followed by a number indicating how
many of that element are present in the formula (if omitted the number is
assumed to be one).  Some parts of a formula may be repeated: they are
enclosed in parentheses, and the closing parenthesis is followed by a
number indicating how many are present in the formula.  A monomer is a
simple compound, i.e.\ it does not have any repeated portions; compounds
with repeated portions are called polymers.  E.g.\ glucose, a monomer, has
the chemical formula $C_{6}H_{12}O_{6}$, and hydrocarbon, a polymer, is
$CH_{3}{(CH_{2})}_{50}CH_{3}$.  The atomic weight of an individual element
is given in periodic tables, but one element in particular will be
important in the later explanation: Hydrogen, abbreviated H, has an atomic
weight of 1.  The atomic weight of a given compound is the sum of the
atomic weights of each of its elements multiplied by the number of times
they occur; this is simple to calculate for a monomer, but it is harder for
a polymer because the atomic weight of the repeated monomer must be
multiplied by the number of times the monomer is present in the compound
(e.g.\ for hydrocarbon the atomic weight of $CH_{2}$ must be multiplied by
$50$).

Calculating the atomic weight of a monomer requires a single rule and
action.  The action separates each element in the monomer, looks up its
atomic weight, multiplies it by the number of times it is present (one if
unspecified), and adds up the results.  The rule's regex is
\verb!^([A-Z]([a-z])?\d*)+$!, which does not guarantee that the chemical
element abbreviations in the formula are correct; it could be modified to
check that by replacing \verb![A-Z]([a-z])?! with all the valid
abbreviations, slightly simplifying the action at the cost of complicating
the regex, which could be automatically generated.

Parsing a polymer is more difficult because of the repeated monomers: each
monomer's atomic weight needs to be calculated separately, multiplied by
the number of times it is present in the compound, and that figure added to
the sum.  Without cascaded parsing the action would need to maintain state
while recursively parsing the monomers embedded in the polymer.  Using
cascaded parsing, three rules and actions can be used instead as shown in
\tableref{Regexes and Actions to calculate the atomic weight of a chemical
compound}.

% These figures come from experimentation; set the length to 0pt, see how
% big a hbox latex complains about, and then set the length.  Stupid latex
% cannot do \settowidth{\verb}.  Poo.
\newlength{\chemicalactionwidth}
\setlength{\chemicalactionwidth}{\textwidth}
% This is the width of the number column, roughly.
\addtolength{\chemicalactionwidth}{-30pt}
% The width of the regex column.
\addtolength{\chemicalactionwidth}{-130pt}
% This is the overhead of space around columns, on either side, etc.
\addtolength{\chemicalactionwidth}{-24pt}

\begin{table}[ht]
    \caption{Regexes and Actions to calculate the atomic weight of a
    chemical compound}
    \empty{}\label{Regexes and Actions to calculate the atomic weight of a
    chemical compound}
    \begin{tabular}{llp{\chemicalactionwidth}}

        \tabletopline{}%
        No.   & Regex                       & Action \\
        \tablemiddleline{}%
        1     & \verb'((?!H\d+([()]|$))'    & Parse a monomer as described above,  \\
              & \verb' ([A-Z]([a-z])?\d*)+)'& returning \verb!Hnnn!, where \verb!nnn! is
                                              the atomic weight of the monomer.  The first
                                              part of the regex prevents it from parsing
                                              the return value of this action
                                              (\verb!Hnnn!), ensuring the parsing process
                                              does not get stuck in a loop.  \\
        2     & \verb!\(H(\d+)\)(\d+)!      & Parse a repeated monomer that has been
                                              rewritten by the first rule and action,
                                              returning \verb!Hnnn!, where \verb!nnn! is
                                              the product of the first and second numbers
                                              captured by the regex.  \\
        3     & \verb!^H(\d+)+$!            & Repeated applications of rules and
                                              actions one and two will have rewritten the
                                              input to \verb!Hnnn!, where \verb!nnn! is
                                              the atomic weight of the compound.  This
                                              action should do whatever is required with
                                              the calculated atomic weight.  \\
        \tablebottomline{}%

    \end{tabular}
\end{table}

Applying the rules and actions above to the hydrocarbon formula results in
this sequence:

\begin{tabular}{lrl}

    1 & Start with              & $CH_{3}{(CH_{2})}_{50}CH_{3}$ \\
    2 & Apply rule and action 1 & $H_{15}{(CH_{2})}_{50}CH_{3}$ \\
    3 & Apply rule and action 1 & $H_{15}{(H_{14})}_{50}CH_{3}$ \\
    3 & Apply rule and action 1 & $H_{15}{(H_{14})}_{50}H_{15}$ \\
    5 & Apply rule and action 2 & $H_{15}H_{700}H_{15}$ \\
    6 & Apply rule and action 1 & $H_{730}$ \\
    7 & Apply rule and action 3 & 730 \\

\end{tabular}

\section{Rules}

\label{rules in architecture}

Rules are responsible for categorising inputs: each rule should recognise
one and only one input variant; an input category with multiple input
variants should have multiple rules, one for each variant.  Rules will
typically use a regex when recognising inputs, but other approaches may
prove useful for some applications, e.g.\ comparing fixed strings to the
input, or checking the length of the input; for the remainder of this
thesis it will be assumed that a regex is used.  Each rule must specify at
least the regex to recognise inputs and the action to invoke when
recognition is successful, but implementations are free to add any other
attributes they require; \sectionref{rule attributes} describes the
attributes used in \parsername{}, and some generally useful attributes will
be discussed later in this section.

The architecture is line oriented at present: there is no facility for
rules to consume more input or push unused input back onto the input
stream.  This was not a deliberate design decision, rather a consequence of
the line oriented nature of Postfix log files; more flexible approaches
could be pursued.  Classifying inputs using the rules is simple: the first
rule to recognise the input determines the action that will be invoked;
there is no backtracking to try alternate rules; no attempt is made to pick
a \textit{best\/} rule.  \sectionref{attaching conditions to rules}
contains an example which requires that the rules are used in a specific
order to correctly parse the input, so a mechanism is needed to allow the
author of the rules to specify that ordering.  The framework should sort
the rules according to a priority attribute specified by the rule author,
allowing fine-grained control over the order that rules are used in.  The
priority attribute may be implemented as a number, or as a range of values,
e.g.\ low, medium, and high, or in a different fashion entirely if it suits
the implementation.  Rule ordering for efficiency is a separate topic that
is covered in \sectionref{rule ordering for efficiency}; overlapping rules
are discussed in \sectionref{overlapping rules in architecture}.

In \acronym{CFG} terms the rules could be described as:

$\text{\textless{}input\textgreater{}}~\mapsto{}~\text{rule-1}~|~\text{rule-2}~|~\text{rule-3}~|~\dots~|~\text{rule-n}$

This is not entirely correct because the rules are not truly context free:
rule conditions (described in \sectionref{attaching conditions to rules})
restrict which rules will be used to recognise each input, imposing a
context of sorts.

\subsection{Adding New Rules}

The framework issues a warning for each unparsed input, so it is clearly
evident when the ruleset needs to be augmented.  Parsing new inputs is
achieved in one of three ways:

\begin{enumerate}

    \item Modify an existing rule's regex, because the new input is part of
        an existing variant.

    \item Write a new rule that pairs an existing action with a new regex,
        adding a new variant to an existing category.

    \item Create a new category of inputs and a new action to process
        inputs from the new category, and write a new rule pairing the new
        action with a new regex.

\end{enumerate}

Decoupling the rules from the actions and framework enables other rule
management approaches to be used, e.g.\ instead of manually editing
existing rules or adding new rules, machine learning techniques could be
used to automate the process.  If this approach was taken the choice of
machine learning technique would be constrained by the size of typical data
sets (see \sectionref{parser efficiency}).  Techniques requiring the full
data set when training would be impractical; Instance Based
Learning~\cite{instance-based-learning} techniques that automatically
determine which inputs from the training set are valuable and which inputs
can be discarded might reduce the data required to a manageable size.  A
parser could also dynamically create new rules in response to certain
inputs, e.g.\ parsing a subroutine declaration could cause a rule to be
created that parses calls to that subroutine, checking that the arguments
used agree with the subroutine's signature.  These avenues of research and
development have not been pursued by the author, but the architecture
allows them to easily be undertaken independently.

\subsection{Attaching Conditions To Rules}

\label{attaching conditions to rules}

Rules can have conditions attached that will be evaluated by the framework
before attempting to use a rule to recognise an input: if the condition is
true the rule will be used, if not the rule will be skipped.  Conditions
can be as simple or complex as the parser requires, though naturally as the
complexity rises so too does the difficulty in understanding how everything
interacts.  The framework has to evaluate each condition, so as the
complexity of conditions increases so will the complexity of the code
required to evaluate them.  Beyond a certain level of complexity,
conditions should probably be written in a proper programming language,
e.g.\ taking advantage of dynamic languages' support for evaluating code at
run-time, or embedding a language like Lua.  If an implementation is going
to use conditions so complex that they will require a Turing-equivilent
programming language to implement them, the design may need to be
re-thought, including the decision to use this architecture --- there may
be other architectures more suitable.

Conditions that examine the input will be the easiest to understand,
because they can be understood in isolation; they do not depend on
variables set by actions or other rules.  Conditions that examine the input
can be complex if required, but simple conditions can be quite useful too,
e.g.\ every Postfix log \empty{}line contains the name of the Postfix
component that produced it, so every rule used in \parsername{} has a
condition specifying the component whose log \empty{}lines it recognises,
reducing the number of rules that will be used when classifying an input
(see \sectionref{rules in implementation} for details) and increasing the
chance that the input will be correctly recognised..

Conditions can also check the value of variables that have been set by
either actions or rules; it is easier to understand how a variable's value
will be used and changed if it is set by rules only rather than by actions,
because the chain of checking and setting can be followed from rule to
rule.  The downside to actions setting variables used in rule conditions is
action at a distance: understanding when a rule's condition will be true or
false requires understanding not just every other rule but also every
action.  The framework probably does not need to support a high level of
complexity and flexibility when rules are setting variables; however, if
the framework supports complex conditions, that code can probably be easily
extended to support complex variable assignments too.  The level of
complexity the framework supports when evaluating conditions and setting
variables has two costs that must be taken into account when designing a
parser: the difficulty of implementation, and the difficulty of
understanding rules and writing correct rules.  XXX NEEDS TO BE IMPROVED\@.

An example of how rule conditions can be useful is parsing C-style
comments, which start with \texttt{/*} and end with \texttt{*/}; the start
and end tokens can be on one line, or may have many lines between them.
\Tableref{Rules to parse C-style comments} shows the regexes, conditions,
and state changes of the four rules required to parse C-style comments.
These rules are somewhat simplified, e.g.\ rules one and two will recognise
the comment start token even if it is within a quoted string, which is
incorrect.  Rules one and two will be used when the parser is parsing code,
not comments: rule one recognises a comment that is contained within one
line and leaves the parser's state unchanged; rule two recognises the start
of a comment and changes the parser's state to parsing comments.  Rules
three and four will be used when the parser is parsing a comment: rule
three recognises the end of a comment and switches the parser's state back
to parsing code; rule four recognises a comment line without an end token
and keeps the parser's state unchanged.  It is important that the rules are
applied in the order listed in \tableref{Rules to parse C-style comments};
\sectionref{rules in architecture} has explained how this is achieved, and
\sectionref{overlapping rules in architecture} will discuss overlapping
rules.

\begin{table}[ht]
    \caption{Rules to parse C-style comments}
    \empty{}\label{Rules to parse C-style comments}
    \begin{tabular}{llll}
        \tabletopline{}%
        No.   & Regex             & Condition           & Variable Changes    \\
        \tablemiddleline{}%
        % XXX WHAT SHOULD THE VARIABLE BE CALLED?
        1     & \verb!/\*.*?\*/!  & parsing == code     & parsing = code      \\
        2     & \verb!/\*.*!      & parsing == code     & parsing = comment   \\
        3     & \verb!.*\*/!      & parsing == comment  & parsing = code      \\
        4     & \verb!.*!         & parsing == comment  & parsing = comment   \\
        \tablebottomline{}%
    \end{tabular}
\end{table}

\subsection{Overlapping Rules}

\label{overlapping rules in architecture}

XXX IS THERE ANY PREVIOUS RESEARCH IN THIS AREA\@?

When adding new rules, the rule author must be aware that the new rule may
overlap with one or more existing rules, i.e.\ some inputs could be parsed
by more than one rule.  Unintentionally overlapping rules lead to
inconsistent parsing and data extraction because the first matching rule
wins, and the order in which rules are tried against each input might
change between parser invocations.  Overlapping rules are frequently a
requirement, allowing a more specific rule to match some inputs and a more
general rule to match the remainder, e.g.\ separating \acronym{SMTP}
delivery to specific sites from \acronym{SMTP} delivery to the rest of the
world.  Allowing overlapping rules simplifies both the general rule and the
more specific rule; additionally rules from different sources can be
combined with a minimum of prior cooperation or modification required.
Overlapping rules should have a priority attribute to specify their
relative ordering; negative priorities may be useful for catchall rules.
The architecture does not try to detect overlapping rules: that
responsibility is left to the author of the rules.

Overlapping rules can be detected by visual inspection, or a program may be
written to analyse the rule's regexes.  Traditional regexes are equivalent
in computational power to \acronym{FA} and can be converted to
\acronym{FA}, so regex overlap can be detected by finding a non-empty
intersection of two \acronym{FA}\@.  Perl 5.10 regexes~\cite{perlre} are
more powerful than traditional regexes: it is possible to match correctly
balanced brackets nested to an arbitrary depth, e.g.\
\verb!/^[^<>]*(<(?:(?>[^<>]+)|(?1))*>)[^<>]*$/!  matches
\verb!z<123<pq<>rs>j<r>ml>s!.  Matching balanced brackets requires the
regex engine to maintain state on a stack, so Perl 5.10 regexes are
equivalent in computational power to \acronym{PDA}; detecting overlap may
require calculating the intersection of two \acronym{PDA} instead of two
\acronym{FA}.  \acronym{PDA} intersection is not closed, i.e.\ the result
cannot always be implemented using a \acronym{PDA}, so intersection may be
intractable in some cases, e.g.:
$a^{*}b^{n}c^{n}~\cap~a^{n}b^{n}c^{*}~\rightarrow~a^{n}b^{n}c^{n}$.
Detecting overlap amongst $n$ regexes requires calculating
$\frac{n\left(n-1\right)}{2}$ intersections, resulting in
$O\left(n^{2}x\right)$ complexity, where $x$ is the cost of calculating
\acronym{FA} or \acronym{PDA} intersection.  This is certainly not a task
to be performed every time a parser runs: naive detection of overlap
amongst \parsernames{} \numberOFrules{} rules would require calculating
\numberOFruleINTERSECTIONS{} intersections.

When detecting overlap any conditions attached to rules must be taken into
account, because two rules whose regexes overlap may have conditions
attached which prevent the rules overlapping.  A less naive approach to
overlap detection would first check for overlapping conditions amongst
rules, and then check for overlap between the regexes of each pair of rules
with overlapping conditions.  Depending on the conditions employed a rule
may overlap with multiple rules, and rule overlap is not transitive, e.g.\
given these three conditions:

\begin{enumerate}

    % Reduce inter-item spacing because each item is a single line.
    \squeezeitems{}

    \item $total < 10$

    \item $total > 20$

    \item $total < 30$

\end{enumerate}

\noindent{}The first and second conditions do not overlap, but the third
condition overlaps with both the first and second conditions.  When rules
are paired based on how their conditions overlap, the complexity of
detecting overlap amongst $n$ rules is $O\left(n^{2}y+|o|x\right)$, where:

\begin{tabular}[]{rcl}

    $x$ & = & cost of checking for overlap between two regexes    \\
    $y$ & = & cost of checking for overlap between two conditions \\
    $o$ & = & set of pairs of rules with overlapping conditions   \\

\end{tabular}

For this approach to be more efficient than the naive approach $y$ must be
significantly lower than $x$.  If $y$ is higher than $x$ then the opposite
approach should be taken: check for regex overlap first, then check
for condition overlap only between pairs of rules with overlapping regexes.

Once conditions pass a certain level of complexity determining whether two
conditions overlap becomes intractable, because it requires so much
knowledge of other rules and possibly even actions.  E.g.\ given two rules
with conditions \verb!verbose == true! and \verb!silent == true!, logically
these rules should not overlap, yet there is nothing to stop both variables
being set to true by one or more rules or actions.  If variables used in
conditions can be set by actions, determining whether two conditions
overlap is impossible, because understanding how an action will change the
variables is impossible.  XXX DO I NEED TO MENTION CHURCH-TURING OR
ANYTHING\@?

\subsection{Pathological Rules}

It is possible to define pathological regexes which fall into two main
categories: regexes that match inputs they should not, and regexes that
consume excessive amounts of CPU time during matching.  Defining a regex
that matches inputs it should not is trivial: \verb!/^/! matches the start
of every input.  This regex would be found by a tool that detects
overlapping rules, and would easily be noticed by visual inspection, but
more complex regexes would be harder to spot.  Regexes that match more than
they should are a problem not because of excessive resource usage, but
because they may prevent the correct rule from recognising the input.  If
an adaptive ordering system is used that prioritises rules that frequently
recognise inputs (see \sectionref{rule ordering for efficiency}), then a
rule with a regex that matches inputs it should not may be promoted up
through the list, displacing an increasing number of correct rules as it
moves.

Usually excessive CPU time is consumed when a regex with a lot of
backtracking, caused by alteration and/or nested quantifiers, fails to
match; successful matching is generally quite fast with such regexes, so
problematic regexes may go unnoticed for some time.  E.g.\ with most regex
engines matching double quoted strings with \verb!"([^"\\]+|\\.)*"! is very
fast when there is a match, but when the match fails its computational
complexity is $O(2^{n})$; see~\cite{mastering-regular-expressions} for
in-depth discussion of nested quantifiers, backtracking, alteration, and
capturing groups.  Pathological regexes which consume excessive CPU time
can be difficult to detect, both by visual inspection and by machine
inspection, but if a regex is converted to a \acronym{FA} or the internal
representation used by the regex engine, it may be possible to determine if
there are nested quantifiers or other troublesome constructs present.
Modern regex engines have addressed many of these problems, e.g.\ the regex
to match double quoted strings given above fails immediately with Perl
5.10, regardless of the input length, because the regex engine looks for
both of the required double quotes first.  Similarly Perl 5.10's regex
engine optimises alterations starting with literal text into a trie, which
has matching time proportional to the length of the alternatives, rather
than the number of alternatives.  Perl regexes can use \verb!(?>pattern)!,
which matches \verb!pattern! the first time the regex engine passes over
it, and does not change what it originally matched if the regex engine
backtracks over it, so troublesome regexes can use this to reduce the
impact of backtracking; Prolog users will notice a similarity to the
\verb'!' (cut) operator.  A presentation showing some of Perl 5.10's new
regex features is available at
\urlLastChecked{http://www.regex-engineer.org/slides/perl510_regex.html}{2009/03/03}.

Conditions can vary in complexity from simple equality through to a
Turing-equivalent language, so enumerating pathological conditions is
difficult if not pointless.  Conditions that examine the input using
regexes can suffer from the same problems as rule regexes; conditions that
check variables in uncomplicated ways may exhibit unexpected or incorrect
behaviour, but are unlikely to exhibit pathological behaviour.  Deciding
whether a more complex condition's behaviour is pathological or simply a
bug is difficult and to some extent is a matter of opinion.  When this
architecture has received more widespread usage consensus should be reached
on the topic of pathological conditions.

\section{Summary}

This chapter has presented the parser architecture developed for this
project.  It started with a high level view of the architecture, describing
how it achieves its design aim of being easily extensible for users, and
the advantages that being easily extensible brings to parser authors too.
The three main components of the architecture were documented in detail,
explaining each component's responsibilities and the functionality it
provides, plus any difficulties associated with the components.  The
framework, unsurprisingly, was quite quick to document, because it is the
simplest of the three components.  The actions are also quite easy to
understand, because the architecture does not impose any structure or
requirements upon them: parser authors are free to do anything they want
within an action.  The architecture's support for cascaded parsing was
described in the actions section, with an example to illustrate how it can
be useful.  The rules section was the longest, because although the rules
appear conceptually simple --- recognise an input and specify the action
that handles that input --- they have subtle behaviour that needs to be
clearly explained.  When extending a ruleset a decision needs to be taken
about whether the input should be recognised by extending an existing rule,
by adding a new rule to an existing rule category, or by adding a new rule
category, rule, and action.  Rules can have conditions attached to them,
restricting the set of rules used to recognise an input; the complexity of
the conditions used greatly influences the difficulty of writing a correct
ruleset or understanding and extending an existing ruleset.  Overlapping
rules are frequently a requirement in a parser, and their use can greatly
simplify some rules, but they can be a source of bugs because they can
recognise inputs unexpectedly.  The framework does not try to detect
overlapping rules, because overlap amongst rules may be valid and is quite
often intentional; that responsibility falls to the author of the rules.
The difficulty involved in detecting overlap is proportional to the
complexity of a ruleset's regexes and conditions, and may be possible for a
human yet intractable or impossible for a program.  The rules section
finishes with a discussion of pathological rules, concentrating on
pathological regexes.
