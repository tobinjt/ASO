\chapter{Parser Architecture}

\label{parser architecture}

XXX REWRITE\@: MERGE CONTENT FROM ELSEWHERE IN THE THESIS AND PAPER\@.

XXX WHEN FINISHED, CHECK ALL REFERENCES TO ENSURE THEY POINT AT THE CORRECT
CHAPTER\@.

To avoid cluttering the explanation of the parser architecture with details
of the difficulties encountered while parsing Postfix log files, the
description of the parser architecture has been separated from the
description of \parsername{}, the implementation developed for this
project.  This chapter presents the architecture that has been designed and
developed for this project, beginning with the overall architecture and
design, followed by the three components of the architecture: Framework,
Actions, and finally Rules.  The discussion in this chapter centers on the
theoretical, XXX EXTEND WHEN THE CHAPTER IS FINISHED\@.
The practical difficulties of writing a parser for Postfix log files are
covered in detail in \sectionref{Postfix Parser Implementation}.

\section{Parser Architecture and Design}

\label{parser design}

It should be clear from the earlier Postfix background (\sectionref{postfix
background}) that log files produced by Postfix vary widely from host to
host, depending on the set of restrictions chosen by the administrator.
With this in mind, one of the parser's design aims was to make adding new
rules as easy as possible, to enable administrators to properly parse their
own log files.  To enable this the architecture is divided into three
parts: framework, actions and rules.  Each will be discussed separately,
but first an overview:

\begin{eqlist}

    \item [Framework]  The framework is the structure that actions and
        rules plug into.  It provides the parsing loop, shared data
        storage, loading and validation of rules, storage of results, and
        other support functions.

    \item [Actions] Each action performs the work required to deal with a
        single category of inputs, e.g.\ processing data from rejections.
        Actions are invoked to deal with a log line once it has been
        identified by the rules: actions modify data structures, handle
        complications, and cause data to be saved to the database.

    \item [Rules]  The rules are responsible for classifying inputs; they
        specify the action to invoke and the regex that matches the inputs
        and extracts data.  Rules provide an easily extensible method of
        associating log lines with actions.

\end{eqlist}

For each input the framework tries each rule in turn until it finds a rule
that matches the input, then invokes the action specified by that rule.

\label{why separate rules, actions, and framework?}

Decoupling the parsing rules from their associated actions allows new rules
to be written and tested without requiring modifications to the parser
source code, significantly lowering the barrier to entry for casual users
who need to parse new inputs, e.g.\ part-time systems administrators
attempting to combat and reduce spam; it also allows companies to develop
user-extensible parsers without divulging their source code.  Decoupling
the framework, actions, and rules simplifies all three and creates a clear
separation of functionality: the framework provides services to the
actions, but does not need to perform any tasks specific to the input being
parsed; actions benefit from having services provided by the framework,
freeing them to concentrate on the task of accurately and correctly
processing the information provided by rules; rules handle the low level
details of classifying inputs and extracting data from those inputs.

Separating the rules from the actions and framework makes it possible to
parse new log lines without modifying the core parsing algorithm.  Adding a
new rule with the action to invoke and a regex to match the log lines is
trivial in comparison to understanding a program's entire parser,
identifying the correct location to change, and making the appropriate
changes.  Bear in mind that changes to the parser must be made without
adversely affecting existing parsing, particularly as there may be edge
cases that are not immediately obvious ---
\sectionref{yet-more-aborted-delivery-attempts} describes a complication
that occurs only four times in \numberOFlogFILES{} log files.  Requiring
changes to the parser's source code also complicates upgrades, as the
changes must be preserved during the upgrade, and may clash with changes
made by the developer.  This architecture allows the user to add new rules
without changing the parser, unless the new log lines require functionality
not already provided by the existing actions.  If the new log lines do
require new functionality, new actions can be added to the parser without
modifying existing actions (\sectionref{adding new actions in
implementation} describes how to safely add new actions); only in the rare
event that the new actions require support from other sections of the code
will more extensive changes be required.

There is some similarity between the parser's design and William Wood's
\acronym{ATN}~\cite{atns}, used in Computational Linguistics for creating
grammars to parse or generate sentences.  The resemblance between the two
(shown in \tableref{Similarities between ATN and this architecture}) is
accidental, but it is obvious that the two different approaches share a
similar division of responsibilities, despite having different semantics.

% Do Not Reformat!

\begin{table}[ht]
    \caption{Similarities between ATN and this architecture}
    \empty{}\label{Similarities between ATN and this architecture}
    \begin{tabular}[]{lll}
        \tabletopline{}%
        \acronym{ATN}   & Architecture  & Similarity                  \\
        \tablemiddleline{}%
        Networks        & Overall       & Determines the sequence 
                                          of transitions              \\
                        & Algorithm     & or actions that 
                                          constitutes a valid input.  \\
        Transitions     & Actions       & Assembles data and
                                          imposes conditions          \\
                        &               & the input must meet to be
                                          accepted as                 \\
                        &               & valid.                      \\
        Abbreviations   & Rules         & Responsible for 
                                          classifying input.          \\
        \tablebottomline{}%
    \end{tabular}
\end{table}

\section{Framework}

XXX EXTEND FRAMEWORK SECTION\@: EXPAND ON ALL OF THE POINTS IN THE LIST IN
THE NEXT PARAGRAPH\@.

\label{framework in architecture}

The framework takes care of miscellaneous support functions and low level
details of parsing, freeing the programmers writing actions to concentrate
on writing productive code.  It links actions and rules, allowing either to
be improved independently of the other.  It provides shared storage to pass
data between actions, loads and saves state, loads and validates rules,
manages parsing, invokes actions, tracks how often each rule matches to
optimise rule ordering (\sectionref{rule ordering for efficiency}), stores
results of parsing, and miscellaneous other tasks.
Most parsers will require the same basic functionality from the framework,
plus some specialised support functions.  The framework is the core of the
architecture and is deliberately quite simple: the rules deal with the
variation in inputs, and the actions deal with the intricacies and
complications encountered when parsing.

The function that finds the rule matching the input and invokes the
requested action can be expressed in pseudo-code (indentation denoting flow
of control) as:

% DO NOT REFORMAT!

\begin{verbatim}
for each input:
    for each rule defined by the user: 
        if this rule matches the input:
            perform the action specified by the rule
            skip the remaining rules
            process the next input
    warn the user that the input was not parsed
\end{verbatim}

\section{Actions}

\label{actions in architecture}

XXX TO BE WRITTEN\@.  START WITH THE CONTENT FROM THE PAPER, THEN CHECK THE
IMPLEMENTATION SECTION\@.

\section{Rules}

\label{rules in architecture}

XXX TO BE WRITTEN\@.  TAKE CONTENT FROM THE PAPER AND POSSIBLY FROM THE
IMPLEMENTATION SECTION\@.

\subsection{Rule Characteristics}

\label{rule characteristics in architecture}

Rule have certain characteristics that may help in understanding the
parser:

\begin{itemize}

    \item Rules are annotated with the name of a Postfix program, and will
        only be used when parsing log lines produced by that
        program.\footnote{There are also generic rules which are used when
        parsing log lines produced by any Postfix program, but only if
        there are also rules specific to that program, and those rules have
        already have been tried and have failed on the current log line.}
        Any given rule will only be used to parse a subset of the log
        lines, and any given log line will only have a subset of the rules
        tried against it.

    \item The first matching rule wins: no further rules are tried against
        that log line, but there is a mechanism for prioritising the rules
        so that more specific rules can be tried first.

    \item Rules are completely self-contained and can be understood in
        isolation, without reference to any other rules.

    \item Rule processing time is a linear function of the number of rules.
        XXX IMPROVE THIS\@.

\end{itemize}

\label{comparison against context-free grammars}

In context-free grammar terms the parser rules could be described as:

$\text{\textless{}log-line\textgreater{}} \mapsto \text{rule-1} |
\text{rule-2} | \text{rule-3} | \dots | \text{rule-n}$


\subsection{Overlapping Rules}

\label{overlapping rules in architecture}

XXX IS THERE ANY PREVIOUS RESEARCH IN THIS AREA\@?

XXX DFA COMPARISON\@.

The parser does not try to detect overlapping rules; that responsibility is
left to the author of the rules.  Unintentionally overlapping rules lead to
inconsistent parsing and data extraction because the order in which rules
are tried against each line may change between log files, and the first
matching rule wins.  Overlapping rules are frequently a requirement,
allowing a more specific rule to match some log lines and a more general
rule to match the majority, e.g.\ separating \acronym{SMTP} delivery to
specific sites from \acronym{SMTP} delivery to the rest of the world.  The
algorithm provides a mechanism for ordering overlapping rules: the priority
field in each rule (defaults to zero).  Negative priorities may be useful
for catchall rules.

Detecting overlapping rules is difficult, but the following approaches may
be helpful:

\begin{itemize}

    \item Sort by regex and visually inspect the list, e.g.\ with
        \acronym{SQL} similar to: \textbf{select regex from rules order by
        regex;}

    \item Compare the results of parsing using sorted, shuffled, and
        reversed rules (described in \sectionref{rule ordering for
        efficiency}).  Parse several log files using optimal ordering, then
        dump a textual representation of the rules, connections, and
        results tables.  Repeat with shuffled and reversed ordering,
        starting with a fresh database.  If there are no overlapping rules
        the tables from each run will be identical; differences indicate
        overlapping rules.  The rules that overlap can be determined by
        examining the differences in the tables: each result contains a
        reference to the rule which created it, if the references differ
        between runs the two rules referenced in the differing results
        overlap.  Unfortunately this method cannot prove the absence of
        overlapping rules; it can detect overlapping rules, but only if
        there are log lines in the log files that are matched by more than
        one rule.

\end{itemize}


\section{Conclusion}

XXX TO BE WRITTEN\@.
