\chapter{Conclusion}

\label{conclusion}

XXX THIS NEEDS TO BE EXTENDED\@.

XXX THERE IS NO MENTION OF RESULTS/EFFICIENCY\@.

Parsing Postfix log files appears at first sight to be an almost trivial
task, especially if one has previous experience in parsing log files, but
it turns out to be a much more taxing project than initially expected.  The
variety and breadth of log lines produced by Postfix is quite surprising,
because a quick survey of sample log files gives the impression that the
number of distinct log lines is quite small; this impression is due to the
uneven distribution exhibited by log lines produced in normal operation
(see \graphref{rule hits graph} for a vivid illustration of this).


Given the diverse nature of the log lines and the ease with which
administrators can cause new log lines to be logged (\sectionref{postfix
background}), enabling users to easily extend the parser to deal with new
log lines is a design imperative (\sectionref{parser design}).  Despite the
resulting initial increase in complexity the task is quite tractable,
though it does raise efficiency and optimisation questions (answered in
\sectionref{parser efficiency}).  Providing a tool to ease the generation
of regexes from unparsed log lines should greatly help users add rules
to parse previously unparsed log lines (\sectionref{creating new rules in
implementation}).


The architecture not only substantially eases the parsing of new log lines,
it makes adding new actions (\sectionref{adding new actions in
implementation}) a relatively simple task.  The simplicity of adding a new
action frees the implementor from worrying about how their action might
interfere with existing parsing, so they can focus on correctly
implementing their new action.


The division of the parser into rules, actions, and framework is unusual
because rules are separated so completely from the actions and framework;
although parsers are often divided (the combination of \texttt{lex} and
\texttt{yacc}~\cite{lex-and-yacc-book} being a common example), the parts
are usually quite internally interdependent, and will be combined into a
complete parser by the compilation process; in contrast \parsername{} keeps
the rules and actions separate until the parser runs.  The actions and
framework are not as completely separated, because the actions depend on
services provided by the framework; however the actions and framework are
not tightly integrated: it would be possible, with some work, to completely
separate the two.

The flow of control in this parser is quite different from that of most
other parsers.  In most traditional parsers the parser has a current state,
and each state has a fixed set of acceptable next states, with unacceptable
states causing parsing to fail --- e.g.\ when a \textbf{C} parser sees the
keyword \textbf{for} it expects to immediately see a left parenthesis, with
any other input causing parsing to fail (comments will already have been
removed by the preprocessor).  This parser is different: the rule which
matches the next log line for a mail dictates the action that will be
invoked, which is equivalent to the next state in other parsers; thus the
next log line for a mail dictates the next state for that mail.  XXX THINK
ABOUT HOW TO MAKE THE PARSER MORE STATEFUL IF REQUIRED\@; ADD IT TO THE
ARCHITECTURE SECTION\@.

The real difficulties arise once the parser is successfully dealing with
90\% of the log lines, as the irregularities and complications explained in
\sectionref{complications} only become apparent once the vast majority of
log lines have been parsed successfully.  Adding new rules to deal with
numerous, infrequently occurring log lines is a simple, albeit tiresome,
task, (though alleviated by the tool described in \sectionref{adding new
actions in implementation}), whereas dealing with mails where information
appears to be missing is much more grueling.  Trawling through the log
files, looking for something out of the ordinary, possibly hundreds or even
thousands of log lines away from the last mention of the queueid in
question, is extremely time consuming and error prone.  Sometimes the task
is not to spot the line that is unusual, but to spot that a line that is
normally present is missing, i.e.\ to realise that one line amongst
thousands is absent.  In all cases the evidence must be used to construct a
hypothesis to explain the irregularities, and that hypothesis must then be
tested in the parser; if successful, the parser must be modified to deal
with the irregularities, without adversely affecting the existing
functionality.  The complications described in \sectionref{complications}
were solved in the order they are described in, and that order closely
resembles the frequency with which they occur; the most frequently
occurring complications dominate the warning messages produced, and so
naturally they are the first complications to be dealt with.

A parser's ability to correctly parse its input is extremely important; the
parser's coverage of its test log files is discussed in \sectionref{parsing
coverage}.  Both its success at parsing individual log lines
(\sectionref{log-lines-covered}) and its correctness in reconstructing each
mail's journey through Postfix (\sectionref{mails-covered}) are described
in detail, including the results of manually verifying the correct parsing
of a subset of the test log files.

\newpage


