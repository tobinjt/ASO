\section{Coverage}

\label{parsing coverage}

The discussion of the parser's coverage of Postfix log files is separated
into two parts: log lines covered and mails covered.  The first is
important because the parser should handle all (relevant) log lines it is
given; the second is equally important because the parser must properly
deal with every mail if it is to be useful.  Improving the former is
less intrusive, as it just requires new rules to be written; improving the
latter is much more intrusive as it requires changes to the parser
algorithm, and it can also be much harder to notice a deficiency.

\subsection{Log lines covered}

\label{log-lines-covered}

Parsing a log line is a three stage process:

\begin{enumerate}

    \item Check if there are any rules for the Postfix component which
        produced the log line; if not then skip the log line.

    \item Try each rule until a matching rule is found; if no match is
        issue a warning and move to the next log line.

    \item Execute the action specified by the rule.

\end{enumerate}

Full coverage of log lines requires the following:

\begin{enumerate}

    \item Each Postfix component whose log lines are of interest must have
        at least one rule or its log lines will be silently skipped; in the
        extreme case of zero rules the parser would happily skip every log
        line.  There may be any number of log lines from other programs
        intermingled in the log file, and there are some Postfix programs
        which do not produce any log lines of interest.

    \item There must be a rule to match each different log line produced by
        each program; if a log line is not successfully matched the parser
        will issue a warning.  Rules should be as specific and tightly
        bound as possible to ensure accurate parsing:\footnote{A rule which
        matches zero or more of any character will successfully parse every
        log line, but not in a meaningful way.} most log lines contain
        fixed strings and have a rigid pattern, so this is not a problem.

    \item The appropriate action to take --- discussed in
        \sectionref{mails-covered}.

\end{enumerate}

Full coverage of log lines is easy to achieve yet hard to maintain.  It is
easy to achieve full coverage for a limited set of log files (at the time
of writing the parser is tested with \numberOFrules{} rules, fully parsing
\numberOFlogFILES{} contiguous log files from Postfix 2.2 and 2.3), and new
rules are easy to add.  Maintaining full coverage is hard because other
servers have different restrictions with custom messages, \DNSBL{} messages
change over time, major releases of Postfix change warning messages
(usually adding more information), etc.,\ so over time the log lines drift
and change.  \Graphref{rule hits graph} shows the number of hits
for each rule over all \numberOFlogFILES{} log files; it is obvious that a
small number of rules match the vast majority of the lines, and more than
half the rules match fewer than 100 times.

Warnings are issued for any log lines which are not parsed; no warnings are
issued for unparsed log lines while testing with the \numberOFlogFILES{}
test log files, so it can be safely concluded that there are zero false
negatives.  False positives are harder to quantify: short of examining each
of the 60,721,709 log lines and determining which \regex{} parsed it, there
is no way to be sure that every line was parsed by the correct \regex{},
making it impossible to quantify the false positive rate; however a random
sample of 6,039 log lines was parsed and the results checked manually to
ensure that the correct \regex{} parsed each log line.\footnote{Each log
line was examined and the correct \regex{} identified from the
\numberOFrules{} rules in the database; the correct \regex{} was then
compared to the \regex{} which was used by the parser.}  The sample was
generated by running the following command:

\verb!    perl -e 'print if (rand 1 < 0.0001)' -n LOG_FILES!

\noindent{}to randomly extract roughly one line in every 10,000.  Although
on initial appearances exercising only 36 rules (from a total of
\numberOFrules{}) when parsing 6039 log lines seems quite low, after
examining \graphref{rule hits graph} it becomes apparent that such
a low hit rate is to be expected; the reader should also bear in mind that
even when parsing all \numberOFlogFILES{} log files not all the rules are
exercised (some of the rules are for parsing log lines which only appear in
other log files).

\subsection{Mails covered}

\label{mails-covered}

Coverage of mails is much more difficult to determine accurately than
coverage of log lines.  The parser can dump its state tables in a human
readable form; examining these tables with reference to the log files is
the best way to detect mails which were not handled properly (many of the
complications discussed in \sectionref{additional complications} were
detected in this way).  The parser issues warnings when it detects any
errors, some of which may alert the user to a problem, e.g.\ when a queueid
is reused before the previous mail is fully dealt with, when a queueid or
\pid{} is not found in the state tables,\footnote{There will often be
warnings about a missing queueid or \pid{} in the first few hundred or
thousand log lines because the earlier log lines for those connections or
mails are in the previous log file; loading the saved state from the
previous log file will solve this problem.} or when there are problems
tracking a child mail (see \sectionref{tracking re-injected mail}).  There
should be few or no warnings when parsing, and when finished parsing the
state table should only contain entries for mails which had yet to be
delivered when the log files ended, or were accepted before the log files
began.

At the time of writing the parser is being tested with \numberOFlogFILES{}
log files.  There are 5 warnings produced, but because the parser errs on
the side of producing more warnings rather than fewer, those 5 warnings
represent 3 instances of 1 problem: 3 connections started before the first
log file, so their initial log entries are missing, leading to warnings
when their log lines are parsed.

The state tables contain entries for mails not yet delivered when the
parser finishes execution.  Ideally all they should contain are mails which
are awaiting delivery after the period covered by the log files, though
they may also contain mails whose initial entries are not contained in the
log files.  Any other entries are evidence of a failure in parsing or an
aberration in the log files.  After parsing the \numberOFlogFILES{} test
log files the state tables contain 18 entries, breaking down into:

\begin{itemize}

    \item 1 connection which started only seconds before the log files
        ended and had not yet been fully transferred from client to server.

    \item 1 mail which had been accepted only seconds before the log files
        ended and had not yet been delivered.

    \item 9 mails whose initial log entries were not present in the log
        files.

    \item 7 mails which had yet to be delivered due to repeated failures.

\end{itemize}

There are no mails in the state tables which should not be present, thus it
can be concluded that there are zero false negatives.  Once again,
determining the false positive rate is much harder, as manually checking
the results of parsing 13,850,793 connections and mails accepted, rejected,
bounced or delivered is infeasible.  There is considerable circumstantial
evidence that the false positive rate is quite low:

\begin{itemize}

    \item The parser is quite verbose when complaining about known problems
        (e.g.\ if a mail is tracked twice as described in
        \sectionref{tracking re-injected mail}), and no such warnings are
        produced during the test runs.

    \item Queueids and \pids{} naturally group together log lines belonging
        to one mail or connection respectively; it is extremely unlikely
        that a log line would be associated with the wrong connection.

    \item When dealing with the complications described in
        \sectionref{complications} and \sectionref{additional
        complications} the solutions are as specific and restrictive as
        possible, with the goal of minimising the number of false
        positives.  In addition the solution to the \textit{Out of order
        log files\/} complication described in \sectionref{out of order log
        lines} imposes conditions which each reassembled mail must comply
        with to be acceptable.

    \item Every effort has been made while developing to make the parser as
        precise, demanding and particular as possible.

\end{itemize}

Whereas verifying by inspection that the parser correctly deals with all
60,721,709 lines in the test log files is infeasible, verifying a subset of
those log files is a tractable, if extremely time consuming, task.  A
sample of log lines was obtained by randomly selecting a log file:

\verb!    perl -Mstrict -Mwarnings -MList::Util=shuffle \!\newline
\verb!            -e 'print [shuffle(@ARGV)]->[0];'!

The first 6000 lines of this log file (roughly 0.01\% of the total number
of log lines used in testing) was extracted:

\verb!    sed -n -e '1,6000p' logfile > test-log-segment!

It is important that the log lines used are contiguous so that all log
entries are present for as many of the connections as possible.  This log
segment was parsed with all debugging options enabled, resulting in 167,448
lines of output.\footnote{A mean of 27.908 lines of output per line of
input; each connection has 30 debugging lines, plus 21 debugging lines per
result.  Connections which have been cloned will have the cloned connection
in their debugging output, plus another 33 debugging lines.  Those numbers
are approximate, and may vary $\pm{}$ 2.  There is a linear relationship
between the number of log lines and debugging lines: $33(connections) +
30(accepted~~mails) + 21(results)$.  This formula is an approximation only,
and has not been rigorously verified.}  All 167,448 lines were examined in
conjunction with the input log and a dump of the resulting database,
verifying that for each of the input lines the parser used the correct rule
and executed the correct action, which in turn produced the correct result
and inserted the correct data in the database.  The log segment produced 4
warnings, 10 mails remaining in the state tables, and 1625 connections
correctly entered in the database.

Given the circumstantial and experimental evidence detailed above, the
author is confident that the false positive rate when reconstructing a mail
is exceedingly low, if not approaching zero.

\subsection{Summary}

Parser coverage is divided into two topics in this section: log lines
covered and mails covered.  The former is initially more important, as the
parser must successfully parse every line if it is to be complete, but
subsequently the latter takes precedence because reproducing the path a
mail takes through Postfix is the aim of the parser.  Increasing the
percentage of log lines parsed is relatively simple and non-intrusive:
adding new rules or modifying existing rules is simplified by the
separation of rules, actions and framework.  Improving the logical coverage
is harder, as the actions taken by Postfix must be reconstructed by the
author, and the new sequence of actions integrated into the existing model
without breaking the existing parsing.  Detecting a deficiency in the
parsing algorithm is also significantly harder than detecting unparsed log
lines, as the parser will warn about any unparsed line, whereas discovering
a flaw in the parser requires understanding of the warnings produced and
the mails remaining in the state table.  Rectifying a flaw in the parser
requires an understanding of both the parser and Postfix's log files, and
investigative work to determine the cause of the deficiency, followed by
further examination of the log files in developing a solution.

