\newpage
\section{Postfix Parser Implementation}

\subsection{Assumptions}

The algorithm described and the program implementing it make a small number
of (hopefully safe and reasonable) assumptions:

\begin{itemize}

    \item The log files are whole and complete: nothing has been removed,
        either deliberately or accidentally (e.g.\ log rotation gone awry,
        file system filling up, logging system unable to cope with the
        volume of log messages).  On a well run system it is extremely
        unlikely that any of these problems will arise, though it is of
        course possible, particularly when suffering a deluge of spam or a
        mail loop.

    \item Postfix logs enough information to make it possible to accurately
        reconstruct the actions it has taken.  There are several heuristics
        used when parsing; see
        \sectionref{identifying-bounce-notifications},
        \sectionref{aborted-delivery-attempts} and \sectionref{pickup
        logging after cleanup} for details.

    \item The Postfix queue has not been tampered with, causing unexplained
        appearance or disappearance of mail.  This may happen if the
        administrator deletes mail from the queue without using
        \daemon{postsuper}, or if there is filesystem corruption.

\end{itemize}

In some ways this task is similar to reverse engineering or replicating a
black box program based solely on its inputs and outputs.  Although the
source code is available,\footnote{Reading and understanding the source
code would require a significant investment of time: there are 375,750
lines of code, documentation, etc.\ in Postfix 2.5.1's 17MB of source
code.} there are advantages to treating Postfix as a black box during
parser development:

\begin{itemize}

    \item The parser is developed using real world log files rather than
        the idealised log files someone would naturally envisage reading
        the source code.

    \item The source code cannot accurately communicate the variety of
        orderings in which log lines are written to the log file, as
        process scheduling independently interferes with logging and other
        processing.

    \item The parser acts as a second source of information, with the
        information gathered from empirical evidence.  A separate project
        could compare the empirical knowledge inherent in the parsing
        algorithm with the documentation and source code of Postfix to see
        how closely the two agree.

\end{itemize}



\subsection{Database Schema}
\label{database schema}

The database is an integral part of the parser presented here: it stores
the rules and the data gleaned by applying those rules to Postfix log
files.  Understanding the database schema is important in understanding the
actions of the parser, and essential to developing further applications
which utilise the data gathered.

The database schema can be conceptually divided in two: the rules, which
are used to parse log files, and the data saved from the parsing of log
files.  Rules have the fields required to parse the log lines, extract data
to be saved, and the action to be executed; they also have several fields
that aid the user in understanding the meaning of the log lines parsed by
each rule.  The rules are described in detail in \sectionref{rules} but the
fields are covered in \sectionref{rule attributes}.

The data saved from parsing the log files is also divided into two tables
as described below: connections and results.  The connections table
contains a row for every mail accepted and every connection where there was
a rejection; the individual fields will be described in
\sectionref{connections table}.  There will be at least one row in the
results table for each row in the connections table; the fields will be
covered in detail in \sectionref{results table}.

An important but easily overlooked benefit of storing the rules in the
database is the connection between rules and results: if more information
is required when examining a result, the rule that produced the result is
available for inspection --- each result references the rule that created
it.  There is no ambiguity about which rule resulted in a particular
result, eliminating one potential source of confusion.

\subsubsection{Rules table}

\label{rule attributes}

Rules are discussed in detail in \sectionref{rules}, but the structure of
the rules table is covered here.  Rules are created by the user, not the
parser, and will not be modified by the parser (except for the hits and
hits\_total fields).  Rules parse the individual log lines, extracting data
to be saved in the connections and results tables, and specifying the
action to take for that log line.

The following attributes are defined for each rule:

\begin{description}

    \item [id] A unique identifier for each rule that other tables can use
        to refer to a specific rule.

    \item [name] A short name for the rule.

    \item [description] Something must have occurred to cause Postfix to
        log each line (e.g.\ a remote client connecting causes a connection
        line to be logged).  This field describes the event causing the log
        lines this rule matches.

    \item [restriction\_name] The restriction that caused the mail to be
        rejected.  Only applicable to rules that have an action of
        \texttt{REJECTION}, other rules should have an empty string.

    \item [postfix\_action] This is the action Postfix must have taken to
        generate this log line.  This field is mostly ignored, but two
        values (IGNORED and INFO) have special meaning, as described below
        in the list of typical values.\label{postfix_action}

        \begin{description}

            \item [ACCEPTED] Postfix has accepted a mail, and will
                subsequently attempt to deliver it.

            \item [BOUNCED] The mail has bounced, because of a mail loop,
                delivery failure, or five day timeout.

            \item [DELETED] The mail was deleted from the queue by an
                administrator.

            \item [DISCARDED] Postfix discarded the mail it was in the
                process of accepting, because it was either larger than the
                limit set by the administrator, or the client timed out or
                disconnected.

            \item [EXPIRED] The mail has been in the queue for five
                days\footnote{As with much of Postfix's behaviour, this is
                the default value but can be changed by the administrator
                if they choose.} without successful delivery.  A bounce
                mail will be generated and sent to the sender address.

            \item [INFO] Represents an unspecified intermediate action that
                the parser is not interested in per se, but that does log
                useful information, supplementing other log lines.

            \item [IGNORED] An action that is not only uninteresting in
                itself, but that also provides no useful data.

            \item [POSTFIX\_RELOAD] The administrator has instructed
                Postfix to start or stop, and all existing \daemon{smtpd}
                processes will be terminated.  This does not negatively
                affect the log files or mail queued by Postfix for
                delivery.

            \item [PROCESSING] \daemon{cleanup} is processing a mail
                (see~\cite{postfix-cleanup} for details).

            \item [REJECTED] Postfix rejected a command from the remote
                client, causing at least one recipient to be rejected.

            \item [SENT] Postfix has successfully sent a mail.

        \end{description}

        Uninteresting log lines are parsed so that any lines the parser
        is not capable of handling become immediately obvious errors.

    \item [program] The program (\daemon{smtpd}, \daemon{qmgr}, etc.) whose
        log lines the rule applies to.  This avoids needlessly trying rules
        that will not match the log line, or worse, might match
        unintentionally.  Rules whose program is \texttt{*} will be tried
        against any log lines that are not parsed by program specific
        rules.

    \item [regex] The \regex{} to match the log line against.  The \regex{}
        will first have several keywords expanded: this simplifies reading
        and writing rules; avoids needless repetition of complex \regex{}
        components; allows the components to be corrected and/or improved
        in one location; and makes each \regex{} largely self-documenting.

        For efficiency the keywords are expanded and every rule's \regex{}
        is compiled before attempting to parse the log file --- otherwise
        each \regex{} would be recompiled each time it was used, resulting
        in a large, data dependent slowdown.  Efficiency concerns are
        discussed in \sectionref{parser efficiency}, with caching compiled
        \regexes{} covered in \sectionref{Caching compiled regexes}.

    \item [result\_cols, connection\_cols] Specifies how the fields in the
        log line will be extracted.  The format is: \newline{} \tab{}
        \texttt{smtp\_code = 1; recipient = 2, sender = 4;} \newline{} i.e.\
        semi-colon or comma separated assignment statements, with the
        variable name on the left and the matching capture from the
        \regex{} on the right hand side.  Data specified by
        \texttt{result\_cols} will be entered in the \texttt{results}
        table, whereas \texttt{connection\_cols} populates the
        \texttt{connection} table.  The list of acceptable variable names
        is:

        \texttt{connection\_cols: client\_hostname, client\_ip, server\_ip,
        \newline{} \tab{} server\_hostname} and \texttt{helo}.\newline{}
        \texttt{result\_cols: sender, recipient, smtp\_code, message\_id,
        \newline{} \tab{} size,} and \texttt{data}

        Additionally \texttt{child} and \texttt{pid} are accepted and used
        respectively by the \texttt{TRACK}, \texttt{BOUNCE} and
        \texttt{SMTPD\_DIED} actions, but do not cause data to be entered
        in the database.

    \item [result\_data, connection\_data] Sometimes rules need to supply a
        piece of data that is not present in the log line: e.g.\ setting
        \texttt{smtp\_code} when mail is accepted.  The format and allowed
        variables are the same as for \texttt{result\_cols} and
        \texttt{connection\_cols}, except that arbitrary
        data\footnote{Commas and semi-colons cannot be escaped and thus
        cannot be used.  This feature is intended for use with small
        amounts of data rather than large amounts in any one rule, so
        dealing with escape sequences was deemed unnecessary.} is permitted
        on the right hand side of the assignment.

    \item [action] The action that will be invoked when this rule matches a
        log line; a full list of actions and the parameters they are
        invoked with can be found in \sectionref{actions-in-detail}.

    \item [queueid] Specifies the capturing field in the \regex{}
        containing the queueid, or zero if the log line does not contain a
        queueid.  Many log lines will not contain a queueid, e.g.\
        rejections logged before a mail has been accepted (a queueid will
        not have been allocated), or log lines that are not tied to one
        particular mail.

    \item [hits] This counter is maintained for every rule and incremented
        each time the rule successfully matches.  At the start of each run
        the program sorts the rules in descending order of hits, and at the
        end of the run updates every rule's hits field in the database.
        Assuming that the distribution of log lines is reasonably
        consistent between log files, rules matching more commonly
        occurring log lines will be tried before rules matching less
        commonly occurring log lines, reducing the program's execution
        time.  Rule ordering for efficiency is discussed in
        \sectionref{rule ordering for efficiency}.

    \item [hits\_total] The total number of hits for this rule over all
        runs of the parser.

    \item [priority] This is the user-configurable companion to hits: when
        the list of rules is sorted, priority overrides hits.  This allows
        more specific rules to take precedence over more general rules
        (described in \sectionref{overlapping rules}).

    \item [cluster\_group] A reference to the \texttt{cluster\_group}
        table.  That table is used by the Decision Tree algorithm described
        in a separate document.

\end{description}


\subsubsection{Connections table}

\label{connections table}

Every accepted mail and every connection where there was a rejection will
have a single entry in the connections table containing the following
fields:

\begin{description}

    \item [id] This field uniquely identifies the row.

    \item [server\_ip] The \IP{} address (IPv4 or IPv6) of the server: the
        local address when receiving mail, the remote address when sending
        mail.

    \item [server\_hostname] The hostname of the server, it will be
        \texttt{unknown} if the \IP{} address could not be resolved to a
        hostname via \DNS{}\@.

    \item [client\_ip] The client \IP{} address (IPv4 or IPv6): the remote
        address when receiving mail, the local address when sending mail.

    \item [client\_hostname] The hostname of the client, it will be
        \texttt{unknown} if the \IP{} address could not be resolved to a
        hostname via \DNS{}\@.

    \item [helo] The hostname used in the HELO command.  The HELO hostname
        occasionally changes during a connection, presumably because spam
        or virus senders think it is a good idea.  By default Postfix only
        logs the HELO hostname when it rejects an \SMTP{} command, but it
        is quite simple to rectify this, as described in
        \sectionref{logging helo}.

    \item [queueid] The queueid of the mail if the connection represents an
        accepted mail, or \texttt{NOQUEUE} otherwise.

    \item [start] The timestamp of the first log line, in seconds since the
        epoch (explained in the glossary, \sectionref{Glossary}).

    \item [end] The timestamp of the last log line, in seconds since the
        epoch.

\end{description}

\subsubsection{Results table}

\label{results table}

XXX REWRITE THE SENTENCE AND FOOT NOTE\@.

Every interesting\footnote{All log lines are interesting unless the
postfix\_action of the rule that parses them is \texttt{INFO} or
\texttt{IGNORE}, as described in \sectionref{actions-in-detail}.} log line
will have an entry in the results table, e.g.\ rejecting an \SMTP{}
command, delivering a mail, or bouncing a mail (see
\sectionref{postfix_action} for typical values of
\texttt{postfix\_action}).  Each row is associated with a single
connection, though there may be many results per connection.

\begin{description}

    \item [connection\_id] The id of the row in the connections table this
        result is associated with.

    \item [rule\_id] The id of the entry in the rules table that matched
        the log line and created this result.

    \item [id] A unique identifier for this result.

    \item [warning] Postfix can be configured to log a warning instead of
        enforcing a restriction that would reject an \SMTP{} command --- a
        facility that is quite useful for testing new restrictions.  This
        field will be 1 if the log line parsed was a warning rather than a
        real rejection, or 0 for a real rejection.  This field should be
        ignored if the result is not a rejection, i.e.\ the action field of
        the associated rule is not \texttt{REJECTION}.

    \item [smtp\_code] The \SMTP{} code associated with the log line.  In
        general an \SMTP{} code is only present for a rejection or final
        delivery; results initially missing an \SMTP{} code will duplicate
        the \SMTP{} code of other results in the connection.  Some final
        delivery log lines do not contain an \SMTP{} code: in those cases
        the code is specified by the rule, based on the success or failure
        represented by the log line.

    \item [sender] The sender's email address.  Because multiple mails may
        be delivered during one connection, there may be different sender
        addresses in the results for one connection; however there should
        not be different sender addresses in the results for one email.
        Mails sent over the same connection can be distinguished by their
        queueid.

    \item [recipient] The recipient address; there may be multiple
        recipient addresses per mail, but only one per result.

    \item [size] The size of the mail; it will only be present for
        delivered mails.

    \item [message\_id] The message-id of the accepted mail, or
        \texttt{NULL} if no mail was accepted.

    \item [data] A field available for anything not covered by other
        fields, e.g.\ the rejection message from an \DNSBL{}\@.

    \item [timestamp] The time the log line was logged, in seconds since
        the epoch.

\end{description}

\subsubsection{Summary}

XXX DO I NEED A SUMMARY HERE\@?

The table containing the rules used by the parser and both tables
containing the data extracted from the Postfix log files were described,
with the purpose of each field discussed in detail.  A clear,
comprehensible schema is essential when using the extracted data; it is
more important when using the data than when storing it, because storing
the data is a write-once operation, whereas utilising the data requires
frequent searching, sorting and manipulation of the data to produce
customised reports or statistics.


\subsection{Rules}

\label{rules}

\subsubsection{Example rule}

\label{example rule}

This example rule matches the message logged by Postfix when it rejects
mail from a sender address because the appropriate \DNS{} entries are
missing, i.e.\ mail could not be delivered to the sender's address (for
full details see~\cite{reject-unknown-sender-domain}).

The example rule below matches the following log line:

\begin{verbatim}
NOQUEUE: reject: RCPT from example.com[10.1.1.1]: 550
  <foo@example.com>: Sender address rejected: Domain not found;
  from=<foo@example.com> to=<info@example.net>
  proto=SMTP helo=<smtp.example.com>
\end{verbatim}

% do not reformat this!
\begin{tabular}[]{ll}

\textbf{Field}      & \textbf{Value}                                    \\
name                & Unknown sender domain                             \\
description         & We do not accept mail from unknown domains        \\
restriction\_name   & reject\_unknown\_sender\_domain                   \\
postfix\_action     & REJECTED                                          \\
program             & \daemon{smtpd}                                    \\
regex               & \verb!^__RESTRICTION_START__ <(__SENDER__)>: !    \\
                    & \verb!Sender address rejected: Domain not found;! \\
                    & \verb!from=<\5> to=<(__RECIPIENT__)> !            \\
                    & \verb!proto=E?SMTP helo=<(__HELO__)>$!            \\
result\_cols        & recipient = 6; sender = 5                         \\
connection\_cols    & helo = 7                                          \\
result\_data        &                                                   \\
connection\_data    &                                                   \\
action              & REJECTION                                         \\
queueid             & 1                                                 \\
hits                & 0                                                 \\
hits\_total         & 0                                                 \\
priority            & 0                                                 \\
cluster\_group      & 400                                               \\

\end{tabular}

\vspace{1em}

Additional data will be captured automatically when the \regex{} contains 
\_\_RESTRICTION\_START\_\_, hence the capture numbers in result\_cols and
connection\_cols start at 5.  The various fields are used as follows;

\begin{description}

    \item [name, description, restriction\_name and postfix\_action:] are
        not \newline{} used by the algorithm, they serve to document the rule
        for the user's benefit.

    \item [program and regex:] If the program in the rule equals the
        program that logged the log line, a match using the rule's
        \regex{} will be attempted against the log line; if the match is
        successful the action will be executed, if not the next rule will
        be tried.  If the program-specific rules do not match the log line,
        the parser will fall back to generic rules; if those rules are
        unsuccessful a warning will be issued.

    \item [action:] The action to be executed if the \regex{} matches
        successfully (see \sectionref{actions-in-detail} for full details).

    \item [result\_cols, connection\_cols, result\_data and
        connection\_data:] are \newline{} used by the action to extract and
        save data matched by the \regex{}.

    \item [queueid:] The index of the capture in the \regex{} that
        supplies the queueid, or zero if the log line does not contain a
        queueid.  This allows the correct mail to be found by queueid and
        actions performed on it.

    \item [hits, hits\_total and priority:] hits and priority are used in
        ordering the rules (see \sectionref{rule ordering for efficiency});
        hits is set to the number of successful matches at the end of the
        parsing run; hits\_total is the sum of hits over every parsing run,
        but is otherwise unused by the algorithm.

    \item [cluster\_group] The cluster\_group attribute is used by the
        Decision Tree algorithm described in a separate document; the
        parser does not use it in any way.

\end{description}



\subsubsection{Creating new rules}

\label{creating new rules}

The log files produced by Postfix differ from installation to installation,
because administrators have the freedom to choose the subset of available
restrictions which suits their needs, including using different \DNSBL{}
services, policy servers, or custom rejection messages.  To facilitate
parsing new log lines, the parser's design separates parsing rules from
parsing actions: adding new actions is difficult, but adding new rules to
parse new rejection messages is trivial, and also occurs much more
frequently.  The implementation provides a program to ease the process of
creating new rules from unparsed log lines, based on the algorithm
developed by Risto Vaarandi~\cite{risto-vaarandi} for his
\SLCT{}~\cite{slct-paper}.  The differences between the two algorithms will
be outlined as part of the general explanation below.

The core of the algorithm is quite simple: log lines are generally created
by substituting variable words into a fixed pattern, and analysis of the
frequency with which each word occurs can be used to determine whether the
word is variable or part of the fixed pattern.  This classification can be
used to group similar log lines and generate a \regex{} to match each group
of log lines.

There are 4 steps in the algorithm:

\begin{description}

    \item [Pre-process the file]  The new algorithm leverages the knowledge
        gained when writing rules and performs a large number of
        substitutions on the log input lines, replacing commonly occurring
        variable terms (e.g.\ email addresses, \IP{} addresses, the
        standard start of rejection messages, etc.) with \regex{} keywords
        that the parser will expand when it loads the rule (see the
        \regex{} entry in \sectionref{rule attributes}).  The purpose of
        this step is to utilise existing knowledge to create more accurate
        \regexes{}.  The new log lines are written to a temporary file,
        which all subsequent stages use instead of the original input file.

        In the original algorithm the purpose of the preprocessing stage
        was to reduce the memory consumption of the program.  In the first
        pass it generated a hash from a small range of values for each word
        of each log line, incrementing a counter for each hash.  The
        counters will be used in the next pass to filter out words: if the
        word's hash does not have a high frequency, the word itself cannot
        have a high frequency, and there is no need to maintain a counter
        for it, reducing the number of counters and thus the program's
        memory consumption.

    \item [Calculate word frequencies]  The position of words within a log
        line is important: a common word does not indicate similarity
        between log lines unless it occupies the same position within both
        log lines.\footnote{If a variable term within a line contains
        spaces, it will appear to the algorithm as two words rather than
        one.  This will alter the position of subsequent words, so a word
        occurring in different positions in two log lines \textit{may\/}
        indicate similarity, but the algorithm does not attempt to deal
        with this possibility.}  The algorithm maintains a counter for each
        \textit{(word, word's position within the log line)\/} tuple,
        incrementing it each time that word occurs in that position.

        The original algorithm only maintains counters for words whose hash
        result from the previous step has a high frequency; this reduces
        the number of counters maintained by the algorithm, reducing the
        memory requirements of the algorithm.
        
        As time goes on the amount of memory typically available increases
        and the requirement to reduce memory requirements decreases, hence
        the modified algorithm omits the hashing step and maintains
        counters for all tokens. In addition most uncommon or infrequently
        occurring words will have been substituted with keywords during the
        first step, vastly reducing the number of tuples to maintain
        counters for.

    \item [Classify words based on their frequency]  The frequency of each
        \textit{(word, word's position within the log line)\/} tuple is
        checked: if its frequency is greater than the threshold supplied by
        the user (1\% of all log lines is generally a good starting point)
        it is classified as a fixed word, otherwise it is classified as a
        variable term.  If a variable term appears sufficiently often it
        will be misclassified as a fixed term, but that should be noticed
        by the user when reviewing the new \regexes{}.  Variable terms are
        replaced by \texttt{.+}, which means to match zero or more of any
        character.  

    \item [Build regexes]  The words are reassembled to produce a \regex{}
        matching the log line, and a counter maintained for each \regex{}.
        Contiguous sequences of \texttt{.+} in the newly reassembled
        \regexes{} are collapsed to a single \texttt{.+}; any resulting
        duplicate \regexes{} are combined, and their counters added
        together.  If the \regex{}'s counter is lower than the threshold
        supplied by the user the \regex{} is discarded; this second
        threshold is independent of the threshold used to differentiate
        between fixed and variable words, but once again 1\% of input log
        lines is a good starting point; obviously the threshold depends on
        the number and content of input log lines.  The new \regexes{} are
        displayed for the user to add to the database, either as new rules
        or merged into the \regexes{} of existing rules; the counter for
        each \regex{} is also displayed, giving the user an indication of
        how many of the input log lines that \regex{} should match.
        Discarding \regexes{} will result in some of the input log lines
        not being matched; the utility should be run again once the
        unmatched log lines have been isolated by parsing the original log
        files once the parser has been augmented with the new rules.

\end{description}

A second utility is also provided that reads a list of new \regexes{} and
the input given to the first utility.  It tries to match each input log
line against each \regex{}, counting the number of log lines that match
each \regex{}, warning the user if an input log line is matched by more
than one \regex{}, and additionally warning if an input log line is not
matched by any \regex{}.  It displays a summary of how many input log lines
each \regex{} matched, comparing it to the expected number of matches; this
provides the user with an easy method of checking if the \regexes{}
produced by the first utility are correctly matching the input log lines
they are based upon.  A future version of this utility will also group
input log lines by \regex{}, so the user can tweak the \regexes{} if
required.

These utilities are not intended to create perfect \regexes{}, but they
greatly reduce the effort required to parse new or different log lines.

\subsection{Parsing Algorithm}

\label{parsing-algorithm}

EMPHASISE THAT THE LOGS CAUSE THE INTERDEPENDENCIES, NOT THE
ARCHITECTURE\@.

Whereas the rules are quite simple and each rule is completely independent
of the other rules, the algorithm is significantly more complicated and
highly internally interdependent.  The algorithm deals with all the
complications of parsing, the eccentricities and oddities of Postfix log
files, presenting the resulting data in a normalised, simple to use
representation.  The algorithm's task is to follow the journey each mail
takes through Postfix, combining the data extracted by rules into a
coherent whole, saving it in a useful and consistent form, and performing
housekeeping duties.

Please refer to \sectionref{parser design} for a discussion of why the
rules, actions and framework have been separated in the parser's design.
In this section algorithm can be taken to mean the combination of framework
and actions.

\subsubsection{Introduction}

XXX MERGE WITH PREVIOUS PARAGRAPHS\@.

This section covers the following topics:

\begin{itemize}

    \item A high level overview of the algorithm.

    \item The first set of complications encountered: initially obvious
        difficulties that had to be overcome.

    \item A flow chart showing common paths a mail can take through the
        algorithm, a discussion of the emergent behaviour observed in the
        parser, and an explanation of the paths shown in the flow chart.

    \item A brief description of the framework.

    \item The actions the parser makes available to rules are covered in
        detail.

    \item Additional complications that have arisen during the development
        of this parser are documented, the solutions to those
        complications, and where those solutions are implemented in the
        algorithm

\end{itemize}

\subsubsection{A high level overview}

From the viewpoint of an individual mail passing through the parser the
experience could be summarised as:

\begin{enumerate}

    \item Mail enters the system via \SMTP{} or local submission.

    \item If the mail is rejected, save all data and finish.

    \item Follow the progress of the accepted mail until it is either
        delivered, bounced, or deleted, then save all data, and finish.

\end{enumerate}

Unfortunately both high level views ignore the many complications
encountered.


\subsubsection{Flow chart}

\label{flow-chart}

XXX CONSIDER REMOVING PART TWO OF THE FLOW CHART\@: I DO NOT THINK THAT
RE-INJECTION IS IMPORTANT ENOUGH TO WARRANT IT\@.  THE EXPLANATION OF
TRACKING (\textsection{}5.4.3) COULD BE MOVED TO ADDITIONAL
COMPLICATIONS\@.  POSSIBLY MERGE MAIL ENTRY AND DELIVERY IN HERE\@;
TRACKING SHOULD BE IN COMPLICATIONS --- MERGE WITH RE-INJECTED MAILS\@.

\Figureref{flow chart image part 1} and \figureref{flow chart image part 2}
show the paths the data representing a mail can take through the parser
algorithm.  The flow chart covers the most common paths only; there are
additional, uncommon paths excluded for the sake of clarity; details of the
deviations can be found in \sectionref{complications}.

\showgraph{build/logparser-flow-chart-part-1}{Parser flow chart part
1}{flow chart image part 1}

\showgraph{build/logparser-flow-chart-part-2}{Parser flow chart part
2}{flow chart image part 2}

\clearpage

\subsubsection{Mail enters the system}

\label{mail-enters-the-system}

Everything starts off with a mail entering the system, whether by local
submission via \daemon{postdrop} or sendmail, by \SMTP{}, by re-injection
due to forwarding, or internally generated by Postfix.  Local submission is
the simplest case: a queueid is assigned immediately and the sender address
is logged (action: PICKUP\@; flowchart:~2).

\SMTP{} is more complicated:

\begin{enumerate}

    \item First there is a connection from the remote client (action:
        CONNECT\@; flowchart:~1).

    \item This is followed by rejection of sender address, recipient
        addresses, client \IP{} address or hostname, HELO hostname, etc.\
        (action: REJECTION\@; flowchart:~4); acceptance of one or more
        mails (action: CLONE\@; flowchart:~5); or some interleaving of
        both.

    \item The client disconnects (action: DISCONNECT\@; flowchart:~6).  If
        Postfix has rejected any \SMTP{} commands the data will be saved to
        the database; if not there will not be any data to save (any mails
        accepted will already have been cloned so their data is in another
        data structure).

    \item If one or more mails were accepted there will be more log entries
        for those mails later, see \sectionref{mail-delivery}.

\end{enumerate}

Re-injection due to forwarding sadly lacks explicit log lines of its
own;\footnote{Previously discussed in \sectionref{complications},
complication 3.} re-injection is somewhat awkward to explain because it
overlaps both the mail acceptance and mail delivery sections, so discussion
is deferred to \sectionref{tracking re-injected mail}.

Internally generated mails lack any explicit origin in Postfix 2.2.x and
must be detected using heuristics (see
\sectionref{identifying-bounce-notifications} for details).  Bounce
notifications are the primary example of internally generated mails, though
there may be other types.\footnote{Postfix may generate mails to the
administrator when it encounters configuration errors, but such mails are
presumably rare.}

\subsubsection{Mail delivery}

\label{mail-delivery}

The obvious counterpart to mail entering the system is mail leaving the
system, whether by deletion, bouncing, local delivery, or remote delivery.
All four are handled in exactly the same way:

\begin{enumerate}

    \item Postfix will log the sender and recipient addresses separately
        (action: SAVE\_BY\_QUEUEID\@; flowchart:~9).

    \item Sometimes mail is re-injected and the child mail needs to be
        tracked by the parent mail (action: TRACK\@; flowchart:~10) ---
        \sectionref{tracking re-injected mail} discusses this in
        detail.

    \item Eventually the mail will be delivered, bounced, or deleted by the
        administrator (action: COMMIT\@; flowchart:~12).  This is the last
        log line for this particular mail (though it may be indirectly
        referred to if it was re-injected).  If it is neither parent nor
        child of re-injection the data is cleaned up and entered in the
        database (flowchart:~14), then deleted from the state tables.
        Re-injected mails are described in \sectionref{tracking re-injected
        mail}.

\end{enumerate}

It should be reiterated that the actions above happen whether the mail is
delivered to a mailbox, piped to a command, delivered to a remote server,
bounced (due to a mail loop, delivery failure, or five day timeout), or
deleted by the administrator, \textit{unless\/} the mail is either parent
or child of re-injection, as explained in \sectionref{tracking re-injected
mail}.

\subsubsection{Tracking re-injected mail}

\label{tracking re-injected mail}

The crux of the problem is that re-injected mails appear in the log files
without explicit logging indicating their source.  There are two implicit
indications:

\begin{enumerate}

    \item The indicator which more commonly introduces re-injection is when
        \daemon{qmgr} selects a mail with a previously unseen queueid for
        delivery (action: MAIL\_QUEUED\@; flowchart:~3), in which case a
        new data structure will be created for that mail.  The mail will be
        flagged as having unknown origins; this flag should be subsequently
        cleared once the origin has been established.  This may also be an
        indicator that the mail is a bounce notification, see
        \sectionref{identifying-bounce-notifications} for details.

    \item Local delivery re-injects the mail and logs a relayed delivery
        rather than delivering directly to a mailbox or program as it
        usually would (action: TRACK\@; flowchart:~10).\footnote{Relayed
        delivery is performed by the \SMTP{} client; local delivery means
        local to the server, i.e.\ an address the server is final
        destination for.} In this case the mail may already have been
        created (described above) and the unknown origin flag will be
        cleared; if not a new data structure will be created.  In both
        cases the re-injected mail is marked as a child of the original
        mail.  The log line in question is:

        \texttt{3FF7C4317: to=<username@example.com>, relay=local, \newline{}
        delay=0, status=sent (forwarded as 56F5B43FD)}

        This second indicator always occurs for re-injected mail but
        typically occurs after the first indicator explained above.  This
        is required to tie the parent and child mails together and so is
        central to the process of tracking re-injected mails.

\end{enumerate}

The algorithm for tracking and saving re-injected mail to the database can
finally be described:

\begin{itemize}

    \item If the mail is of unknown origin it is assumed to be a child mail
        whose parent has not yet been identified (action: COMMIT\@;
        flowchart:~15).  Mark the mail as ready for entry in the database
        (flowchart:~16), and wait for the parent to deal with it
        (flowchart:~17).  The mail should not have subsequent log entries;
        only its parent should refer to it.

    \item If the mail is a child mail then it has already been tracked
        (action: COMMIT\@; flowchart:~18): as with all other mail, the data
        is cleaned up, the child is entered in the database
        (flowchart:~19), then deleted from the state tables.  The child
        mail will be removed from the parent mail's list of children
        (flowchart:~20); if this is the last child and the parent has also
        been entered in the database the parent will be deleted from the
        state tables.

    \item The last alternative is that the mail is a parent mail (action:
        COMMIT\@; flowchart:~21).  Regardless of the state of its children
        its data is cleaned up and entered in the database (flowchart:~22).
        The parent may have children that are waiting to be entered in the
        database (flowchart:~23); each of those children's data is cleaned
        up and entered in the database, then deleted from the state tables.
        The parent may also have outstanding children which are not yet
        delivered (flowchart:~24), in which case the parent must wait for
        those children to be finished with.  As soon as the last child is
        deleted from the state tables the parent will also be finished with
        (flowchart:~25), and deleted from the state tables.

\end{itemize}

A parent mail can have multiple children, which may be delivered before or
after the parent mail.

\subsubsection{Emergent behaviour}

\label{Emergent behaviour}

The rules and actions exhibit emergent behaviour~\cite{Wikipedia-Emergence}
that is far more complicated than the behaviour explicitly encoded in the
algorithm.  The top-level parser could be written in pseudo-code like so
(with indentation level indicating flow control):

\begin{verbatim}
for each line in the input files:
    for each rule defined by the user:
        if this rule matches the input line:
            perform the action specified by the rule
            skip the remaining rules
            process the next input line
    warn the user that the input line was not parsed
\end{verbatim}

Yet despite the brevity of the algorithm above the (simplified) flow chart
in \sectionref{flow-chart} contains:

\begin{itemize}

    \item Twenty one states (not including explicit branches).

    \item Three entry points (1, 2, 3).

    \item Five exit points (8, 14, 17, 20, 25).

    \item Five loops (4--4, 4--5--4, 5--5, 9--9, 9--10--9).

    \item Four explicit branches (7, 13, 15, 18); these are decisions taken
        by an action, determining what will happen next to a mail.

    \item Four implicit branches, where the transition is determined by the
        next log line that is processed for that mail, e.g.\ state 1 can
        transition to either state 4 or state 5.  (1--\{4,5\},
        4--\{4,5,6\}, 5--\{4,5,6\}, 9--\{9,10,11,12\})

\end{itemize}

Several more states and corresponding branches are omitted for the sake of
clarity (these extra states are caused by solving the complications
described in \sectionref{complications}).

The actions introduce some of the complexity seen in the flow chart: all
explicit branches and the transitions between some stages (19--20,
21--22--23--24) in the second half of the flow chart
(\figureref{flow chart image part 2}) are encoded within the
actions; the transitions between the other stages (16--17, 24--25) in the
second half arise from the interaction between actions

However the transitions between stages in the first half of the flow chart
(\figureref{flow chart image part 1}) are not encoded anywhere in
the parser; the transitions are determined by the ordering of log lines in
the log file.  The complexity of the first 12 stages in the flow chart
emerges from the interaction of simple rules and actions, easing the
process of adding new actions (described in \sectionref{adding new
actions}), as there is no requirement to explicitly insert the action into
an algorithmic version of the flow chart.\footnote{The solution to the
complication in \sectionref{out of order log lines} (out of order log
lines) requires a list of the acceptable combinations of Postfix programs a
mail can pass through; however this does not correspond to the
\textit{parser actions\/} a mail must pass through.  Before that
complication was overcome, i.e.\ for the first half of the development of
this parser, there was nothing in the parser explicitly encoding the states
or paths a mail could take; no other component in the parser has any need
for this flow of data to be specified.}



\subsubsection{Actions in detail}

\label{actions-in-detail}

Each action is passed the same arguments:

\begin{description}

    \item [line] The log line, separated into fields:

        \begin{description}

            \item [timestamp] The time the line was logged at.

            \item [host] The hostname of the server that logged the line.

            \item [program] The name of the program that logged the line.

            \item [pid] The \pid{} of the program that logged the line.

            \item [text] The remainder of the line.

        \end{description}

    \item [rule] The matching rule.

    \item [matches] The fields in the line captured by the rule's \regex{}.

\end{description}

The actions:

\begin{description}

    \item [BOUNCE] Postfix 2.3 and subsequent versions log the creation of
        bounce messages.  This action creates a new mail if necessary; if
        the mail already exists the unknown origin flag will be removed.
        The action also marks the mail as a bounce notification.  To deal
        with complication \sectionref{Further out of order log lines} this
        action checks a cache of recent bounce mails to avoid creating
        bogus bounce mails when lines are logged out of order.

    \item [CLONE] Multiple mails may be accepted on a single connection, so
        each time a mail is accepted the connection's state table entry
        must be cloned and saved in the state tables under its queueid; if
        the original data structure was used then second and subsequent
        mails would overwrite one another's data.

    \item [COMMIT] Enter the data from the mail into the database. Entry
        will be postponed if the mail is a child waiting to be tracked.
        Once entered, the mail will be deleted from the state tables.
        Deletion will be postponed if the mail is the parent of re-injected
        mail.

    \item [CONNECT] Handle a remote client connecting: create a new state
        table entry (indexed by \daemon{smtpd} \pid{}) and save both the
        client hostname and \IP{} address.

    \item [DELETE] Deals with mail deleted using Postfix's administrative
        command \daemon{postsuper}.  This action adds a dummy recipient
        address if required, then invokes the COMMIT action to handle
        adding the mail to the database.  The complication this action
        deals with is described fully in \sectionref{Mail deleted before
        delivery is attempted}.  

    \item [DELIVERY\_REJECTED] Deals with Postfix rejecting an \SMTP{}
        command from the remote client: log the rejection with a mail if
        there is a queueid in the log line, or with the connection if not.

    \item [DISCONNECT] Deal with the remote client disconnecting: enter the
        connection in the database, perform any required cleanup, and
        delete the connection from the state tables.  This action deals
        with aborted delivery attempts
        (\sectionref{aborted-delivery-attempts}).

    \item [EXPIRY] If Postfix has not managed to deliver a mail after
        trying for five days it will give up and return the mail to the
        sender.  When this happens the mail will not have a combination of
        Postfix programs which passes the valid combinations check (see
        \sectionref{out of order log lines}).  To ensure that the mail can
        be committed the EXPIRY action sets a flag marking the mail as
        expired; the flag later causes the valid combinations check to be
        skipped, so the mail will be committed.

    \item [MAIL\_QUEUED] This action represents Postfix
        picking a mail from the queue to deliver. This action is used for
        both \daemon{qmgr} and \daemon{cleanup} as it needs to deal with
        out of order log lines; see \sectionref{discarding cleanup lines}
        for details.

    \item [MAIL\_TOO\_LARGE] When a client tries to send a message larger
        than the local server accepts, the mail will be discarded, and the
        client informed.  See TIMEOUT for further discussion; the two are
        handled in exactly the same way.

    \item [PICKUP] The PICKUP action corresponds to the \daemon{pickup}
        service dealing with a locally submitted mail.  Out of order log
        entries may have caused the state table entry to already exist (see
        \sectionref{pickup logging after cleanup}); otherwise it is
        created.  The data extracted from the log line is then saved to the
        state table entry.

    \item [POSTFIX\_RELOAD] When Postfix stops or reloads its configuration
        it kills all \daemon{smtpd} processes,\footnote{Possibly other
        programs are killed also, but the parser is only affected by and
        interested in \daemon{smtpd} processes exiting.} requiring any
        active connections to be cleaned up, entered in the database, and
        deleted from the state tables.

    \item [SAVE\_DATA] Find the correct mail based on the queueid in
        the log line, and save the data extracted by the \regex{} to it.

    \item [SMTPD\_DIED] Sometimes a \daemon{smtpd} dies or exits
        unsuccessfully; the active connection for that \daemon{smtpd} must
        be cleaned up, entered in the database, and deleted from the state
        tables.

    \item [SMTPD\_KILLED] Sometimes an \daemon{smtpd} is killed by a
        signal~\cite{Wikipedia-unix-signals}; the active connection for
        that \daemon{smtpd} must be cleaned up, entered in the database,
        and deleted from the state tables.

    \item [SMTPD\_WATCHDOG] \daemon{smtpd} processes have a watchdog timer
        to deal with unusual situations --- after five hours the timer will
        expire and the \daemon{smtpd} will exit.  This occurs very
        infrequently, as there are many other timeouts that should occur
        in the intervening hours: \DNS{} timeouts, timeouts reading data
        from the client, etc\@.  The active connection for that
        \daemon{smtpd} must be cleaned up, entered in the database, and
        deleted from the state tables.

    \item [TIMEOUT] The connection timed out so the mail currently being
        transferred must be discarded. The mail may have been accepted, in
        which case there's a data structure to dispose of, or it may not in
        which case there is not.  See
        \sectionref{timeouts-during-data-phase} for the gory details.

    \item [TRACK] Track a mail when it is re-injected for forwarding to
        another mail server; this happens when a local address is aliased
        to a remote address.  TRACK will be called when dealing with the
        parent mail, and will create the child mail if necessary. TRACK
        checks if the child has already been tracked, either by this parent
        or by another parent, and issues appropriate warnings in either
        case.

    \item [UNINTERESTING] This rule just returns successfully; it is used when a
        line needs to be parsed for completeness but does not either
        provide any useful data or require anything to be done.

\end{description}

The distribution of rules per action is shown in \graphref{Distribution of
rules per action}.

\showgraph{build/graph-action-distribution}{Distribution of rules per
action}{Distribution of rules per action}

\subsubsection{Adding new actions}

\label{adding new actions}

Adding new actions is not as simple as adding new rules, though care has
been taken in the parser's design and implementation to make adding new
actions as painless as possible.  The implementor writes a subroutine that
accepts the standard arguments given to actions, and registers it as an
action\footnote{The new action must be registered before the rules are
loaded, as it is an error for a rule to reference an unregistered action;
this helps catch mistakes made when adding new rules.} by calling the
framework subroutine add\_actions() with the name of the new action
subroutine as a parameter.  No other work is required on the implementor's
part to integrate the action into the parser; all of their attention and
effort can be focused on the correctness of their action.  The only
negative aspect is that the process involves editing the parser source
code, which makes upgrading to a later version of the parser more
difficult, though by no means impossible.  If the author of the new action
wishes, they can take advantage of the parser's object oriented
implementation~\cite{Wikipedia-object-orientation} by subclassing it and
implementing their changes in the derived class, allowing future upgrading
of the parser with greatly reduced chance of conflicts.\footnote{The real
difference between the two approaches is where the new code is placed.  The
simpler option is to change the parser code directly, but those changes
will then have to be made to subsequent versions of the parser, and as the
scope of the changes increases so does the chance of conflict, or mistakes
when copying the action.  The more time consuming option is to write a
subclass containing the new actions and change the program which invokes
the parser so that it uses the subclass rather than the parser; the changes
required to the program invoking the parser are minor and much less likely
to lead to conflicts when upgrading to future versions of the parser.  An
alternative is to submit new actions to the author of the parser for
inclusion in future versions, resulting in two benefits: the new actions do
not need to be maintained separately, and other users of the parser will
avail of the new functionality.} The action may need to extend the list of
valid combinations described in \sectionref{out of order log lines} if the
addition creates a different set of acceptable programs, but this is
unlikely to occur, as it would require parsing log lines from Postfix
components the parser currently ignores.\footnote{The mail server used for
development does not utilise either the \daemon{lmtp} or \daemon{virtual}
delivery agents, so this parser does not have rules to handle log lines
from those components.  Adding new rules to parse those component's log
lines is a simple process, though if their behaviour differs significantly
from the \daemon{smtp} or \daemon{local} delivery agents new actions may be
required.  The mail server in question is a production mail server handling
mail for a university department; the benefit is that the log files used
exhibit the idiosyncrasies and peculiarities a mail server in the wild must
deal with, but the downside is that significantly altering the
configuration just to log messages from a different Postfix component is
not an option.}


\subsubsection{Summary}

This section presented the core of the parser, starting with a very high
level view and the initial complications that arose.  A flow chart showing
the paths a mail may take through the nascent simplified algorithm was
provided, followed by an explanation of those paths, and a discussion of
the parser's emergent behaviour --- the data from the log files creates the
paths in the flow chart, they are not specified anywhere in the parser.
The framework which holds the parser together was covered next, after which
came a description of the current actions provided by the parser, and the
algorithm for analysing unparsed log lines to create \regexes{} for new
rules.  Detecting, diagnosing and defeating complications forms the largest
single portion of this section, mirroring the development of the parser.
The complications are described in the order they were overcome, with
subsequent problems affecting fewer mails (often by an order or magnitude),
though the time required to solve problems increased with each successive
problem.


\subsection{Complications encountered}

\label{complications}

The complications described in this section are listed in the order in
which they were encountered during development of the parser.  Each of
these complications caused the parser to operate incorrectly, generating
either warning messages or leaving mails in the state table.  The frequency
of occurrence is much higher at the start of the list, with the first
complication occurring several orders of magnitude more frequently than the
last.  When deciding which problem to address next, the most common was
always chosen, as resolving the most common problem would yield the biggest
improvement in the parser, prune the greatest number of mails from the
state tables and error messages, and make the remaining problems more
apparent.  The first three complications were encountered early in the
parser's implementation and guided its design and development.

\subsubsection{Queueid vs pid}

The mail lacks a queueid until it has been accepted, so log lines must
first be correlated by the \daemon{smtpd} \pid{}, then transition to being
correlated by the queueid.  This is relatively minor, but does require:

\begin{itemize}

    \item Two versions of several functions: \texttt{by\_pid} and
        \texttt{by\_queueid}.

    \item Two state tables to hold the data structure for each connection.

    \item Most importantly: every section of code must know whether it
        needs to lookup the data structures by \pid{} or queueid.

\end{itemize}

\subsubsection{Connection reuse}

\label{connection reuse}

Multiple independent mails may be delivered during one connection: this
requires the algorithm to clone the current data as soon as a mail is
accepted, so that subsequent mails will not trample over each other's data.
This must be done every time a mail is accepted, as it is impossible to
tell in advance which connections will accept multiple mails.  Once the
mail has been accepted its log entries will not be correlated by \pid{} any
more (its queueid will be used instead), so there is no longer any
ambiguity about which mail a given log line belongs to (except when
timeouts occur during the data phase
\sectionref{timeouts-during-data-phase}).  If the original connection has
any interesting data (e.g.\ rejections) it will be saved to the database
when the client disconnects.  One unsolved difficulty is distinguishing
between different groups of rejections, e.g.\ when dealing with the
following sequence:

\begin{enumerate}

    \item The client attempts to deliver a mail, but it is rejected.

    \item The client issues the RSET command to reset the session.

    \item The client attempts to deliver another mail, likewise rejected.

\end{enumerate}

There should ideally be two separate entries in the database resulting from
the above sequence, but currently there will only be one.



\subsubsection{Re-injected mails}

The most difficult complication initially encountered is that locally
addressed mails are not always delivered directly to a mailbox: sometimes
they are addressed to and accepted for a local address but need to be
delivered to one or more remote addresses due to aliases.  When this occurs
a child mail will be injected into the Postfix queue, but without the
explicit logging \daemon{smtpd} or \daemon{postdrop} injected mails have.
Thus the source is not immediately discernible from the log line in which
the mail first appears; from a strictly chronological reading of the log
files it usually appears as if the child mail has appeared from thin air.
Subsequently the parent mail will log the creation of the child mail:

\texttt{3FF7C4317: to=<username@example.com>, relay=local, \newline{}
delay=0, status=sent (forwarded as 56F5B43FD)}

The child mail has been created with queueid \texttt{56F5B43FD}.  Different
delivery methods result in different log lines:

\begin{description}

    \item [Re-injected for forwarding:] forwarded as 56F5B43FD

    \item [Delivered to a remote \SMTP{} server:] 250 Ok: queued as
        BD07D3C49

    \item [Local delivery:] delivered to command:
        /mail/procmail/bin/procmail -p -t /mail/procmail/etc/procmailrc

\end{description}

Unfortunately, while all log lines from an individual process appear in
chronological order, the order in which log lines from different processes
are interleaved is subject to the vagaries of process scheduling.  In
addition, the first log line belonging to the child mail (the log line
cited above belongs to the parent mail) is logged by \daemon{qmgr}, so the
order also depends on how soon \daemon{qmgr} processes the new mail.

Because of the uncertain order the parser cannot complain when it
encounters a log line from \daemon{qmgr} for a previously unseen mail;
instead it must flag the mail as coming from an unknown origin, and
subsequently clear the flag if and when the origin of the mail becomes
clear.  Obviously the parser could omit checking where mails originate
from, but requiring an explicit source helps to expose bugs in the parser;
such checks helped to identify the complications described in
\sectionref{discarding cleanup lines} and \sectionref{pickup logging after
cleanup}.

Process scheduling can have a still more confusing effect: quite often the
child mail will be created, delivered and entirely finished with
\textbf{before} the parent logs the creation log line!  Thus, mails flagged
as coming from an unknown origin cannot be entered into the database when
their final log line is parsed; instead they must be marked as ready for
entry and subsequently entered by the parent mail once it has been
identified.

\subsubsection{Identifying bounce notifications}

\label{identifying-bounce-notifications}

Postfix 2.2.x (and presumably previous versions) lacks explicit logging
when bounce notifications are generated; suddenly there will be log entries
for a mail that lacks an obvious source.  There are similarities to the
problem of re-injected mails discussed in \sectionref{tracking re-injected
mail}, but unlike the solution described therein bounce notifications do
not eventually have a log line that identifies their source.  Heuristics
must be used to identify bounce notifications, and those heuristics are:

\begin{enumerate}

    \item The sender address is $<>$.

    \item Neither \daemon{smtpd} nor \daemon{pickup} have logged any
        messages associated with the mail, indicating it was generated
        internally by Postfix, not accepted via \SMTP{} or submitted
        locally by \daemon{postdrop} or sendmail.

    \item The message-id has a specific format: \newline{}
        \texttt{YYYYMMDDhhmmss.queueid@server.hostname} \newline{}
        e.g.\ \texttt{20070321125732.D168138A1@smtp.example.com}

    \item The queueid in the message-id must be the same as the queueid of
        the mail: this is what distinguishes bounce notifications generated
        locally from bounce notifications which are being re-injected as a
        result of aliasing.  In the latter case the message-id will be
        unchanged from the original bounce notification, and so even if it
        happens to be in the correct format (e.g.\ if it was generated by
        Postfix on this or another server) the queueid in the message-id
        will not match the queueid of the mail.

\end{enumerate}

Once a mail has been identified as a bounce notification, the unknown
origin flag is cleared and the mail can be cleaned up and entered in the
database.

There is a small chance that a mail will be incorrectly identified as a
bounce notification, as the heuristics used may be too broad.  For this to
occur the following conditions would have to be met:

\begin{enumerate}

    \item The mail must have been generated internally by Postfix.

    \item The sender address must be $<>$.

    \item The message-id must have the correct format and match the queueid
        of the mail.  While a mail sent from elsewhere could easily have
        the correct message-id format, the chance that the queueid in the
        message-id would match the queueid of the mail is extremely small.

\end{enumerate}

If a mail is mis-classified as a bounce message it will almost certainly
have been generated internally by Postfix; arguably mis-classification in
this case is a benefit rather than a drawback, as other mails generated
internally by Postfix will be handled correctly.

Postfix 2.3 (and hopefully subsequent versions) log the creation of a
bounce message.

This check is performed during the COMMIT action.

\subsubsection{Aborted delivery attempts}

\label{aborted-delivery-attempts}

Some mail clients behave unexpectedly during the \SMTP{} dialogue: the
client aborts the first delivery attempt after the first recipient is
accepted, then makes a second delivery attempt for the same recipient which
it continues with until delivery is complete.\footnote{Microsoft Outlook is
one client that behaves in this fashion; no attempt has been made to
identify other clients that act in a similar way.}  An example dialogue
exhibiting this behaviour is presented below (lines starting with a three
digit number are sent by the server, the other lines are sent by the
client):

\begin{verbatim}
220 smtp.example.com ESMTP
EHLO client.example.com
250-smtp.example.com
250-PIPELINING
250-SIZE 15240000
250-ENHANCEDSTATUSCODES
250-8BITMIME
250 DSN
MAIL FROM: <sender@example.com>
250 2.1.0 Ok
RCPT TO: <recipient@example.net>
250 2.1.5 Ok
RSET
250 2.0.0 Ok
RSET
250 2.0.0 Ok
MAIL FROM: <sender@example.com>
250 2.1.0 Ok
RCPT TO: <recipient@example.net>
250 2.1.5 Ok
DATA
354 End data with <CR><LF>.<CR><LF>
The mail transfer is not shown.
250 2.0.0 Ok: queued as 880223FA69
QUIT
221 2.0.0 Bye
\end{verbatim}

Once again Postfix does not log a message making the client's behaviour
clear, so once again heuristics are required to identify when this
behaviour occurs.  In this case a list of all mails accepted during a
connection is saved in the connection state, and the accepted mails are
examined when the disconnection action is executed.  Each mail is checked
for the following: 

\begin{itemize}

    \item Are there two or more \daemon{smtpd} log lines?  Does the second
        result have a postfix\_action attribute of ACCEPTED\@?  The first
        two \daemon{smtpd} log lines will be a connection log line and a
        mail acceptance log line (Postfix logs acceptance as soon as the
        first recipient has been accepted).

    \item Is \daemon{smtpd} the only Postfix component that produced a log
        line for this mail?  Every mail which passes normally through
        Postfix will have a \daemon{cleanup} line, and later a
        \daemon{qmgr} log line; lack of a \daemon{cleanup} line is a sure
        sign the mail did not make it too far.  

    \item Does the queueid exist in the state tables?  If not it cannot be
        an aborted delivery attempt.

    \item If there are third and subsequent results, are all their
        postfix\_action attributes equal to INFO\@?  If there are any log
        lines after the first two they should be informational lines only.

\end{itemize}

If all the checks above are successful then the mail is assumed to be an
aborted delivery attempt and is discarded.  There will be no further
entries logged for such mails, so without identifying and discarding them
they accumulate in the state table and will cause clashes if the queueid is
reused.  The mail cannot be entered in the database as the only data
available is the client hostname and \IP{} address, but the database schema
requires many more fields be populated (see \sectionref{connections table}
and \sectionref{results table}).  This heuristic is quite restrictive, and
it appears there is little scope for false positives; if there are any
false positives there will be warnings when the next log line for that mail
is parsed.  False negatives are less likely to be detected: there may be
queueid clashes (and warnings) if mails remain in the state tables after
they should have been removed, otherwise the only way to detect false
negatives is to examine the state tables after each parsing run.

This check is performed in the DISCONNECT action; it requires support in
the CLONE action where a list of cloned connections is maintained.

\subsubsection{Further aborted delivery attempts}

Some mail clients disconnect abruptly if a second or subsequent recipient
is rejected; they may also disconnect after other errors, but such
disconnections are either unimportant or are handled elsewhere in the
parser (\sectionref{timeouts-during-data-phase}).  Sadly, Postfix does not
log a message saying the mail has been discarded, as should be expected by
now.  The checks to identify this happening are:

\begin{itemize}

    \item Is the mail missing its \daemon{cleanup} log message?  Every mail
        which passes through Postfix will have a \daemon{cleanup} line;
        lack of a \daemon{cleanup} line is a sure sign the mail did not
        make it too far.

    \item Were there three or more \daemon{smtpd} log lines for the mail?
        There should be a connection line and a mail accepted line,
        followed by one or more rejection lines.

    \item Is the last \daemon{smtpd} log line a rejection line?

\end{itemize}

These checks are made during the DISCONNECT action: if all checks are
successful then the mail is assumed to have been discarded when the client
disconnected.  There will be no further entries logged for such mails, so
without identifying and entering them in the database immediately they
accumulate in the state table and will cause clashes if the queueid is
reused.

\subsubsection{Timeouts during DATA phase}

\label{timeouts-during-data-phase}

The DATA phase of the \SMTP{} conversation is where the headers and body of
the mail are transferred.  Sometimes there is a timeout or the connection
is lost\footnote{For brevity's sake \textit{timeout\/} will be used
throughout this section, but everything applies equally to lost
connections.} during the DATA phase; when this occurs Postfix will discard
the mail and the parser needs to discard the data associated with that
mail.  It seems more intuitive to save the data to the database, but if a
timeout occurs there is no data available to save; the timeout is recorded
with the connection data instead.

To deal properly with timeouts the parsing algorithm needs to do the
following in the TIMEOUT action:

\begin{enumerate}

    \item Record the timeout and associated data in the connection's
        results.

    \item If no mails have been accepted yet nothing needs to be done; the
        TIMEOUT action ends.  

    \item If one or more recipients have been accepted Postfix will have
        allocated a queueid for the incoming mail, and there will be a mail
        in the state tables that needs to be dealt with.

\end{enumerate}

A timeout may thus apply either to an accepted mail or a rejected mail.  To
distinguish between the two cases the algorithm compares the timestamp of
the last accepted mail against the timestamp of the last line logged by
\daemon{smtpd} for that connection (the TIMEOUT action is dependant on the
CLONE action keeping a list of all mails accepted on each connection).  If
the mail acceptance timestamp is later then the timeout applies to the
just-accepted mail, which will be discarded.  If the \daemon{smtpd}
timestamp is later there was a rejection between the accepted mail and the
timeout: the action assumes that the timeout applies to a rejected delivery
attempt and finishes.  This assumption is not necessarily correct, because
Postfix may have accepted an earlier recipient and rejected a later one, in
which case the timeout applies to the accepted mail, which should be
discarded.  This has not been a problem in practice, though it may be in
future.

This complication is further complicated by the presence of out of order
\daemon{cleanup} lines: see \sectionref{discarding cleanup lines} for
details.

\subsubsection{Discarding cleanup lines}

\label{discarding cleanup lines}

The author has only observed this complication occurring after a timeout,
though there may be other circumstances that trigger it.  Sometimes the
\daemon{cleanup} line is logged after the timeout line; parsing this line
causes the creation of a new state table entry for the queueid in the log
line.  This is incorrect because the line actually belongs to the mail
that has just been discarded; the next log line for that queueid will be
seen when the queueid is reused for a different mail, causing a queueid
clash and the appropriate warning.

In the case where the \daemon{cleanup} line is still pending during the
TIMEOUT action the algorithm updates a global cache of queueids, adding the
queueid and the timestamp from the timeout line.  When the next
\daemon{cleanup} line is parsed for that queueid the cache will be checked
(during the MAIL\_QUEUED action),
and the line will be deemed part of the discarded mail and discarded if it
meets the following requirements:

\begin{itemize}

    \item The queueid must not have been reused yet, i.e.\ there is not an
        entry in the state tables for the queueid.

    \item The timestamp of the \daemon{cleanup} line must be within ten
        minutes of the mail acceptance timestamp.  Timeouts happen after
        five minutes, but some data may have been transferred slowly
        (perhaps because either the client or server is suffering from
        network congestion or rate limiting), and empirical evidence shows
        that ten minutes is not unreasonable; hopefully it is a good
        compromise between false positives and false negatives.

\end{itemize}

The next \daemon{cleanup} line must meet the criteria above for it to be
discarded because not every connection where a timeout occurs will have a
\daemon{cleanup} line logged for it; if the algorithm blindly discarded the
next \daemon{cleanup} line after a TIMEOUT it would in some cases be
mistaken.  Whether or not the next \daemon{cleanup} line is discarded the
queueid will be removed from the cache of timeout queueids when the next
\daemon{pickup} line containing that queueid is parsed.

\subsubsection{Pickup logging after cleanup}

\label{pickup logging after cleanup}

Occasionally the \daemon{pickup} line logged when mail is submitted locally
via sendmail appears later in the log file than the \daemon{cleanup} line
for that mail.  This seems to occur during periods of particularly heavy
load, so is most likely due to process scheduling vagaries.  Normally if
the queueid given in the PICKUP line exists in the state tables a warning
is generated by the \daemon{pickup} action, but if the following conditions
are met it is assumed that the lines were out of order:

\begin{itemize}

    \item The only program which has logged anything thus far for the mail
        is \daemon{cleanup}.

    \item There is less than a five second difference between the
        timestamps of the \daemon{cleanup} and \daemon{pickup} lines.

\end{itemize}

As always with heuristics there may be circumstances in which these
heuristics match incorrectly,  but none have been identified so far.

This complication is dealt with during the PICKUP action.

\subsubsection{Smtpd stops logging}

\label{smtpd stops logging}

Occasionally a \daemon{smtpd} will just stop logging, without an
immediately obvious reason.  After poring over log files for some time
there are several reasons for this infrequent occurrence:

\begin{enumerate}

    \item Postfix is stopped or its configuration is reloaded.  When this
        happens all \daemon{smtpd} processes exit, and all entries in the
        connections state table must be cleaned up, entered in the database
        if there is sufficient data, and deleted.  The queueid state table
        is untouched.

    \item Sometimes a \daemon{smtpd} is killed by a signal (sent by Postfix
        for some reason, by the administrator, or by the OS), so its active
        connection must be cleaned up, entered in the database if there is
        sufficient data, and deleted from the connections state table.

    \item Occasionally a \daemon{smtpd} will exit uncleanly, so the active
        connection must be cleaned up, entered in the database if there is
        sufficient data, and deleted from the connections state table.

\end{enumerate}

The above descriptions appear to cover all situations identified thus far
where a \daemon{smtpd} suddenly stops logging.  In addition to removing an
active connection the last accepted mail may need to be discarded, as
detailed in \sectionref{timeouts-during-data-phase}.

These occurrences are handled by the three actions POSTFIX\_RELOAD,
SMTPD\_DIED and SMTPD\_WATCHDOG\@.

\subsubsection{Out of order log lines}

\label{out of order log lines}

Occasionally a log file will have out of order log lines which cannot be
dealt with by the techniques described in \sectionref{tracking re-injected
mail}, \sectionref{discarding cleanup lines} or \sectionref{pickup logging
after cleanup}.  In the \numberOFlogFILES{} log files used for testing this
occurs only five times in 60,721,709 log lines, but for completeness of the
algorithm it should be dealt with.  The five occurrences in the test log
files have the same characteristics: the \daemon{local} log line showing
delivery to a local mailbox occurs after the \daemon{qmgr} log line showing
removal of the mail from the queue because delivery is completed.  This
causes problems: the mail is not complete, so entry into the database
fails; a new mail is created when the \daemon{local} line is parsed and
remains in the state tables; four warnings are issued per pair of out of
order log lines.

The solution to this problem is to examine the list of programs that have
logged lines for each mail, comparing the list against a table of
known-good combinations of programs (this check is performed during the
COMMIT action).  If the mail's combination is found in the valid list the
mail can be entered in the database; if the combination is not found entry
must be postponed and the mail flagged for later entry.  The
SAVE\_BY\_QUEUEID action checks for that flag and retries entry if it is
found; if the additional log lines have caused the mail to reach a valid
combination entry will proceed, otherwise it must be postponed once more.

The list of valid combinations is explained below.  Every mail will
additionally have log entries from \daemon{cleanup} and \daemon{qmgr}; any
mail may also have log entries from \daemon{bounce}, \daemon{postsuper}, or
both.

\begin{description}

    \item [\daemon{local}:] Local delivery of a bounce notification, or
        local delivery of a forwarded or tracked mail.

    \item [\daemon{local}, \daemon{pickup}:] Mail submitted locally on the
        server, delivered locally on the server.

    \item [\daemon{local}, \daemon{pickup}, \daemon{smtp}:] Locally
        submitted mail, \newline{} both local and remote delivery.

    \item [\daemon{local}, \daemon{smtp}, \daemon{smtpd}:] Mail accepted
        from a remote client, both local and remote delivery.

    \item [\daemon{local}, \daemon{smtpd}:] Mail accepted from a remote
        client, local delivery only.

    \item [\daemon{pickup}, \daemon{smtp}:] Locally submitted mail, remote
        delivery only.

    \item [\daemon{smtp}:] Remote delivery of forwarded or tracked mail, or
        a bounce notification.

    \item [\daemon{smtp}, \daemon{smtpd}:] Mail accepted from a remote
        client, then remotely delivered (typically relaying mail for
        clients on the local network).

    \item [\daemon{smtpd}, \daemon{postsuper}:] Mail accepted from a remote
        client, then deleted by the administrator before any delivery
        attempt was made (the unwanted mail is typically due to a mail loop
        or joe~job).  Notice that \daemon{postsuper} is required, not
        optional, for this combination.

\end{description}

This check applies to accepted mails only, not to rejected mails.

\subsubsection{Yet more aborted delivery attempts}

\label{yet-more-aborted-delivery-attempts}

The aborted delivery attempts described in
\sectionref{aborted-delivery-attempts} occur frequently, but the aborted
delivery attempts described in this section only occur four times in the
\numberOFlogFILES{} log files used for testing.  The symptoms are the same
as in \sectionref{aborted-delivery-attempts}, except that there
\textit{is\/} a \daemon{cleanup} log line; there does not appear to be
anything in the log file to explain why there are no further log messages.
The only way to detect these mails is to periodically scan all mails in the
state tables, deleting any mails displaying the following characteristics:

\begin{itemize}

    \item The timestamp of the last log line for the mail must be 12 hours
        or more earlier than the last log line parsed.

    \item There must be exactly two \daemon{smtpd} and one \daemon{cleanup}
        log entries for the mail, with no additional log entries.

\end{itemize}

12 hours is a somewhat arbitrary time period, but it is far longer than
Postfix would delay delivery of a mail in the queue.\footnote{This may be a
problem if Postfix is not running for an extended period of time.}  The
state tables are scanned for mails matching the characteristics above each
time the end of a log file is reached, and matching mails are deleted.

\subsubsection{Mail deleted before delivery is attempted}

\label{Mail deleted before delivery is attempted}

Postfix logs the recipient address when delivery of a mail is attempted, so
if delivery has yet to be attempted the parser cannot determine the
recipient address or addresses.  This is a problem when mail is arriving
faster than Postfix can attempt delivery, and the administrator deletes
some of the mail (because it is the result of a mail loop, mail bomb, or
joe~job) before Postfix has had a chance to try to deliver it.  In this
case the recipient address will not have been logged, so a dummy recipient
address needs to be added, as every mail is required to have at least one
recipient.  This complication has been observed in few log files, but
typically when it does occur there will be many instances of it, closely
grouped.  The DELETE action is responsible for handling this complication.

\subsubsection{Further out of order log lines}

\label{Further out of order log lines}

This is yet another complication that only occurs during periods of
extremely high load, when process scheduling vagaries and even hard disk
access times cause strange behaviour.  In this complication bounce
notification mails are created, delivered and deleted from the queue,
\textit{before\/} the log line from \daemon{bounce} is logged.  To deal
with this the COMMIT action maintains a cache of recently committed bounce
notification mails, which the BOUNCE action subsequently checks if the
bounce mail is not already in the state tables. If a mail exists in the
cache under the correct queueid, and its start time is less than ten
seconds before the timestamp of the bounce log line, it is assumed that the
bounce notification mail has already been processed and the BOUNCE action
does not create one.  If the queueid exists it is removed from the cache,
as it has either just been used or it is too old to use in future.  Whether
the BOUNCE action creates a mail or finds one existing in the state tables,
it flags the mail as having been seen by the BOUNCE action; if this flag is
present the COMMIT action will not add the mail to the cache of recent
bounce notification mails.\footnote{This is not required to correctly deal
with the complication, but is an optimisation to reduce memory usage of the
parser; on the occasions the author has observed this action occurring
there have been a huge number of bounce notification mails generated --- if
every bounce notification mail was cached it would dramatically increase
the memory requirements of the parser.}  The cache of bounce notification
mails will be pruned whenever the parser's state is saved, though if the
size of the cache ever becomes a problem it could be pruned periodically to
keep the size in check.

\subsubsection{Mails deleted during delivery}

\label{Mails deleted during delivery}

The administrator can delete mails using \daemon{postsuper}; occasionally
mails in the process of being delivered will be deleted.  This results in
the log lines from the delivery agent (\daemon{local}, \daemon{virtual} or
\daemon{smtp}) appearing in the log file \textit{after\/} the mail has been
removed from the state tables and saved in the database.  The DELETE action
adds deleted mails to a cache, which is checked by the SAVE\_BY\_QUEUEID
action, and the current log line discarded if the following conditions are
met:

\begin{enumerate}

    \item The queueid is not found in the state tables. 

    \item The queueid is found in the cache of deleted mails.

    \item The timestamp of the log line is within 5 minutes of the end time
        of the mail.

\end{enumerate}

Sadly this solution involves discarding some data, but the complication
only arises eight times (tightly grouped in one log file, which is not in
the group used for testing); if this complication occurred more frequently
it might be desirable to find the mail in the database and add the log
line to it.

\subsection{Limitations and possible improvements}

\label{limitations-improvements}

Every piece of software suffers from some limitations and there is almost
always room for improvement.

\subsubsection{Limitations}

\label{logging helo}

\begin{enumerate}

    \item Each new Postfix release requires new rules to be written to cope
        with the new log lines.  Similarly using a new \DNSBL{}, new policy
        server or new administrator defined rejection messages require new
        rules.

    \item It appears that the hostname used in the HELO command is not
        logged if the mail is accepted.\footnote{Tested with Postfix
        2.2.10, 2.3.11 and 2.4.7; this may possibly have changed in Postfix
        2.5. XXX TEST THIS AGAIN.}  Rectifying this is relatively simple:
        create a restriction that is guaranteed to warn for every accepted
        mail, as follows:

        \begin{enumerate}

            \item Create \texttt{/etc/postfix/log\_helo.pcre}
                containing:\newline{} \tab{}\texttt{/\^/~~~~WARN~Logging~HELO}

            \item Modify \texttt{smtpd\_data\_restrictions} in
                \texttt{/etc/postfix/main.cf} to contain\newline{}
                \tab{}\texttt{check\_helo\_access~/etc/postfix/log\_helo.pcre}

        \end{enumerate}

        Although \texttt{smtpd\_helo\_restrictions} seems like the natural
        place to log the HELO hostname, there will not be a queueid
        associated with the mail for the first recipient, so the log line
        cannot be associated with the correct mail.  There is guaranteed
        to be a queueid when the DATA command has been reached, and thus it
        will be logged by any restrictions taking effect in
        \texttt{smtpd\_data\_restrictions}.  There is no difficulty in
        specifying a HELO-based restriction in
        \texttt{smtpd\_data\_restrictions}, Postfix will perform the check
        correctly.

        Logging the HELO hostname in this fashion also prevents the
        complication described in \sectionref{Mail deleted before delivery
        is attempted} from occurring, but only in the case where there is a
        single recipient; in that case the recipient address will be logged
        also, but when there are multiple recipients no addresses are
        logged.  It is also possible to warn for every recipient,
        preventing the complication in \sectionref{Mail deleted before
        delivery is attempted} entirely.

    \item The algorithm does not distinguish between mails where one or
        more mails are rejected and a subsequent mail is accepted; it will
        appear in the database as one mail with lots of rejections followed
        by acceptance (this has already been mentioned in
        \sectionref{connection reuse}).  It does not appear to be possible
        to make this distinction given the data Postfix logs, though it
        might be possible to write a policy server to provide additional
        logging.

    \item The TIMEOUT action uses potentially incorrect heuristics to
        decide whether the timeout applies to an accepted mail or not,
        potentially leaving a mail in the state tables
        (\sectionref{timeouts-during-data-phase}).

    \item The program will not detect parsing the same log file twice,
        resulting in the database containing duplicate entries.

    \item The parser does not distinguish between log files produced by
        different sources when parsing; all results will be saved to the
        same database.  This may be viewed as an advantage, as log files
        from different sources can be combined in the same database, or it
        may be viewed as a limitation as there is no facility to
        distinguish between log files from different sources in the same
        database.  If the results of parsing log files from different
        sources must remain separate, the parser can easily be instructed
        to use a different database to store the results in.

    \item The solution to complication \sectionref{Mails deleted during
        delivery} involves discarding data.

\end{enumerate}

\subsubsection{Possible improvements}

\begin{itemize}

    \item Investigate and write the policy server referred to in limitation
        3 above.

    \item Improve the solution to complication \sectionref{Mails deleted
        during delivery} so that data is not discarded.

    \item Improve the heuristics used in
        \sectionref{timeouts-during-data-phase}, or develop another
        solution, to avoid incorrectly leaving a mail in the state tables.

\end{itemize}


\subsection{Summary}

XXX TO BE WRITTEN\@.
