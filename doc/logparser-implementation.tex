\chapter{Postfix Parser Implementation}

\label{Postfix Parser Implementation}

XXX WRITE AN INTRODUCTION\@.  MOST OF THIS IS CRAP, BUT I MIGHT BE ABLE TO
RESCUE SOME OF IT\@.

The parser deals with all the complications of parsing, the eccentricities
and oddities of Postfix log files, presenting the resulting data in a
normalised, simple to use representation.  The parser's task is to follow
the journey each mail takes through Postfix, combining the data captured by
rules into a coherent whole, saving it in a useful and consistent form, and
performing housekeeping duties.

Please refer to \sectionref{parser design} for a discussion of why the
rules, actions, and framework have been separated in the parser's design.
In this section algorithm can be taken to mean the combination of framework
and actions.

The intermingling of log entries from different mails immediately rules out
the possibility of handling each mail in isolation; the parser must be able
to handle multiple mails in parallel, each potentially at a different stage
in its journey, without any interference between mails --- except in the
minority of cases where intra-mail interference is required.  The best way
to implement this is to maintain state information for every unfinished
mail and manipulate the appropriate mail correctly for each log line
encountered.

There is a similarity between this design and the event-driven programming
paradigm commonly used in GUI programs, where one part of the program
responds to events (mouse clicks in a GUI program, log lines being matched
in the parser) and invokes the correct action.


\section{Assumptions}

\parsername{} makes a small number of (hopefully safe and reasonable)
assumptions:

\begin{itemize}

    \item The log files are whole and complete: nothing has been removed,
        either deliberately or accidentally (e.g.\ log file rotation gone
        awry, file system filling up, logging system unable to cope with
        the volume of log messages).  On a well run system it is extremely
        unlikely that any of these problems will arise, though it is of
        course possible, particularly when suffering from a deluge of spam
        or a mail loop.

    \item Postfix logs enough information to make it possible to accurately
        reconstruct the actions it has taken.  There are several heuristics
        used when parsing; see \sectionref{identifying bounce
        notifications}, \sectionref{aborted delivery attempts}, and
        \sectionref{pickup logging after cleanup} for details.

    \item The Postfix queue has not been tampered with, causing unexplained
        appearance or disappearance of mail.  This may happen if the
        administrator deletes mail from the queue without using
        \daemon{postsuper}, or if there is filesystem corruption.

\end{itemize}

In some ways this task is similar to reverse engineering or replicating a
black box program based solely on its inputs and outputs.  Thus far
analysis of the log files has been sufficient to reconstruct Postfix's
behaviour, but for more difficult programs the techniques described in
\cite{black-box-error-reporting} may be useful.  There are some advantages
to treating Postfix as a black box during parser development:

\begin{itemize}

    \item Reading and understanding the source code would require a
        significant investment of time: Postfix 2.5.5 has 17MB of source
        code.

    \item The parser is developed using real world log files rather than
        the idealised log files someone would naturally envisage reading
        the source code.  The source code cannot accurately communicate the
        variety of orderings in which log lines are written to the log
        file, as process scheduling independently interferes with logging
        and other processing.

    \item The parser acts as a second source of information about Postfix's
        operation, using information gathered from empirical evidence.  A
        separate project could compare the empirical knowledge inherent in
        the parsing algorithm with Postfix's documentation and source code
        to see how closely the two agree.

\end{itemize}



\section{Parser Flow Chart}

\label{flow chart}

A picture is said to be worth a thousand words: the flow chart in
\figureref{flow chart image} shows the most common paths a connection or
mail can take through \parsername{}; the parsing complications described in
\sectionref{complications} are excluded for the sake of clarity.  The flow
chart is intended to be a graphical overview of how a mail progresses
through both Postfix and \parsername{}, providing an overall context into
which the detailed descriptions in the remainder of this chapter will fit,
in particular the actions (\sectionref{actions in implementation}) and
complications (\sectionref{complications}).

\showgraph{build/logparser-flow-chart-part-1}{Parser flow chart}{flow chart
image}

Everything starts off with a mail entering the system, whether by local
submission via \daemon{postdrop}, by \acronym{SMTP}, by re-injection due to
forwarding, or internally generated by Postfix.  Local submission is the
simplest case: a queueid is assigned immediately and the sender address is
logged \flowchart{PICKUP}{2}.  Re-injection due to forwarding sadly lacks
explicit log lines of its own; it is explained fully in
\sectionref{Re-injected mails}.  Internally generated mails lack any
explicit origin in Postfix 2.2.x and must be detected using heuristics as
described in \sectionref{identifying bounce notifications}; later versions
of Postfix do provide log lines for internally generated mails.  Bounce
notifications are the primary example of internally generated mails, though
there are other types, e.g.\ Postfix may generate mails to the
administrator when it encounters configuration errors, but such mails are
presumably rare.

\acronym{SMTP} is more complicated than the others:

\begin{enumerate}

    \item First there is a connection from the remote client
        \flowchart{CONNECT}{1}.

    \item This is followed by rejection of sender address, recipient
        addresses, client \acronym{IP} address or hostname, HELO hostname,
        etc.\ \flowchart{DELIVERY\_REJECTED}{4}; acceptance of one or more
        mails \flowchart{CLONE}{5}; or some interleaving of both.

    \item The client disconnects \flowchart{DISCONNECT}{6}.  If Postfix has
        rejected any \acronym{SMTP} commands the data will be saved to the
        database; if not there will not be any data to save (any mails
        accepted will already have been cloned so their data is in another
        data structure).

    \item If one or more mails were accepted there will be more log lines
        for those mails.

\end{enumerate}

The obvious counterpart to mail entering the system is mail leaving the
system, whether by deletion, bouncing, local delivery, or remote delivery.
All four are handled in exactly the same way:

\begin{enumerate}

    \item Postfix will log the sender and recipient addresses separately
        \flowchart{SAVE\_DATA}{9}.

    \item Sometimes mail is re-injected and the child mail needs to be
        tracked by the parent mail \flowchart{TRACK}{10}; the handling of
        re-injected mails is described in \sectionref{tracking re-injected
        mail}.

    \item Eventually the mail will be delivered, bounced, or deleted by the
        administrator \flowchart{COMMIT}{12}.  This is the last log line
        for this particular mail (though it may be indirectly referred to
        if it was re-injected).  If it is neither parent nor child of
        re-injection the data is cleaned up and entered in the database,
        then deleted from the state tables.

\end{enumerate}

It should be emphasised that the mail delivery process above happens
whether the mail is delivered to a mailbox, piped to a command, delivered
to a remote server, bounced (due to a mail loop, delivery failure, or five
day timeout), or deleted by the administrator, \textit{unless\/} the mail
is either parent or child of re-injection, as explained in
\sectionref{tracking re-injected mail}.

\section{Database}

\label{database}

The database is an integral part of the parser presented here: it stores
the rules and the data gleaned by using those rules to parse Postfix log
files.  Understanding the database schema is important in understanding the
actions of the parser, and essential to developing further applications
which utilise the data gathered; \sectionref{database as API} describes
how the database schema functions as an \acronym{API}.

The database schema can be conceptually divided in two: the rules used to
recognise log lines, and the data saved from the parsing of log files.
Each rule has a regex to recognise log lines and capture data from them,
and specifies the action to be invoked when a log line is recognised; they
also have several fields that aid the user in understanding the meaning of
the log lines recognised by each rule.  The rules are described in detail
in \sectionref{rules in implementation}, but the rules table is documented
in \sectionref{rules table} with the rest of the database schema.

The data saved from parsing the log files is also divided into two tables:
connections and results.  The connections table contains an entry for every
mail accepted and every connection where there was a rejection; the
individual fields will be described in \sectionref{connections table}.
There will be at least one entry in the results table for each entry in the
connections table; the result table's fields will be covered in detail in
\sectionref{results table}.  A diagram of the database schema is provided
in \figureref{Diagram of the Database Schema picture}, alongside the
in-depth descriptions of each table.

An important but easily overlooked benefit of storing the rules in the
database is the link between rules and results: if more information is
required when examining a result, the rule that produced the result is
available for inspection because each result references the rule that
created it.  There is no ambiguity about which rule resulted in a
particular result, eliminating one potential source of confusion.

A clear, comprehensible schema is essential when using the data extracted
from log lines; it is more important when using the data than when storing
it, because storing the data is a write-once operation, whereas utilising
the data requires frequent searching, sorting, and manipulation of the data
to produce customised reports or statistics.

\subsection{Using A Database To Provide An Application Programming Interface}

\label{database as API}

The database populated by \parsername{} provides a simple interface to
Postfix log files.  Although the interface is a database schema rather than
a set of functions in a library, it has the same benefits as any other
\acronym{API}: it provides a stable interface, allowing code on either side
of the interface to be changed without adverse effects, as long as the
interface is adhered to.  Programs that use the database can range from the
simple examples in \sectionref{motivation} to far more complex data mining
tools.

Using a database simplifies writing programs that need to interact with the
data in several ways:

\begin{enumerate}

    \item Most programming languages have facilities for database access,
        allowing a developer to write programs that use the gathered data
        in their preferred programming language, rather than being
        restricted to the language the parser is written in.

    \item Databases provide complex querying and sorting functionality for
        the user without requiring large amounts of programming.  All
        databases provide a program, of varying complexity and
        sophistication, that can be used for ad hoc queries with minimal
        investment of time.

    \item Databases are easily extensible, e.g.:

        \begin{itemize}

            \item New columns can be added to the tables used by the
                program, using DEFAULT clauses or TRIGGERS to populate
                them.

            \item A VIEW gives a custom arrangement of data with minimal
                effort.

            \item Some databases support granting access on a fine-grained
                basis, e.g.\ allowing the finance department to produce
                invoices, the helpdesk to run limited queries as part of
                dealing with support calls, and the administrators to have
                full access to the data.

            \item Triggers can be written to perform actions when certain
                events occur.  In pseudo-\acronym{SQL}\@:

\begin{verbatim}
CREATE TRIGGER ON INSERT INTO results
    WHERE sender = 'boss@example.com'
        AND action = 'DELIVERY_REJECTED'
    SEND PANIC EMAIL TO 'postmaster@example.com';
\end{verbatim}

            \item Other tables can be added to the database, to cache
                historical, summary, or computed data.

        \end{itemize}


    \item \acronym{SQL} is reasonably standard and many people will already
        be familiar with it; for those unfamiliar with it there are lots of
        readily available resources from which to learn, e.g.\
        \urlLastChecked{http://philip.greenspun.com/sql/}{2009/02/23}.
        Although every vendor implements a different dialect of
        \acronym{SQL}, the basics are the same everywhere, analogous to the
        overall similarities and minor differences amongst Irish English,
        British English, American English, and Australian English.
        Depending on the database in use there may be tools available that
        reduce or remove the requirement to know \acronym{SQL}; several are
        available for \gls{SQLite}, the default database used by
        \parsername{}.

\end{enumerate}

Storing the results in a database will also increase the efficiency of
using those results, because the log files need only be parsed once rather
than each time the data is used; indeed the results may be used by someone
with no access to the original log files.

\subsection{Rules Table}

\label{rules table}

\newlength{\belowcaptionskipORIG}
\setlength{\belowcaptionskipORIG}{\belowcaptionskip}
\setlength{\belowcaptionskip}{10pt}
\showgraph{database-schema}{Diagram of the Database Schema}{Diagram
of the Database Schema picture}
\setlength{\belowcaptionskip}{\belowcaptionskipORIG}

Rules are discussed in detail in \sectionref{rules in implementation}, but
the structure of the rules table is documented here along with the other
tables in the database.  Rules are created by the user, not the parser, and
will not be modified by the parser (except for the hits and hits\_total
fields).  Rules classify the individual log lines, capturing data to be
saved in the connections and results tables, and specifying the action to
take for that log line.

The following attributes are defined for each rule:

\begin{boldeqlist}

    \item [id] A unique identifier for each rule that other tables can use
        when referring to a specific rule.

    \item [name] A short name for the rule.

    \item [description] Something must have occurred to cause Postfix to
        log each line (e.g.\ a remote client connecting causes a connection
        line to be logged).  This field describes the event causing the log
        lines this rule matches.

    \item [restriction\_name] The restriction that caused the mail to be
        rejected.  Only applicable to rules that recognise rejection log
        lines, i.e.\ rules that have an action of
        \texttt{DELIVERY\_REJECTED}, other rules should have an empty
        string.

    \item [program] The Postfix component (e.g.\ \daemon{smtpd}) whose log
        lines the rule applies to; see \sectionref{rule conditions in
        implementation} for full details of how this attribute is used.

    \item [regex] The regex to recognise log lines with, documented in
        \sectionref{regex components}.

    \item [connection\_data] Sometimes rules need to save data that is not
        present in the log line: e.g.\ setting \texttt{client\_ip} when a
        mail is being delivered to another server.  The format is:
        \newline{} \tab{} \texttt{ client\_hostname = localhost,}
        \newline{} \tab{} \tab{} \texttt{client\_ip = 127.0.0.1} \newline{}
        i.e.\ semi-colon or comma separated assignment statements, with the
        variable name on the left and the value on the right hand side.
        Any field in the connections table can be set in this way.  Commas
        and semi-colons cannot be escaped and thus cannot be included in
        data.  This feature is intended for use with small amounts of data,
        so dealing with escape sequences was deemed unnecessary.

    \item [result\_data] The result table equivalent of
        \texttt{connection\_data}.

    \item [action] The action that will be invoked when this rule
        recognises a log line; a full list of actions and the parameters
        they are invoked with can be found in \sectionref{actions in detail
        in implementation}.

    \item [hits] This counter is maintained for every rule and incremented
        each time the rule successfully recognises a log line.  At the
        start of each run the parser sorts the rules by how frequently each
        rule recognised log lines, and at the end of the run it updates
        every rule's hits field in the database.  Assuming that the
        distribution of log lines is reasonably consistent across log
        files, ordering rules by their recognition frequency will reduce
        the parser's execution time.  Rule ordering for efficiency is
        discussed in \sectionref{rule ordering for efficiency}.

    \item [hits\_total] The total number of hits for this rule over all
        runs of the parser.

    \item [priority] This is the user-configurable companion to hits: when
        the list of rules is sorted, priority overrides hits.  This allows
        more specific rules to take precedence over more general rules
        (described in \sectionref{rules in architecture}).

\end{boldeqlist}

\subsection{Connections Table}

\label{connections table}

Every accepted mail and every connection where there was a rejection will
have a single entry in the connections table containing the following
fields:

\begin{boldeqlist}

    \item [id] This field uniquely identifies the row.

    \item [server\_ip] The \acronym{IP} address (IPv4 or IPv6) of the
        server: the local address when receiving mail, the remote address
        when sending mail.

    \item [server\_hostname] The hostname of the server, it will be
        \texttt{unknown} if the \acronym{IP} address could not be resolved
        to a hostname via DNS\@.

    \item [client\_ip] The client \acronym{IP} address (IPv4 or IPv6): the
        remote address when receiving mail, the local address when sending
        mail.

    \item [client\_hostname] The hostname of the client, it will be
        \texttt{unknown} if the \acronym{IP} address could not be resolved
        to a hostname via DNS\@.

    \item [helo] The hostname used in the HELO command.  The HELO hostname
        occasionally changes during a connection, presumably because spam
        or virus senders think it is a good idea.  By default Postfix only
        logs the HELO hostname when it rejects an \acronym{SMTP} command,
        but it is quite simple to rectify this, as described in
        \sectionref{logging helo}.

    \item [queueid] The queueid of the mail if the connection represents an
        accepted mail, or \texttt{NOQUEUE} otherwise.

    \item [start] The timestamp of the first log line, in seconds since the
        epoch.\glsadd{Epoch}

    \item [end] The timestamp of the last log line, in seconds since the
        epoch.

\end{boldeqlist}

\subsection{Results Table}

\label{results table}

Every recognised log line will have a row in the results table, associated
with a single connection; a single connection will have at least one result
associated with it, but will usually have several more, and may have
hundreds.

\begin{boldeqlist}

    \item [connection\_id] The id of the row in the connections table this
        result is associated with.

    \item [rule\_id] The id of the rule in the rules table that recognised
        the log line and created this result.

    \item [id] A unique identifier for this result.

    \item [warning] Postfix can be configured to log a warning instead of
        enforcing a restriction that would reject an \acronym{SMTP} command
        --- a mechanism that is quite useful for testing new restrictions.
        This field will be set to 1 if the log line parsed was a warning rather
        than a real rejection, or to 0 for a real rejection.  This field
        should be ignored if the result is not a rejection, i.e.\ the
        action field of the associated rule is not
        \texttt{DELIVERY\_REJECTED}.

    \item [smtp\_code] The \acronym{SMTP} code associated with the log
        line.  In general an \acronym{SMTP} code is only present in a log
        line for a rejection or final delivery; results whose log line did
        not contain an \acronym{SMTP} code will duplicate the
        \acronym{SMTP} code of other results in that connection.  Some
        final delivery log lines do not contain an \acronym{SMTP} code: in
        those cases the \acronym{SMTP} code is specified by the rule's
        \texttt{result\_data} field, based on the success or failure
        represented by the log line.

    \item [enhanced\_status\_code] The enhanced status code~\cite{RFC3463}
        is similar to the \acronym{SMTP} code, but is intended to be parsed
        by mail clients so that error messages can be clearly conveyed to
        the user.  Enhanced status code support was added to Postfix in
        version 2.3; logs generated by previous versions will not contain
        any enhanced status codes.

    \item [sender] The sender's email address.  Because multiple mails may
        be delivered during one connection, there may be different sender
        addresses in the results for one connection; however there should
        not be different sender addresses in the results for one mail.
        Mails sent over the same connection can be distinguished by their
        queueids.

    \item [recipient] The recipient address; there may be multiple
        recipient addresses per mail or connection.

    \item [size] The size of the mail; it will only be present for
        delivered mails.

    \item [delay] How long the mail was delayed while it was being
        delivered.  This will only be present for delivered mails.

    \item [delays] More detailed information about how long the mail was
        delayed while it was being delivered, again only present for
        delivered mails.

    \item [message\_id] The message-id of the accepted mail, or
        \texttt{NULL} if no mail was accepted.

    \item [data] A field available for anything not covered by other
        fields, e.g.\ the rejection message from a \acronym{DNSBL}\@.

    \item [timestamp] The time at which the log line was logged, in seconds
        since the epoch.\glsadd{Epoch}

\end{boldeqlist}



\section{Framework}

The role of the framework in this architecture was described in detail in
\sectionref{framework in architecture}; this section is concerned with the
implementation of the framework within \parsername{}.  The framework
manages the parsing process, taking care of the drudgery and boring tasks,
provides services to the actions, and implements some efficiency measures.

Each time the parser is run the framework performs some initialisation
tasks, setting up data structures that will later be used by actions,
including several state tables.  Data about the connections and mails being
processed is held in \texttt{connections} and \texttt{queueids}
respectively.  There are also state tables used when working around
complications in parsing: \texttt{timeout\_queueids} is used when dealing
with connections that time out during the DATA phase (\sectionref{timeouts
during data phase}); \texttt{bounce\_queueids} is part of the solution to
bounce notification mails being delivered before their creation is logged
(\sectionref{Bounce notification mails delivered before their creation is
logged}); and \texttt{postsuper\_deleted\_queueids} caches information
about mails recently deleted by the administrator so that subsequent log
lines processed by the SAVE\_DATA action can be discarded (\sectionref{Mail
deleted before delivery is attempted}).

When the framework loads the ruleset, it verifies each of the rules,
checking:

\begin{itemize}

    \squeezeitems{}

    \item That the specified action is registered with the framework.

    \item Ensuring the regex is valid and compiling it for efficiency
        (\sectionref{Caching compiled regexes}).

    \item For overlap between the data captured by the regex and additional
        data specified in either \texttt{connection\_data} or
        \texttt{result\_data}.

    \item That \texttt{connection\_data}, \texttt{result\_data}, and the
        regex captures specify valid fields to save data to.

\end{itemize}

If there are state tables from a previous \parsername{} run to be loaded,
they will be loaded now; obviously the framework supports saving its state
tables too.  The need to track re-injected mail (\sectionref{Re-injected
mails}) complicates the process of saving state, as the relationships
between mails must be maintained; loading state is complicated by dealing
with aborted delivery attempts (\sectionref{aborted delivery attempts}),
because a different set of relationships between connections and mails must
be re-created when the state tables are restored.

The framework will now move on to parsing each log file.  For each log line
it will use those rules whose \texttt{program} field matches the program in
the log line (see \sectionref{rule conditions in implementation}), falling
back to generic rules if necessary, and warning if the log line is
unrecognised.  The rules are sorted for increased efficiency; see
\sectionref{rule ordering for efficiency} for details.  The repetitive
nature of log files gives them high compression ratios; the framework
supports reading compressed files, simplifying the process of parsing them
because they do not have to be uncompressed to a temporary file before
parsing begins.  The framework also displays a progress bar to show how far
parsing has progressed through the log file, and to give the user an
indication of how long the remainder of the parsing process is likely to
take.  The progress bar is not as reliable when parsing compressed files
because a compressed block may uncompress to a variable number of log
lines; there is also variation between the recognition and processing time
of individual log lines, but overall the progress bar is a useful addition.

The framework collects data used when evaluating \parsernames{} efficiency
for the results chapter (\sectionref{Results}); in addition some of the
techniques used to improve parsing speed can be turned off or altered to
show the effect they have.  Three sets of data are collected:

\begin{enumerate}

    \item How long it takes to parse each log file.

    \item The number of rules tried when recognising log lines; the
        framework may need to try multiple rules for a given log line
        before it finds one that recognises it.

    \item How many log lines did the framework attempt to recognise, how
        many were skipped because there were no rules whose condition
        matched that log line's program, how many were successfully
        recognised, and how many were not recognised.

\end{enumerate}

There are five ways the framework can adapt its behaviour to demonstrate
how effective the optimisations it uses are, and how efficient
\parsername{} is:

\begin{enumerate}

    \item The rule ordering used can be changed from optimal (the default,
        most efficient) to shuffled (intended to represent an unordered
        ruleset) or reverse (reverse of optimal, least efficient).  The
        results of this are shown in \sectionref{rule ordering for
        efficiency}.

    \item The framework can record which rule recognised each log line, and
        then on a subsequent run use that information so that it uses the
        correct rule for each log line.  This gives the best possible
        running time, as only one rule is used for each log line.  A
        variation of this is to use the correct rule last, to get the worst
        possible running time.  \sectionref{perfect rule ordering}
        discusses this in detail.

    \item Each regex is compiled and the result cached when the rules are
        loaded; this can be disabled and the regex compiled every time it
        is used when recognising log lines, to demonstrate the effect this
        optimisation has on parser efficiency, as shown in
        \sectionref{Caching compiled regexes}.

    \item Normally once a log line has been recognised, the framework
        invokes the action specified by the rule.  Invoking the action can
        be skipped if desired, so the timing information shows how long is
        spent recognising log lines; this time can be subtracted from the
        time taken by a normal run to show how long is spent processing log
        lines.  This data is analysed in \sectionref{recognising vs
        processing}.

    \item The framework can skip inserting data into the database, because
        that dominates the execution time of the parser, and the evaluation
        in \sectionref{Results} is measuring the speed of \parsername{},
        not the speed of the database or the disks it resides on.

\end{enumerate}

The framework also provides several debugging options, to aid in writing or
correcting rules, or figuring out why the parser is not behaving as
expected.  In increasing order of severity they are:

\begin{enumerate}

    \item The regex from the recognising rule and the log line it
        recognised can be printed, so that the user can verify that the
        correct rule recognised each log line.  A possible variant of this
        would be to mark specific rules so only their regexes and the log
        lines they recognise would be printed.

    \item Each result can be extended with extra debugging information, so
        that when a mail's data is dumped for inspection more information
        is available.  The information currently added is: the log line's
        timestamp in human readable form; the entire log line; the name of
        the log file and the number of the log line within it.

    \item Every connection and result added to the database can be dumped
        in a human readable form.  This will result in a huge amount of
        debugging information, so it is only useful for small log file
        snippets, because otherwise the amount of information is
        overwhelming.

\end{enumerate}

\section{Actions}

\label{actions in implementation}

Actions are the component of this architecture responsible for processing
all of the inputs recognised by rules; in \parsername{} they reconstruct
the journey each mail takes through Postfix, dealing with all the
complications and difficulties that arise.  \parsername{} has
\numberOFactions{} actions and \numberOFrules{} rules, with an uneven
distribution of rules to actions as shown in \graphref{Distribution of
rules per action}.  Unsurprisingly, the action with the most associated
rules is \texttt{DELIVERY\_REJECTED}, the action that processes Postfix
rejecting a mail delivery attempt.  The next most common action is, perhaps
surprisingly, \texttt{UNINTERESTING}: this action does nothing when
invoked, allowing uninteresting log lines to be parsed without any effects
(it does not imply that the input is ungrammatical or unparsed).  Generally
rules specifying the \texttt{UNINTERESTING} action recognise log lines that
are not associated with a specific mail, e.g.\ notices about configuration
files changing; these log lines are recognised and processed so that the
framework can warn about unrecognised log lines, informing the user that
they need to add to or extend their ruleset.  Most of the remaining actions
have only one or two associated rules: some are specialised actions
required to address a deficiency in the log files, or a complication that
arises during parsing; other actions will only ever have one log line
variant, e.g.\ all log lines showing that a remote client has connected are
recognised by a single rule and handled by the \texttt{CONNECT} action.
\parsernames{} actions are described in detail in \sectionref{actions in
detail in implementation}, and \sectionref{adding new actions in
implementation} covers the process of adding a new action.

\showgraph{build/graph-action-distribution}{Distribution of rules per
action}{Distribution of rules per action}

\subsection{Actions in the \parsernamelong{}}

\label{actions in detail in implementation}

This section explains the actions found in \parsername{}; it may help to
revisit the flow chart in \sectionref{flow chart} to see how a mail passes
from one action to another as its log lines are recognised.  The words
\textit{mail\/} and \textit{connection\/} are used in the actions
descriptions below because they are less unwieldy and more specific than
\textit{state table entry\/}; a connection becomes a mail during the
\texttt{CLONE} action, which processes Postfix accepting a delivery
attempt, and the data structure moves from one state table to another
within the parser.

The complications and difficulties that arose when parsing log files
generated on a production mail server are documented in
\sectionref{complications}; some action descriptions refer to specific
difficulties they address.  The complications are documented in a separate
section to avoid overwhelming the action descriptions.

If there is enough information in the log line to identify the correct
connection or mail, each action will save all the data captured by the
recognising rule's regex.  Each action is passed the same arguments:

\begin{boldeqlist}

    \squeezeitems{}

    \item [rule] The recognising rule.

    \item [data] The data captured from the log line by the rule's regex.

    \item [line] The log line, separated into fields:

        \begin{boldeqlist}

            \squeezeitems{}

            \item [timestamp] The time the line was logged at.

            \item [program] The name of the program that logged the line.

            \item [pid] The \acronym{pid} of the program that logged the
                line.

            \item [host] The hostname of the server the line was logged on.

            \item [text] The remainder of the log line, i.e.\ the message
                logged by the program and recognised by the rule.

        \end{boldeqlist}

\end{boldeqlist}

\begin{description}

    \item [BOUNCE\_CREATED] Postfix 2.3 and subsequent versions log the
        creation of bounce messages.  This action creates a new mail if
        necessary; if the mail already exists the unknown origin flag will
        be removed.  The action also marks the mail as a bounce
        notification.  To deal with complication \sectionref{Bounce
        notification mails delivered before their creation is logged} this
        action checks a cache of recent bounce mails, to avoid creating
        bogus bounce mails when log lines are out of order.

    \item [CLEANUP\_PROCESSING] \daemon{cleanup} processes every mail that
        passes through Postfix; details of what it does are available in
        \sectionref{Postfix Daemons}.  This action saves all data captured
        by the rule's regex if the log line has not come after a timeout
        (see \sectionref{discarding cleanup log lines}); it also creates
        the mail if necessary, setting its unknown origin flag (see
        \sectionref{pickup logging after cleanup}).

    \item [CLONE] Multiple mails may be accepted on a single connection, so
        each time a mail is accepted the connection's state table entry
        must be cloned and saved in the state tables under its queueid; if
        the original data structure was used then second and subsequent
        mails would overwrite one another's data.

    \item [COMMIT] Enter the data from the mail into the database.  Entry
        will be postponed if the mail is a child waiting to be tracked
        (\sectionref{Re-injected mails}).  Once entered in the database,
        the mail will be usually be deleted from the state tables, but
        deletion will be postponed if the mail is the parent of re-injected
        mail (\sectionref{Re-injected mails}).

    \item [CONNECT] Handle a remote client connecting: create a new
        connection, indexed by \daemon{smtpd} \acronym{pid}.  If a
        connection already exists it is treated as a symptom of a bug in
        \parsername{}, and the action will issue a warning containing the
        full contents of the existing connection plus the log line that has
        just been parsed.

    \item [DELETE] Deals with mail deleted using Postfix's administrative
        command \daemon{postsuper}.  This action adds a dummy recipient
        address if required, then invokes the COMMIT action to save the
        mail to the database.  The complication this action deals with is
        described fully in \sectionref{Mail deleted before delivery is
        attempted}.

    \item [DELIVERY\_REJECTED] Deals with Postfix rejecting an
        \acronym{SMTP} command from the remote client.  This is the action
        most frequently specified by rules, because so many different
        restrictions are used to reject delivery attempts.  This action is
        quite simple: if the log line contains a queueid, save the data
        captured by the rule's regex to the mail identified by that
        queueid; otherwise save it to the connection identified by the
        \acronym{pid} in the log line.  No further processing of the log
        line is required.

    \item [DISCONNECT] Invoked when the remote client disconnects, it
        enters the connection in the database, performs any required
        cleanup, and deletes the connection from the state tables.  This
        action deals with aborted delivery attempts (\sectionref{aborted
        delivery attempts}).

    \item [EXPIRY] If Postfix has not managed to deliver a mail after
        trying for five days it will give up and return the mail to the
        sender.  When this happens the mail will not have a combination of
        Postfix programs which passes the valid combinations check as
        described in \sectionref{out of order log lines}; this action, with
        cooperation from the COMMIT and SAVE\_DATA actions, deals with that
        complication.

    \item [MAIL\_BOUNCED] This action behaves exactly like the
        \texttt{SAVE\_DATA} action; it saves all data captured by the
        recognising rule's regex, and does nothing more.  It is a separate
        action to help distinguish delivery attempts that bounce from other
        delivery attempts.

    \item [MAIL\_DISCARDED] Sometimes mail is discarded, e.g.\ mail
        submitted locally that is larger than the limit configured by the
        administrator.  This action is used for those mails; it invokes the
        \texttt{COMMIT} action, but is a separate action to simplify
        further analysis.

    \item [MAIL\_QUEUED] This action represents Postfix picking a mail from
        the queue to deliver.  This action needs to deal with out of order
        log lines when mail is re-injected for forwarding; see
        \sectionref{Re-injected mails} for details.

    \item [MAIL\_SENT] This action behaves exactly like the
        \texttt{SAVE\_DATA} action; it saves all data captured by the
        recognising rule's regex, and does nothing more.  It is a separate
        action to help distinguish successful delivery attempts from other
        delivery attempts.

    \item [MAIL\_TOO\_LARGE] When a client tries to send a mail larger than
        the local server accepts it will be discarded and the client
        informed of the problem.  The mail may have been accepted and
        partially transferred, in which case there's a data structure to
        dispose of, otherwise there will not be.  See \sectionref{timeouts
        during data phase} for the gory details; although that describes
        timeouts the processing is the same for mails that are too large.

    \item [PICKUP] The PICKUP action corresponds to the \daemon{pickup}
        service dealing with a locally submitted mail.  A new mail will be
        created, although out of order log entries may have caused the
        state table entry to already exist, as documented in
        \sectionref{pickup logging after cleanup}.

    \item [POSTFIX\_RELOAD] When Postfix stops or reloads its configuration
        it kills all \daemon{smtpd} processes, requiring any active
        connections to be cleaned up, entered in the database, and deleted
        from the state tables.  Postfix probably kills all the other
        components too, but \parsername{} is only affected by \daemon{smtpd}
        processes exiting.

    \item [SAVE\_DATA] Every action that can locate the correct entry in
        the state tables saves any data captured by the recognising rule's
        regex to it.  The \texttt{SAVE\_DATA} saves data in this way but
        does not do anything else; it is invoked for log lines that contain
        useful data but do not require any further processing.

    \item [SMTPD\_DIED] Sometimes a \daemon{smtpd} dies or exits
        unsuccessfully; the active connection for that \daemon{smtpd} must
        be cleaned up, entered in the database if it has sufficient data,
        and deleted from the state tables.  In some cases the connection
        will not have enough data to satisfy the database schema, so it
        cannot be entered into the database; unfortunately this means that
        some data is lost.

    \item [SMTPD\_KILLED] Sometimes a \daemon{smtpd} is killed by a signal;
        this is processed in the same way as a \daemon{smtpd} dying, see
        the \texttt{SMTPD\_DIED} action for details.

    \item [SMTPD\_WATCHDOG] \daemon{smtpd} processes have a watchdog timer
        to deal with unusual situations; after five hours the timer will
        expire and the \daemon{smtpd} will exit.  This occurs very
        infrequently, as there are many other timeouts that should occur in
        the intervening hours, e.g.\ timeouts for DNS requests or timeouts
        reading data from the client.  The active connection for that
        \daemon{smtpd} must be cleaned up, entered in the database, and
        deleted from the state tables.

    \item [TIMEOUT] The connection with the remote client timed out so the
        mail currently being transferred must be discarded.  The mail may
        have been accepted, in which case there's a data structure to
        dispose of, or it may not in which case there is not.  See
        \sectionref{timeouts during data phase} for full details.

    \item [TRACK] Track a mail when it is re-injected for forwarding to
        another mail server; this happens when a local address is aliased
        to a remote address (see \sectionref{tracking re-injected mail} for
        a full explanation).  \texttt{TRACK} will be called when dealing
        with the parent mail, and will create the child mail if necessary.
        \texttt{TRACK} checks if the child has already been tracked, either
        by this parent or by another parent, and issues appropriate
        warnings in either case.

    \item [UNINTERESTING] This action just returns successfully: it is used
        when a log line needs to be parsed for completeness, but does not
        either provide any useful data to be saved or require anything to
        be done.

\end{description}

\subsection{Adding New Actions}

\label{adding new actions in implementation}

Adding new actions is not as easy as adding new rules, though care has been
taken in the architecture and implementation to make adding new actions as
painless as possible.  The implementer writes a subroutine that accepts the
standard arguments given to actions, and registers it with the framework.
The new action must be registered before the rules are loaded, because it
is an error for a rule to specify an unregistered action; this helps catch
mistakes made when adding new rules.  No other work is required from the
implementer to integrate the action into the parser; all of their attention
and effort can be focused on correctly implementing their new action.  The
action may need to extend the list of valid combinations described in
\sectionref{out of order log lines} if the new action creates a different
set of acceptable programs, but this would only be necessary if the new
action processes log lines from Postfix components \parsername{} currently
does not have rules for.

\section{Rules}

\label{rules in implementation}

The rules are responsible for classifying each log line and specifying the
correct action to be invoked.  For any parser implemented using this
architecture, the rules are the most visible component and most likely to
be modified by users.  Rules need to be as simple as possible so that users
can easily modify or add to them, but the implementation must balance that
simplicity with the need to provide enough flexibility and power to
successfully parse the inputs.

The role of the rules in the architecture is covered in detail in
\sectionref{rules in architecture}; this section is concerned with the
practical aspects of how rules are implemented and used in \parsername{}.
The structure of the rules table has already been documented in
\sectionref{rules table}; that abstract description will not be repeated
here, but should be fleshed out by the concrete example rule in
\sectionref{example rule in implementation}.  The process of creating new
rules from unparsed log lines is dealt with in \sectionref{creating new
rules in implementation}; the implementation of the utility supplied with
\parsername{} to create regexes from unparsed log lines is also described
in that section.  The rule conditions used in \parsername{} are explained
next, and the final portion of this section provides some
suggestions for how to detect overlapping rules.

\subsection{Example Rule}

\label{example rule in implementation}

The example rule in \tableref{Example rule in implementation table}
recognises the log line resulting from Postfix rejecting a delivery attempt
because the domain in the sender address does not have an A, AAAA,
or MX DNS entry, i.e.\ mail could not be delivered to the sender's address
(for full details see~\cite{reject-unknown-sender-domain}).

Example log line recognised by this rule:

% RFC 3330 says that 192.0.2.0/24 is reserved for example use.

\begin{verbatim}
NOQUEUE: reject: RCPT from smtp.example.com[192.0.2.1]:
  550 5.1.8 <foo@example.com>:
  Sender address rejected: Domain not found;
  from=<foo@example.com> to=<info@example.net>
  proto=SMTP helo=<smtp.example.com>
\end{verbatim}

% do not reformat this!
\begin{table}[ht]

    \caption{Example rule}
    \empty{}\label{Example rule in implementation table}
    \begin{tabular}{ll}

    \textbf{Field}      & \textbf{Value}                                    \\
    name                & Unknown sender domain                             \\
    description         & We do not accept mail from unknown domains        \\
    restriction\_name   & reject\_unknown\_sender\_domain                   \\
    program             & \daemon{smtpd}                                    \\
    regex               & \verb!^__RESTRICTION_START__ <(__SENDER__)>: !    \\
                        & \verb!Sender address rejected: Domain not found;! \\
                        & \verb!from=<\k<sender>> to=<(__RECIPIENT__)> !    \\
                        & \verb!proto=E?SMTP helo=<(__HELO__)>$!            \\
    result\_data        &                                                   \\
    connection\_data    &                                                   \\
    action              & DELIVERY\_REJECTED                                \\
    hits                & 0                                                 \\
    hits\_total         & 0                                                 \\
    priority            & 0                                                 \\
    %cluster\_group      & 400                                               \\

    \end{tabular}

\end{table}

The fields in \tableref{Example rule in implementation table} are used as
follows:

\begin{description}

    \item [name, description, and restriction\_name:] are not used by the
        parser, they serve to document the rule for the user's benefit.

    \item [program and regex:] The program is used to restrict the log
        lines this rule will be used to recognise; see \sectionref{rule
        conditions in implementation} for details.  The regex does the
        actual recognition of the log lines, and data captured by the regex
        (sender, recipient, etc.) will be automatically saved to the
        results and connections tables.

    \item [action:] The action to be invoked when the rule recognises a log
        line.  See \sectionref{actions in detail in implementation} for
        details of the actions implemented by \parsername{}, and
        \sectionref{actions in architecture} for the role of actions in the
        parser architecture.

    \item [result\_data and connection\_data:] are used to provide data not
        present in the log line, and are unused in this rule.

    \item [hits, hits\_total, and priority:] hits and priority are used
        when ordering the rules for more efficient parsing (see
        \sectionref{rule ordering for efficiency}).  At the end of each
        parsing run hits is set to the number of log lines recognised by
        the rule.  Hits\_total is the sum of hits over every parsing run,
        but is otherwise unused by the parser.

\end{description}

\subsection{Creating New Rules}

\label{creating new rules in implementation}

The log files produced by Postfix differ from installation to installation,
because administrators have the freedom to choose the subset of available
restrictions which suits their needs, including using different
\acronym{DNSBL} services, policy servers, or custom rejection messages.  To
facilitate parsing new log lines, the architecture separates rules from
actions: adding new actions can be difficult, but adding new rules to
recognise new log lines is trivial, and occurs much more frequently.

To add a new rule a new row must be added to the rules table in the
database, containing all the required attributes: action, name,
description, program, and regex.  The name and description fields should be
set based on the meaning of the log line, to help others understand which
log lines this rule will recognise; the program will be obvious from the
unrecognised log lines.  The action depends on the what the log line
represents, e.g.\ a delivery rejection, a mail being delivered, some useful
information, or something else; examine the list of actions in
\sectionref{actions in detail in implementation} to determine the correct
one.  The regex will be constructed based on the log line, but see below
for a tool to ease the process.

There are other attributes that may be required in a rule:
connection\_data, result\_data, priority, or restriction\_name.  In general
it will only become clear that connection\_data or result\_data are
required when \parsername{} warns about an entry in the connections or
results tables that is missing some required fields, because values for the
missing fields are not present in any of the log lines.  E.g.\ the rule
recognising the \daemon{pickup} component processing a mail sets the
client\_hostname to \texttt{localhost} and the client\_ip to
\texttt{127.0.0.1}, because the mail originates on the local server.  If
the new rule deliberately uses the architecture's overlapping rules feature
it will be obvious that the priority field needs to be set, on this rule and
possibly others; the priority field may be needed on unintentionally
overlapping rules too, but that is more difficult to determine.  Finally
the restriction\_name field should be set when the rule's action is
\texttt{DELIVERY\_REJECTED}; hopefully the restriction in question will be
clear from the content of the log line.

A program is provided with \parsername{} to ease the process of creating
new rules from unrecognised log lines, based on the algorithm developed by
Risto Vaarandi for his \acronym{SLCT}~\cite{slct-paper}.  The differences
between the two algorithms will be outlined as part of the general
explanation below.  The core of the \acronym{SLCT} algorithm is quite
simple: log lines are generally created by substituting variable words into
a fixed pattern, and analysis of the frequency with which each word occurs
can be used to determine whether the word is variable or part of the fixed
pattern.  This classification can be used to group similar log lines and
generate a regex to match each group of log lines.  There are 4 steps in
the algorithm:

\begin{description}

    \item [Pre-process the file.]  The new algorithm leverages the
        knowledge gained when writing rules and developing \parsername{};
        it performs a large number of substitutions on the input log lines,
        replacing commonly occurring variable terms (e.g.\ email addresses,
        \acronym{IP} addresses, the standard start of rejection messages)
        with keywords that \parsername{} will expand when it loads the new
        rule.  The purpose of this step is to utilise existing knowledge to
        create more accurate regexes; it replaces a large number of
        variable words with fixed words, improving the subsequent
        classification of words as fixed or variable.  The altered log
        lines are written to a temporary file, which subsequent stages will
        use instead of the original input file.

        In the original algorithm the purpose of the pre-processing stage
        was to reduce the memory consumption of the program.  In the first
        pass it generates a hash~\cite{hash-functions}\glsadd{hash} (from a
        small range of values) for each word of each log line, incrementing
        a counter for each hash.  The counters will be used in the next
        pass to filter out words: if the word's hash does not have a high
        frequency, the word itself cannot have a high frequency, so there
        is no need to maintain a counter for it, reducing the number of
        counters required and thus the program's memory consumption.

    \item [Calculate word frequencies.]  The position of words within a log
        line is important: a word occurring in two log lines does not
        indicate similarity unless it occupies the same position in both
        log lines.  If a variable term within a line contains
        spaces, it will appear to the algorithm as more than one word.
        This will alter the position of subsequent words, so a word
        occurring in different positions in two log lines \textit{may\/}
        indicate similarity, but the algorithm does not attempt to deal
        with this possibility.  The algorithm maintains a counter for each
        \texttt{(word, word's position within the log line)} tuple,
        incrementing it each time that word occurs in that position.

        The original algorithm hashes each word and checks that result's
        counter from the previous pass; if that counter has a high
        frequency a separate counter will be maintained for this word in
        this position.  This reduces the number of counters maintained by
        the algorithm in this step, reducing the memory requirements of the
        algorithm at a cost of increased CPU usage.

        As time goes on the amount of memory typically available increases
        and the need to reduce memory requirements decreases, so the
        modified algorithm omits the hashing step and maintains counters
        for all tokens.  In addition most of the infrequently occurring
        words will have been substituted with keywords during the first
        step, vastly reducing the number of tuples to maintain counters
        for.

    \item [Classify words based on their frequency.]  The frequency of
        every tuple \texttt{(word, word's position within the log line)} is
        checked: if its frequency is greater than the threshold supplied by
        the user (1\% of all log lines is generally a good starting point),
        it is classified as a fixed word, otherwise it is classified as a
        variable term.  If a variable term appears sufficiently often it
        will be misclassified as a fixed term, but that should be noticed
        by the user when reviewing the new regexes.  Variable terms are
        replaced by \texttt{.+} to match one or more of any character.

    \item [Build regexes.]  The words are reassembled to produce a regex
        matching the log line, and a counter is maintained for each regex.
        Contiguous sequences of \texttt{.+} in the newly reassembled
        regexes are collapsed to a single \texttt{.+}; any resulting
        duplicate regexes are combined, and their counters added together.
        If a regex's counter is lower than the threshold supplied by the
        user the regex is discarded; this second threshold is independent
        of the threshold used to differentiate between fixed and variable
        words, but once again 1\% of log lines is a good starting point.
        The new regexes are displayed for the user to add to the database,
        either as new rules or merged into the regexes of existing rules;
        the counter for each regex is also displayed, giving the user an
        indication of how many of the log lines that regex should match.
        Discarding regexes will result in some of the log lines not being
        matched; when the rules have been augmented with the new regexes,
        the original log files should be parsed again, and any remaining
        unparsed log lines used as input to this utility.

\end{description}

XXX MERGE logs2regexes WITH check-regexes.

A second utility is also provided that reads a list of new regexes and
the input given to the first utility.  It tries to match each input log
line against each regex, counting the number of log lines that match
each regex, warning the user if an input log line is matched by more
than one regex, and additionally warning if an input log line is not
matched by any regex.  It displays a summary of how many input log lines
each regex matched, comparing it to the expected number of matches; this
provides the user with an easy method of checking if the regexes
produced by the first utility are correctly matching the input log lines
they are based upon.

These utilities are not intended to create perfect regexes, but they
greatly reduce the effort required to parse new or different log lines.

\subsection{Rule Conditions}

\label{rule conditions in implementation}

Rule conditions as part of the architecture have already been documented in
\sectionref{rule conditions in architecture}, and they can be very complex
and difficult to evaluate.  In contrast, \parsername{} uses very simple
rule conditions: each rule has a \texttt{program} attribute that specifies
the Postfix component whose log lines it recognises, and each log line
contains the name of the Postfix component that produced it; the framework
will only use rules where the two are equal when trying to recognise log
lines.  This avoids needlessly trying rules that will not match the log
line, or worse, might match unintentionally.  In addition the framework
supports generic rules, whose program attribute is ``*''; these will be
used if none of the program-specific rules recognise the log line.  If none
of the rules are successful the framework will warn the user.

\subsection{Regex Components}

\label{regex components}

Each rule's regex will have keywords expanded when the rules are loaded,
for several reasons:

\begin{itemize}

    \item It simplifies both reading and writing of regexes and makes each
        regex largely self-documenting.  E.g.\ the meaning of
        \_\_CLIENT\_HOSTNAME\_\_ is immediately clear, whereas
        \verb!(?:unknown|(?:[-.\w]+))! needs to be deciphered.

    \item It avoids needless repetition of complex regex components, and
        allows the components to be corrected or improved in one
        location.  E.g.\ \_\_SENDER\_\_ is used in 68 rules; if a mistake
        is discovered it can be corrected in one place only.

    \item It enables automatic extraction and saving of captured data.  The
        regex snippets use Perl 5.10's named capture buffers~\cite{perlre},
        so the mapping between captures and fields does not need to be
        explicitly specified by the rule.

\end{itemize}

For efficiency the keywords are expanded and every rule's regex is compiled
before attempting to parse the log file, otherwise every regex would be
recompiled each time it was used, resulting in a large, data dependent
slowdown, as shown in \sectionref{Caching compiled regexes}.  Most of the
keywords are named after the fields in the connections or results tables
they populate: \_\_CLIENT\_HOSTNAME\_\_, \_\_CLIENT\_IP\_\_, \_\_DELAY\_\_,
\_\_DELAYS\_\_, \_\_ENHANCED\_STATUS\_CODE\_\_, \_\_HELO\_\_,
\_\_MESSAGE\_ID\_\_, \_\_QUEUEID\_\_, \_\_RECIPIENT\_\_, \_\_SENDER\_\_,
\_\_SERVER\_HOSTNAME\_\_, \_\_SERVER\_IP\_\_, \_\_SIZE\_\_, and
\_\_SMTP\_CODE\_\_.

Some of the keywords need more explanation:

\begin{eqlist}

    \squeezeitems{}

    \item [\_\_CHILD\_\_]  The queueid of a child mail; see
        \sectionref{Re-injected mails}.

    \item [\_\_COMMAND\_\_]  All \acronym{SMTP} commands, except
        \texttt{DATA}, which typically has a more specific rule.
        Priorities could have been used instead of excluding \texttt{DATA}.

    \item [\_\_CONN\_USE\_\_]  How many times the connection was reused;
        Postfix tries to reuse connections whenever possible to reduce the
        load on both the sending and receiving servers.

    \item [\_\_DATA\_\_]  This snippet is special: it matches a zero-length
        string, so it needs to be followed by a pattern that matches
        appropriately, but it captures whatever is matched so that it can
        be saved.  E.g.\ \verb!/(__DATA__connection (?:refused|reset!
        \newline{} \tab{}\verb!by peer))/! matches either ``connection
        refused'' or ``connection reset by peer'' and causes it to be saved
        to the data field of that log line's result.

    \item [\_\_PID\_\_]  The \acronym{pid} of a \daemon{smtpd} process that
        dies or is killed; see the SMTPD\_DIED action in
        \sectionref{actions in detail in implementation}.

    \item [\_\_RESTRICTION\_START\_\_]  Matches the start of a delivery
        rejected log line: XXX SHOULD I INCLUDE THIS\@? \newline{}
        \verb!/(__QUEUEID__): reject(?:_warning)?: ! \newline{}
        \verb!(?:RCPT|DATA) from ! \newline{}
        \verb!(?>(__CLIENT_HOSTNAME__)\\[)! \newline{}
        \verb!(?>(__CLIENT_IP__)\\]): (__SMTP_CODE__)! \newline{}
        \verb!(?: __ENHANCED_STATUS_CODE__)?/!

    \item [\_\_SHORT\_CMD\_\_]  Postfix sometimes logs \acronym{SMTP}
        commands in a short, single word form.

\end{eqlist}

\subsection{Overlapping Rules}

\label{overlapping rules in implementation}

\parsername{} does not try to detect overlapping rules; that responsibility
is left to the author of the rules.  The advantages and difficulties of
overlapping rules have already been addressed in \sectionref{overlapping
rules in architecture} and will not be repeated here.  \parsername{}
provides a mechanism for ordering overlapping rules: the priority field in
each rule (defaulting to zero); negative priorities may be useful for
catchall rules.

Detecting overlapping rules is difficult, but the following approaches may
be helpful:

\begin{itemize}

    \item Sort by program and regex, then visually inspect the list, e.g.\
        with \acronym{SQL} similar to:      \newline{}
        \verb!SELECT program, regex!        \newline{}
        \verb!    FROM rules!               \newline{}
        \verb!    ORDER BY program, regex;! \newline{}
        The rules are sorted first by program, then by regex, because rules
        with overlapping regexes will not overlap if their programs are
        different.  Note that this does not properly deal with rules whose
        program is \texttt{*}; those rules will be used on all log lines
        that have not been recognised by program-specific rules.

    \item Compare the results of parsing using different rule orderings
        (described in \sectionref{rule ordering for efficiency}).  Parse
        several log files using optimal ordering, then dump a textual
        representation of the rules, connections, and results tables.
        Repeat with shuffled and reversed ordering, starting with a fresh
        database.  If there are no overlapping rules the tables from each
        run will be identical; differences indicate overlapping rules.  The
        rules that overlap can be determined by examining the differences
        in the tables: each result contains a reference to the rule which
        created it, which will change if that rule overlaps with another.
        Unfortunately this method cannot prove the absence of overlapping
        rules; it can detect overlapping rules, but only if there are log
        lines in the log files that are recognised by more than one rule.

\end{itemize}

\section{Complications Encountered}

\label{complications}

XXX IMPROVE THE INTRO\@: ``needs more of a lead-in''.

The complications described in this section are listed in the order in
which they were encountered during development of the parser.  Each of
these complications caused the parser to operate incorrectly, generating
either warning messages or leaving mails in the state table.  The frequency
of occurrence is much higher at the start of the list, with the first
complication occurring several orders of magnitude more frequently than the
last.  When deciding which problem to address next, the most common was
always chosen, as resolving the most common problem would yield the biggest
improvement in the parser, prune the greatest number of mails from the
state tables and error messages, and make the remaining problems more
apparent.  The first three complications were encountered early in the
parser's implementation and guided its design and development.

\subsection{Queueid Vs Pid}

The mail lacks a queueid until it has been accepted, so log lines must
first be correlated by the \daemon{smtpd} \acronym{pid}, then transition to
being correlated by the queueid.  This is relatively minor, but does
require:

\begin{itemize}

    \item Two versions of several functions: \texttt{by\_pid} and
        \texttt{by\_queueid}.

    \item Two state tables to hold the data structure for each connection
        and mail.

    \item Most importantly: every section of code must know whether it
        needs to lookup the data structures by \acronym{pid} or queueid.

\end{itemize}

\subsection{Connection Reuse}

\label{connection reuse}

Multiple independent mails may be delivered across one connection: this
requires the algorithm to clone the current data as soon as a mail is
accepted, so that subsequent mails will not trample over each other's data.
This must be done every time a mail is accepted, as it is impossible to
tell in advance which connections will accept multiple mails.  Once the
mail has been accepted its log lines will not be correlated by
\acronym{pid} any more, its queueid will be used instead (except when
timeouts occur during the data phase \sectionref{timeouts during data
phase}).  If the original connection has any useful data (e.g.\ rejections)
it will be saved to the database when the client disconnects.  One unsolved
difficulty is distinguishing between different groups of rejections, e.g.\
when dealing with the following sequence:

\begin{enumerate}

    \item The client attempts to deliver a mail, but it is rejected.

    \item The client issues the RSET command to reset the \acronym{SMTP}
        session.

    \item The client attempts to deliver another mail, likewise rejected.

\end{enumerate}

There should ideally be two separate entries in the database resulting from
the above sequence, but currently there will only be one.



\subsection{Re-injected Mails}

\label{Re-injected mails}

\label{tracking re-injected mail}

The most difficult complication initially encountered is that locally
addressed mails are not always delivered directly to a mailbox: sometimes
they are addressed to and accepted for a local address but need to be
delivered to one or more remote addresses due to aliases.  When this
occurs a child mail will be injected into the Postfix queue, but without
the explicit logging that \daemon{smtpd} or \daemon{postdrop} injected
mails have.  Thus the source of the mail is not immediately discernible
from the log line in which the mail first appears; from a strictly
chronological reading of the log files it usually appears as if the child
mail has appeared from thin air.  Subsequently the parent mail will log the
creation of the child mail, e.g.\ parent mail 3FF7C4317 creates child mail
56F5B43FD\@:

\texttt{3FF7C4317: to=<username@example.com>, relay=local, \newline{}
\tab{} delay=0, status=sent (forwarded as 56F5B43FD)}

Unfortunately, while all log lines from an individual process appear in
chronological order, the order in which log lines from different processes
are interleaved is subject to the vagaries of process scheduling.  In
addition, the first log line belonging to the child mail (the log line
cited above belongs to the parent mail) is logged by \daemon{qmgr}, so the
order also depends on how soon \daemon{qmgr} processes the new mail.

Because of the uncertain order the parser cannot complain when it
encounters a log line from \daemon{qmgr} for a previously unseen mail;
instead it must flag the mail as coming from an unknown origin, and
subsequently clear the flag if and when the origin of the mail becomes
clear.  Obviously the parser could omit checking where mails originate
from, but requiring an explicit source helps to expose bugs in the parser;
such checks helped to identify the complications described in
\sectionref{discarding cleanup log lines} and \sectionref{pickup logging
after cleanup}.

Process scheduling can have a still more confusing effect: quite often the
child mail will be created, delivered, and entirely finished with
\textbf{before} the parent logs the creation log line!  Thus, mails flagged
as coming from an unknown origin cannot be entered into the database when
their final log line is parsed; instead they must be marked as ready for
entry and subsequently entered once the parent mail has been identified.

XXX MERGE THE REMAINDER OF THIS SECTION INTO THE PRIOR MATERIAL\@.

The crux of this complication is that re-injected mails appear in the log
files without explicit logging indicating their source.  There are two
implicit indications:

\begin{enumerate}

    \item The indicator which more commonly introduces re-injection is when
        \daemon{qmgr} selects a mail with a previously unseen queueid for
        delivery, in which case a new data structure will be created for
        that mail.  The mail will be flagged as having unknown origins;
        this flag will subsequently be cleared once the origin has been
        established.  This may also be an indicator that the mail is a
        bounce notification, see \sectionref{identifying bounce
        notifications} for details.

    \item Local delivery re-injects the mail and logs a relayed delivery
        rather than delivering directly to a mailbox or program as it
        usually would.\footnote{Relayed delivery is performed by the
        \acronym{SMTP} client; local delivery means local to the server,
        i.e.\ an address the server is final destination for.}  In this
        case the mail may already have been created (described above) and
        the unknown origin flag will be cleared; if not a new data
        structure will be created.  In both cases the re-injected mail is
        marked as a child of the original mail.  The log line in question
        is:

        \texttt{3FF7C4317: to=<username@example.com>, relay=local,
        \newline{} \tab{} delay=0, status=sent (forwarded as 56F5B43FD)}

        This always occurs for re-injected mail but typically occurs after
        the first indicator.  This log line is required to tie the parent
        and child mails together and so is central to the process of
        tracking re-injected mails.

\end{enumerate}

The algorithm for tracking and saving re-injected mail to the database can
finally be described:

\begin{itemize}

    \item If the mail is of unknown origin it is assumed to be a child mail
        whose parent has not yet been identified.  Mark the mail as ready
        for entry in the database and wait for the parent to deal with it.
        The mail should not have any subsequent log lines; only its parent
        should refer to it.

    \item If the mail is a child mail then it has already been tracked: as
        with all other mail, the data is cleaned up, the child is entered
        in the database, and then deleted from the state tables.  The child
        mail will be removed from the parent mail's list of children; if
        this is the last child and the parent has already been entered in
        the database, the parent will also be deleted from the state
        tables.

    \item The last alternative is that the mail is a parent mail.
        Regardless of the state of its children its data is cleaned up and
        entered in the database.  The parent may have children that are
        waiting to be entered in the database; the data for each of those
        children is cleaned up and entered in the database, then deleted
        from the state tables.  The parent may also have outstanding
        children which are not yet delivered, in which case the parent must
        be retained in the state tables until those children are finished
        with.  As soon as the last child is deleted from the state tables
        the parent will also be deleted from the state tables.

\end{itemize}

A parent mail can have multiple children, which may be delivered before or
after the parent mail.


\subsection{Identifying Bounce Notifications}

\label{identifying bounce notifications}

Postfix 2.2.x (and presumably previous versions) does not generate a log
line when it generates a bounce notification; suddenly there will be log
entries for a mail that lacks an obvious source.  There are similarities to
the problem of identifying re-injected mails discussed in
\sectionref{tracking re-injected mail}, but unlike the solution described
therein bounce notifications do not eventually have a log line that
identifies their source.  Heuristics must be used to identify bounce
notifications, and those heuristics are:

\begin{enumerate}

    \item The sender address is \verb!<>!.\glsadd{<>}

    \item Neither \daemon{smtpd} nor \daemon{pickup} have logged any
        messages associated with the mail, indicating it was generated
        internally by Postfix, not accepted via \acronym{SMTP} or submitted
        locally by \daemon{postdrop}.

    \item The message-id has a specific format: \newline{}
        \tab{} \texttt{YYYYMMDDhhmmss.queueid@server.hostname} \newline{}
        e.g.\ \texttt{20070321125732.D168138A1@smtp.example.com}

    \item The queueid in the message-id must be the same as the queueid of
        the mail: this is what distinguishes bounce notifications generated
        locally from bounce notifications which are being re-injected as a
        result of aliasing.  In the latter case the message-id will be
        unchanged from the original bounce notification, and so even if it
        happens to be in the correct format (e.g.\ if it was generated by
        Postfix on this or another server) it will not match the queueid of
        the mail.

\end{enumerate}

Once a mail has been identified as a bounce notification, the unknown
origin flag is cleared and the mail can be entered in the database.

There is a small chance that a mail will be incorrectly identified as a
bounce notification, as the heuristics used may be too broad.  For this to
occur the following conditions would have to be met:

\begin{enumerate}

    \item The mail must have been generated internally by Postfix.

    \item The sender address must be \verb!<>!.\glsadd{<>}

    \item The message-id must have the correct format and match the queueid
        of the mail.  While a mail sent from elsewhere could easily have
        the correct message-id format, the chance that the queueid in the
        message-id would match the queueid of the mail is extremely small.

\end{enumerate}

If a mail is misclassified as a bounce message it will almost certainly
have been generated internally by Postfix; arguably misclassification in
this case is a benefit rather than a drawback, as other mails generated
internally by Postfix will be handled correctly.  Postfix 2.3 (and
hopefully subsequent versions) log the creation of a bounce message.

This check is performed during the COMMIT action.

\subsection{Aborted Delivery Attempts}

\label{aborted delivery attempts}

Some mail clients behave unexpectedly during the \acronym{SMTP} dialogue:
the client aborts the first delivery attempt after the first recipient is
accepted, then makes a second delivery attempt for the same recipient which
it continues with until delivery is complete.  Microsoft Outlook is one
client that behaves in this fashion; other clients may act in a similar
way.  An example dialogue exhibiting this behaviour is presented below
(lines starting with a three digit number are sent by the server, the other
lines are sent by the client):

\begin{verbatim}
220 smtp.example.com ESMTP
EHLO client.example.com
250-smtp.example.com
250-PIPELINING
250-SIZE 15240000
250-ENHANCEDSTATUSCODES
250-8BITMIME
250 DSN
MAIL FROM: <sender@example.com>
250 2.1.0 Ok
RCPT TO: <recipient@example.net>
250 2.1.5 Ok
RSET
250 2.0.0 Ok
RSET
250 2.0.0 Ok
MAIL FROM: <sender@example.com>
250 2.1.0 Ok
RCPT TO: <recipient@example.net>
250 2.1.5 Ok
DATA
354 End data with <CR><LF>.<CR><LF>
The mail transfer is not shown.
250 2.0.0 Ok: queued as 880223FA69
QUIT
221 2.0.0 Bye
\end{verbatim}

Once again Postfix does not log a message making the client's behaviour
clear, so once again heuristics are required to identify when this
behaviour occurs.  In this case a list of all mails accepted during a
connection is saved in the connection state, and the accepted mails are
examined when the disconnection action is executed.  Each mail is checked
for the following:

\begin{itemize}

    \item Was the second result processed by the \texttt{CLONE} action?
        The first two \daemon{smtpd} log lines will be a connection log
        line and a mail acceptance log line; Postfix logs acceptance as
        soon as the first recipient has been accepted.

    \item Is \daemon{smtpd} the only Postfix component that produced a log
        line for this mail?  Every mail which passes normally through
        Postfix will have a \daemon{cleanup} line, and later a
        \daemon{qmgr} log line; lack of a \daemon{cleanup} line is a sure
        sign the mail did not make it too far.

    \item Does the queueid exist in the state tables?  If not it cannot be
        an aborted delivery attempt.

    \item If there are third and subsequent results, were those log lines
        processed by the \texttt{SAVE\_DATA} action?  If there are any log
        lines after the first two they should be informational only.

\end{itemize}

If all the checks above are successful then the mail is assumed to be an
aborted delivery attempt and is discarded.  There will be no further
entries logged for such mails, so without identifying and discarding them
they accumulate in the state table and will cause clashes if the queueid is
reused.  The mail cannot be entered in the database as the only data
available is the client hostname and \acronym{IP} address, but the database
schema requires many more fields be populated (see \sectionref{connections
table} and \sectionref{results table}).  This heuristic is quite
restrictive, and it appears there is little scope for false positives; if
there are any false positives there will be warnings when the next log line
for that mail is parsed.  False negatives are less likely to be detected:
there may be queueid clashes (and warnings) if mails remain in the state
tables after they should have been removed, otherwise the only way to
detect false negatives is to examine the state tables after each parsing
run.

This check is performed in the DISCONNECT action; it requires support in
the CLONE action where a list of cloned connections is maintained.


\subsection{Further Aborted Delivery Attempts}

Some mail clients disconnect abruptly if a second or subsequent recipient
is rejected; they may also disconnect after other errors, but such
disconnections are either unimportant or are handled elsewhere in the
parser (\sectionref{timeouts during data phase}).  Sadly, Postfix does not
log a message saying the mail has been discarded, as should be expected by
now.  The checks to identify this happening are:

\begin{itemize}

    \item Is the mail missing its \daemon{cleanup} log line?  Every mail
        which passes through Postfix will have a \daemon{cleanup} line;
        lack of a \daemon{cleanup} line is a sure sign the mail did not
        make it too far.

    \item Were there three or more \daemon{smtpd} log lines for the mail?
        There should be a connection log line and a mail accepted log line,
        followed by one or more rejection log lines.

    \item Is the last \daemon{smtpd} log line a rejection line?

\end{itemize}

If all checks are successful then the mail is assumed to have been
discarded when the client disconnected.  There will be no further entries
logged for such mails, so without identifying and entering them in the
database immediately they accumulate in the state table and will cause
clashes if the queueid is reused.

These checks are made during the DISCONNECT action.

\subsection{Timeouts During DATA Phase}

\label{timeouts during data phase}

The DATA phase of the \acronym{SMTP} conversation is where the headers and
body of the mail are transferred.  Sometimes there is a timeout or the
connection is lost\footnote{For brevity's sake \textit{timeout\/} will be
used throughout this section, but everything applies equally to lost
connections.} during the DATA phase; when this occurs Postfix will discard
the mail and the parser needs to discard the data associated with that
mail.  It seems more intuitive to save the mail's data to the database, but
if a timeout occurs there is no data available to save; the timeout is
recorded with the connection data instead, which is saved.

To deal properly with timeouts the parsing algorithm needs to do the
following in the TIMEOUT action:

\begin{enumerate}

    \item Record the timeout and associated data in the connection's
        results.

    \item If no mails have been accepted yet nothing needs to be done; the
        TIMEOUT action ends.

    \item If one or more recipients have been accepted Postfix will have
        allocated a queueid for the incoming mail, and there will be a mail
        in the state tables that needs to be dealt with.

\end{enumerate}

XXX MAKE THIS PARAGRAPH CLEARER\@.  A timeout may thus apply either to an
accepted mail or a rejected mail.  To distinguish between the two cases the
algorithm compares the timestamp of the last accepted mail against the
timestamp of the last line logged by \daemon{smtpd} for that connection
(the TIMEOUT action is dependant on the CLONE action keeping a list of all
mails accepted on each connection).  If the mail acceptance timestamp is
later then the timeout applies to the just-accepted mail, which will be
discarded.  If the \daemon{smtpd} timestamp is later there was a rejection
between the accepted mail and the timeout: the action assumes that the
timeout applies to a rejected delivery attempt and finishes.  This
assumption is not necessarily correct, because Postfix may have accepted an
earlier recipient and rejected a later one, in which case the timeout
applies to the accepted mail, which should be discarded.  This has not been
a problem in practice, though it may be in future.  This complication is
further complicated by the presence of out of order \daemon{cleanup} log
lines: see \sectionref{discarding cleanup log lines} for details.

This complication is dealt with in the TIMEOUT action.

\subsection{Discarding Cleanup Log Lines}

\label{discarding cleanup log lines}

The author has only observed this complication occurring after a timeout,
though there may be other circumstances that trigger it.  Sometimes the
\daemon{cleanup} line for a mail being accepted is logged after the timeout
line; parsing this line causes the MAIL\_QUEUED action to create a new
state table entry for the queueid in the log line.  This is incorrect
because the line actually belongs to the mail that has just been discarded;
the next log line for that queueid will be seen when the queueid is reused
for a different mail, causing a queueid clash and the appropriate warning.

When the \daemon{cleanup} line is still pending during the TIMEOUT action,
the action updates a global list of queueids, adding the queueid and the
timestamp from the log line.  When the next \daemon{cleanup} line is parsed
for that queueid the list will be checked (during the MAIL\_QUEUED action),
and the log line will be deemed part of the mail where the timeout occurred
and discarded if it meets the following requirements:

\begin{itemize}

    \item The queueid must not have been reused yet, i.e.\ it does not have
        an entry in the state tables.

    \item The timestamp of the \daemon{cleanup} log line must be within ten
        minutes of the mail acceptance timestamp.  Timeouts happen after
        five minutes, but some data may have been transferred slowly
        (perhaps because either the client or server is suffering from
        network congestion or rate limiting), and empirical evidence shows
        that ten minutes is not unreasonable; hopefully it is a good
        compromise between false positives (log lines incorrectly
        discarded) and false negatives (new state table entries incorrectly
        created).

\end{itemize}

The next \daemon{cleanup} line must meet the criteria above for it to be
discarded because some, but not all connections where a timeout occurs will
have an associated \daemon{cleanup} line logged; if the algorithm blindly
discarded the next \daemon{cleanup} line after a timeout it would sometimes
be mistaken.  When the next \daemon{pickup} line containing that queueid is
parsed the queueid will be removed from the cache of timeout queueids,
regardless of whether it meets the criteria above.

This complication is handled by the TIMEOUT and MAIL\_QUEUED actions.

\subsection{Pickup Logging After Cleanup}

\label{pickup logging after cleanup}

When mail is submitted locally, \daemon{pickup} accepts the new mail and
generates a log line showing the source.  Occasionally this log line will
occur later in the log file than the \daemon{cleanup} log line, so the
PICKUP action will find that a state table entry exists for that queueid.
Normally if the queueid given in the PICKUP line exists in the state tables
a warning is generated by the \daemon{pickup} action, but if the following
conditions are met it is assumed that the log lines were out of order:

\begin{itemize}

    \item The only program which has logged anything thus far for the mail
        is \daemon{cleanup}.

    \item There is less than a five second difference between the
        timestamps of the \daemon{cleanup} and \daemon{pickup} log lines.

\end{itemize}

As always with heuristics there may be circumstances in which these
heuristics match incorrectly, but none have been identified so far.  This
complication seems to occur during periods of particularly heavy load, so
is most likely caused by process scheduling vagaries.

This complication is dealt with during the PICKUP action.

\subsection{Smtpd Stops Logging}

\label{smtpd stops logging}

Occasionally a \daemon{smtpd} will just stop logging, without an
immediately obvious reason.  After poring over log files for some time
several reasons have been found for this infrequent occurrence:

\begin{enumerate}

    \item Postfix is stopped or its configuration is reloaded.  When this
        happens all \daemon{smtpd} processes exit, so all entries in the
        connections state table must be cleaned up, entered in the database
        if there is sufficient data, and deleted.

    \item Sometimes a \daemon{smtpd} is killed by a signal (sent by Postfix
        for some reason, by the administrator, or by the OS), so its active
        connection must be cleaned up, entered in the database if there is
        sufficient data, and deleted from the connections state table.

    \item Occasionally a \daemon{smtpd} will exit uncleanly, so the active
        connection must be cleaned up, entered in the database if there is
        sufficient data, and deleted from the connections state table.

    \item Every Postfix process uses a watchdog which kills the process if
        it is not reset for a considerable period of time (five hours by
        default).  This safeguard prevents Postfix processes from running
        indefinitely and consuming resources if a failure causes them to
        enter a stuck state.

\end{enumerate}

The circumstances above account for all occasions identified thus far where
a \daemon{smtpd} suddenly stops logging.  In addition to removing an active
connection the last accepted mail may need to be discarded, as detailed in
\sectionref{timeouts during data phase}; otherwise the queueid state table
is untouched.

These occurrences are handled by the three actions POSTFIX\_RELOAD,
SMTPD\_DIED, and SMTPD\_WATCHDOG\@.

\subsection{Out of Order Log Lines}

\label{out of order log lines}

Occasionally a log file will have out of order log lines which cannot be
dealt with by the techniques described in \sectionref{tracking re-injected
mail}, \sectionref{discarding cleanup log lines} or \sectionref{pickup
logging after cleanup}.  In the \numberOFlogFILES{} log files used for
testing this occurs only five times in 60,721,709 log lines, but for parser
correctness it must be dealt with.  The five occurrences have the same
characteristics: the \daemon{local} log line showing delivery to a local
mailbox occurs after the \daemon{qmgr} log line showing removal of the mail
from the queue because delivery is completed.  This causes problems: the
data in the state tables for the mail is not complete, so entering it into
the database fails; a new mail is created when the \daemon{local} line is
parsed and remains in the state tables; four warnings are issued per pair
of out of order log lines.

The COMMIT action examines the list of programs that have produced a log
lines for each mail, comparing the list against a table of known-good
program combinations.  If the mail's combination is found in the table the
mail can be entered in the database; if the combination is not found entry
must be postponed and the mail flagged for later entry.  The SAVE\_DATA
action checks for that flag; if the additional log lines have caused the
mail to reach a valid combination then entry in the database will proceed,
otherwise it must be postponed once more.

The list of valid combinations is explained below.  Every mail will
additionally have log line from \daemon{cleanup} and \daemon{qmgr}; any
mail may also have log line from \daemon{bounce}, \daemon{postsuper}, or
both.

% This will put the text on the line following the item name, if the
% enumitem package is loaded.
%\begin{description}[style=nextline]
\begin{description}

    \item [\daemon{local}:] Local delivery of a bounce notification, or
        local delivery of a re-injected mail.

    \item [\daemon{local}, \daemon{pickup}:] Mail submitted locally on the
        server, delivered locally on the server.

    \item [\daemon{local}, \daemon{pickup}, \daemon{smtp}:] Mail submitted
        locally \newline{} on the server, for both local and remote
        delivery.

    \item [\daemon{local}, \daemon{smtp}, \daemon{smtpd}:] Mail accepted
        from a remote client, for both local and remote delivery.

    \item [\daemon{local}, \daemon{smtpd}:] Mail accepted from a remote
        client, for local delivery only.

    \item [\daemon{pickup}, \daemon{smtp}:] Mail submitted locally on the
        server, for remote delivery only.

    \item [\daemon{smtp}:] Remote delivery of either a re-injected mail or
        a bounce notification.

    \item [\daemon{smtp}, \daemon{smtpd}:] Mail accepted from a remote
        client, then remotely delivered (typically relaying mail for
        clients on the local network to addresses outside the local
        network).

    \item [\daemon{smtpd}, \daemon{postsuper}:] Mail accepted from a remote
        client, then deleted by the administrator before any delivery
        attempt was made (the unwanted mail is typically due to a mail loop
        or joe~job\glsadd{Joe-job}).  Notice that \daemon{postsuper} is
        required, not optional, for this combination.

\end{description}

This check applies to accepted mails only, not to rejected mails.  This
check is performed during the COMMIT action.

\subsection{Yet More Aborted Delivery Attempts}

\label{yet more aborted delivery attempts}

The aborted delivery attempts described in \sectionref{aborted delivery
attempts} occur frequently, but the aborted delivery attempts described in
this section only occur four times in the \numberOFlogFILES{} log files
used for testing.  The symptoms are the same as in \sectionref{aborted
delivery attempts}, except that there \textit{is\/} a \daemon{cleanup} log
line; there is nothing in the log file to explain why there are no further
log lines.  The only way to detect these mails is to periodically scan all
mails in the state tables, deleting any mails displaying the following
characteristics:

\begin{itemize}

    \item The timestamp of the last log line for the mail must be 12 hours
        or more earlier than the last log line parsed from the current log
        file.

    \item There must be exactly two \daemon{smtpd} and one \daemon{cleanup}
        log lines for the mail, with no additional log lines.

\end{itemize}

12 hours is a somewhat arbitrary time period, but it is far longer than
Postfix would delay delivery of a mail in the queue (unless Postfix is not
running for an extended period of time).  The state tables are scanned for
mails matching the characteristics above each time the end of a log file is
reached, and matching mails are deleted.

\subsection{Mail Deleted Before Delivery is Attempted}

\label{Mail deleted before delivery is attempted}

Postfix logs the recipient address when delivery of a mail is attempted, so
if delivery has yet to be attempted the parser cannot determine the
recipient address or addresses.  This is a problem when mail is arriving
faster than Postfix can attempt delivery, and the administrator deletes
some of the mail (because it is the result of a mail loop\glsadd{mail
loop}, mail bomb\glsadd{mail bomb}, or joe~job\glsadd{Joe-job}) before
Postfix has had a chance to try to deliver it.  In this case the recipient
address will not have been logged, so a dummy recipient address needs to be
added, as every mail is required by the database schema
(\sectionref{results table}) to have at least one recipient.  Typically
when this complication occurs there are many instances of it, closely
grouped.

This lack of information cannot easily be overcome: it is trivial to log a
warning for every recipient accepted, but when the warning for the first
recipient is logged Postfix will not yet have allocated a queue file and
queueid for the mail, so the warning will be associated with the connection
rather than the accepted mail.  A queue file and queueid will be allocated
after Postfix accepts the MAIL FROM command if
\texttt{smtpd\_delay\_open\_until\_valid\_rcpt} is set to ``no'', but that
setting will cause disk IO for almost every delivery attempt, instead of
just for delivery attempts where recipients are accepted, and consequently
a drastic reduction in the performance of the mail server.

The DELETE action is responsible for handling this complication.

\subsection{Bounce Notification Mails Are Delivered Before Their Creation
Is Logged}

\label{Bounce notification mails delivered before their creation is logged}

This is yet another complication that only occurs during periods of
extremely high load, when process scheduling and even hard disk access
times cause strange behaviour.  In this complication bounce notification
mails are created, delivered, and deleted from the queue, \textit{before\/}
the log line from \daemon{bounce} that explains their source is logged.  To
deal with this the COMMIT action maintains a cache of recently committed
bounce notification mails, which the BOUNCE action subsequently checks if
the bounce mail is not already in the state tables.  If the queueid exists
in the cache, and its start time is less than ten seconds before the
timestamp of the bounce log line, it is assumed that the bounce
notification mail has already been processed and the BOUNCE action does not
create one.  If the queueid exists it is removed from the cache, because it
has either just been used or it is too old to use in future.  Whether the
BOUNCE action creates a mail or finds an existing mail in the state tables,
it flags the mail as having been seen by the BOUNCE action; if this flag is
present the COMMIT action will not add the mail to the cache of recent
bounce notification mails.  This is not required to correctly deal
with the complication, but is an optimisation to reduce the parser's memory
usage; on the occasions the author has observed this complication occurring
there have been a huge number of bounce notification mails generated --- if
every bounce notification mail was cached it would dramatically increase
the memory requirements of the parser.  The cache of bounce notification
mails will be pruned whenever the parser's state is saved, though if the
size of the cache ever becomes a problem it could be pruned periodically to
keep the size in check.

\subsection{Mails Deleted During Delivery}

\label{Mails deleted during delivery}

The administrator can delete mails using \daemon{postsuper}; occasionally
mails that are in the process of being delivered will be deleted.  This
results in the log lines from the delivery agent (\daemon{local},
\daemon{virtual} or \daemon{smtp}) appearing in the log file
\textit{after\/} the mail has been removed from the state tables and saved
in the database.  The DELETE action adds deleted mails to a cache, which is
checked by the SAVE\_DATA action, and the current log line discarded if the
following conditions are met:

\begin{enumerate}

    \item The queueid is not found in the state tables.

    \item The queueid is found in the cache of deleted mails.

    \item The timestamp of the log line is within 5 minutes of the final
        timestamp of the mail.

\end{enumerate}

Sadly this solution involves discarding some data, but the complication
only arises eight times in quick succession in one log file, which is not
in the \numberOFlogFILES{} log files used for testing; if this complication
occurred more frequently it might be desirable to find the mail in the
database and add the log line to it.

\section{Limitations and Possible Improvements}

\label{limitations and improvements in implementation}

Every piece of software suffers from some limitations, and there is almost
always room for improvement.  Below are the limitations and possible
improvements that have been identified in \parsername{}.

\subsection{Limitations}

\label{logging helo}

\begin{enumerate}

    \item Each new Postfix release requires writing new rules or modifying
        existing rules to cope with the new or changed log lines.
        Similarly using a new \acronym{DNSBL}, a new policy server, or new
        administrator-defined rejection messages requires new rules.

    \item The hostname used in the HELO command is not logged if the
        incoming delivery attempt is successful.  Configuring Postfix to do
        this is relatively simple: create a restriction that is guaranteed
        to warn for every accepted mail, as follows:

        \begin{enumerate}

            \item Create \texttt{/etc/postfix/log\_helo.pcre}
                containing:\newline{}
                \tab{}\texttt{/\^/~~~~WARN~Logging~HELO}

            \item Modify \texttt{smtpd\_data\_restrictions} in
                \texttt{/etc/postfix/main.cf} to contain:\newline{}
                \tab{}\texttt{check\_helo\_access~pcre:/etc/postfix/log\_helo.pcre}

        \end{enumerate}

        Although \texttt{smtpd\_helo\_restrictions} seems like the natural
        place to log the HELO hostname, there will not yet be a queueid
        associated with the mail when \texttt{smtpd\_helo\_restrictions} is
        evaluated for the first recipient, so the log line cannot be
        associated with the correct mail.  There is guaranteed to be a
        queueid when the DATA command has been reached, and thus the
        queueid will be logged by any restrictions taking effect in
        \texttt{smtpd\_data\_restrictions}, and the log line can be
        associated with the correct mail.  There is no difficulty in
        specifying a HELO-based restriction in
        \texttt{smtpd\_data\_restrictions}, Postfix will perform the check
        correctly.

        Logging the HELO hostname in this fashion also partially prevents
        the complication described in \sectionref{Mail deleted before
        delivery is attempted} from occurring, but only when there is a
        single recipient; in that case the recipient address will be logged
        also, but when there are multiple recipients no addresses are
        logged.

    \item \parsername{} does not create separate mails where one or more
        delivery attempts are rejected and subsequently a mail is accepted;
        it will appear in the database as one mail with lots of rejections
        followed by acceptance (this has already been mentioned in
        \sectionref{connection reuse}).  It does not appear to be possible
        to make this distinction given the data Postfix logs, though it
        might be possible to write a policy server to provide additional
        logging.

    \item \parsername{} will not detect that it is parsing the same log file
        twice, resulting in the database containing duplicate entries.

    \item \parsername{} does not distinguish between log files produced by
        different sources when parsing; all results will be saved to the
        same database.  This may be viewed as an advantage, because log
        files from different sources can be combined in the same database,
        or it may be viewed as a limitation because there is no facility to
        distinguish between log files from different sources in the same
        database.  If the results of parsing log files from different
        sources must remain separate, the parser can easily be instructed
        to use a different database.

    \item The solution to complication \sectionref{Mails deleted during
        delivery} involves discarding data.

\end{enumerate}

\subsection{Possible Improvements}

\begin{enumerate}

    \item Investigate and write the policy server referred to in limitation
        3 above.

    \item Improve the solution to complication \sectionref{Mails deleted
        during delivery} so that data is not discarded.

\end{enumerate}


\section{Summary}

XXX TO BE WRITTEN\@.  START WITH THE OLD CONTENT BELOW\@.

This section presented the core of the parser, starting with a very high
level view and the initial complications that arose.  A flow chart showing
the paths a mail may take through the nascent simplified algorithm was
provided, followed by an explanation of those paths, and a discussion of
the parser's emergent behaviour --- the data from the log files creates the
paths in the flow chart, they are not specified anywhere in the parser.
The framework which holds the parser together was covered next, after which
came a description of the current actions provided by the parser, and the
algorithm for analysing unparsed log lines to create regexes for new rules.
Detecting, diagnosing, and defeating complications forms the largest single
portion of this section, mirroring the development of the parser.  The
complications are described in the order they were overcome, with
subsequent problems affecting fewer mails (often by an order or magnitude),
though the time required to solve problems increased with each successive
problem.


