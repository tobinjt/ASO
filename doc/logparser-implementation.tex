\chapter{Postfix Log Parser}

\label{Postfix Parser Implementation}

This chapter documents the implementation of \parsername{}, a parser for
Postfix log files based on the architecture described in \sectionref{parser
architecture}.  Any design may look good when examined in the abstract, but
the real test of the design comes with the first concrete implementation;
only then do any practical difficulties come to light.  Implementing this
architecture was straightforward --- the difficulties came not from the
architecture, but from anomalies in the log files and Postfix's behaviour,
as described in \sectionref{complications}.  \parsername{} successfully
deals with all the difficulties that were discovered during its
development, but further difficulties may arise during future usage;
unfortunately solving all potential difficulties that may arise is an
impossible task, but the descriptions of the solutions developed thus far
should help if someone needs to solve a problem of their own.

\parsername{} deals with all the eccentricities and oddities of parsing
Postfix log files, presenting the resulting data in a normalised, simple to
use representation.  Unfortunately, dealing with the complications that
arise sometimes requires the parser to discard log lines (e.g.\
\sectionref{aborted delivery attempts}), and, less frequently, to discard a
data structure (e.g.\ \sectionref{timeouts during data phase}).
\parsername{} can parse log files from Postfix version 2.2 through to
version 2.5, and should be able to parse log files from later versions with
only minor modifications or an updated ruleset.  The parser's task is to
follow the journey each mail takes through Postfix, combining the data
captured by rules into a coherent whole, and saving it in a useful and
consistent form.  The intermingling of
log lines from different mails immediately rules out the possibility of
handling each mail in isolation; the parser must handle multiple mails in
parallel, each potentially at a different stage in its journey, without any
interference between mails --- except in the minority of cases where
intra-mail interference is required, e.g.\ mail re-injected for forwarding
(\sectionref{Re-injected mails}).  The best way to deal with intermingling
of log lines is to maintain state information for every unfinished mail,
and to manipulate the appropriate mail correctly for each log line
encountered.

This chapter begins with the assumptions under which \parsername{} was
designed and written, followed by a flowchart showing the most common paths
taken through Postfix and \parsername{}, with a description of the stages
and stage transitions.  The second topic is the \acronym{SQL} database that
provides storage for the parser: any future work that analyses gathered
data will do so using the database, so the database schema acts as an
\acronym{API}.  A diagram of the database schema is provided, plus
documentation for every table and field.

The next three sections document the implementation of the three components
of the architecture.  First is the framework, including its initialisation
phase, the parsing process, and the conveniences it offers to users of
\parsername{}; that section finishes by describing features that are
important for this thesis but not strictly necessary for parsing: the
performance data collected by the framework, the optimisations that can be
disabled to show their effect, and the debugging options the framework
provides.  The second component of the architecture is the actions,
starting with a graph showing how often each action is specified by rules,
a description of why some actions are more popular than others, and how
this popularity has no influence on how often an action is invoked.  Every
action that is part of \parsername{} is documented, and the actions section
concludes with a description of the process of adding a new action.  Rules
are the third component of the architecture, and also the most visible to
advanced users, e.g.\ systems administrators, because it is likely that
they will need to add their own rules to recognise their own log lines.  A
sample rule used by \parsername{} is examined, with every field clearly
documented; that is followed by a description of adding new rules, and how
to determine the value of each of the new rule's fields.  \parsername{}
provides a utility to create regexes from unrecognised log lines; the
algorithm it uses is documented, including the differences between it and
the original algorithm it is based on.  The rules section finishes with a
discussion of how \parsername{} uses rule conditions and overlapping rules,
plus a description of the regex snippets provided to ease the process of
writing regexes.

On first inspection, Postfix log files look like they will be simple to
parse, but this impression turns out to be incorrect.  The many
complications and difficulties encountered while writing \parsername{}, and
the solutions developed to overcome them, are documented in
\sectionref{complications}.  This chapter finishes with a list of
\parsernames{} limitations, and some possible improvements that could be
implemented.

\section{Assumptions}

\parsername{} makes a few (hopefully safe and reasonable) assumptions:

\begin{itemize}

    \item The log files are whole and complete: nothing has been removed,
        either deliberately or accidentally (e.g.\ log file rotation gone
        awry, file system filling up, logging system unable to cope with
        the volume of log messages).  On a well run mail server it is
        extremely unlikely that any of these problems will arise, though
        the likelihood increases when suffering from a deluge of spam or a
        mail loop.  When parsing individual log files in isolation, it is
        highly likely that some mails will have log lines in previous log
        files, and others will have log lines in subsequent log files; to
        alleviate this problem \parsername{} supports saving and loading
        its state tables, so they can be preserved between log files.

    \item Postfix logs enough information to make it possible to accurately
        reconstruct the actions it has taken.  Heuristics are used in
        several places when parsing; see \sectionref{identifying bounce
        notifications}, \sectionref{aborted delivery attempts}, and
        \sectionref{pickup logging after cleanup} for details.  At least
        one difficulty encountered while writing \parsername{}
        (\sectionref{yet more aborted delivery attempts}) could not be
        solved using the data in the log files, and requires a brute force
        approach.

    \item The Postfix queue has not been tampered with, causing unexplained
        appearance or disappearance of mail.  This may happen if the
        administrator deletes mail from the queue without using
        \daemon{postsuper}, or if the server suffers from filesystem
        corruption.

\end{itemize}

In some ways this task is similar to reverse engineering or replicating a
black box system based solely on its inputs and outputs.  Thus far,
analysis of the log files has been enough to reconstruct Postfix's
behaviour, but for other programs the techniques described in
\cite{black-box-error-reporting} may be useful.  Some advantages come from
treating Postfix as a black box during parser development:

\begin{itemize}

    \item Reading and understanding the source code would require a
        significant investment of time: Postfix 2.5.5 has 17MB of source
        code.  Each subsequent version would require further work to
        investigate the changes; many of those changes, although they
        improve Postfix's internals, would not have any effect on its
        externally observable behaviour.

    \item \parsername{} is developed using real world log files rather than
        the idealised log files someone would naturally envisage when
        reading the source code, which cannot accurately communicate the
        variety of orderings in which log lines are found in log files.

    \item The parser acts as a second source of information about Postfix's
        operation, based on empirical evidence gathered from log files.  A
        separate project could compare the empirical knowledge inherent in
        \parsername{} with Postfix's documentation and source code to see
        how closely the two agree.

\end{itemize}



\section{Parser Flow Chart}

\label{flow chart}

The flow chart in \figureref{flow chart image} shows the most common paths
a connection or mail can take through \parsername{}; decision boxes and the
difficulties described in \sectionref{complications} are excluded for the
sake of clarity.  The flow chart is intended to be a graphical overview of
how a mail progresses through both Postfix and \parsername{}, providing an
overall context into which the detailed descriptions in the remainder of
this chapter will fit, in particular the actions (\sectionref{actions in
implementation}) and complications (\sectionref{complications}).  The
states and state transitions shown in the flow chart will be explained
in this section.

\showgraph{build/logparser-flow-chart-part-1}{\parsernamelong{} flow
chart}{flow chart image}

Everything starts off with a mail entering the system, whether by local
submission via \daemon{postdrop}, by \acronym{SMTP}, by re-injection
because of forwarding, or generated internally by Postfix.  Local
submission is the simplest of the four: a queueid is assigned immediately
\flowchart{PICKUP}{2}, and the mail moves on to the delivery stage.
Re-injection because of forwarding lacks explicit log lines of its own; it
is explained fully in \sectionref{Re-injected mails}.  Internally generated
mails lack any explicit origin in Postfix 2.2.x and must be detected using
heuristics as described in \sectionref{identifying bounce notifications};
later versions of Postfix do provide log lines for internally generated
mails \flowchart{BOUNCE\_CREATED}{3}.  Bounce notifications are the primary
example of internally generated mails, though other types exist, e.g.\
Postfix may generate mails to the administrator when it encounters
configuration errors.

\acronym{SMTP} is more complicated than the others:

\begin{enumerate}

    \item The remote client connects \flowchart{CONNECT}{1}.

    \item This is followed by rejection of one or more mail delivery attempts
        \flowchart{DELIVERY\_REJECTED}{4}; acceptance of one or more mails
        \flowchart{CLONE}{5}; failure of the remote client's connection
        \flowchart{DELIVERY\_ERROR}{6}; or some random interleaving of two
        or more of the above.

    \item The client disconnects \flowchart{DISCONNECT \textnormal{or}
        TIMEOUT}{6}, either normally or with an error.  If Postfix has
        rejected any mail delivery attempts the data gathered from those
        rejections will be saved to the database \flowchart{DISCONNECT}{7};
        if there were no rejections there will not be any data to save.
        Any mails accepted will already have been cloned so their data is
        in another data structure, and will be delivered in the same way as
        mails that entered the system by any other route.

\end{enumerate}

The obvious counterpart to mail entering the system is mail leaving the
system, whether by deletion, bouncing, expiry, local delivery, or remote
delivery.  All five are handled in the same way:

\begin{enumerate}

    \item The mail will have one or more delivery attempts
        \flowchart{MAIL\_SENT \textnormal{or} SAVE\_DATA}{8}.

    \item Sometimes mail is re-injected for forwarding and the child mail
        needs to be tracked with the parent mail \flowchart{TRACK}{9}; the
        handling of re-injected mails is described in \sectionref{tracking
        re-injected mail}.

    \item After one or more delivery attempts the mail will be delivered
        \flowchart{MAIL\_SENT}{8}, bounced \flowchart{MAIL\_BOUNCED}{8},
        expired \flowchart{EXPIRY}{8}, or deleted by the administrator
        \flowchart{DELETE}{8}.

    \item The mail is removed from the Postfix queue.  This is the last log
        line for this particular mail, though it may be indirectly referred
        to if it was re-injected.  The mail is cleaned up and entered in
        the database, then deleted from the state tables
        \flowchart{COMMIT}{10}.

\end{enumerate}

It should be emphasised that the sequence above happens whether the mail is
delivered to a mailbox, piped to a command, delivered to a remote server,
bounced (because of a mail loop, delivery failure, or five day timeout), or
deleted by the administrator, \textit{unless\/} the mail is either parent
or child of re-injection, as explained in \sectionref{tracking re-injected
mail}.

\section{Database}

\label{database}

An \acronym{SQL} database is used to store both the rules and the data
gleaned by parsing Postfix log files.  Understanding the database schema is
helpful in understanding the actions of the parser, and essential to
developing further applications that utilise the data; \sectionref{database
as API} describes how the database schema functions as an \acronym{API}.

The database schema can be conceptually divided in two: the rules used to
recognise log lines, and the data saved from the parsing of log files.
Each rule has a regex to recognise log lines and capture data from them,
and specifies the action to be invoked when a log line is recognised; they
also have several other fields used by the parser, and several fields that
aid the user in understanding the meaning of the log lines recognised by
each rule.  The rules are described in detail in \sectionref{rules in
implementation}, but the rules table is documented in \sectionref{rules
table} with the rest of the database schema.

The data saved from parsing log files is divided into two tables:
connections and results.  The connections table contains an entry for every
mail accepted and every connection that rejected a delivery attempt; the
individual fields will be described in \sectionref{connections table}.  The
results table will have at least one entry for each entry in the
connections table; its fields will be covered in detail in
\sectionref{results table}.  A diagram of the database schema is provided
in \figureref{Diagram of the database schema picture}, to complement the
in-depth descriptions of each table.

An important but easily overlooked benefit of storing the rules in the
database is the link between rules and results: if more information is
required when examining a result, the rule that produced the result is
available for inspection because each result references the rule that
created it.  No ambiguity is possible about which rule or action created a
particular result, eliminating one potential source of confusion.

A clear, comprehensible schema is essential when using the data extracted
from log lines; it is more important when using the data than when storing
it, because storing the data is a once-off operation, whereas utilising
the data requires frequent searching, sorting, and manipulation of the
data.

\subsection{Using A Database To Provide An Application Programming Interface}

\label{database as API}

The database populated by \parsername{} provides a simple interface to
Postfix log files.  Although the interface is a database schema rather than
a set of functions in a library, it provides the same benefits as any other
\acronym{API}: a stable interface between the user and the creator of the
data, allowing code on either side of the interface to be changed without
adverse effects, as long as the interface is adhered to.  Programs that use
the database can range from the simple examples in \sectionref{motivation}
to far more complex data mining tools and machine learning algorithms.

Using a database simplifies writing programs that need to interact with the
data in several ways:

\begin{enumerate}

    \item Most programming languages have facilities for database access,
        allowing a developer to write programs that use the gathered data
        in their preferred programming language, rather than being
        restricted to the language the parser is written in.

    \item Databases provide complex querying and sorting functionality for
        the user without requiring large amounts of programming.  All
        databases have one or more programs, of varying complexity and
        sophistication, that can be used for ad hoc queries with minimal
        investment of time.

    \item Databases are easily extensible, e.g.:

        \begin{itemize}

            \item New columns can be added to tables, using DEFAULT clauses
                or TRIGGERS to populate them.

            \item A VIEW gives a custom arrangement of data with minimal
                effort.

            \item Triggers can be written to perform actions when certain
                events occur.  In pseudo-\acronym{SQL}\@:

\begin{verbatim}
CREATE TRIGGER ON INSERT INTO results
    WHERE sender = "boss@example.com"
        AND rule_id = rules.id
        AND rules.action = "DELIVERY_REJECTED"
    SEND PANIC EMAIL TO "postmaster@example.com";
\end{verbatim}

            \item Other tables can be added to the database, e.g.\ to cache
                historical or computed data, or to incorporate data from
                other sources.

        \end{itemize}

    \item Some databases support granting access on a fine-grained basis,
        e.g.\ allowing the finance department to produce invoices, the
        helpdesk to run limited queries as part of dealing with support
        calls, and the administrators to have full access to the data.


    \item \acronym{SQL} is reasonably standard and many people will already
        be familiar with it; for those unfamiliar with \acronym{SQL}, lots of
        resources are available from which to learn, e.g.\
        \urlLastChecked{http://philip.greenspun.com/sql/}{2009/02/23}.
        Although every vendor implements a different dialect of
        \acronym{SQL}, the basics are the same everywhere.  Depending on
        the database in use there may be tools available that reduce or
        remove the requirement to know \acronym{SQL}.

\end{enumerate}

Storing the results in a database will also increase the efficiency of
using those results, because the log files only need to be parsed once
rather than each time the data is used; indeed the database may be used by
someone with no access to the original log files.

\subsection{Rules Table}

\label{rules table}

\newlength{\belowcaptionskipORIG}
\setlength{\belowcaptionskipORIG}{\belowcaptionskip}
\setlength{\belowcaptionskip}{10pt}
\showgraph{build/database-schema}{Diagram of the database schema}{Diagram
of the database schema picture}
\setlength{\belowcaptionskip}{\belowcaptionskipORIG}

Rules are discussed in detail in \sectionref{rules in implementation}, but
the structure of the rules table is documented here alongside the other
tables in the database.  Rules are created by the user, and will not be
modified by \parsername{}, except when it updates the hits and hits\_total
fields.  Rules recognise individual log lines, capturing data to be saved
in the connections and results tables, and specifying the action to invoke
for each recognised log line.

Each rule must provide values for most of the fields below; all fields are
required unless otherwise stated in the description.

\begin{boldeqlist}

    \item [id] A unique identifier that other tables can use when referring
        to a specific rule.  This will be assigned by the database.

    \item [name] A short name for the rule.

    \item [description] This field should describe the event causing the
        log lines this rule recognises, e.g.\ ``Mail has been delivered to
        the LDA (typically procmail)''.

    \item [restriction\_name] The name of the Postfix restriction that
        caused the rejection of the mail delivery attempt.  This field is
        valid only for rules that recognise rejection log lines, i.e.\
        rules that have an action of \action{DELIVERY\_REJECTED}.

    \item [program] The Postfix component (e.g.\ \daemon{smtpd}) whose log
        lines the rule recognises; see \sectionref{rule conditions in
        implementation} for full details of how this attribute is used.

    \item [regex] The regex to recognise log lines with, as documented in
        \sectionref{regex components}.

    \item [connection\_data] Sometimes rules need to provide data that is
        not present in the log line, e.g.\ setting \texttt{client\_ip} when
        a mail is being delivered to another server; any field in the
        connections table can be set in this way.  The format is:
        \newline{} \tab{} \texttt{ client\_hostname = localhost,}
        \newline{} \tab{} \tab{} \texttt{client\_ip = 127.0.0.1} \newline{}
        i.e.\ semi-colon or comma separated assignment statements.  Commas
        and semi-colons cannot be escaped and thus cannot be included in
        data, because this feature is intended for use with small amounts
        of data and dealing with escape sequences was deemed unnecessary.
        This field is optional.

    \item [result\_data] The result table equivalent of
        \texttt{connection\_data}, also optional.

    \item [action] The action to be invoked when this rule recognises a log
        line; a full list of actions and the parameters they are invoked
        with can be found in \sectionref{actions in detail in
        implementation}.

    \item [hits] This counter is maintained for every rule and incremented
        each time the rule successfully recognises a log line.  At the
        start of each run \parsername{} sorts the rules by hits, and at the end of the run it updates
        every rule's hits field in the database.  Assuming that the
        distribution of log lines is reasonably consistent across log
        files, ordering rules by their recognition frequency will reduce
        the parser's execution time.  Rule ordering for efficiency is
        discussed in \sectionref{rule ordering for efficiency}.  This field
        will be set by the parser rather than the rule author.

    \item [hits\_total] The total number of log lines recognised by this
        rule over all runs of the parser; hits starts from zero each time
        the parser is run, but hits\_total is not.  This field will be set
        by the parser rather than the rule author.

    \item [priority] This is the user-configurable companion to hits: when
        the list of rules is sorted by the parser, priority overrides hits.
        This allows more specific rules to take precedence over more
        general rules, as described in \sectionref{rules in architecture}.
        This field is optional.

    \item [debug] If this field is true, a warning will be issued with
        information about the rule and the log line every time this rule
        recognises a log line.  This field is optional.

\end{boldeqlist}

\subsection{Connections Table}

\label{connections table}

Every accepted mail and every connection that rejected a mail delivery
attempt will have a single entry in the connections table containing all of
the fields below.  For an incoming connection, the client is the remote
machine, and the server is the local machine; for outbound mail delivery
attempts, the roles are reversed.

\begin{boldeqlist}

    \item [id] This field uniquely identifies the row.  This will be
        assigned by the database.

    \item [server\_ip] The IP address of the server.

    \item [server\_hostname] The hostname of the server, it will be
        \texttt{unknown} if the IP address could not be resolved to a
        hostname via DNS\@.

    \item [client\_ip] The client IP address.

    \item [client\_hostname] The hostname of the client, it will be
        \texttt{unknown} if the IP address could not be resolved to a
        hostname via DNS\@.

    \item [helo] The hostname used in the HELO command.  The HELO hostname
        occasionally changes during a connection, presumably because spam
        or virus senders think it is a good idea.  Postfix only logs the
        HELO hostname when it rejects a mail delivery attempt, but it is
        quite simple to rectify this as described in \sectionref{logging
        helo}.

    \item [queueid] The queueid of the mail if the connection represents an
        accepted mail, or \texttt{NOQUEUE} if not.

    \item [start] The timestamp of the first log line.

    \item [end] The timestamp of the last log line.

\end{boldeqlist}

\subsection{Results Table}

\label{results table}

Every recognised log line will have a row in the results table, and each
row is associated with a single connection; a single connection will have
at least one result associated with it, but will usually have several, and
may have hundreds.

\begin{boldeqlist}

    \item [connection\_id] The id of the row in the connections table this
        result is associated with.

    \item [rule\_id] The id of the rule in the rules table that recognised
        the log line.

    \item [id] A unique identifier for this result.  This will be assigned
        by the database.

    \item [warning] Administrators can configure Postfix to log a warning
        instead of enforcing a restriction that would reject a mail
        delivery attempt --- a mechanism that is quite useful for testing
        new restrictions.  This field will be false for a real rejection,
        or true if the log line was a warning.  This field should be
        ignored if the result is not a rejection, i.e.\ the action field of
        the associated rule is not \action{DELIVERY\_REJECTED}.

    \item [smtp\_code] The \acronym{SMTP} code associated with the log
        line.  In general an \acronym{SMTP} code is only present in a log
        line representing a mail being delivered or a mail delivery attempt
        being rejected; results whose log line did not contain an
        \acronym{SMTP} code will duplicate the \acronym{SMTP} code of other
        results in that connection.  Some mail delivery log lines do not
        contain an \acronym{SMTP} code, e.g.\ when Postfix delivers to a
        user's mailbox; in those cases the \acronym{SMTP} code is specified
        by the rule's \texttt{result\_data} field, based on the success or
        failure represented by the log line.

    \item [enhanced\_status\_code] The enhanced status code~\cite{RFC3463}
        is similar to the \acronym{SMTP} code, but is intended to be
        interpreted by mail clients so that error messages can be clearly
        conveyed to the user.  Enhanced status code support was added to
        Postfix in version 2.3; log lines from previous versions will not
        contain any enhanced status codes.

    \item [sender] The sender's email address.  This may change during a
        connection if the client uses different sender addresses for
        multiple rejected delivery attempts; however, the results for one
        accepted mail will only have one sender address.

    \item [recipient] The recipient address; there may be multiple
        recipient addresses per mail or connection.

    \item [size] The size of the mail, only available for delivered mails.

    \item [delay] How long the mail was delayed while it was being
        delivered.  This will only be present for delivered mails.

    \item [delays] More detailed information about how long the mail was
        delayed while it was being delivered, again only present for
        delivered mails.

    \item [message\_id] The message-id of the accepted mail, again it is
        present for delivered mails only.

    \item [data] A field available to store a piece of captured data that
        does not have its own specific field, e.g.\ the rejection message
        from a \acronym{DNSBL}\@.

    \item [timestamp] The log line's timestamp.

\end{boldeqlist}



\section{Framework}

\label{framework in implementation}

The role of the framework in this architecture was described in detail in
\sectionref{framework in architecture}; this section is concerned with the
implementation of the framework within \parsername{}.  The framework
manages the parsing process, taking care of the drudgery and boring tasks,
provides services to the actions, and implements some optimisations.

The framework performs some initialisation tasks each time the parser is
run, setting up several state tables that will later be used by actions.
Data about the connections and mails being processed is held in
\texttt{connections} and \texttt{queueids} respectively.  The other state
tables are used when solving complications that arise during parsing:
\texttt{timeout\_queueids} is used when dealing with connections that time
out during the DATA phase (\sectionref{timeouts during data phase});
\texttt{bounce\_queueids} is part of the solution to bounce notification
mails being delivered before their creation is logged (\sectionref{Bounce
notification mails delivered before their creation is logged}); and
\texttt{postsuper\_deleted\_queueids} caches information about mails that
were recently deleted by the administrator, so that subsequent log lines
processed by the \action{SAVE\_DATA} action can be discarded
(\sectionref{Mail deleted before delivery is attempted}).

The framework verifies each of the rules when it loads the ruleset,
checking:

\begin{itemize}

    \squeezeitems{}

    \item That the specified action is registered with the framework.

    \item That the regex is valid.  The regex will have regex components
        expanded (see \sectionref{regex components}), and will also be
        compiled for efficiency (\sectionref{Caching compiled regexes}).

    \item For overlap between the data captured by the regex and additional
        data specified in either \texttt{connection\_data} or
        \texttt{result\_data}.

    \item That \texttt{connection\_data}, \texttt{result\_data}, and the
        regex captures specify valid fields to save data to.

\end{itemize}

State tables from a previous \parsername{} run, if any, will be loaded now;
the framework supports saving state at any time, without adversely
affecting the parsing process.  The need to track re-injected mail
(\sectionref{Re-injected mails}) complicates the process of saving state,
because the relationships between mails must be maintained; loading state
is also complicated by dealing with aborted delivery attempts
(\sectionref{aborted delivery attempts}), because a separate set of
relationships between connections and mails must be re-created when the
state tables are restored.  The last step in loading the ruleset is to sort
the rules as described in \sectionref{rule ordering for efficiency}.

The framework is now ready to begin parsing.  For each log line it will use
those rules whose \texttt{program} field equals the program in the log line
(described in \sectionref{rule conditions in implementation}), falling back
to generic rules if necessary, and finally warning if the log line is
unrecognised.  The repetitive nature of log files gives them high
compression ratios; the framework uses a Perl module named
\texttt{IO::Uncompress::AnyUncompress} to read compressed log files, saving
users the trouble of uncompressing them to a temporary file before parsing
begins.  When used interactively, the framework displays a progress bar to
show how far parsing has progressed through the log file and how long the
remainder of the parsing process is likely to take.  The progress bar is
not as accurate when parsing compressed log files, because each compressed
block uncompresses to a variable number of log lines; variation between the
recognition and processing time of individual log lines also affects its
accuracy, but overall the progress bar is a useful addition.  \parsername{}
is primarily intended for parsing complete log files, but with minor
modifications it could be used to parse a live log file, periodically
checking it for new log lines; this could be very useful for programs that
work best with up to date data, e.g.\ a program for monitoring the health
of the mail system, or graphs showing activity over the last five minutes.

The framework collects data used when evaluating \parsernames{} efficiency
for the evaluation chapter (\sectionref{Evaluation}); some of the
techniques used to improve parsing speed can be turned off or altered to
measure the effect they have.  The data gathered with each optimisation
disabled will be compared to the data gathered with it enabled, to quantify
the benefit each optimisation provides.  Three sets of data are collected:

\begin{enumerate}

    \item How long it takes to parse each log file (\graphref{parsing time
        vs log file size vs number of log lines graph}).

    \item The number of rules used when recognising log lines
        (\graphref{Mean number of rules used per log line}); the framework
        may need to try multiple rules for each log line before it finds
        one that recognises it.

    \item The number of log lines and rules used per log line for each
        Postfix component (\graphref{Mean number of rules used per log line
        for each Postfix component}).

\end{enumerate}

The framework has five ways that it can adapt its behaviour to demonstrate
how effective each optimisation is:

\begin{enumerate}

    \item The rule ordering used can be changed from optimal (the default,
        most efficient) to shuffled (intended to represent an unordered
        ruleset) or reverse (reverse of optimal, least efficient).  The
        results of parsing with the three different orderings are shown in
        \sectionref{rule ordering for efficiency}.

    \item The framework can record which rule recognised each log line, and
        then on a subsequent run consult that information so that it uses
        the correct rule for each log line.  This gives the best possible
        running time, because only one rule is used to recognise each log
        line, as discussed in \sectionref{perfect rule ordering}.

    \item Each regex is compiled and the result cached when the ruleset is
        loaded; this optimisation can be disabled and the regex compiled
        every time it is used when recognising log lines.  The effect this
        optimisation has is addressed shown in \sectionref{Caching compiled
        regexes}.

    \item Normally, when a log line has been recognised the framework
        invokes the action specified by the rule.  Invoking the action can
        be skipped if desired, so the timing information shows how long is
        spent recognising log lines only; this time can be subtracted from
        the time taken by a normal run to show how long is spent processing
        log lines.  This data is analysed in \sectionref{recognising vs
        processing}.

    \item The framework can skip inserting data into the database, because
        that dominates the execution time of the parser; the evaluation
        chapter measures the speed of \parsername{}, not the speed of the
        database or the disks it resides on.  The effect on parsing time of
        storing results in the database is described at the beginning of
        \sectionref{parser efficiency}.

\end{enumerate}

The framework also provides several debugging options, to aid in writing or
correcting rules, or figuring out why \parsername{} is not behaving as
expected.  In increasing order of severity they are:

\begin{enumerate}

    \item Individual rules can set their debug field to true, and debugging
        information will be printed each time they recognise a log line.

    \item Each result can be extended with extra debugging information,
        which is useful when a warning dumps a data structure for
        inspection.  The extra information added is: the log line's
        timestamp in human readable form; the entire log line; the name of
        the log file and the line number of the log line within it.  This
        extra information is not stored in the database.

    \item Every time a log line is recognised, the recognising rule's regex
        and the log line can be printed, so that the user can verify that
        the correct rule recognised each log line.  This is equivalent to
        setting every rule's debug field to true.

    \item Every connection and result added to the database can be dumped
        in a human readable form.  This will result in a huge amount of
        debugging information, so it is only useful for small log file
        snippets, because otherwise the amount of information is
        overwhelming.

\end{enumerate}

A user, typically a mail administrator, would use these options when having
difficulty extending the ruleset to recognise log lines not recognised by
\parsernames{} \numberOFrules{} rules.  Option 1 is useful for debugging a
single rule, to check if it is recognising log lines it should not;
figuring out that a rule is not recognising log lines that it should
recognise is a more difficult task.  Option 2 is useful when \parsername{}
is warning about parsing problems, because extra information about a mail
will be present in the warning message.  Option 3 is less useful because of
the volume of data it produces, though for small log file snippets, e.g.\
1000 log lines, it is possible to manually verify that the correct rule
recognised each log line.  Option 4 is not useful for most users: it is for
debugging problems within \parsername{}, and sufficient safeguards are in
place that there should be plenty of warning messages explaining what is
wrong without using this option.

\section{Actions}

\label{actions in implementation}

Actions are the component of this architecture responsible for processing
all of the inputs recognised by rules; in \parsername{} they reconstruct
the journey each mail takes through Postfix, dealing with all the
complications and difficulties that arise.  \parsername{} has
\numberOFactions{} actions and \numberOFrules{} rules, with an uneven
distribution of rules to actions as shown in \graphref{Distribution of
rules per action}.  Unsurprisingly, the action with the most associated
rules is \action{DELIVERY\_REJECTED}, the action that processes Postfix
rejecting a mail delivery attempt; next is \action{SAVE\_DATA}, the action
that saves useful information without doing any other processing.  The
third most common action is, perhaps surprisingly, \action{UNINTERESTING}:
this action does nothing when invoked, allowing uninteresting log lines to
be parsed without any effects; it does not imply that the input is
ungrammatical or unparsed.  Generally, rules specifying the
\action{UNINTERESTING} action recognise log lines that are not associated
with a specific mail, e.g.\ notices about configuration files changing;
these log lines are recognised and processed so that the framework can warn
about unrecognised log lines, informing the user that they need to augment
the ruleset.  Most of the remaining actions have only one or two associated
rules, because that input category will only ever have one or two log line
variants, e.g.\ all log lines showing that a remote client has connected
are recognised by a single rule and processed by the \action{CONNECT}
action.

No correlation exists between how often an action is specified by rules and
how often an action is invoked because a rule recognises a log line.
\Graphref{Number of times each action was invoked excluding 22 and 62--68}
shows the number of times each action was invoked when parsing the
\numberOFlogFILES{} log files used to generate the statistics in
\sectionref{parser efficiency}, \textit{excluding\/} log files 22 \&
62--68, because their contents are extremely skewed by two mail loops,
described in \sectionref{Characteristics of the 93 log files}.  The action
most commonly specified by rules, \action{DELIVERY\_REJECTED}, is the third
most commonly invoked action; the most commonly invoked actions,
\action{CONNECT} and \action{DISCONNECT}, are each specified by only one
rule.  The \action{SAVE\_DATA} and \action{UNINTERESTING} actions, the
second and third most commonly specified actions respectively, are halfway
down the graph.

As should be expected, some actions have been invoked almost exactly the
same number of times: almost every \action{CONNECT} will have a
\action{DISCONNECT}, with only a 0.00042\% difference between the number of
times the two were invoked; every mail that enters Postfix's queue will be
managed by \daemon{qmgr} (\action{MAIL\_QUEUED}) and processed by
\daemon{cleanup} (\action{CLEANUP\_PROCESSING}), and again the two have
only a 0.01648\% difference between their number of invocations.
\action{CLEANUP\_PROCESSING} is invoked slightly more often than
\action{MAIL\_QUEUED}: occasionally, an accepted mail in the process of
being transferred is interrupted by a timeout, and there is a log line from
\daemon{cleanup} but not from \daemon{qmgr}, as described in
\sectionref{discarding cleanup log lines}.

\showgraph{build/graph-action-distribution}{Number of rules specifying each
action}{Distribution of rules per action}

\showgraph{build/graph-number-of-action-invocations-excluding-22-and-62--68}{Number
of times each action was invoked when parsing \numberOFlogFILES{} log
files, excluding log files 22 \& 62--68}{Number of times each action was
invoked excluding 22 and 62--68}

\subsection{Description Of Each Action}

\label{actions in detail in implementation}

This section documents the actions found in \parsername{}; it may help to
revisit the flow chart in \sectionref{flow chart} to see how a mail passes
from one action to another as its log lines are recognised.  The words
\textit{mail\/} and \textit{connection\/} are used in the action
descriptions below because they are less unwieldy and more specific than
\textit{state table entry\/}; a connection becomes a mail during the
\action{CLONE} action, which processes Postfix accepting a delivery
attempt, and the data structure is copied from the connections state table
to the queueids state table.

The complications and difficulties that arose when parsing real-world log
files are documented in \sectionref{complications}; some action
descriptions refer to specific difficulties they address.  The
complications are documented in a separate section to avoid overwhelming
the action descriptions.

If the log line has enough information to identify the correct connection
or mail, each action will save all the data captured by the recognising
rule's regex to the connection or mail; usually, log lines that lack
identifying information will be processed by the \action{UNINTERESTING}
action.  Each action is passed the same arguments:

\begin{boldeqlist}

    \squeezeitems{}

    \item [rule] The recognising rule.

    \item [data] The data captured from the log line by the rule's regex.

    \item [line] The log line, separated into fields:

        \begin{boldeqlist}

            \squeezeitems{}

            \item [timestamp] The time the line was logged at.

            \item [program] The name of the program that generated the log
                line.

            \item [pid] The \acronym{pid} of the process that logged the
                line.

            \item [host] The hostname of the server the line was logged on.

            \item [text] The remainder of the log line, i.e.\ the message
                logged by the program and recognised by the rule.

        \end{boldeqlist}

\end{boldeqlist}

\noindent{}\textbf{\parsernames{} actions:}

\begin{description}

    \item [BOUNCE\_CREATED] Postfix 2.3 and subsequent versions log the
        creation of bounce messages, and this action processes those log
        lines.  This action creates a new mail if necessary; if the mail
        already exists the unknown origin flag will be removed from it.  To
        deal with complication \sectionref{Bounce notification mails
        delivered before their creation is logged}, this action checks a
        cache of recent bounce mails, to avoid incorrectly creating bounce
        mails when log lines are out of order, and also marks the mail as a
        bounce notification.

    \item [CLEANUP\_PROCESSING] \daemon{cleanup} processes every mail that
        passes through Postfix; details of what it does are available in
        \sectionref{Postfix Daemons}.  This action saves all data captured
        by the rule's regex if the log line has not come after a timeout
        (see \sectionref{discarding cleanup log lines}); it also creates
        the mail if necessary, setting its unknown origin flag (see
        \sectionref{pickup logging after cleanup}).

    \item [CLONE] Multiple mails may be accepted on a single connection, so
        each time a mail is accepted the connection's state table entry
        must be cloned and saved in the state tables under its queueid; if
        the original data structure was used then second and subsequent
        mails would overwrite one another's data.

    \item [COMMIT] Enter the data from the mail into the database.  Entry
        will be postponed if the mail is a child waiting to be tracked
        (\sectionref{Re-injected mails}).  Once entered into the database,
        the mail will be usually be deleted from the state tables, but
        deletion will be postponed if the mail is the parent of mail
        re-injected for forwarding (\sectionref{Re-injected mails}).

    \item [CONNECT] Process a remote client connecting: create a new
        connection, indexed by \daemon{smtpd} \acronym{pid}.  If a
        connection already exists it is treated as a symptom of a bug in
        \parsername{}, and the action will issue a warning containing the
        full contents of the existing connection plus the log line that has
        just been parsed.

    \item [DELETE] Deals with mail deleted using Postfix's administrative
        command \daemon{postsuper}.  This action adds a dummy recipient
        address if required (see \sectionref{Mail deleted before delivery
        is attempted}), then invokes the \action{COMMIT} action to save the
        mail to the database.

    \item [DELIVERY\_ERROR] Process log lines indicating that an error
        occurred and the remote client disconnected; the log lines
        processed by this action will be followed by log lines processed by
        the \action{DISCONNECT} action, so all this action does is save
        data from the log line.

    \item [DELIVERY\_REJECTED] Postfix rejected a mail delivery attempt
        from the remote client.  This is the action most frequently
        specified by rules, because so many different restrictions are used
        to reject delivery attempts.  This action is quite simple: if the
        log line contains a queueid, save the data captured by the rule's
        regex to the mail identified by that queueid; otherwise save it to
        the connection identified by the \acronym{pid} in the log line.

    \item [DISCONNECT] Invoked when the remote client disconnects, it
        enters the connection in the database if it has any useful data,
        performs any required cleanup, and deletes the connection from the
        state tables.  This action deals with aborted delivery attempts
        (\sectionref{aborted delivery attempts}).

    \item [EXPIRY] If Postfix has not managed to deliver a mail after
        trying for five days, it will give up and return the mail to the
        sender.  When this happens the mail will not have a combination of
        Postfix programs that passes the valid combinations check,
        implemented to deal with the complication described in
        \sectionref{out of order log lines}; this action tags the mail as
        having expired, so the \action{COMMIT} action will skip the valid
        combinations check.

    \item [MAIL\_BOUNCED] This action behaves in exactly the same way as
        the \action{SAVE\_DATA} action; it saves all data captured by the
        recognising rule's regex, and does nothing more.  It is a separate
        action to distinguish delivery attempts that bounce from other
        delivery attempts.

    \item [MAIL\_DISCARDED] Sometimes mail is discarded by Postfix, e.g.\
        mail submitted locally that is larger than the limit configured by
        the administrator.  This action is used for those mails; it invokes
        the \action{COMMIT} action, but is a separate action to simplify
        further analysis.

    \item [MAIL\_QUEUED] This action represents Postfix picking a mail from
        the queue to deliver.  This action needs to deal with out of order
        log lines when mail is re-injected for forwarding; see
        \sectionref{Re-injected mails} for details.

    \item [MAIL\_SENT] This action behaves in exactly the same way as the
        \action{SAVE\_DATA} action; it saves all data captured by the
        recognising rule's regex, and does nothing more.  It is a separate
        action to distinguish successful delivery attempts from other
        delivery attempts.

    \item [MAIL\_TOO\_LARGE] When a client tries to send a mail larger than
        the local server accepts, the mail will be discarded by Postfix and
        the client informed of the problem.  The mail may have been
        accepted and partially transferred; if so the parser will have a
        data structure that must be disposed of.  See \sectionref{timeouts
        during data phase} for full details; although that describes
        timeouts, the processing is the same for mails that are too large.

    \item [PICKUP] The \action{PICKUP} action corresponds to the
        \daemon{pickup} service processing a locally submitted mail.  A new
        mail will usually be created, although out of order log lines may
        have caused it to already exist, as documented in
        \sectionref{pickup logging after cleanup}.

    \item [POSTFIX\_RELOAD] When Postfix stops running or reloads its
        configuration, it kills all \daemon{smtpd} processes, requiring all
        of the connections in \parsernames{} state tables to be cleaned up,
        entered in the database, and deleted from the state tables.
        Postfix probably kills all the other components too, but
        \parsername{} is only affected by \daemon{smtpd} processes exiting.

    \item [SAVE\_DATA] Every action that can locate the correct data
        structure in the state tables saves any data captured by the
        recognising rule's regex to it.  The \action{SAVE\_DATA} saves data
        in this way but does not do anything else; it is invoked for log
        lines that contain useful data but do not require any further
        processing.

    \item [SMTPD\_DIED] Sometimes a \daemon{smtpd} dies, is killed by a
        signal, or exits unsuccessfully; the associated connection must be
        cleaned up, entered in the database if it has enough data, and
        deleted from the state tables.  Sometimes the connection will not
        have enough data to satisfy the database schema, so it cannot be
        entered into the database for future analysis; unfortunately this
        means that the small amount of data that has been gathered by
        \parsername{} will be lost.

    \item [SMTPD\_WATCHDOG] \daemon{smtpd} processes have a watchdog timer
        to deal with unusual situations; after five hours the timer will
        expire and the \daemon{smtpd} will exit.  This occurs very
        infrequently, because there are many other timeouts that should
        occur in the intervening hours, e.g.\ timeouts for DNS requests or
        timeouts reading data from the client.  The active connection for
        that \daemon{smtpd} must be cleaned up, entered in the database,
        and deleted from the state tables.

    \item [TIMEOUT] The connection with the remote client timed out, so the
        mail being transferred must be discarded by Postfix.  The mail may
        have been accepted: if so the parser will have a data structure to
        dispose of.  See \sectionref{timeouts during data phase} for full
        details.

    \item [TRACK] Track a mail when it is re-injected for forwarding to
        another mail server; this happens when a local address is aliased
        to a remote address (see \sectionref{tracking re-injected mail} for
        a full explanation).  \action{TRACK} will be called when dealing
        with the parent mail, and will create the child mail if necessary.
        \action{TRACK} checks if the child has already been tracked, either
        with this parent or with another parent, and issues appropriate
        warnings if so.

    \item [UNINTERESTING] This action just returns successfully: it is used
        when a log line needs to be recognised to avoid warning about
        unrecognised log lines, but does not either provide any useful data
        to be saved or require any processing.

\end{description}

\subsection{Adding New Actions}

\label{adding new actions in implementation}

Adding new actions is not as simple as adding new rules, though care has
been taken in the architecture and implementation to make adding new
actions as painless as possible; one of the few limitations is that
\parsername{} is written in Perl, so new actions must also be written in
Perl.  The implementer writes a subroutine that accepts the standard
arguments given to actions, and registers it with the framework by calling
the framework's \texttt{add\_actions()} subroutine.  No other work is
required from the implementer to integrate the action into the parser; all
of their attention and effort can be focused on correctly implementing
their new action.  The action may need to extend the list of valid
combinations described in \sectionref{out of order log lines} if the new
action creates a different set of acceptable programs, but this would only
be necessary if the new action processes log lines from Postfix components
that \parsername{} does not have rules for, e.g.\ \daemon{virtual} or
\daemon{lmtp}.  The new action must be registered before the rules are
loaded, because it is an error for a rule to specify an unregistered
action; this helps catch mistakes made when adding new rules.

\section{Rules}

\label{rules in implementation}

The rules are responsible for recognising each log line and specifying the
correct action to be invoked.  The rules will be the most visible component
in any parser based on this architecture, and also the component most
likely to be modified by users.  Rules need to be as simple as possible so
that users can easily modify them or add new rules, but each parser must
balance that simplicity with the need to provide enough flexibility and
power to successfully parse inputs.

The role of the rules in the architecture is covered in detail in
\sectionref{rules in architecture}; this section is concerned with the
practical aspects of how rules are implemented and used in \parsername{}.
The structure of the rules table has already been documented in
\sectionref{rules table}; that description will not be repeated here, but
should be fleshed out by the sample rule in \sectionref{example rule in
implementation}.  The process of creating new rules from unparsed log lines
is dealt with in \sectionref{creating new rules in implementation},
followed by the algorithm used by the utility supplied with \parsername{}
that creates regexes from unparsed log lines; the regex components provided
by \parsername{} to ease writing of complex regexes are covered in
\sectionref{regex components}.  The rule conditions used in \parsername{}
are the penultimate topic (\sectionref{rule conditions in implementation}),
and this section concludes with some suggestions for how to detect
overlapping rules.

\subsection{Sample Rule}

\label{example rule in implementation}

The sample rule in \tableref{Example rule in implementation table}
recognises the log line that results from Postfix rejecting a delivery
attempt because the domain part of the sender address does not have an A,
AAAA, or MX DNS entry, i.e.\ mail could not be delivered to any address in
the sender's domain (for full details
see~\cite{reject-unknown-sender-domain}).  An example log line that would
be recognised by this rule:

% RFC 3330 says that 192.0.2.0/24 is reserved for example use.

\begin{verbatim}
NOQUEUE: reject: RCPT from smtp.example.com[192.0.2.1]:
  550 5.1.8 <alice@example.com>:
  Sender address rejected: Domain not found;
  from=<alice@example.com> to=<bob@example.net>
  proto=SMTP helo=<smtp.example.com>
\end{verbatim}

% do not reformat this!
\begin{table}[thbp]
    \caption{Sample rule used by \parsernameshort{}}
    \empty{}\label{Example rule in implementation table}
    \centering{}
    \begin{tabular}{ll}
        \tabletopline{}%
        Attribute           & Value                                             \\
        \tablemiddleline{}%
        name                & Unknown sender domain                             \\
        description         & We do not accept mail from unknown domains        \\
        restriction\_name   & reject\_unknown\_sender\_domain                   \\
        program             & \daemon{smtpd}                                    \\
        regex               & \verb!^__RESTRICTION_START__ <(__SENDER__)>: !    \\
                            & \verb!Sender address rejected: Domain not found;! \\
                            & \verb!from=<\k<sender>> to=<(__RECIPIENT__)> !    \\
                            & \verb!proto=E?SMTP helo=<(__HELO__)>$!            \\
        result\_data        &                                                   \\
        connection\_data    &                                                   \\
        action              & \action{DELIVERY\_REJECTED}                       \\
        hits                & 0                                                 \\
        hits\_total         & 0                                                 \\
        priority            & 0                                                 \\
        debug               & 0                                                 \\
        %cluster\_group      & 400                                               \\
        \tablebottomline{}%
    \end{tabular}
\end{table}

\noindent{}The attributes in \tableref{Example rule in implementation
table} are used as follows:

\begin{description}

    \item [name, description, and restriction\_name:] are not used by the
        parser, they serve to document the rule for the user's benefit.

    \item [program and regex:] program is used to restrict the log lines
        this rule will be used to recognise; see \sectionref{rule
        conditions in implementation} for details.  The regex does the
        actual recognition of log lines, and data captured by the regex
        (e.g.\ sender, recipient) will be automatically saved to the
        results and connections tables.  The regex components used in the
        regex are described in \sectionref{regex components}.

    \item [action:] The action to be invoked when the rule recognises a log
        line.  See \sectionref{actions in detail in implementation} for
        details of the actions implemented by \parsername{}, and
        \sectionref{actions in architecture} for the role of actions in the
        architecture.

    \item [result\_data and connection\_data:] are used to provide data not
        present in the log line, but are unused in this rule.

    \item [hits, hits\_total, and priority:] hits and priority are used
        when ordering the rules for more efficient parsing (see
        \sectionref{rule ordering for efficiency}).  At the end of each
        parsing run hits is set to the number of log lines recognised by
        the rule.  Hits\_total is the sum of hits over every parsing run,
        but is otherwise unused by the parser.

    \item [debug] enables or disables printing of debugging information
        when this rule recognises a log line.

\end{description}

\subsection{Creating New Rules}

\label{creating new rules in implementation}

The log files produced by Postfix differ from installation to installation,
because administrators have the freedom to choose the subset of available
restrictions that suits their needs, including using different
\acronym{DNSBL} services, policy servers, or custom rejection messages.  To
ease the process of parsing new log lines, the architecture separates rules
from actions: adding new actions requires some effort, but adding new rules
to recognise new log lines is trivial, and occurs much more frequently.

To add a new rule a new row must be added to the rules table in the
database, containing all the required attributes: action, name,
description, program, and regex; all the other attributes are either
optional, are set by the parser, or have sensible defaults.  The name and
description fields should be set based on the meaning of the log line, to
help others understand which log lines this rule will recognise; the value
for the program field will be obvious from the unrecognised log lines.  The
action depends on the what the log line represents, e.g.\ a delivery
rejection, a mail being delivered, some useful information, or something
else; examine the list of actions in \sectionref{actions in detail in
implementation} to determine the correct one.  The regex needs to be based
on the log line, but see below for a tool to ease the process of creating
regexes from unrecognised log lines.

Other attributes may be required: connection\_data, result\_data, priority,
or restriction\_name.  In general it will only become clear that
connection\_data or result\_data are required when \parsername{} warns
about an entry in the connections or results tables that is missing some
required fields, because values for those fields are not present in any of
the log lines for that connection or mail.  For example, the rule that
recognises the \daemon{pickup} component processing a mail sets
client\_hostname to \texttt{localhost} and client\_ip to
\texttt{127.0.0.1}, because the mail originates on the local server.  If
the new rule deliberately uses the architecture's overlapping rules feature
the priority field needs to be set, on this rule and possibly others; the
priority field may be needed on unintentionally overlapping rules too, but
that is more difficult to determine.  Finally, the restriction\_name field
should be set if the rule's action is \action{DELIVERY\_REJECTED}; the name
of the restriction should be clear from the content of the log line.

A program is provided with \parsername{} to ease the process of creating
new rules from unrecognised log lines, based on the algorithm developed by
Risto Vaarandi for his \acronym{SLCT}~\cite{slct-paper}.  The differences
between the two algorithms will be outlined as part of the explanation
below.  The core of the \acronym{SLCT} algorithm is quite simple: programs
generally create log lines by substituting variable words into a fixed
pattern, and analysis of the frequency with which each word occurs can be
used to determine whether the word is variable or part of the fixed
pattern.  This classification can be used to group similar log lines and
generate a regex to match each group of log lines.  The algorithm has five
steps:

\begin{description}

    \item [Pre-process the file.]  The modified algorithm begins by
        leveraging the knowledge gained from writing rules and developing
        \parsername{}; it performs many substitutions on the input log
        lines, replacing commonly occurring variable terms (e.g.\ email
        addresses, IP addresses, the standard start of rejection messages)
        with keywords described in \sectionref{regex components}.  The
        purpose of this step is to utilise existing knowledge to create
        more accurate regexes; it replaces many variable words with fixed
        words, improving the subsequent classification of words as fixed or
        variable.  Regex metacharacters in the log line will be escaped, to
        avoid generating invalid or incorrect regexes.  The altered log
        lines are written to a temporary file, which the next stage will
        use instead of the original input file.

        In the original algorithm the purpose of the pre-processing stage
        was to reduce the memory consumption of the program.  In the first
        pass it generates a hash~\cite{hash-functions}\glsadd{hash} (from a
        small range of values) for each word of each log line, incrementing
        a counter for each hash.  The counters will be used in the next
        pass to filter out words.

    \item [Calculate word frequencies.]  The position of words within a log
        line is important: a word occurring in two log lines does not
        indicate similarity unless it occupies the same position in both
        log lines.  If a variable term substituted into a log line contains
        spaces, it will appear to the algorithm as more than one word.
        This will alter the position of subsequent words, so a word
        occurring in different positions in two log lines \textit{may\/}
        indicate similarity, but the algorithm does not attempt to deal
        with this possibility.  The modified algorithm maintains a counter
        for each \texttt{(word, word's position within the log line)}
        tuple, incrementing it each time that word occurs in that position.

        The original algorithm hashes each word and checks that result's
        counter from the previous pass: if the word's hash does not have a
        high frequency, the word itself cannot have a high frequency, so it
        must be variable and does not need a counter maintained for the
        \texttt{(word, word's position within the log line)} tuple,
        reducing the number of counters required and thus the program's
        memory consumption.  This reduces the number of counters maintained
        in this step, reducing memory requirements at the cost of increased
        CPU usage.

        As time goes by, the amount of memory typically available to a
        program or algorithm increases, and the need to reduce memory
        requirements correspondingly decreases, so the modified algorithm
        omits the hashing step and maintains counters for all tuples.  Most
        of the infrequently occurring words will have been substituted with
        keywords during the first step, vastly reducing the number of
        tuples to maintain counters for; the original algorithm does not
        have the detailed knowledge leveraged by the modified algorithm,
        because it is a generic tool.

    \item [Classify words based on their frequency.]  The frequency of each
        \texttt{(word, word's position within the log line)} tuple is
        checked: if its frequency is greater than the threshold supplied by
        the user (1\% of all log lines is generally a good starting point),
        it is classified as a fixed word, otherwise it is classified as a
        variable term.  If a variable term appears sufficiently often it
        will be misclassified as a fixed term, but that should be noticed
        by the user when reviewing the new regexes, and will be obvious
        when the new rules do not recognise some log lines they are
        expected to.  Variable terms are replaced by \texttt{.+} to match
        one or more of any character.

    \item [Build regexes.]  The words are reassembled to produce a regex
        matching the log line, and a counter is maintained for each regex.
        Contiguous sequences of \texttt{.+} in the newly assembled regexes
        are collapsed to a single \texttt{.+}; any resulting duplicate
        regexes and their counters are combined.  If a regex's counter is
        lower than the threshold supplied by the user the regex is
        discarded; this second threshold is independent of the threshold
        used to differentiate between fixed and variable words, but once
        again 1\% of log lines is a good starting point.

    \item [Test the new regexes.]  The final step replaces keywords in the
        new regexes in the same way as \parsername{} does, and compiles
        each regex to check they are valid.  A match will be attempted
        between all of the new regexes and each of the original unparsed
        log lines (not the pre-processed log lines); the user will be
        warned if a log line is not matched by any regex, or if a log line
        is matched by more than one regex.  The number of log lines each
        regex matches is counted, as is the number of log lines matched by
        all regexes, though a log line is counted once only, even if
        matched by more than one regex.

        This step is not performed by the original algorithm.

\end{description}

The new regexes are displayed for the user to add to the ruleset, either as
new rules or merged into the regexes of existing rules; also displayed are
the number of unrecognised log lines each regex was expected to match, and
the number it actually matched, to help the user notice problems in the new
regexes.  Discarding regexes will result in some of the unrecognised log
lines not being matched; when the ruleset has been augmented with the new
regexes, the original log files should be parsed again, and any remaining
unparsed log lines used as input to this utility.

This utility is not expected to create perfect regexes, but it greatly
reduces the effort required to deal with unrecognised log lines.  The
regexes it generates will be self-contained: a parser that relies on using
cascaded parsing would require a modified algorithm, perhaps by replacing
the pre-processing stage with one that applies existing cascading rules to
each log line, and uses the resulting modified log lines for the remainder
of the algorithm.

\subsection{Regex Components}

\label{regex components}

Each rule's regex will have keywords expanded when the ruleset is loaded,
for several reasons:

\begin{itemize}

    \item It simplifies both reading and writing of regexes and helps to
        make each regex largely self-documenting.  For example, the meaning
        of \_\_CLIENT\_HOSTNAME\_\_ is immediately clear, whereas its
        expansion \verb!(?:unknown|(?:[-.\w]+))! needs to be deciphered
        each time it is encountered.

    \item It avoids needless repetition of complex regex components, and
        allows the components to be corrected or improved in one location.
        For example, \_\_SENDER\_\_ is used in 68 rules; if a mistake is
        discovered in it the mistake only needs to be corrected in one
        place.

    \item It enables automatic extraction and saving of captured data.  The
        regex snippets use Perl 5.10's named capture buffers~\cite{perlre}
        to capture data, so the mapping between captures and fields does
        not need to be explicitly specified by the rule.

\end{itemize}

To improve efficiency, the keywords are expanded and every rule's regex is
compiled before attempting to parse the log file, otherwise every regex
would be recompiled each time it was used, resulting in a large, data
dependent slowdown, as described in \sectionref{Caching compiled regexes}.
Most of the keywords are named after the fields in the connections or
results tables they populate: \_\_CLIENT\_HOSTNAME\_\_, \_\_CLIENT\_IP\_\_,
\_\_DELAY\_\_, \_\_DELAYS\_\_, \_\_ENHANCED\_STATUS\_CODE\_\_,
\_\_HELO\_\_, \_\_MESSAGE\_ID\_\_, \newline{} \_\_QUEUEID\_\_,
\_\_RECIPIENT\_\_, \_\_SENDER\_\_, \_\_SERVER\_HOSTNAME\_\_,
\_\_SERVER\_IP\_\_, \_\_SIZE\_\_, and \_\_SMTP\_CODE\_\_.

The other keywords need more explanation:

\begin{eqlist}

    \squeezeitems{}

    \item [\_\_CHILD\_\_]  The queueid of a child mail; see
        \sectionref{Re-injected mails}.

    \item [\_\_COMMAND\_\_]  All \acronym{SMTP} commands.

    \item [\_\_CONN\_USE\_\_]  How many times the connection was reused;
        Postfix tries to reuse connections whenever possible to reduce the
        load on both the sending and receiving servers.

    \item [\_\_DATA\_\_]  This snippet is special: it does not match
        anything by itself, so it must be followed by a pattern written by
        the rule author, but it captures whatever is matched by that
        pattern.  For example, \verb!/(__DATA__connection! \newline{}
        \tab{}\verb!(?:refused|reset by peer))/! matches either
        ``connection refused'' or ``connection reset by peer'' and causes
        it to be saved to the data field of that log line's result.

    \item [\_\_PID\_\_]  The \acronym{pid} of a \daemon{smtpd} process that
        dies or is killed; see the \action{SMTPD\_DIED} action in
        \sectionref{actions in detail in implementation}.

    \item [\_\_RESTRICTION\_START\_\_]  Matches the standard information
        Postfix includes at the start of most log lines resulting from
        rejecting a delivery attempt.

    \item [\_\_SHORT\_CMD\_\_]  Postfix sometimes logs \acronym{SMTP}
        commands in a short, single word form; this snippet matches all of
        those, except \texttt{DATA}, which typically has a more specific
        rule.  Priorities could have been used instead of excluding
        \texttt{DATA}.


\end{eqlist}

Some similarity exists between regex components and cascaded parsing: each
regex component resembles a rule that recognises part of a log line and
consumes it, leaving the remainder to be recognised by other components or
cascaded rules.  The major difference between the two is that regex
components are explicitly used by the author of the ruleset, whereas
cascaded parsing would be dynamically applied by the framework.

\subsection{Rule Conditions}

\label{rule conditions in implementation}

Rule conditions as part of the architecture have already been documented in
\sectionref{rule conditions in architecture}, and they can be very complex
and difficult to evaluate.  In contrast, \parsername{} uses quite simple
rule conditions: each rule has a \texttt{program} attribute that specifies
the Postfix component whose log lines it recognises, and each log line
contains the name of the Postfix component that produced it; when trying to
recognise log lines, the framework will only use rules where the two are
equal.  This avoids needlessly trying rules that will not recognise the log
line, or worse, might recognise it unintentionally.  In addition the
framework supports generic rules, whose program attribute is ``*''; these
will be used if none of the program-specific rules recognise the log line.
If none of the rules are successful the framework will warn the user,
informing them that they need to augment their ruleset, and alerting them
that the results stored in the database may be incomplete because
\parsername{} failed to recognise some log lines.

\subsection{Overlapping Rules}

\label{overlapping rules in implementation}

The advantages and difficulties of overlapping rules have already been
addressed in \sectionref{overlapping rules in architecture} and will not be
repeated here.  \parsername{} does not try to detect overlapping rules;
that responsibility is left to the author of the rules.  A mechanism is
provided for ruleset authors to specify the order that overlapping rules
will be tried in: the priority field in each rule.  Negative priorities may
be useful for catchall rules.

Detecting overlapping rules is difficult, but the following approaches may
be helpful:

\begin{itemize}

    \item Sort rules by program and regex e.g.\ with an \acronym{SQL} query
        similar to:                                     \newline{}
        \verb!SELECT program, regex!                    \newline{}
        \verb!    FROM rules!                           \newline{}
        \verb!    ORDER BY program, regex;!             \newline{}
        The rules are sorted first by program, then by regex, because rules
        cannot overlap if their programs are different.  Note that this
        query does not properly deal with generic rules whose program is
        \texttt{*}; those rules will be used on all log lines that have not
        been recognised by program-specific rules.

        Inspect the list to see if any pair of regexes look suspiciously
        similar; overlapping regexes will often lie beside one another when
        sorted.

    \item Compare the results of parsing using different rule orderings, as
        described in \sectionref{rule ordering for efficiency}.  Parse
        several log files using optimal ordering, then dump a textual
        representation of the rules, connections, and results tables.
        Repeat with shuffled and reversed ordering, starting with a fresh
        database.  If the ruleset does not have overlapping rules the
        tables from each run will be identical; differences indicate
        overlapping rules.  The rules that overlap can be determined by
        examining the differences in the tables: each result contains a
        reference to the rule that created it, which will change if that
        rule overlaps with another.  Unfortunately this method cannot prove
        the absence of overlapping rules; it can detect overlapping rules,
        but only if the log files have log lines that are recognised by
        more than one rule.

\end{itemize}

\input{logparser-complications}

\section{Limitations And Possible Improvements}

\label{limitations and improvements in implementation}

Every piece of software suffers from some limitations, and almost always
has room for improvement.  Below are the limitations and possible
improvements that have been identified in \parsername{}.

\label{logging helo}

\begin{enumerate}

    \item Each new Postfix release requires writing new rules or modifying
        existing rules to cope with the new or changed log lines.
        Similarly, using a new \acronym{DNSBL}, a new policy server, or new
        administrator-defined rejection messages also requires new rules.

    \item The hostname used in the HELO command is not logged if the
        incoming delivery attempt is successful.  Configuring Postfix to
        log the HELO hostname for accepted mails is relatively simple;
        create a restriction that is guaranteed to warn for every accepted
        mail, as follows:

        \begin{enumerate}

            \item Create \texttt{/etc/postfix/log\_helo.pcre}
                containing:\newline{}
                \tab{}\texttt{/\^/~~~~WARN~Logging~HELO}

            \item Modify \texttt{smtpd\_data\_restrictions} in
                \texttt{/etc/postfix/main.cf} to contain:\newline{}
                \tab{}\texttt{check\_helo\_access~pcre:/etc/postfix/log\_helo.pcre}

        \end{enumerate}

        Although \texttt{smtpd\_helo\_restrictions} seems like the natural
        place to log the HELO hostname, when it is evaluated for the first
        recipient there will not yet be a queueid allocated for the
        delivery attempt, so the log line will be associated with the
        connections rather than the mail.  A queueid is guaranteed to have
        been allocated when the DATA command has been reached, and thus the
        queueid will be logged by any restrictions taking effect in
        \texttt{smtpd\_data\_restrictions}, and the log line can be
        associated with the correct mail.  Specifying a HELO-based
        restriction in \texttt{smtpd\_data\_restrictions} does not cause
        any problems; Postfix will perform the check correctly.

        Logging the HELO hostname in this fashion also partially prevents
        the complication described in \sectionref{Mail deleted before
        delivery is attempted} from occurring, but only when the mail has a
        single recipient.  When a mail has a single recipient address it
        will be logged, but when a mail has multiple recipients no
        addresses are logged.

    \item \parsername{} will not detect that it is parsing the same log file
        twice, resulting in the database containing duplicate entries.

    \item \parsername{} does not distinguish between log files produced by
        different servers when parsing; all results will be saved to the
        same database.  This may be viewed as an advantage, because log
        files from different servers can be combined in the same database,
        or it may be viewed as a limitation because it is impossible to
        distinguish between log files from different servers in the same
        database.  If the results of parsing log files from different
        servers must remain separate, \parsername{} can easily be
        instructed to use a different database.

    \item Further complications may arise when parsing log files, and
        \parsername{} will need to be modified to deal with them.

    \item \parsername{} does not limit the size of the database, which will
        grow without bounds unless the user deletes connections and results
        from it.  This is both a limitation and a benefit: the benefit is
        that data will never be unexpectedly deleted, but the limitation is
        that the user must manage the size of the database.

\end{enumerate}


\section{Summary}

This chapter has presented \parsername{}, the parser implemented for this
project, beginning with the assumptions made during its development.  A
simplified flowchart shows the most common paths taken through Postfix and
\parsername{}, accompanied by a description of the stages and transitions.
A database provides storage for rules and for data gathered from log files;
any further use of that data is dependent on a clear understanding of the
database schema, so the role of the database schema as an \acronym{API} is
described, followed by a diagram and a detailed description of the database
schema.  The framework is documented next, including the steps it takes
during initialisation, the parsing process, and the conveniences it offers
to users.  The performance data collected by the framework is described, as
are the ways in which various optimisations can be disabled to demonstrate
their effect, and the debugging options the framework provides.  The
implementation of actions in \parsername{} is documented, including how
frequently each action is specified by rules, and brief descriptions of why
some actions are more popular than others.  All the actions that are part
of \parsername{} are described in detail, followed by the process of adding
a new action to the parser.  The final component of the architecture, the
rules, is also the most visible component, and its implementation is
examined in detail with: a sample rule and an explanation of how every
field in that rule is used; a description of how to add new rules and
determine the values that should be used for each field; an explanation of
the algorithm used by the utility that creates new regexes from unparsed
log lines, and how it differs from the original algorithm it is based on.
How \parsername{} uses rule conditions and overlapping rules is discussed,
accompanied by a description of the regex snippets the framework provides
to simplify the regexes used in rules.  The many complications and
difficulties encountered while writing \parsername{}, and the solutions
developed to overcome them, are documented in depth; also documented are
how solutions interact, and which action or actions each solution is
implemented in.  This chapter concludes with a list of the limitations
identified in \parsername{}.  The next chapter will evaluate this
implementation, examining both \parsernames{} efficiency and the coverage
it achieves when parsing log files.
