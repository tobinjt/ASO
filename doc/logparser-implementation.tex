\chapter{Postfix Parser Implementation}

\label{Postfix Parser Implementation}

XXX WRITE AN INTRODUCTION\@.

XXX UPDATE REFERENCES WHEN ARCHITECTURE AND IMPLEMENTATION ARE MORE FLESHED
OUT\@.  XXX WHEN FINISHED, CHECK ALL REFERENCES TO ENSURE THEY POINT AT THE
CORRECT CHAPTER\@.

XXX FIGURE OUT WHEN TO USE ACTIONS AND WHEN TO USE ALGORITHM, ETC\@.

XXX THIS MIGHT BE USEFUL SOMEWHERE\@:

XXX SHOULD I SAY ``cleaned up'' EVERYWHERE OR NOT\@?

Whereas the rules are quite simple and each rule is completely independent
of the other rules, the algorithm is significantly more complicated and
highly internally interdependent.  The algorithm deals with all the
complications of parsing, the eccentricities and oddities of Postfix log
files, presenting the resulting data in a normalised, simple to use
representation.  The algorithm's task is to follow the journey each mail
takes through Postfix, combining the data extracted by rules into a
coherent whole, saving it in a useful and consistent form, and performing
housekeeping duties.

Please refer to \sectionref{parser design} for a discussion of why the
rules, actions, and framework have been separated in the parser's design.
In this section algorithm can be taken to mean the combination of framework
and actions.

The intermingling of log entries from different mails immediately rules out
the possibility of handling each mail in isolation; the parser must be able
to handle multiple mails in parallel, each potentially at a different stage
in its journey, without any interference between mails --- except in the
minority of cases where intra-mail interference is required.  The best way
to implement this is to maintain state information for every unfinished
mail and manipulate the appropriate mail correctly for each log line
encountered.

There is a similarity between this design and the event-driven programming
paradigm commonly used in GUI programs, where one part of the program
responds to events (mouse clicks in a GUI program, log lines being matched
in the parser) and invokes the correct action.


\section{Assumptions}

\parsername{} makes a small number of (hopefully safe and reasonable)
assumptions:

\begin{itemize}

    \item The log files are whole and complete: nothing has been removed,
        either deliberately or accidentally (e.g.\ log file rotation gone
        awry, file system filling up, logging system unable to cope with
        the volume of log messages).  On a well run system it is extremely
        unlikely that any of these problems will arise, though it is of
        course possible, particularly when suffering a deluge of spam or a
        mail loop.

    \item Postfix logs enough information to make it possible to accurately
        reconstruct the actions it has taken.  There are several heuristics
        used when parsing; see
        \sectionref{identifying-bounce-notifications},
        \sectionref{aborted-delivery-attempts} and \sectionref{pickup
        logging after cleanup} for details.

    \item The Postfix queue has not been tampered with, causing unexplained
        appearance or disappearance of mail.  This may happen if the
        administrator deletes mail from the queue without using
        \daemon{postsuper}, or if there is filesystem corruption.

\end{itemize}

In some ways this task is similar to reverse engineering or replicating a
black box program based solely on its inputs and outputs.  There are
advantages to treating Postfix as a black box during parser development:

\begin{itemize}

    \item Reading and understanding the source code would require a
        significant investment of time: Postfix 2.5.5 has 17MB of source
        code.  

    \item The parser is developed using real world log files rather than
        the idealised log files someone would naturally envisage reading
        the source code.

    \item The source code cannot accurately communicate the variety of
        orderings in which log lines are written to the log file, as
        process scheduling independently interferes with logging and other
        processing.

    \item The parser acts as a second source of information about Postfix's
        operation, using information gathered from empirical evidence.  A
        separate project could compare the empirical knowledge inherent in
        the parsing algorithm with the documentation and source code of
        Postfix to see how closely the two agree.

\end{itemize}



\section{Database Schema}
\label{database schema}

XXX ADD A DIAGRAM OF THE DATABASE SCHEMA\@.

The database is an integral part of the parser presented here: it stores
the rules and the data gleaned by applying those rules to Postfix log
files.  Understanding the database schema is important in understanding the
actions of the parser, and essential to developing further applications
which utilise the data gathered.

The database schema can be conceptually divided in two: the rules used to
parse log files, and the data saved from the parsing of log files.  Rules
have the fields required to parse the log lines, extract data to be saved,
and specify the action to be executed; they also have several fields that
aid the user in understanding the meaning of the log lines parsed by each
rule.  The rules are described in detail in \sectionref{rules in
implementation} but the fields are covered in \sectionref{rule attributes}.

The data saved from parsing the log files is also divided into two tables
as described below: connections and results.  The connections table
contains a row for every mail accepted and every connection where there was
a rejection; the individual fields will be described in
\sectionref{connections table}.  There will be at least one row in the
results table for each row in the connections table; the fields will be
covered in detail in \sectionref{results table}.

An important but easily overlooked benefit of storing the rules in the
database is the link between rules and results: if more information is
required when examining a result, the rule that produced the result is
available for inspection: each result references the rule that created it.
There is no ambiguity about which rule resulted in a particular result,
eliminating one potential source of confusion.

A clear, comprehensible schema is essential when using the extracted data;
it is more important when using the data than when storing it, because
storing the data is a write-once operation, whereas utilising the data
requires frequent searching, sorting, and manipulation of the data to
produce customised reports or statistics.

\subsection{Using A Database To Provide An Application Programming Interface}

\label{database as API}

The database populated by this program provides a simple interface to
Postfix log files.  Although the interface is a database schema rather than
a set of functions provided by a library, it performs the same function as
any other \acronym{API}: it provides a stable interface, allowing code on
either side of the interface to be changed without adverse effects, as long
as the interface is maintained.  The parser can be improved to handle
additional complications, support earlier or later releases of Postfix,
bugs can be fixed or limitations removed.  Programs that use the database
can range from the simple examples in \sectionref{motivation} to far more
complex data mining tools.

Using a database simplifies writing programs that need to interact with the
data in several ways:

\begin{enumerate}

    \item Facilities are provided to access databases from most programming
        languages, allowing a developer to access the data gathered using
        their preferred programming language, rather than being restricted
        to the language the parser is written in.  It is often possible to
        write an interface layer allowing code written in one language to
        be used in another language, but this greatly increases the effort
        required to use the parser.

    \item Databases provide complex querying and sorting functionality to
        the user without requiring large amounts of programming.  All
        databases provide a program, of varying complexity and
        sophistication, that can be used for ad hoc queries with minimal
        investment of time.

    \item Databases are easily extensible, e.g.:

        \begin{itemize}

            \item New columns can be added to the tables used by the
                program, using DEFAULT clauses or TRIGGERS to populate
                them.

            \item A VIEW gives a custom arrangement of data with minimal
                effort.

            \item If the database supports it, access can be granted on a
                fine-grained basis, e.g.\ allowing the finance department
                to produce invoices, the helpdesk to run limited queries as
                part of dealing with support calls, and the administrators
                to have full access to the data.

            \item Triggers can be written to perform actions when certain
                events occur.  In pseudo-\acronym{SQL}\@:

\begin{verbatim}
CREATE TRIGGER ON INSERT INTO results
    WHERE sender = 'boss@example.com'
        AND postfix_action = 'REJECTED'
    SEND PANIC EMAIL TO 'postmaster@example.com';
\end{verbatim}

            \item Other tables can be added to the database, e.g.\ to cache
                historical, summary, or computed data.

        \end{itemize}


    \item \acronym{SQL} is reasonably standard and many people will already
        be familiar with it; for those unfamiliar with it there are lots of
        readily available resources from which to learn (a good
        introduction to \acronym{SQL} can be found at
        \urlLastChecked{http://philip.greenspun.com/sql/}{2009/02/23},
        others are
        \urlLastChecked{http://www.w3schools.com/sql/default.asp}{2009/02/23},
        \urlLastChecked{http://sqlcourse.com/}{2009/02/23}).  Although
        every vendor implements a different dialect of \acronym{SQL}, the
        basics are the same everywhere (analogous to the overall
        similarities and minor differences amongst Irish English, British
        English, American English, and Australian English).  Depending on
        the database in use there may be tools available that reduce or
        remove the requirement to know \acronym{SQL}; several are available
        for \gls{SQLite}, the default database used by the parser
        implementation.

\end{enumerate}

Storing the results in a database will also increase the efficiency of
using those results, because the log files need only be parsed once rather
than each time the data is used; indeed the results may be used by someone
with no access to the original log files.

\subsection{Rules Table}

\label{rule attributes}

Rules are discussed in detail in \sectionref{rules in implementation}, but
the structure of the rules table is covered here.  Rules are created by the
user, not the parser, and will not be modified by the parser (except for
the hits and hits\_total fields).  Rules parse the individual log lines,
extracting data to be saved in the connections and results tables, and
specifying the action to take for that log line.

The following attributes are defined for each rule:

\begin{eqlist}

    \item [id] A unique identifier for each rule that other tables can use
        when referring to a specific rule.

    \item [name] A short name for the rule.

    \item [description] Something must have occurred to cause Postfix to
        log each line (e.g.\ a remote client connecting causes a connection
        line to be logged).  This field describes the event causing the log
        lines this rule matches.

    \item [restriction\_name] The restriction that caused the mail to be
        rejected.  Only applicable to rules that have an action of
        \texttt{REJECTION}, other rules should have an empty string.

    \item [postfix\_action] The action Postfix must have taken to generate
        this log line; the possible actions are described later in
        \sectionref{list of postfix actions in implementation}.

    \item [program] The program (\daemon{smtpd}, \daemon{qmgr}, etc.) whose
        log lines the rule applies to.  This avoids needlessly trying rules
        that will not match the log line, or worse, might match
        unintentionally.  Rules whose program is \texttt{*} will be tried
        against any log lines that are not parsed by program specific
        rules.

    \item [regex] The regex to match the log line against.  The regex will
        have several keywords expanded when the rules are loaded: this
        simplifies reading and writing rules; avoids needless repetition of
        complex regex components; allows the components to be corrected
        and/or improved in one location; enables automatic extraction and
        saving of data; and makes each regex largely self-documenting.  For
        efficiency the keywords are expanded and every rule's regex is
        compiled before attempting to parse the log file --- otherwise each
        regex would be recompiled each time it was used, resulting in a
        large, data dependent slowdown.  Efficiency concerns are discussed
        in \sectionref{parser efficiency}, with caching compiled regexes
        covered in \sectionref{Caching compiled regexes}.

    \item [connection\_data] Sometimes rules need to save data that is not
        present in the log line: e.g.\ setting \texttt{client\_ip} when a
        mail is being delivered to another server.  The format is:
        \newline{} \tab{} \texttt{ client\_hostname = localhost,}
        \newline{} \tab{} \tab{} \texttt{client\_ip
        = 127.0.0.1} \newline{} i.e.\ semi-colon or comma separated
        assignment statements, with the variable name on the left and the
        value on the right hand side.\footnote{Commas and semi-colons
        cannot be escaped and thus cannot be used.  This feature is
        intended for use with small amounts of data rather than large
        amounts in any one rule, so dealing with escape sequences was
        deemed unnecessary.}  Any field in the connections table can be set
        in this way.

    \item [result\_data] The result table equivalent of
        \texttt{connection\_data}.

    \item [action] The action that will be invoked when this rule matches a
        log line; a full list of actions and the parameters they are
        invoked with can be found in \sectionref{actions in detail in
        implementation}.

    \item [hits] This counter is maintained for every rule and incremented
        each time the rule successfully matches.  At the start of each run
        the parser sorts the rules in descending order of hits, and at the
        end of the run it updates every rule's hits field in the database.
        Assuming that the distribution of log lines is reasonably
        consistent across log files, rules matching more commonly occurring
        log lines will be tried before rules matching less commonly
        occurring log lines, reducing the parser's execution time.  Rule
        ordering for efficiency is discussed in \sectionref{rule ordering
        for efficiency}.

    \item [hits\_total] The total number of hits for this rule over all
        runs of the parser.

    \item [priority] This is the user-configurable companion to hits: when
        the list of rules is sorted, priority overrides hits.  This allows
        more specific rules to take precedence over more general rules
        (described in \sectionref{overlapping rules in architecture}).

%    \item [cluster\_group] A reference to the \texttt{cluster\_group}
%        table.  That table is used by the Decision Tree algorithm
%        described in a separate document.

\end{eqlist}

\subsection{Connections Table}

\label{connections table}

Every accepted mail and every connection where there was a rejection will
have a single entry in the connections table containing the following
fields:

\begin{eqlist}

    \item [id] This field uniquely identifies the row.

    \item [server\_ip] The \acronym{IP} address (IPv4 or IPv6) of the
        server: the local address when receiving mail, the remote address
        when sending mail.

    \item [server\_hostname] The hostname of the server, it will be
        \texttt{unknown} if the \acronym{IP} address could not be resolved
        to a hostname via DNS\@.

    \item [client\_ip] The client \acronym{IP} address (IPv4 or IPv6): the
        remote address when receiving mail, the local address when sending
        mail.

    \item [client\_hostname] The hostname of the client, it will be
        \texttt{unknown} if the \acronym{IP} address could not be resolved
        to a hostname via DNS\@.

    \item [helo] The hostname used in the HELO command.  The HELO hostname
        occasionally changes during a connection, presumably because spam
        or virus senders think it is a good idea.  By default Postfix only
        logs the HELO hostname when it rejects an \acronym{SMTP} command,
        but it is quite simple to rectify this, as described in
        \sectionref{logging helo}.

    \item [queueid] The queueid of the mail if the connection represents an
        accepted mail, or \texttt{NOQUEUE} otherwise.

    \item [start] The timestamp of the first log line, in seconds since the
        epoch.\glsadd{Epoch}

    \item [end] The timestamp of the last log line, in seconds since the
        epoch.

\end{eqlist}

\subsection{Results Table}

\label{results table}

Every log line a row in the results table, unless the
\texttt{postfix\_action} field of the rule which parsed it was INFO or
IGNORE\@.  Each row in the results table is associated with a single
connection, and there may be many results per connection.

\begin{eqlist}

    \item [connection\_id] The id of the row in the connections table this
        result is associated with.

    \item [rule\_id] The id of the entry in the rules table that matched
        the log line and created this result.

    \item [id] A unique identifier for this result.

    \item [warning] Postfix can be configured to log a warning instead of
        enforcing a restriction that would reject an \acronym{SMTP} command
        --- a mechanism that is quite useful for testing new restrictions.
        This field will be 1 if the log line parsed was a warning rather
        than a real rejection, or 0 for a real rejection.  This field
        should be ignored if the result is not a rejection, i.e.\ the
        action field of the associated rule is not \texttt{REJECTION}.

    \item [smtp\_code] The \acronym{SMTP} code associated with the log
        line.  In general an \acronym{SMTP} code is only present in a log
        line for a rejection or final delivery; results initially missing
        an \acronym{SMTP} code will duplicate the \acronym{SMTP} code of
        other results in that connection.  Some final delivery log lines do
        not contain an \acronym{SMTP} code: in those cases the code is
        specified by the rule, based on the success or failure represented
        by the log line.

    \item [enhanced\_status\_code] The enhanced status code~\cite{RFC3463}
        is similar to the \acronym{SMTP} code, but is intended to be parsed
        by mail clients so that error messages can be clearly conveyed to
        the user.  Enhanced status code support was added to Postfix in
        version 2.3; logs generated by previous versions will not contain
        any enhanced status codes.

    \item [sender] The sender's email address.  Because multiple mails may
        be delivered during one connection, there may be different sender
        addresses in the results for one connection; however there should
        not be different sender addresses in the results for one mail.
        Mails sent over the same connection can be distinguished by their
        queueid.

    \item [recipient] The recipient address; there may be multiple
        recipient addresses per mail or connection.

    \item [size] The size of the mail; it will only be present for
        delivered mails.

    \item [delay] How long the mail was delayed for while it was being
        delivered.  This will only be present for delivered mails.

    \item [delays] More details information about how long the mail was
        delayed for while it was being delivered  This will only be present
        for delivered mails..

    \item [message\_id] The message-id of the accepted mail, or
        \texttt{NULL} if no mail was accepted.

    \item [data] A field available for anything not covered by other
        fields, e.g.\ the rejection message from a \acronym{DNSBL}\@.

    \item [timestamp] The time at which the log line was logged, in seconds
        since the epoch.\glsadd{Epoch}

\end{eqlist}



\section{Rules}

\label{rules in implementation}

XXX NEED AN INTRODUCTION\@.


\subsection{Overlapping Rules}

\label{overlapping rules in architecture}

XXX IS THERE ANY PREVIOUS RESEARCH IN THIS AREA\@?

XXX DFA COMPARISON\@.

The parser does not try to detect overlapping rules; that responsibility is
left to the author of the rules.  Unintentionally overlapping rules lead to
inconsistent parsing and data extraction because the order in which rules
are tried against each line may change between log files, and the first
matching rule wins.  Overlapping rules are frequently a requirement,
allowing a more specific rule to match some log lines and a more general
rule to match the majority, e.g.\ separating \acronym{SMTP} delivery to
specific sites from \acronym{SMTP} delivery to the rest of the world.  The
algorithm provides a mechanism for ordering overlapping rules: the priority
field in each rule (defaults to zero).  Negative priorities may be useful
for catchall rules.

Detecting overlapping rules is difficult, but the following approaches may
be helpful:

\begin{itemize}

    \item Sort by regex and visually inspect the list, e.g.\ with
        \acronym{SQL} similar to: \textbf{select regex from rules order by
        regex;}

    \item Compare the results of parsing using sorted, shuffled, and
        reversed rules (described in \sectionref{rule ordering for
        efficiency}).  Parse several log files using optimal ordering, then
        dump a textual representation of the rules, connections, and
        results tables.  Repeat with shuffled and reversed ordering,
        starting with a fresh database.  If there are no overlapping rules
        the tables from each run will be identical; differences indicate
        overlapping rules.  The rules that overlap can be determined by
        examining the differences in the tables: each result contains a
        reference to the rule which created it, if the references differ
        between runs the two rules referenced in the differing results
        overlap.  Unfortunately this method cannot prove the absence of
        overlapping rules; it can detect overlapping rules, but only if
        there are log lines in the log files that are matched by more than
        one rule.

\end{itemize}

\subsection{Example Rule}

\label{example rule in implementation}

This example rule matches the message logged by Postfix when it rejects
mail from a sender address because the domain in the sender address does
not have an A, AAAA, or MX DNS entry, i.e.\ mail could not be delivered
to the sender's address (for full details
see~\cite{reject-unknown-sender-domain}).

Example log line matched by this rule:

% RFC 3330 says that 192.0.2.0/24 is reserved for example use.

\begin{verbatim}
NOQUEUE: reject: RCPT from example.com[192.0.2.1]: 550 5.1.8
  <foo@example.com>: Sender address rejected: Domain not found;
  from=<foo@example.com> to=<info@example.net>
  proto=SMTP helo=<smtp.example.com>
\end{verbatim}

% do not reformat this!
\begin{tabular}[]{ll}

\textbf{Field}      & \textbf{Value}                                    \\
name                & Unknown sender domain                             \\
description         & We do not accept mail from unknown domains        \\
restriction\_name   & reject\_unknown\_sender\_domain                   \\
postfix\_action     & REJECTED                                          \\
program             & \daemon{smtpd}                                    \\
regex               & \verb!^__RESTRICTION_START__ <(__SENDER__)>: !    \\
                    & \verb!Sender address rejected: Domain not found;! \\
                    & \verb!from=<\k<sender>> to=<(__RECIPIENT__)> !    \\
                    & \verb!proto=E?SMTP helo=<(__HELO__)>$!            \\
result\_data        &                                                   \\
connection\_data    &                                                   \\
action              & DELIVERY\_REJECTED                                \\
hits                & 0                                                 \\
hits\_total         & 0                                                 \\
priority            & 0                                                 \\
%cluster\_group      & 400                                               \\

\end{tabular}

\vspace{1em}

Fields matched by the regex will be automatically saved to the results and
connections tables.

\begin{description}

    \item [name, description, restriction\_name, and postfix\_action:] are
        not \newline{} used by the parser, they serve to document the rule
        for the user's benefit.

    \item [program and regex:] If the program in the rule equals the
        program in the log line, a match using the rule's regex will be
        attempted against the log line; if the match is successful the
        action will be executed, if not the next rule will be tried.  If
        the program-specific rules do not match the log line, the parser
        will fall back to generic rules; if those rules are unsuccessful a
        warning will be issued.

    \item [action:] The action to be executed if the regex matches
        successfully.  See \sectionref{actions in detail in implementation}
        for details of the actions implemented by the parser,
        \sectionref{actions in architecture} for the role of actions in the
        parser architecture.

    \item [result\_data and connection\_data:] are used to provide XXX

    \item [hits, hits\_total, and priority:] hits and priority are used in
        ordering the rules (see \sectionref{rule ordering for efficiency});
        hits is set to the number of successful matches at the end of the
        parsing run; hits\_total is the sum of hits over every parsing run,
        but is otherwise unused by the parser.

%    \item [cluster\_group] The cluster\_group attribute is used by the
%        Decision Tree algorithm described in a separate document; the
%        parser does not use it in any way.

\end{description}


\subsection{Creating New Rules}

\label{creating new rules in implementation}

The log files produced by Postfix differ from installation to installation,
because administrators have the freedom to choose the subset of available
restrictions which suits their needs, including using different
\acronym{DNSBL} services, policy servers, or custom rejection messages.  To
facilitate parsing new log lines, the parser's design separates parsing
rules from parsing actions: adding new actions can be difficult, but adding
new rules to parse new rejection messages is trivial, and occurs much more
frequently.  A program is provided to ease the process of creating new
rules from unparsed log lines, based on the algorithm developed by Risto
Vaarandi for his \acronym{SLCT}~\cite{slct-paper}.  The differences between
the two algorithms will be outlined as part of the general explanation
below.

The core of the \acronym{SLCT} algorithm is quite simple: log lines are
generally created by substituting variable words into a fixed pattern, and
analysis of the frequency with which each word occurs can be used to
determine whether the word is variable or part of the fixed pattern.  This
classification can be used to group similar log lines and generate a regex
to match each group of log lines.

There are 4 steps in the algorithm:

\begin{description}

    \item [Pre-process the file]  The new algorithm leverages the knowledge
        gained when writing rules and performs a large number of
        substitutions on the input log lines, replacing commonly occurring
        variable terms (e.g.\ email addresses, \acronym{IP} addresses, the
        standard start of rejection messages, etc.) with regex keywords
        that the parser will expand when it loads the rule (see the
        regex entry in \sectionref{rule attributes}).  The purpose of
        this step is to utilise existing knowledge to create more accurate
        regexes; it replaces a large number of variable words with fixed
        words, improving the subsequent classification of words as fixed or
        variable.  The new log lines are written to a temporary file, which
        all subsequent stages use instead of the original input file.

        In the original algorithm the purpose of the preprocessing stage
        was to reduce the memory consumption of the program.  In the first
        pass it generated a hash~\cite{hash-functions} from a small range
        of values for each word of each log line, incrementing a counter
        for each hash.  The counters will be used in the next pass to
        filter out words: if the word's hash does not have a high
        frequency, the word itself cannot have a high frequency, so there
        is no need to maintain a counter for it, reducing the number of
        counters required and thus the program's memory consumption.

    \item [Calculate word frequencies]  The position of words within a log
        line is important: a word occurring in two log lines does not
        indicate similarity unless it occupies the same position within
        both log lines.\footnote{If a variable term within a line contains
        spaces, it will appear to the algorithm as two or more words rather
        than one.  This will alter the position of subsequent words, so a
        word occurring in different positions in two log lines
        \textit{may\/} indicate similarity, but the algorithm does not
        attempt to deal with this possibility.}  The algorithm maintains a
        counter for each \texttt{(word, word's position within the log
        line)} tuple, incrementing it each time that word occurs in that
        position.

        The original algorithm only maintains counters for words whose hash
        result from the previous step has a high frequency; this reduces
        the number of counters maintained by the algorithm, reducing the
        memory requirements of the algorithm at a cost of increased CPU
        usage.
        
        As time goes on the amount of memory typically available increases
        and the requirement to reduce memory requirements decreases, hence
        the modified algorithm omits the hashing step and maintains
        counters for all tokens. In addition most of the infrequently
        occurring words will have been substituted with keywords during the
        first step, vastly reducing the number of tuples to maintain
        counters for.

    \item [Classify words based on their frequency]  The frequency of every
        tuple \texttt{(word, word's position within the log line)} is
        checked: if its frequency is greater than the threshold supplied by
        the user (1\% of all log lines is generally a good starting point)
        it is classified as a fixed word, otherwise it is classified as a
        variable term.  If a variable term appears sufficiently often it
        will be misclassified as a fixed term, but that should be noticed
        by the user when reviewing the new regexes.  Variable terms are
        replaced by \texttt{.+}, which means to match zero or more of any
        character.  XXX SHOULD I USE \verb![\S]+! INSTEAD\@?

    \item [Build regexes]  The words are reassembled to produce a regex
        matching the log line, and a counter is maintained for each
        regex.  Contiguous sequences of \texttt{.+} in the newly
        reassembled regexes are collapsed to a single \texttt{.+}; any
        resulting duplicate regexes are combined, and their counters
        added together.  If a regex's counter is lower than the
        threshold supplied by the user the regex is discarded; this
        second threshold is independent of the threshold used to
        differentiate between fixed and variable words, but once again 1\%
        of log lines is a good starting point.  The new regexes are
        displayed for the user to add to the database, either as new rules
        or merged into the regexes of existing rules; the counter for
        each regex is also displayed, giving the user an indication of
        how many of the log lines that regex should match.  Discarding
        regexes will result in some of the log lines not being matched;
        when the rules have been augmented with the new regexes, the
        original log files should be parsed again, and any remaining
        unparsed log lines used as input to this utility.

\end{description}

XXX MERGE logs2regexes WITH check-regexes.

A second utility is also provided that reads a list of new regexes and
the input given to the first utility.  It tries to match each input log
line against each regex, counting the number of log lines that match
each regex, warning the user if an input log line is matched by more
than one regex, and additionally warning if an input log line is not
matched by any regex.  It displays a summary of how many input log lines
each regex matched, comparing it to the expected number of matches; this
provides the user with an easy method of checking if the regexes
produced by the first utility are correctly matching the input log lines
they are based upon.  A future version of this utility will also group
input log lines by regex, so the user can tweak the regexes if
required.

These utilities are not intended to create perfect regexes, but they
greatly reduce the effort required to parse new or different log lines.

\section{Actions}

\label{actions in implementation}

XXX INTRODUCTION\@.

The actions reconstruct the journey a mail takes through Postfix.  Details
of the actions available in the Postfix parser can be found in
\sectionref{actions in detail in implementation}, and \sectionref{adding
new actions in implementation} describes the process of adding new actions.

In the Postfix log parser developed for this project there are
\numberOFactions{} actions and \numberOFrules{} rules, with an uneven
distribution of rules to actions as shown in \graphref{Distribution of
rules per action}.  Unsurprisingly, the action with the most associated
rules is \texttt{DELIVERY\_REJECTED}, the action that handles Postfix
rejecting a mail delivery attempt; it is followed by \texttt{SAVE\_DATA},
the action responsible for handling informative log lines, supplementing
the data gathered from other log lines.  The third most common action is,
perhaps surprisingly, \texttt{UNINTERESTING}: this action does nothing when
executed, allowing uninteresting log lines to be parsed without causing any
effects (it does not imply that the input is ungrammatical or unparsed).
Generally rules specifying the \texttt{UNINTERESTING} action parse log
lines that are not associated with a specific mail, e.g.\ notices about
configuration files changing.  The remaining actions have only one or two
associated rules: some actions are required to address a deficiency in the
log files, or a complication that arises during parsing;  other actions
will only ever have one log line variant, e.g.\ all log lines showing that
a remote client has connected are matched by a single rule and handled by
the \texttt{CONNECT} action.

Using the \texttt{CONNECT} action as an example: it creates a new data
structure in memory for the new client connection, saving the data
extracted by the rule into it; this data will be entered into the database
when the mail delivery attempt is complete.  If a data structure already
exists for the new connection it is treated as a symptom of a bug, and the
action issues a warning containing the full contents of the existing data
structure, plus the log line that has just been parsed.

Each action is passed the same arguments:

%\begin{eqlist}[\def\makelabel#1{\bfseries#1:}]

XXX IMPROVE HOW THIS LOOKS

\begin{eqlist}

    \item [line] The log line, separated into fields:

        \begin{eqlist}

            \item [timestamp] The time the line was logged at.

            \item [host] The hostname of the server that logged the line.

            \item [program] The name of the program that logged the line.

            \item [pid] The \acronym{pid} of the program that logged the
                line.

            \item [text] The remainder of the line.

        \end{eqlist}

    \item [rule] The matching rule.

    \item [matches] The data extracted from the line by the rule's regex.

\end{eqlist}


\subsection{Actions in Detail}

\label{actions in detail in implementation}

Each action saves all data extracted by the rule's regex, if there is
enough information in the log line to identify the correct connection to
save the data to.

XXX MAKE ALL ACTION DESCRIPTIONS FLOW BETTER\@.  

\begin{description}

    \item [BOUNCE] Postfix 2.3 and subsequent versions log the creation of
        bounce messages.  This action creates a new mail if necessary; if
        the mail already exists the unknown origin flag will be removed.
        The action also marks the mail as a bounce notification.  To deal
        with complication \sectionref{Bounce notification mails delivered
        before their creation is logged} this action checks a cache of
        recent bounce mails to avoid creating bogus bounce mails when log
        lines are out of order.

    \item [CLONE] Multiple mails may be accepted on a single connection, so
        each time a mail is accepted the connection's state table entry
        must be cloned and saved in the state tables under its queueid; if
        the original data structure was used then second and subsequent
        mails would overwrite one another's data.

    \item [COMMIT] Enter the data from the mail into the database. Entry
        will be postponed if the mail is a child waiting to be tracked.
        Once entered, the mail will be deleted from the state tables.
        Deletion will be postponed if the mail is the parent of re-injected
        mail (\sectionref{Re-injected mails}).

    \item [CONNECT] Handle a remote client connecting: create a new state
        table entry (indexed by \daemon{smtpd} \acronym{pid}) and save both
        the client hostname and \acronym{IP} address.

    \item [DELETE] Deals with mail deleted using Postfix's administrative
        command \daemon{postsuper}.  This action adds a dummy recipient
        address if required, then invokes the COMMIT action to handle
        adding the mail to the database.  The complication this action
        deals with is described fully in \sectionref{Mail deleted before
        delivery is attempted}.  

    \item [DELIVERY\_REJECTED] Deals with Postfix rejecting an
        \acronym{SMTP} command from the remote client: log the rejection
        with a mail if there is a queueid in the log line, or with the
        connection if not.

    \item [DISCONNECT] Deal with the remote client disconnecting: enter the
        connection in the database, perform any required cleanup, and
        delete the connection from the state tables.  This action deals
        with aborted delivery attempts
        (\sectionref{aborted-delivery-attempts}).

    \item [EXPIRY] If Postfix has not managed to deliver a mail after
        trying for five days it will give up and return the mail to the
        sender.  When this happens the mail will not have a combination of
        Postfix programs which passes the valid combinations check (see
        \sectionref{out of order log lines}).  To ensure that the mail can
        be committed the EXPIRY action sets a flag marking the mail as
        expired; the flag later causes the valid combinations check to be
        skipped, so the mail will be committed.

    \item [MAIL\_QUEUED] This action represents Postfix picking a mail from
        the queue to deliver. This action is used for both \daemon{qmgr}
        and \daemon{cleanup} as it needs to deal with out of order log
        lines; see \sectionref{discarding cleanup log lines} for details.
        XXX EXPLAIN THIS BETTER\@.

    \item [MAIL\_TOO\_LARGE] When a client tries to send a mail larger than
        the local server accepts it will be discarded and the client
        informed of the problem.  This is handled in exactly the same way
        as the TIMEOUT action, further details are given there.

    \item [PICKUP] The PICKUP action corresponds to the \daemon{pickup}
        service dealing with a locally submitted mail.  Out of order log
        entries may have caused the state table entry to already exist (see
        \sectionref{pickup logging after cleanup}); otherwise it is
        created.  The data extracted from the log line is then saved to the
        state table entry.

    \item [POSTFIX\_RELOAD] When Postfix stops or reloads its configuration
        it kills all \daemon{smtpd} processes,\footnote{Possibly other
        programs are killed also, but the parser is only affected by and
        interested in \daemon{smtpd} processes exiting.} requiring any
        active connections to be cleaned up, entered in the database, and
        deleted from the state tables.

    \item [SAVE\_DATA] Find the correct mail based on the queueid in
        the log line, and save the data extracted by the regex to it.

    \item [SMTPD\_DIED] Sometimes a \daemon{smtpd} dies or exits
        unsuccessfully; the active connection for that \daemon{smtpd} must
        be cleaned up, entered in the database, and deleted from the state
        tables.

    \item [SMTPD\_KILLED] Sometimes an \daemon{smtpd} is killed by a
        signal; the active connection for
        that \daemon{smtpd} must be cleaned up, entered in the database,
        and deleted from the state tables.

    \item [SMTPD\_WATCHDOG] \daemon{smtpd} processes have a watchdog timer
        to deal with unusual situations --- after five hours the timer will
        expire and the \daemon{smtpd} will exit.  This occurs very
        infrequently, as there are many other timeouts that should occur
        in the intervening hours: DNS timeouts, timeouts reading data
        from the client, etc\@.  The active connection for that
        \daemon{smtpd} must be cleaned up, entered in the database, and
        deleted from the state tables.

    \item [TIMEOUT] The connection timed out so the mail currently being
        transferred must be discarded. The mail may have been accepted, in
        which case there's a data structure to dispose of, or it may not in
        which case there is not.  See
        \sectionref{timeouts-during-data-phase} for the gory details.

    \item [TRACK] Track a mail when it is re-injected for forwarding to
        another mail server; this happens when a local address is aliased
        to a remote address.  TRACK will be called when dealing with the
        parent mail, and will create the child mail if necessary. TRACK
        checks if the child has already been tracked, either by this parent
        or by another parent, and issues appropriate warnings in either
        case.

    \item [UNINTERESTING] This rule just returns successfully; it is used
        when a line needs to be parsed for completeness but does not either
        provide any useful data or require anything to be done.

\end{description}

The distribution of rules per action is shown in \graphref{Distribution of
rules per action}.

\showgraph{build/graph-action-distribution}{Distribution of rules per
action}{Distribution of rules per action}

\subsection{Postfix Actions}

\label{list of postfix actions in implementation}

XXX WHY DO I NEED HAVE postfix\_actions AT ALL\@?

XXX SEE TABLE BELOW\@: THERE IS NEARLY A 1--1 MAPPING BETWEEN action AND
postfix\_action.  NEW ACTIONS COULD BE ADDED TO FORCE A 1--1 MAPPING, E.G.
COMMIT + DISCARDED COULD BECOME A DISCARD ACTION\@.  MAIL\_QUEUED HAVING
INFO AND PROCESSING CAN PROBABLY BE DEALT WITH, THE BIGGEST DIFFICULTY WILL
BE WITH SAVE\_DATA\@.

\begin{tabular}{lll}

1 & BOUNCE & BOUNCED \\
1 & CLONE & ACCEPTED \\
3 & COMMIT & DISCARDED \\
1 & COMMIT & INFO \\
1 & CONNECT & INFO \\
1 & DELETE & DELETED \\
64 & DELIVERY\_REJECTED & REJECTED \\
1 & DISCONNECT & INFO \\
1 & EXPIRY & EXPIRED \\
1 & MAIL\_QUEUED & INFO \\
1 & MAIL\_QUEUED & PROCESSING \\
1 & MAIL\_TOO\_LARGE & DISCARDED \\
1 & PICKUP & INFO \\
2 & POSTFIX\_RELOAD & POSTFIX\_RELOAD \\
13 & SAVE\_DATA & BOUNCED \\
38 & SAVE\_DATA & INFO \\
1 & SAVE\_DATA & REJECTED \\
5 & SAVE\_DATA & SENT \\
2 & SMTPD\_DIED & IGNORED \\
1 & SMTPD\_WATCHDOG & IGNORED \\
2 & TIMEOUT & DISCARDED \\
1 & TRACK & SENT \\
42 & UNINTERESTING & IGNORED \\

\end{tabular}

postfix\_action IS USED IN A FEW ACTIONS, BUT IT COULD PROBABLY BE EASILY
REMOVED, OR MAYBE I SHOULD KEEP TRACK OF actions INSTEAD\@?

XXX IF I KEEP THESE I NEED TO EXPLAIN WHY I HAVE THEM\@.

\begin{description}

    \item [ACCEPTED] Postfix has accepted a mail, and will
        subsequently attempt to deliver it.

    \item [BOUNCED] The mail has bounced, because of a mail loop,
        delivery failure, or five day timeout.\footnote{As with
        much of Postfix's behaviour, five days is the default value
        but can be changed by the administrator if they choose.}

    \item [DELETED] The mail was deleted from the queue by an
        administrator.

    \item [DISCARDED] Postfix discarded the mail it was in the
        process of accepting, because it was either larger than the
        size limit set by the administrator, or the client timed
        out or disconnected.

    \item [EXPIRED] The mail has been in the queue for five days without
        successful delivery.  A bounce mail will be generated and sent to
        the sender address.

    \item [INFO] Represents an unspecified intermediate action that
        the parser is not interested in per se, but that does log
        useful information, supplementing other log lines.

    \item [IGNORED] An action that is not only uninteresting in
        itself, but that also provides no useful data.

    \item [POSTFIX\_RELOAD] The administrator has instructed
        Postfix to start or stop, and all \daemon{smtpd} processes
        will be terminated.  This does not negatively affect the
        log files or mail queued by Postfix for delivery.

    \item [PROCESSING] \daemon{cleanup} is processing a mail.
        XXX ADD MORE DETAIL HERE

    \item [REJECTED] Postfix rejected a command from the remote
        client, causing at least one recipient to be rejected.

    \item [SENT] Postfix has successfully sent a mail.

\end{description}

\subsection{Adding New Actions}

\label{adding new actions in implementation}

XXX HALF OF THIS CONTENT IS IN FOOTNOTES, WHICH SHOULD BE INLINED\@.

Adding new actions is not as easy as adding new rules, though care has been
taken in the parser's design and implementation to make adding new actions
as painless as possible.  The implementor writes a subroutine that accepts
the standard arguments given to actions, and registers it as an action by
calling the framework subroutine add\_actions() with the name of the new
action as a parameter.  The new action must be registered before the rules
are loaded, because it is an error for a rule to specify an unregistered
action; this helps catch mistakes made when adding new rules.  No other
work is required from the implementor to integrate the action into the
parser; all of their attention and effort can be focused on correctly
implementing their action.  The only negative aspect is that the process
involves editing the parser source code, which makes upgrading to a later
version of the parser more difficult, though by no means impossible.  If
the author of the new action wishes, they can take advantage of the
parser's object oriented implementation by subclassing it and implementing
their changes in the derived class, allowing future upgrades of the parser
with greatly reduced chance of conflicts.\footnote{The real difference
between the two approaches is where the new code is placed.  The simpler
option is to change the parser code directly, but those changes will then
have to be made to subsequent versions of the parser, and as the scope of
the changes increases so does the chance of conflict, or mistakes when
copying the action.  The more time consuming option is to write a subclass
containing the new actions and change the program that invokes the parser
so that it uses the subclass rather than the parser; the changes required
to the program invoking the parser are minor and much less likely to lead
to conflicts when upgrading to future versions of the parser.  An
alternative is to submit new actions to the author of the parser for
inclusion in future versions, resulting in two benefits: the new actions do
not need to be maintained separately, and other users of the parser can
avail of the new functionality.} The action may need to extend the list of
valid combinations described in \sectionref{out of order log lines} if the
new action creates a different set of acceptable programs, but this is
unlikely to occur, as it would require parsing log lines from Postfix
components the parser currently ignores.\footnote{The mail server used for
development does not utilise either the \daemon{lmtp} or \daemon{virtual}
delivery agents, so this parser does not have rules to handle log lines
from those components.  Adding new rules to parse those component's log
lines is a simple process, though if their behaviour differs significantly
from the \daemon{smtp} or \daemon{local} delivery agents new actions may be
required.  The mail server in question is a production mail server handling
mail for a university department; the benefit of using this server is that
the log files used exhibit the idiosyncrasies and peculiarities a mail
server in the wild must deal with, but the downside is that significantly
altering the configuration just to log messages from a different Postfix
component is not an option.}


\section{Framework}

XXX TO BE WRITTEN\@.


\section{Parser Flow Chart}

\label{flow-chart}

XXX MIGHT NEED A SMALL INTRO\@.

From the viewpoint of an individual mail passing through the parser the
experience could be summarised as:

\begin{enumerate}

    \item Mail enters the system via \acronym{SMTP} or local submission.

    \item If the mail is rejected, save all data and finish.

    \item Follow the progress of the accepted mail until it is either
        delivered, bounced, or deleted, then save all data, and finish.

\end{enumerate}

\Figureref{flow chart image} shows the most common paths the data
representing a mail can take through the parser; the complications
described in \sectionref{complications} are excluded for the sake of
clarity.

\showgraph{build/logparser-flow-chart-part-1}{Parser flow chart}{flow chart
image}

\label{mail-enters-the-system}

Everything starts off with a mail entering the system, whether by local
submission via \daemon{postdrop}, by \acronym{SMTP}, by re-injection due to
forwarding, or internally generated by Postfix.  Local submission is the
simplest case: a queueid is assigned immediately and the sender address is
logged (action: PICKUP\@; flowchart:~2).  Re-injection due to forwarding
sadly lacks explicit log lines of its own; it is explained in
\sectionref{Re-injected mails}.

Internally generated mails lack any explicit origin in Postfix 2.2.x and
must be detected using heuristics as described in
\sectionref{identifying-bounce-notifications}.  Bounce notifications are
the primary example of internally generated mails, though there may be
other types.  Postfix may generate mails to the administrator when it
encounters configuration errors, but such mails are presumably rare.

\acronym{SMTP} is more complicated than local submission:

\begin{enumerate}

    \item First there is a connection from the remote client (action:
        CONNECT\@; flowchart:~1).

    \item This is followed by rejection of sender address, recipient
        addresses, client \acronym{IP} address or hostname, HELO hostname,
        etc.\ (action: REJECTION\@; flowchart:~4); acceptance of one or
        more mails (action: CLONE\@; flowchart:~5); or some interleaving of
        both.

    \item The client disconnects (action: DISCONNECT\@; flowchart:~6).  If
        Postfix has rejected any \acronym{SMTP} commands the data will be
        saved to the database; if not there will not be any data to save
        (any mails accepted will already have been cloned so their data is
        in another data structure).

    \item If one or more mails were accepted there will be more log lines
        for those mails later, see \sectionref{mail-delivery}.

\end{enumerate}

\label{mail-delivery}

The obvious counterpart to mail entering the system is mail leaving the
system, whether by deletion, bouncing, local delivery, or remote delivery.
All four are handled in exactly the same way:

\begin{enumerate}

    \item Postfix will log the sender and recipient addresses separately
        (action: SAVE\_DATA\@; flowchart:~9).

    \item Sometimes mail is re-injected and the child mail needs to be
        tracked by the parent mail (action: TRACK\@; flowchart:~10) ---
        \sectionref{tracking re-injected mail} discusses this in
        detail.

    \item Eventually the mail will be delivered, bounced, or deleted by the
        administrator (action: COMMIT\@; flowchart:~12).  This is the last
        log line for this particular mail (though it may be indirectly
        referred to if it was re-injected).  If it is neither parent nor
        child of re-injection the data is cleaned up and entered in the
        database (flowchart:~14), then deleted from the state tables.
        Re-injected mails are described in \sectionref{tracking re-injected
        mail}.

\end{enumerate}

It should be reiterated that the actions above happen whether the mail is
delivered to a mailbox, piped to a command, delivered to a remote server,
bounced (due to a mail loop, delivery failure, or five day timeout), or
deleted by the administrator, \textit{unless\/} the mail is either parent
or child of re-injection, as explained in \sectionref{tracking re-injected
mail}.

\section{Complications Encountered}

\label{complications}

XXX IMPROVE THE INTRO\@: ``needs more of a lead-in''.

XXX SHOULD I HAVE A LIST OF HOW MANY COMPLICATIONS ARE HANDLED BY EACH
ACTION\@?

The complications described in this section are listed in the order in
which they were encountered during development of the parser.  Each of
these complications caused the parser to operate incorrectly, generating
either warning messages or leaving mails in the state table.  The frequency
of occurrence is much higher at the start of the list, with the first
complication occurring several orders of magnitude more frequently than the
last.  When deciding which problem to address next, the most common was
always chosen, as resolving the most common problem would yield the biggest
improvement in the parser, prune the greatest number of mails from the
state tables and error messages, and make the remaining problems more
apparent.  The first three complications were encountered early in the
parser's implementation and guided its design and development.

\subsection{Queueid Vs Pid}

The mail lacks a queueid until it has been accepted, so log lines must
first be correlated by the \daemon{smtpd} \acronym{pid}, then transition to
being correlated by the queueid.  This is relatively minor, but does
require:

\begin{itemize}

    \item Two versions of several functions: \texttt{by\_pid} and
        \texttt{by\_queueid}.

    \item Two state tables to hold the data structure for each connection
        and mail.

    \item Most importantly: every section of code must know whether it
        needs to lookup the data structures by \acronym{pid} or queueid.

\end{itemize}

\subsection{Connection Reuse}

\label{connection reuse}

Multiple independent mails may be delivered across one connection: this
requires the algorithm to clone the current data as soon as a mail is
accepted, so that subsequent mails will not trample over each other's data.
This must be done every time a mail is accepted, as it is impossible to
tell in advance which connections will accept multiple mails.  Once the
mail has been accepted its log lines will not be correlated by
\acronym{pid} any more, its queueid will be used instead (except when
timeouts occur during the data phase
\sectionref{timeouts-during-data-phase}).  If the original connection has
any useful data (e.g.\ rejections) it will be saved to the database when
the client disconnects.  One unsolved difficulty is distinguishing between
different groups of rejections, e.g.\ when dealing with the following
sequence:

\begin{enumerate}

    \item The client attempts to deliver a mail, but it is rejected.

    \item The client issues the RSET command to reset the \acronym{SMTP}
        session.

    \item The client attempts to deliver another mail, likewise rejected.

\end{enumerate}

There should ideally be two separate entries in the database resulting from
the above sequence, but currently there will only be one.



\subsection{Re-injected Mails}

\label{Re-injected mails}

\label{tracking re-injected mail}

The most difficult complication initially encountered is that locally
addressed mails are not always delivered directly to a mailbox: sometimes
they are addressed to and accepted for a local address but need to be
delivered to one or more remote addresses due to aliases.  When this
occurs a child mail will be injected into the Postfix queue, but without
the explicit logging that \daemon{smtpd} or \daemon{postdrop} injected
mails have.  Thus the source of the mail is not immediately discernible
from the log line in which the mail first appears; from a strictly
chronological reading of the log files it usually appears as if the child
mail has appeared from thin air.  Subsequently the parent mail will log the
creation of the child mail, e.g.\ parent mail 3FF7C4317 creates child mail
56F5B43FD\@:

\texttt{3FF7C4317: to=<username@example.com>, relay=local, \newline{}
\tab{} delay=0, status=sent (forwarded as 56F5B43FD)}

Unfortunately, while all log lines from an individual process appear in
chronological order, the order in which log lines from different processes
are interleaved is subject to the vagaries of process scheduling.  In
addition, the first log line belonging to the child mail (the log line
cited above belongs to the parent mail) is logged by \daemon{qmgr}, so the
order also depends on how soon \daemon{qmgr} processes the new mail.

Because of the uncertain order the parser cannot complain when it
encounters a log line from \daemon{qmgr} for a previously unseen mail;
instead it must flag the mail as coming from an unknown origin, and
subsequently clear the flag if and when the origin of the mail becomes
clear.  Obviously the parser could omit checking where mails originate
from, but requiring an explicit source helps to expose bugs in the parser;
such checks helped to identify the complications described in
\sectionref{discarding cleanup log lines} and \sectionref{pickup logging
after cleanup}.

Process scheduling can have a still more confusing effect: quite often the
child mail will be created, delivered, and entirely finished with
\textbf{before} the parent logs the creation log line!  Thus, mails flagged
as coming from an unknown origin cannot be entered into the database when
their final log line is parsed; instead they must be marked as ready for
entry and subsequently entered once the parent mail has been identified.

XXX MERGE THE REMAINDER OF THIS SECTION INTO THE PRIOR MATERIAL\@.

The crux of this complication is that re-injected mails appear in the log
files without explicit logging indicating their source.  There are two
implicit indications:

\begin{enumerate}

    \item The indicator which more commonly introduces re-injection is when
        \daemon{qmgr} selects a mail with a previously unseen queueid for
        delivery, in which case a new data structure will be created for
        that mail.  The mail will be flagged as having unknown origins;
        this flag will subsequently be cleared once the origin has been
        established.  This may also be an indicator that the mail is a
        bounce notification, see
        \sectionref{identifying-bounce-notifications} for details.

    \item Local delivery re-injects the mail and logs a relayed delivery
        rather than delivering directly to a mailbox or program as it
        usually would.\footnote{Relayed delivery is performed by the
        \acronym{SMTP} client; local delivery means local to the server,
        i.e.\ an address the server is final destination for.}  In this
        case the mail may already have been created (described above) and
        the unknown origin flag will be cleared; if not a new data
        structure will be created.  In both cases the re-injected mail is
        marked as a child of the original mail.  The log line in question
        is:

        \texttt{3FF7C4317: to=<username@example.com>, relay=local,
        \newline{} \tab{} delay=0, status=sent (forwarded as 56F5B43FD)}

        This always occurs for re-injected mail but typically occurs after
        the first indicator.  This log line is required to tie the parent
        and child mails together and so is central to the process of
        tracking re-injected mails.

\end{enumerate}

The algorithm for tracking and saving re-injected mail to the database can
finally be described:

\begin{itemize}

    \item If the mail is of unknown origin it is assumed to be a child mail
        whose parent has not yet been identified.  Mark the mail as ready
        for entry in the database and wait for the parent to deal with it.
        The mail should not have any subsequent log lines; only its parent
        should refer to it.

    \item If the mail is a child mail then it has already been tracked: as
        with all other mail, the data is cleaned up, the child is entered
        in the database, and then deleted from the state tables.  The child
        mail will be removed from the parent mail's list of children; if
        this is the last child and the parent has already been entered in
        the database, the parent will also be deleted from the state
        tables.

    \item The last alternative is that the mail is a parent mail.
        Regardless of the state of its children its data is cleaned up and
        entered in the database.  The parent may have children that are
        waiting to be entered in the database; the data for each of those
        children is cleaned up and entered in the database, then deleted
        from the state tables.  The parent may also have outstanding
        children which are not yet delivered, in which case the parent must
        be retained in the state tables until those children are finished
        with.  As soon as the last child is deleted from the state tables
        the parent will also be deleted from the state tables.

\end{itemize}

A parent mail can have multiple children, which may be delivered before or
after the parent mail.


\subsection{Identifying Bounce Notifications}

\label{identifying-bounce-notifications}

Postfix 2.2.x (and presumably previous versions) does not generate a log
line when it generates a bounce notification; suddenly there will be log
entries for a mail that lacks an obvious source.  There are similarities to
the problem of identifying re-injected mails discussed in
\sectionref{tracking re-injected mail}, but unlike the solution described
therein bounce notifications do not eventually have a log line that
identifies their source.  Heuristics must be used to identify bounce
notifications, and those heuristics are:

\begin{enumerate}

    \item The sender address is \verb!<>!.\glsadd{<>}

    \item Neither \daemon{smtpd} nor \daemon{pickup} have logged any
        messages associated with the mail, indicating it was generated
        internally by Postfix, not accepted via \acronym{SMTP} or submitted
        locally by \daemon{postdrop}.

    \item The message-id has a specific format: \newline{}
        \tab{} \texttt{YYYYMMDDhhmmss.queueid@server.hostname} \newline{}
        e.g.\ \texttt{20070321125732.D168138A1@smtp.example.com}

    \item The queueid in the message-id must be the same as the queueid of
        the mail: this is what distinguishes bounce notifications generated
        locally from bounce notifications which are being re-injected as a
        result of aliasing (described in \sectionref{Re-injected mails}).
        In the latter case the message-id will be unchanged from the
        original bounce notification, and so even if it happens to be in
        the correct format (e.g.\ if it was generated by Postfix on this or
        another server) it will not match the queueid of the mail.

\end{enumerate}

Once a mail has been identified as a bounce notification, the unknown
origin flag is cleared and the mail can be entered in the database.

There is a small chance that a mail will be incorrectly identified as a
bounce notification, as the heuristics used may be too broad.  For this to
occur the following conditions would have to be met:

\begin{enumerate}

    \item The mail must have been generated internally by Postfix.

    \item The sender address must be \verb!<>!.\glsadd{<>}

    \item The message-id must have the correct format and match the queueid
        of the mail.  While a mail sent from elsewhere could easily have
        the correct message-id format, the chance that the queueid in the
        message-id would match the queueid of the mail is extremely small.

\end{enumerate}

If a mail is mis-classified as a bounce message it will almost certainly
have been generated internally by Postfix; arguably mis-classification in
this case is a benefit rather than a drawback, as other mails generated
internally by Postfix will be handled correctly.  Postfix 2.3 (and
hopefully subsequent versions) log the creation of a bounce message.

This check is performed during the COMMIT action.

\subsection{Aborted Delivery Attempts}

\label{aborted-delivery-attempts}

Some mail clients behave unexpectedly during the \acronym{SMTP} dialogue:
the client aborts the first delivery attempt after the first recipient is
accepted, then makes a second delivery attempt for the same recipient which
it continues with until delivery is complete.  Microsoft Outlook is one
client that behaves in this fashion; other clients may act in a similar
way.  An example dialogue exhibiting this behaviour is presented below
(lines starting with a three digit number are sent by the server, the other
lines are sent by the client):

\begin{verbatim}
220 smtp.example.com ESMTP
EHLO client.example.com
250-smtp.example.com
250-PIPELINING
250-SIZE 15240000
250-ENHANCEDSTATUSCODES
250-8BITMIME
250 DSN
MAIL FROM: <sender@example.com>
250 2.1.0 Ok
RCPT TO: <recipient@example.net>
250 2.1.5 Ok
RSET
250 2.0.0 Ok
RSET
250 2.0.0 Ok
MAIL FROM: <sender@example.com>
250 2.1.0 Ok
RCPT TO: <recipient@example.net>
250 2.1.5 Ok
DATA
354 End data with <CR><LF>.<CR><LF>
The mail transfer is not shown.
250 2.0.0 Ok: queued as 880223FA69
QUIT
221 2.0.0 Bye
\end{verbatim}

Once again Postfix does not log a message making the client's behaviour
clear, so once again heuristics are required to identify when this
behaviour occurs.  In this case a list of all mails accepted during a
connection is saved in the connection state, and the accepted mails are
examined when the disconnection action is executed.  Each mail is checked
for the following: 

\begin{itemize}

    \item XXX IS THIS CORRECT NOW\@?\newline{}  Are there two or more
        \daemon{smtpd} log lines?  Does the second result have a
        postfix\_action attribute of ACCEPTED\@?  The first two
        \daemon{smtpd} log lines will be a connection log line and a mail
        acceptance log line (Postfix logs acceptance as soon as the first
        recipient has been accepted).

    \item Is \daemon{smtpd} the only Postfix component that produced a log
        line for this mail?  Every mail which passes normally through
        Postfix will have a \daemon{cleanup} line, and later a
        \daemon{qmgr} log line; lack of a \daemon{cleanup} line is a sure
        sign the mail did not make it too far.  

    \item Does the queueid exist in the state tables?  If not it cannot be
        an aborted delivery attempt.

    \item If there are third and subsequent results, are all their
        postfix\_action attributes equal to INFO\@?  If there are any log
        lines after the first two they should be informational only.

\end{itemize}

If all the checks above are successful then the mail is assumed to be an
aborted delivery attempt and is discarded.  There will be no further
entries logged for such mails, so without identifying and discarding them
they accumulate in the state table and will cause clashes if the queueid is
reused.  The mail cannot be entered in the database as the only data
available is the client hostname and \acronym{IP} address, but the database
schema requires many more fields be populated (see \sectionref{connections
table} and \sectionref{results table}).  This heuristic is quite
restrictive, and it appears there is little scope for false positives; if
there are any false positives there will be warnings when the next log line
for that mail is parsed.  False negatives are less likely to be detected:
there may be queueid clashes (and warnings) if mails remain in the state
tables after they should have been removed, otherwise the only way to
detect false negatives is to examine the state tables after each parsing
run.

This check is performed in the DISCONNECT action; it requires support in
the CLONE action where a list of cloned connections is maintained.


\newpage{} % XXX

\subsection{Further Aborted Delivery Attempts}

XXX MAYBE FIND OUT WHAT CLIENTS DISCONNECT\@?  Some mail clients disconnect
abruptly if a second or subsequent recipient is rejected; they may also
disconnect after other errors, but such disconnections are either
unimportant or are handled elsewhere in the parser
(\sectionref{timeouts-during-data-phase}).  Sadly, Postfix does not log a
message saying the mail has been discarded, as should be expected by now.
The checks to identify this happening are:

\begin{itemize}

    \item Is the mail missing its \daemon{cleanup} log line?  Every mail
        which passes through Postfix will have a \daemon{cleanup} line;
        lack of a \daemon{cleanup} line is a sure sign the mail did not
        make it too far.

    \item Were there three or more \daemon{smtpd} log lines for the mail?
        There should be a connection log line and a mail accepted log line,
        followed by one or more rejection log lines.

    \item Is the last \daemon{smtpd} log line a rejection line?

\end{itemize}

If all checks are successful then the mail is assumed to have been
discarded when the client disconnected.  There will be no further entries
logged for such mails, so without identifying and entering them in the
database immediately they accumulate in the state table and will cause
clashes if the queueid is reused.

These checks are made during the DISCONNECT action.

\subsection{Timeouts During DATA Phase}

\label{timeouts-during-data-phase}

The DATA phase of the \acronym{SMTP} conversation is where the headers and
body of the mail are transferred.  Sometimes there is a timeout or the
connection is lost\footnote{For brevity's sake \textit{timeout\/} will be
used throughout this section, but everything applies equally to lost
connections.} during the DATA phase; when this occurs Postfix will discard
the mail and the parser needs to discard the data associated with that
mail.  It seems more intuitive to save the mail's data to the database, but
if a timeout occurs there is no data available to save; the timeout is
recorded with the connection data instead, which is saved.

To deal properly with timeouts the parsing algorithm needs to do the
following in the TIMEOUT action:

\begin{enumerate}

    \item Record the timeout and associated data in the connection's
        results.

    \item If no mails have been accepted yet nothing needs to be done; the
        TIMEOUT action ends.  

    \item If one or more recipients have been accepted Postfix will have
        allocated a queueid for the incoming mail, and there will be a mail
        in the state tables that needs to be dealt with.

\end{enumerate}

XXX MAKE THIS PARAGRAPH CLEARER\@.  A timeout may thus apply either to an
accepted mail or a rejected mail.  To distinguish between the two cases the
algorithm compares the timestamp of the last accepted mail against the
timestamp of the last line logged by \daemon{smtpd} for that connection
(the TIMEOUT action is dependant on the CLONE action keeping a list of all
mails accepted on each connection).  If the mail acceptance timestamp is
later then the timeout applies to the just-accepted mail, which will be
discarded.  If the \daemon{smtpd} timestamp is later there was a rejection
between the accepted mail and the timeout: the action assumes that the
timeout applies to a rejected delivery attempt and finishes.  This
assumption is not necessarily correct, because Postfix may have accepted an
earlier recipient and rejected a later one, in which case the timeout
applies to the accepted mail, which should be discarded.  This has not been
a problem in practice, though it may be in future.  This complication is
further complicated by the presence of out of order \daemon{cleanup} log
lines: see \sectionref{discarding cleanup log lines} for details.

This complication is dealt with in the TIMEOUT action.

\subsection{Discarding Cleanup Log Lines}

\label{discarding cleanup log lines}

The author has only observed this complication occurring after a timeout,
though there may be other circumstances that trigger it.  Sometimes the
\daemon{cleanup} line for a mail being accepted is logged after the timeout
line; parsing this line causes the MAIL\_QUEUED action to create a new
state table entry for the queueid in the log line.  This is incorrect
because the line actually belongs to the mail that has just been discarded;
the next log line for that queueid will be seen when the queueid is reused
for a different mail, causing a queueid clash and the appropriate warning.

When the \daemon{cleanup} line is still pending during the TIMEOUT action,
the action updates a global list of queueids, adding the queueid and the
timestamp from the log line.  When the next \daemon{cleanup} line is parsed
for that queueid the list will be checked (during the MAIL\_QUEUED action),
and the log line will be deemed part of the mail where the timeout occurred
and discarded if it meets the following requirements:

\begin{itemize}

    \item The queueid must not have been reused yet, i.e.\ it does not have
        an entry in the state tables.

    \item The timestamp of the \daemon{cleanup} log line must be within ten
        minutes of the mail acceptance timestamp.  Timeouts happen after
        five minutes, but some data may have been transferred slowly
        (perhaps because either the client or server is suffering from
        network congestion or rate limiting), and empirical evidence shows
        that ten minutes is not unreasonable; hopefully it is a good
        compromise between false positives (log lines incorrectly
        discarded) and false negatives (new state table entries incorrectly
        created).

\end{itemize}

The next \daemon{cleanup} line must meet the criteria above for it to be
discarded because some, but not all connections where a timeout occurs will
have an associated \daemon{cleanup} line logged; if the algorithm blindly
discarded the next \daemon{cleanup} line after a timeout it would sometimes
be mistaken.  When the next \daemon{pickup} line containing that queueid is
parsed the queueid will be removed from the cache of timeout queueids,
regardless of whether it meets the criteria above.

This complication is handled by the TIMEOUT and MAIL\_QUEUED actions.

\subsection{Pickup Logging After Cleanup}

\label{pickup logging after cleanup}

When mail is submitted locally, \daemon{pickup} accepts the new mail and
generates a log line showing the source.  Occasionally this log line will
occur later in the log file than the \daemon{cleanup} log line, so the
PICKUP action will find that a state table entry exists for that queueid.
Normally if the queueid given in the PICKUP line exists in the state tables
a warning is generated by the \daemon{pickup} action, but if the following
conditions are met it is assumed that the log lines were out of order:

\begin{itemize}

    \item The only program which has logged anything thus far for the mail
        is \daemon{cleanup}.

    \item There is less than a five second difference between the
        timestamps of the \daemon{cleanup} and \daemon{pickup} log lines.

\end{itemize}

As always with heuristics there may be circumstances in which these
heuristics match incorrectly, but none have been identified so far.  This
complication seems to occur during periods of particularly heavy load, so
is most likely caused by process scheduling vagaries.  

This complication is dealt with during the PICKUP action.

\subsection{Smtpd Stops Logging}

\label{smtpd stops logging}

Occasionally a \daemon{smtpd} will just stop logging, without an
immediately obvious reason.  After poring over log files for some time
several reasons have been found for this infrequent occurrence:

\begin{enumerate}

    \item Postfix is stopped or its configuration is reloaded.  When this
        happens all \daemon{smtpd} processes exit, so all entries in the
        connections state table must be cleaned up, entered in the database
        if there is sufficient data, and deleted.

    \item Sometimes a \daemon{smtpd} is killed by a signal (sent by Postfix
        for some reason, by the administrator, or by the OS), so its active
        connection must be cleaned up, entered in the database if there is
        sufficient data, and deleted from the connections state table.

    \item Occasionally a \daemon{smtpd} will exit uncleanly, so the active
        connection must be cleaned up, entered in the database if there is
        sufficient data, and deleted from the connections state table.

    \item Every Postfix process uses a watchdog which kills the process if
        it is not reset for a considerable period of time (five hours by
        default).  This safeguard prevents Postfix processes from running
        indefinitely and consuming resources if a failure causes them to
        enter a stuck state.  XXX IMPROVE THIS SOMEHOW\@.

\end{enumerate}

The circumstances above account for all occasions identified thus far where
a \daemon{smtpd} suddenly stops logging.  In addition to removing an active
connection the last accepted mail may need to be discarded, as detailed in
\sectionref{timeouts-during-data-phase}; otherwise the queueid state table
is untouched.

These occurrences are handled by the three actions POSTFIX\_RELOAD,
SMTPD\_DIED, and SMTPD\_WATCHDOG\@.

\subsection{Out of Order Log Lines}

\label{out of order log lines}

Occasionally a log file will have out of order log lines which cannot be
dealt with by the techniques described in \sectionref{tracking re-injected
mail}, \sectionref{discarding cleanup log lines} or \sectionref{pickup
logging after cleanup}.  In the \numberOFlogFILES{} log files used for
testing this occurs only five times in 60,721,709 log lines, but for parser
correctness it must be dealt with.  The five occurrences have the same
characteristics: the \daemon{local} log line showing delivery to a local
mailbox occurs after the \daemon{qmgr} log line showing removal of the mail
from the queue because delivery is completed.  This causes problems: the
data in the state tables for the mail is not complete, so entering it into
the database fails; a new mail is created when the \daemon{local} line is
parsed and remains in the state tables; four warnings are issued per pair
of out of order log lines.

The COMMIT action examines the list of programs that have produced a log
lines for each mail, comparing the list against a table of known-good
program combinations.  If the mail's combination is found in the table the
mail can be entered in the database; if the combination is not found entry
must be postponed and the mail flagged for later entry.  The SAVE\_DATA
action checks for that flag; if the additional log lines have caused the
mail to reach a valid combination then entry in the database will proceed,
otherwise it must be postponed once more.

The list of valid combinations is explained below.  Every mail will
additionally have log line from \daemon{cleanup} and \daemon{qmgr}; any
mail may also have log line from \daemon{bounce}, \daemon{postsuper}, or
both.

XXX WOULD IT BE BETTER TO BEGIN THE EXPLANATIONS ON THE FOLLOWING LINE\@?

\begin{description}

    \item [\daemon{local}:] Local delivery of a bounce notification, or
        local delivery of a re-injected mail.

    \item [\daemon{local}, \daemon{pickup}:] Mail submitted locally on the
        server, delivered locally on the server.

    \item [\daemon{local}, \daemon{pickup}, \daemon{smtp}:] Mail submitted
        locally \newline{} on the server, for both local and remote
        delivery.

    \item [\daemon{local}, \daemon{smtp}, \daemon{smtpd}:] Mail accepted
        from a remote client, for both local and remote delivery.

    \item [\daemon{local}, \daemon{smtpd}:] Mail accepted from a remote
        client, for local delivery only.

    \item [\daemon{pickup}, \daemon{smtp}:] Mail submitted locally on the
        server, for remote delivery only.

    \item [\daemon{smtp}:] Remote delivery of either a re-injected mail or
        a bounce notification.

    \item [\daemon{smtp}, \daemon{smtpd}:] Mail accepted from a remote
        client, then remotely delivered (typically relaying mail for
        clients on the local network to addresses outside the local
        network).

    \item [\daemon{smtpd}, \daemon{postsuper}:] Mail accepted from a remote
        client, then deleted by the administrator before any delivery
        attempt was made (the unwanted mail is typically due to a mail loop
        or joe~job\glsadd{Joe-job}).  Notice that \daemon{postsuper} is
        required, not optional, for this combination.

\end{description}

This check applies to accepted mails only, not to rejected mails.  This
check is performed during the COMMIT action.

\subsection{Yet More Aborted Delivery Attempts}

\label{yet-more-aborted-delivery-attempts}

The aborted delivery attempts described in
\sectionref{aborted-delivery-attempts} occur frequently, but the aborted
delivery attempts described in this section only occur four times in the
\numberOFlogFILES{} log files used for testing.  The symptoms are the same
as in \sectionref{aborted-delivery-attempts}, except that there
\textit{is\/} a \daemon{cleanup} log line; there does not appear to be
anything in the log file to explain why there are no further log lines.
The only way to detect these mails is to periodically scan all mails in the
state tables, deleting any mails displaying the following characteristics:

\begin{itemize}

    \item The timestamp of the last log line for the mail must be 12 hours
        or more earlier than the last log line parsed from the current log
        file.

    \item XXX IS THIS CORRECT NOW\@?  CAN THERE BE MORE smtpd LOG LINES\@?
        There must be exactly two \daemon{smtpd} and one \daemon{cleanup}
        log lines for the mail, with no additional log lines.

\end{itemize}

12 hours is a somewhat arbitrary time period, but it is far longer than
Postfix would delay delivery of a mail in the queue (unless Postfix is not
running for an extended period of time).  The state tables are scanned for
mails matching the characteristics above each time the end of a log file is
reached, and matching mails are deleted.

\subsection{Mail Deleted Before Delivery is Attempted}

\label{Mail deleted before delivery is attempted}

Postfix logs the recipient address when delivery of a mail is attempted, so
if delivery has yet to be attempted the parser cannot determine the
recipient address or addresses.  This is a problem when mail is arriving
faster than Postfix can attempt delivery, and the administrator deletes
some of the mail (because it is the result of a mail loop\glsadd{mail
loop}, mail bomb\glsadd{mail bomb}, or joe~job\glsadd{Joe-job}) before
Postfix has had a chance to try to deliver it.  In this case the recipient
address will not have been logged, so a dummy recipient address needs to be
added, as every mail is required by the database schema
(\sectionref{results table}) to have at least one recipient.  This
complication has been observed in XXX/\numberOFlogFILES{} log files, but
typically when it does occur there will be many instances of it, closely
grouped.

The DELETE action is responsible for handling this complication.

\subsection{Bounce Notification Mails Are Delivered Before Their Creation
Is Logged}

\label{Bounce notification mails delivered before their creation is logged}

This is yet another complication that only occurs during periods of
extremely high load, when process scheduling and even hard disk access
times cause strange behaviour.  In this complication bounce notification
mails are created, delivered, and deleted from the queue, \textit{before\/}
the log line from \daemon{bounce} that explains their source is logged.  To
deal with this the COMMIT action maintains a cache of recently committed
bounce notification mails, which the BOUNCE action subsequently checks if
the bounce mail is not already in the state tables. If the queueid exists
in the cache, and its start time is less than ten seconds before the
timestamp of the bounce log line, it is assumed that the bounce
notification mail has already been processed and the BOUNCE action does not
create one.  If the queueid exists it is removed from the cache, because it
has either just been used or it is too old to use in future.  Whether the
BOUNCE action creates a mail or finds an existing mail in the state tables,
it flags the mail as having been seen by the BOUNCE action; if this flag is
present the COMMIT action will not add the mail to the cache of recent
bounce notification mails.\footnote{This is not required to correctly deal
with the complication, but is an optimisation to reduce the parser's memory
usage; on the occasions the author has observed this complication occurring
there have been a huge number of bounce notification mails generated --- if
every bounce notification mail was cached it would dramatically increase
the memory requirements of the parser.}  The cache of bounce notification
mails will be pruned whenever the parser's state is saved, though if the
size of the cache ever becomes a problem it could be pruned periodically to
keep the size in check.  XXX EXPLAIN PRUNING\@.

\subsection{Mails Deleted During Delivery}

\label{Mails deleted during delivery}

The administrator can delete mails using \daemon{postsuper}; occasionally
mails that are in the process of being delivered will be deleted.  This
results in the log lines from the delivery agent (\daemon{local},
\daemon{virtual} or \daemon{smtp}) appearing in the log file
\textit{after\/} the mail has been removed from the state tables and saved
in the database.  The DELETE action adds deleted mails to a cache, which is
checked by the SAVE\_DATA action, and the current log line discarded if the
following conditions are met:

\begin{enumerate}

    \item The queueid is not found in the state tables. 

    \item The queueid is found in the cache of deleted mails.

    \item The timestamp of the log line is within 5 minutes of the final
        timestamp of the mail.

\end{enumerate}

Sadly this solution involves discarding some data, but the complication
only arises eight times in quick succession in one log file, which is not
in the \numberOFlogFILES{} log files used for testing; if this complication
occurred more frequently it might be desirable to find the mail in the
database and add the log line to it.

\section{Limitations and Possible Improvements}

\label{limitations and improvements in implementation}

Every piece of software suffers from some limitations and there is almost
always room for improvement.  Below are the limitations and possible
improvements that have been identified in the parser.

\subsection{Limitations}

\label{logging helo}

\begin{enumerate}

    \item Each new Postfix release requires new rules to be written or
        existing rules modified to cope with the new or changed log lines.
        Similarly using a new \acronym{DNSBL}, a new policy server, or new
        administrator-defined rejection messages require new rules.

    \item The hostname used in the HELO command is not logged if the mail
        is accepted.\footnote{Tested with Postfix 2.2.10, 2.3.11, and
        2.4.7; this may possibly have changed in Postfix 2.5. XXX TEST THIS
        AGAIN.}  Rectifying this is relatively simple: create a restriction
        that is guaranteed to warn for every accepted mail, as follows:

        \begin{enumerate}

            \item Create \texttt{/etc/postfix/log\_helo.pcre}
                containing:\newline{}
                \tab{}\texttt{/\^/~~~~WARN~Logging~HELO}

            \item Modify \texttt{smtpd\_data\_restrictions} in
                \texttt{/etc/postfix/main.cf} to contain:\newline{}
                \tab{}\texttt{check\_helo\_access~pcre:/etc/postfix/log\_helo.pcre}

        \end{enumerate}

        Although \texttt{smtpd\_helo\_restrictions} seems like the natural
        place to log the HELO hostname, there will not be a queueid
        associated with the mail when \texttt{smtpd\_helo\_restrictions} is
        evaluated for the first recipient, so the log line cannot be
        associated with the correct mail.  There is guaranteed to be a
        queueid when the DATA command has been reached, and thus the
        queueid will be logged by any restrictions taking effect in
        \texttt{smtpd\_data\_restrictions}.  There is no difficulty in
        specifying a HELO-based restriction in
        \texttt{smtpd\_data\_restrictions}, Postfix will perform the check
        correctly.

        Logging the HELO hostname in this fashion also partially prevents
        the complication described in \sectionref{Mail deleted before
        delivery is attempted} from occurring, but only when there is a
        single recipient; in that case the recipient address will be logged
        also, but when there are multiple recipients no addresses are
        logged.  It is also possible to warn for every recipient,
        preventing the complication in \sectionref{Mail deleted before
        delivery is attempted} entirely.  XXX ADD A PARAGRAPH EXPLAINING
        HOW\@.

    \item The parser does not separate mails where one or more mails are
        rejected and a subsequent mail is accepted; it will appear in the
        database as one mail with lots of rejections followed by acceptance
        (this has already been mentioned in \sectionref{connection reuse}).
        It does not appear to be possible to make this distinction given
        the data Postfix logs, though it might be possible to write a
        policy server to provide additional logging.

    \item The parser will not detect that it is parsing the same log file
        twice, resulting in the database containing duplicate entries.

    \item The parser does not distinguish between log files produced by
        different sources when parsing; all results will be saved to the
        same database.  This may be viewed as an advantage, as log files
        from different sources can be combined in the same database, or it
        may be viewed as a limitation as there is no facility to
        distinguish between log files from different sources in the same
        database.  If the results of parsing log files from different
        sources must remain separate, the parser can easily be instructed
        to use a different database to store the results in.

    \item The solution to complication \sectionref{Mails deleted during
        delivery} involves discarding data.

\end{enumerate}

\subsection{Possible Improvements}

\begin{itemize}

    \item Investigate and write the policy server referred to in limitation
        3 above.

    \item Improve the solution to complication \sectionref{Mails deleted
        during delivery} so that data is not discarded.

    \item Improve the heuristics used in
        \sectionref{timeouts-during-data-phase}, or develop another
        solution, to avoid incorrectly leaving a mail in the state tables.

\end{itemize}


\section{Summary}

XXX TO BE WRITTEN\@.  START WITH THE OLD CONTENT BELOW\@.

This section presented the core of the parser, starting with a very high
level view and the initial complications that arose.  A flow chart showing
the paths a mail may take through the nascent simplified algorithm was
provided, followed by an explanation of those paths, and a discussion of
the parser's emergent behaviour --- the data from the log files creates the
paths in the flow chart, they are not specified anywhere in the parser.
The framework which holds the parser together was covered next, after which
came a description of the current actions provided by the parser, and the
algorithm for analysing unparsed log lines to create regexes for new rules.
Detecting, diagnosing, and defeating complications forms the largest single
portion of this section, mirroring the development of the parser.  The
complications are described in the order they were overcome, with
subsequent problems affecting fewer mails (often by an order or magnitude),
though the time required to solve problems increased with each successive
problem.


