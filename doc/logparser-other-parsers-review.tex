\section{State of the Art Review}

\label{state of the art review}

XXX UPDATE THE BLURBS\@.

Ten other parsers, starting with the only previously published research in
this area that the author is aware of, have been reviewed as part of the
background research for this project.  Each of the parsers was tested with
the \numberOFlogFILES{} test log files described in \sectionref{parser
efficiency}.  None of the parsers reviewed perform the kind of advanced
parsing and correlation required for this project; all except one (XXX IS
THIS ACCURATE\@?) are intended to perform a specific parsing and reporting
task, whereas \PLP{} is designed to be a generic parser, extracting data
and leaving generation of reports from the data to other programs.  Some
parsers save the data extracted to a data store, but the majority discard
all data once they have finished running, making historical analysis
impossible.  The parsers reviewed all produce a report of varying
complexity and detail, whereas this project's parser does not produce a
report at all; that responsibility is deferred to a separate program, to be
developed later.  The parser developed for this project is designed to
enable much more detailed log analysis by providing a stable platform for
subsequent programs to develop upon.

It is important to compare and contrast newly developed programs,
algorithms, and parsers against those already available, to accurately
judge what improvements, if any, are delivered by the newcomers.  There are
not that many previously developed Postfix log parsers --- indeed it was
quite difficult to find ten parsers to review for this project --- and the
functionality offered ranges from quite basic to much more mature,
depending on the needs of the creator.  The programs are not reviewed from
an independent viewpoint; the objective is to compare and contrast each
program with \parsername{}.  There are some important differences between
\parsername{} and the parsers reviewed here:

\begin{enumerate}

    \item Only \parsername{} makes it possible to parse new log lines
        without extensive and intrusive modifications to the parser;
        \parsername{}'s architecture is described in \sectionref{why
        separate rules, actions and framework?}.

    \item \parsername{} does not provide any reports based on the data it
        gathers.  This is by design: the parser is responsible for parsing
        log files and extracting data; further processing of that data is a
        separate concern, deferred to another program.  Using an \SQL{}
        database simplifies the process of generating such reports
        (discussed in \sectionref{database as API}); some simple examples
        are given in \sectionref{motivation}.

    \item Most of the parsers silently ignore log lines they cannot handle,
        whereas \parsername{} complains loudly about every single log line
        it fails to parse.  The exception is AWStats, which outputs the
        percentage of input lines it was unable to parse, but does not
        output the lines themselves.

    \item A minor difference is that most parsers do not handle compressed
        files; both \parsername{} and Splunk handle them transparently,
        without user intervention; Sawmill and Lire can be configured to
        support compressed files, but Sawmill exhibits a dramatic slow down
        when thus configured.  Although this is a minor difference, support
        for reading compressed log files is quite helpful, as it
        dramatically reduces the disk space required to store old log
        files.

\end{enumerate}

XXX EXTEND THIS\@.

\subsection{Log Mail Analyser}

XXX IS THE BDB STRUCTURE DOCUMENTED\@?

XXX DESCRIBE THE \CSV{} FORMAT\@.

XXX DESCRIBE THE MYSQL SCHEMA\@.

IS THE SAME DATA STORED FOR ALL FORMATS\@?

\label{prior art}

There only appears to be one prior published paper about parsing Postfix
log files: \textit{Log Mail Analyzer: Architecture and Practical
Utilizations\/}~\cite{log-mail-analyser}.  The aim of \LMA{} is quite
different from this parser: it attempts to present correlated log data in a
form suitable for a systems administrator to search using the myriad of
standard Unix text processing utilities already available, producing both a
\CSV{} file and either a MySQL (\url{http://www.mysql.com/}) or Berkeley DB
(\url{http://www.oracle.com/database/berkeley-db/index.html}) database.
XXX SHOULD THOSE URLS BE REFERENCES INSTEAD\@?  The decision to support
both \CSV{} and Berkeley DB appears to have been a serious limitation:
\CSV{} is a very simple format where a record is stored in a single line,
with fields separated by a comma or other punctuation symbol.  Problems
with \CSV{} files include the need to escape separators in the data stored,
providing multiple values for a field (e.g.\ multiple recipients), and
adding new fields.  There is no mechanism to document the fields or the
separator, unlike \SQL{} databases where every database includes a schema
naming the fields and the type of data they store (integer, text,
timestamp, etc.).

Berkeley DB only supports storing simple \textbf{(key, value)} pairs,
unlike \SQL{} databases that store arbitrary tuples.  In \LMA{}'s main
table the key is an integer referred to by secondary tables, and the value
is a \CSV{} line containing all of the data for that row.  The
secondary by-sender, by-recipient, by-date, and by-\IP{} tables use the
sender/recipient/date/\IP{} as the key, and the value is a \CSV{} list of integers
referring to the main table.  This effectively re-implements
\SQL{} foreign keys, but without the functionality offered by even the most
basic of \SQL{} databases (joins, ordering, searches, etc.).  It also
requires custom code to search on some combination of the above, though the
authors of \LMA{} did provide some queries: IP-STORY, FROM-STORY,
DAILY-EMAIL and DAILY-REJECT\@.  Berkeley DB appears to be the least useful
of the three output formats: it does not provide the functionality of a
basic \SQL{} database, not can it be used with standard Unix text
processing tools.

\LMA{} stores the following date: time and date, client hostname and \IP{}
address, server \IP{} address, sender and recipient addresses, \SMTP{}
code, and size (for accepted mails only).  It omits XXX XXX\@.  Handling of
multiple recipients, \SMTP{} codes or remote servers\footnote{A single mail
may be sent to multiple remote servers if it was addressed to recipients in
different domains, or Postfix needs to try multiple servers for one or more
recipients.} is not explained in the paper; experimental observation shows
that multiple records are added when there are multiple recipients (sadly
the records are not associated or linked in any way), and presumably the
same approach is taken when there are multiple destination servers.

The schema used with the MySQL database is undocumented, but at least it is
possible to discover the schema with an \SQL{} database, unlike with
Berkeley DB\@; all \SQL{} databases embed the schema into the database and
provide commands for displaying it.  Berkeley DB does not embed a schema,
as there is neither requirement nor benefit; it only provides \textbf{(key,
value)} pairs, so any additional structuring of the data is imposed by the
application, thus it behoves the application to document this structure.

\LMA{} requires major changes to the parser code to parse new log lines or
extract additional data.  The code is structured as a long series of blocks
that each handle all lines matching a single \regex{}, so parsing new log
lines requires modifying an existing \regex{} or carefully inserting a new
block in the correct place; extracting extra data will require modifying
multiple blocks, \regexes{}, or both.

\LMA{} does not appear to deal with any of the complications discussed in
\sectionref{complications}.  It does not differentiate between different
types of rejections, so it is not suitable for the purposes of this
project; the data about which restriction caused the rejection is
discarded, whereas the main goal of this project is to retain that data to
aid optimisation and evaluation of Postfix restrictions.  \LMA{} fails to
parse Postfix log files generated on Solaris hosts because the fields
automatically prepended to each log line differ from those added on Linux
hosts; log files from Solaris hosts (and possibly other operating systems)
thus require preprocessing before parsing by \LMA{}.  Once the
preprocessing has been performed on the \numberOFlogFILES{} test log files
\LMA{} parses the log files without complaint, although it produced 32
entries in its output file for every rejection in the input log file; it
also missed some 40\% of delivered mail.  Once these glaring deficiencies
were discovered the author did not waste any more time checking the
results.

\LMA{} does provide some simple reports: IP-STORY, FROM-STORY, DAILY-EMAIL
and DAILY-REJECT\@.  The first three extract \CSV{} lines for the specified
client \IP{} address, sender address, or date respectively.  DAILY-REJECT
initially failed with an error message (XXX ADD THE ERROR MESSAGE) from the
Perl interpreter; after making some corrections to the code it worked, and
extracted the \CSV{} lines for the specified day where the \SMTP{} code
signifies a rejection.  All of these reports are trivially simple to
produce from the \CSV{} file using the standard Unix tool \texttt{awk}; the
most complicated, DAILY-REJECT, is merely:

\begin{verbatim}
awk -F\| '$1 ~ /2007-01-26/ && $7 ~ /^[450]|deferred/
    { print $0; print " "; }' lma_output.txt
\end{verbatim}

Notes about the command above:

\begin{enumerate}

    \item It outputs an unnecessary line containing only a single space
        after each matching line, to accurately replicate the output of
        DAILY-REJECT\@.

    \item Some records have an \SMTP{} code of \textit{0}, though this is
        not a valid \SMTP{} code; presumably this is because \LMA{} did not
        parse a log line properly.  DAILY-REJECT considers this code to be
        a rejection, so the command above must too.

    \item In a small fraction of records the \SMTP{} code is
        \textit{deferred\/} (again, not a valid \SMTP{} code, and most
        likely because of mis-parsing by \LMA{}); it is treated as a
        rejection by DAILY-REJECT, so the equivalent \texttt{awk} command
        must do so, too.

\end{enumerate}

There are two differences between the output from DAILY-REJECT and the
\texttt{awk} command:

\begin{itemize}

    \item \LMA{} does not escape or replace the separators used in the
        \CSV{} records when they are present in sender or recipient
        addresses, leading to some mangled records; these records are not
        present in the output from \texttt{awk}.  XXX WHY\@?  GIVE AN
        EXAMPLE RECORD\@.

    \item Where there are multiple records in the \CSV{} file, DAILY-REJECT
        extracts only one record, whereas the \texttt{awk} command above
        extracts all matching records.

\end{itemize}

In summary, \LMA{} appears to be a proof of concept, written to demonstrate
the point of their paper (that having this information in an accessible
fashion is useful to systems administrators), rather than a program
designed to be useful in a production environment; no consideration was
given to extensibility when writing it.

% Literature review notes:
%
% Hard-coded parsing, requiring code changes to add more.  Attempts to
% correlate log lines, saves data to database for data mining purposes.
% Hard to extend/expand/understand.  Appears to only save: date and hour,
% \DNS{} name and \IP{} address host, mail server \IP{} address, sender,
% receiver and e-mail status (sent, rejected).  Undocumented schema.
% Design decision to use \CSV{} as an intermediate format between the log
% file and the database seems to have been restrictive.  Appears to require
% a queueid but majority of log entries (e.g.\ rejections) lack a queueid.
% Supports whitelisting \IP{} addresses when parsing logs, but whitelisting
% when generating reports/data mining would be preferable.  Supporting
% Berkeley DB is probably limiting the software - an example is the
% difficulty in searching a pipe-delimited string, so they have
% re-implemented foreign keys with tables keyed by ip address etc.\
% pointing at the main table - this also will not scale well.  There does
% not appear to be any attempt to deal with the complications I have
% encountered: their parsing is not detailed enough to encounter them.  It
% does not run properly; does not create any output; throws up errors.

\subsection{Pflogsumm}

\begin{quotation}

    pflogsumm is designed to provide an over-view of Postfix activity, with
    just enough detail to give the administrator a ``heads up'' for
    potential trouble spots.

\end{quotation}

\noindent{}\url{http://jimsun.linxnet.com/postfix_contrib.html} \newline{}
(Last checked 2008/04/09.)

Pflogsumm produces a report designed for troubleshooting rather than for
in-depth analysis.  It does not support saving any data or extracting more
data than is required for producing the report.  Both the parsing and
reporting are difficult to extend as it is a specialised tool, unlike the
easily extensible design of \parsername{}.  It does not correlate log lines
by queueid or \pid{}, and does not need to deal with the complications
encountered during this project.  Pflogsumm produces a useful report, and
successfully parsed the \numberOFlogFILES{} log files tested
with,\footnote{The results it reported were not verified in detail, but it
did not report any errors, and has a very good reputation amongst Postfix
users.} but it is not a suitable base for this project.

Pflogsumm's report includes the following:

\begin{itemize}

    \item Total number of mails accepted, delivered, and rejected.  Total
        size of mails accepted and delivered.  Total number of sender and
        recipient addresses and domains.

    \item Per-hour averages and per-day summaries of the number of mails
        received, delivered, deferred, bounced, and rejected.

    \item For received mail: per-domain totals for mails sent, deferred,
        average delay, maximum delay, and bytes delivered.  For received
        mail: per-domain totals for size and number of mails received.

    \item Number and size of mails sent and received for each address.

    \item Summary of why mail delivery was deferred or failed, why mails
        were bounced, why mails were rejected, and warning messages.

\end{itemize}

Many options are available to include or exclude certain sections of the
report.

\subsection{Sawmill Universal Log File Analysis and Reporting}

\begin{quotation}

    Sawmill is a Postfix log analyzer (it also support [sic] 686 other log
    formats).  It can process log files in Postfix format, and generate
    dynamic statistics from them, analyzing and reporting events.  Sawmill
    can parse Postfix logs, import them into a SQL database (or its own
    built-in database), aggregate them, and generate dynamically filtered
    reports, all through a web interface.  Sawmill can perform Postfix
    analysis on any platform, including Window, Linux, FreeBSD, OpenBSD,
    Mac OS, Solaris, other UNIX, and more.

\end{quotation}

\noindent{}\url{http://www.thesawmill.co.uk/formats/postfix.html} \newline{}
\url{http://www.thesawmill.co.uk/formats/postfix_ii.html} \newline{}
\url{http://www.thesawmill.co.uk/formats/beta_postfix.html} \newline{}
(Last checked 2007/12/04.)

XXX CHECK ALL THIS AGAIN\@; GIVE EXAMPLE PRICES\@.

Sawmill is a general purpose commercial product that parses 687 log file
formats (correct as of 2007/12/04) and produces reports.  Its data
extraction facilities are too limited to save sufficient data for the
purposes of this project,\footnote{The data extracted by Sawmill is
described later in this review, and the data extracted by \PLP{} is
described in \sectionref{connections table} and \sectionref{results
table}.} though it does extract three different sets of data for Postfix
(one is beta as of 2007/12/04), but they do not appear to be interlinked.
The documentation does not suggest that any attempt is made to correlate
log lines or deal with the difficulties documented in
\sectionref{complications}.  The source code is available in an obfuscated
form only (presumably for a fee), and the product is quite expensive,
requiring a license per report to be generated; in contrast \parsername{}
is free and the code is freely available.  The web interface allows
creation of dynamic reports based on any field, but because of the
associated cost the author has not experimented with it.  Presumably, as
the company charges per report the user wishes to generate, the data store
(if there is one) is deliberately inaccessible and undocumented to prevent
the user bypassing the program and generating their own reports.

XXX THE BLURB MENTIONS \SQL{} DB\@; CAN THAT BE ACCESSED SEPARATELY, IS THE
FORMAT DOCUMENTED, ETC\@?

\url{http://www.thesawmill.co.uk/formats/postfix.html} \newline{} Fields
extracted: from, to, server, UID, relay, status, number of recipients,
origin hostname, origin \IP{} and virus.  The fields \texttt{server},
\texttt{uid} and \texttt{virus} are not explained in the documentation:
\texttt{server} is probably the server the mail is delivered to, and
\texttt{uid} might be the uid of the user submitting mail locally.  Postfix
does not perform any form of virus checking (though it has many options for
cooperating with an external virus scanner), so the \texttt{virus} field is
a mystery.

\url{http://www.thesawmill.co.uk/formats/postfix_ii.html} \newline{} Fields
extracted: from, to, RBL, client hostname and client \IP{}\@.

\url{http://www.thesawmill.co.uk/formats/beta_postfix.html} \newline{} Fields
extracted: from, to, client hostname, client \IP{}, relay hostname, relay
\IP{}, status, response code, RBL and message id.

Even if the three data sets were linked together Sawmill would extract less
data than \parsername{}, and it does not appear to extract data about
rejections except when the rejection is caused by a \DNSBL{} check.

When tested with the \numberOFlogFILES{} test log files it performed
adequately, though the rate it processed log files at did slow down
noticeably as it progressed.  Sawmill supports reading compressed log files
but it exhibits a dramatic slow down when doing so: it took six hours to
parse the first half of the log files, and twelve hours to parse the next
third; after twenty four hours parsing the remaining sixth it crashed due
to lack of disk space.  On the second parsing attempt the log files were
uncompressed beforehand and parsing took eight hours.

Sawmill's web interface supports searching on any combination of the fields
it extracts, but the interface is neither as simple to use nor as
informative as the interface provided by Splunk.  The administrative
interface is easy to use --- it took only five minutes to start parsing a
directory of log files.

XXX ADD A SUMMARY\@.

\subsection{Splunk}

\begin{quotation}

    Splunk is an IT Search engine. It is software that indexes any format
    of IT data from any source in real time, including logs,
    configurations, scripts, code, messages, traps, alerts, activity
    reports, stack traces and metrics from all of your applications,
    servers and devices. Splunk lets you search, navigate, alert and report
    on all your IT data in real time using an AJAX web interface. You can
    also share knowledge and Splunk solutions with other members of the
    Splunk community via SplunkBase.

\end{quotation}

\noindent{}\url{http://www.splunk.com/} \newline{}
(Last checked 2008/04/29.)

XXX THE FIRST PARAGRAPH NEEDS TO BE REWRITTEN\@; PICK POINTS, THEN COMPARE
AND CONTRAST\@.

Splunk aims to index all an organisation's log files, providing a
centralised view capable of searching and correlating diverse log sources.
The web interface supports complicated searches, providing statistics and
graphs in real time, a facility not provided by \parsername{} (report
generation has been deferred to a subsequent program).  Saved searches can
be run periodically and the results emailed to a recipient or sent to an
external program for further processing (though maybe without the graphs
and detailed statistics); the author was unable to save searches, though
that may have been due to limitations in the free version.  The database is
not available for use by external programs, whereas \parsername{} provides
the database and leaves it to the user to utilise it without limit or
restriction.  The interface is optimised for interactive rather than
automated queries and it does not appear to be possible to write
independent tools to utilise the Splunk database; there are additional
reports available on \url{http://www.splunkbase.com/}, but the author was
unable to find any: every category was empty, even those that the interface
claimed had reports available.  Many types of reports are bundled with the
software, though most are variations of a bar or pie chart, except bubble
and heatmap graphs.  It is easy to drill down through the graphs to extract
a portion of the data (e.g.\ select the hour with the largest number of
events, then select a particular host, and finally a specific address),
though it is not possible to search on partial words.  All searches
performed using the indexed data returned reasonable results.  The web
interface is quite attractive and simple to use when searching, but as an
administrator it seems unnecessarily difficult to perform simple tasks.

When testing it took roughly 30 minutes to add a single log file to be
indexed (log files must be indexed before searching), with the downside
that the log file was copied into a spool directory, doubling the disk
space usage.  The most suitable test would be to index all the log files in
a particular directory, but after three hours, numerous futile attempts,
and reading all the available documentation the author admitted defeat.
Using the \CLI{} was more successful: invoking the command \newline{}
\tab{} \texttt{splunk find logs }\textit{log-directory\/}\newline{} added
40 of the \numberOFlogFILES{} log files to the queue for indexing.
Repeated attempts enqueued the same 40 log files, without explaining why
the others were excluded.\footnote{The log files appear to have been
indexed once only; presumably Splunk keeps track of the files it has
indexed and discards requests to index files for a second time.  This may
or may not be a useful feature for \parsername{}.} There did not appear to
be an option to order the log files to ensure they would be processed in
the order they were created, though this may not be necessary or beneficial
with Splunk.  Subsequently the author was successful in adding a single
file at a time using the \CLI{}; a simple loop to add all desired log files
was sufficient to index all files.  Splunk will periodically check all
those files for updates unless they are manually removed from its list;
this may or may not be useful behaviour.  Splunk did not appear to have any
difficulty in parsing the log files, once instructed to do so.
\parsername{} parses the logs it is instructed to parse, in the order
given; periodic parsing of logs is a task an administrator can easily
achieve with \texttt{cron(8)} and \texttt{logrotate(8)}.

Copious documentation is made available on \url{http://www.splunk.com/},
but poor organisation and sheer abundance makes it extremely hard to find
useful information.  Searching using the website's interface confusingly
tends to return results from old documentation rather than new.  In
general the documentation appears to have been written by someone
intimately acquainted with the software and having difficulty understanding
how a newcomer would approach tasks or the questions they would ask.

Splunk supports reading compressed log files without any configuration by
the user.  The free version of Splunk limits the volume of data indexed per
day to 500MB, though a trial Enterprise licence is available that allows
indexing of up to 5GB of data per day.  In 2007 the cheapest licensed
version cost \$5000 plus \$1000 support, and limited the volume of data
indexed per day to 500MB\@.  Prices were removed from the Splunk website
during 2008; now Splunk's sales team must be contacted for a quote.
Typical log file sizes for a small scale mail server are given in
\sectionref{parser efficiency}.

When parsing Postfix log files Splunk extracts some standard fields: to and
from addresses, HELO hostname, time and date, the host the log files were
generated on, and protocol (\SMTP{} or \ESMTP{}).  It parses the standard
syslog fields at the beginning of the log line, and extracts any
\texttt{key=<value>} pairs occurring after the standard syslog prologue;
these pairs are the fields listed above.  Searches can be based on the
extracted fields or the full text of the log line.  \parsername{} extracts
noticeably more data, though it does not make the full text of the line
available, and the full power of \SQL{} is available when searching,
allowing the user to search on arbitrarily complicated conditions.

Splunk is a generic tool, so it lacks any Postfix specific support over and
above extracting the \texttt{key=<value>} fields from a log line; most
importantly it makes no attempt to correlate log lines by queueid or
\pid{}, or to handle any of the myriad complications discussed in
\sectionref{complications}.  Some additional Postfix reports are supposedly
available at \url{http://www.splunkbase.com/}, but the author was unable to
find Postfix reports, or additional reports for any other log file types.

\newpage{}

\subsection{Isoqlog}

\begin{quotation}

    Isoqlog is an MTA log analysis program written in C. It designed to
    scan qmail, postfix, sendmail and exim logfile and produce usage
    statistics in \HTML{} format for viewing through a browser. It produces
    Top domains output according to Sender, Receiver, Total mails and
    bytes; it keeps your main domain mail statistics with regard to Days
    Top Domain, Top Users values for per day, per month and years.

\end{quotation}

\noindent{}\url{http://www.enderunix.org/isoqlog/} \newline{}
(Last checked 2007/08/13.)

Isoqlog's report lists daily, monthly, and annual totals of mails sent by
each unique sender and bytes transferred (XXX EXPLAIN BYTES TRANSFERRED\@:
TO/FROM, PER-SENDER\@?).  It only produces these reports for the domains
listed in its configuration file, making it impossible to produce reports
for every sender domain.  It ignores all log lines except those for today,
so it is impossible to analyse historical logs, and testing with the
\numberOFlogFILES{} test log files is pointless.  It does maintain a record
of data previously extracted, which the newly extracted data is merged into
(no information is provided on the format of the data store).  The data
extracted is limited to the number of mails sent by each sender, unlike the
copious amounts of data extracted by \parsername{}.  It does not utilise
rejection log lines in any way, so is unsuitable for the purposes of this
project.  Its parsing is completely inextensible, indeed is almost
incomprehensible, relying on \texttt{scanf(3)}, unexplained fixed offsets,
and low level string manipulation; it is the opposite end of the spectrum
to \parsernames{} parsing.  It does not handle any of the complications
discussed in \sectionref{complications}, does not gather the breadth of
data required for this project, and ignores the majority of log lines
produced by Postfix.

XXX FINISH THIS\@.

\subsection{AWStats}

\begin{quotation}

    AWStats is a free powerful and featureful tool that generates advanced
    web, streaming, ftp or mail server statistics, graphically. This log
    analyzer works as a CGI or from command line and shows you all possible
    information your log contains, in few graphical web pages. It uses a
    partial information file to be able to process large log files, often
    and quickly. It can analyze log files from all major server tools like
    Apache log files (NCSA combined/XLF/ELF log format or common/CLF log
    format), WebStar, IIS (W3C log format) and a lot of other web, proxy,
    wap, streaming servers, mail servers and some ftp servers.

\end{quotation}

\noindent{}\url{http://awstats.sourceforge.net/} \newline{}
\url{http://awstats.sourceforge.net/awstats.mail.html} \newline{}
\url{http://awstats.sourceforge.net/docs/awstats_faq.html#MAIL} \newline{}
(Last checked 2007/10/13.)

AWStats will produce simple graphs for many different services, but
supporting many different services without special purpose code limits its
functionality.  The data it will extract from an \MTA{} log file is limited
in comparison to \parsername{}: \newline{} \tab{} time2, email, email\_r,
host, host\_r, method, url, code, and bytesd.\newline{} XXX WHAT FIELDS ARE
MISSING\@?  There does not seem to be an explanation of any of those fields
in the documentation (\parsername{} provides copious documentation).
AWStats coerces Postfix log files into Apache\footnote{The Apache web
server is the most popular HTTP server in use over the past 10 years; more
information is available at \url{http://httpd.apache.org/}.} format log
files, for analysis by AWStats' HTTP log file parser.  The converting
parser only deals with a small portion of the log lines generated by
Postfix, silently skipping those it cannot deal with, and does not
distinguish between different types of rejection; it would be a lot of work
to extend it to handle new log lines.  Although it does correlate log lines
by queueid (not by pid), it does not deal with any of the other
complications described in \sectionref{complications}.  AWStats supports
saving data but the format of the saved data is not documented, as far as
the author could tell.  It also supports reading compressed log files, but
that functionality was not tested.

When tested with the \numberOFlogFILES{} test log files AWStats' reported
that it parsed 9,240,075 (88.70\%) of 10,416,129 log lines, skipping
1,176,050 (11.29\%) corrupt log lines; however there are
\numberOFlogLINES{} lines in the \numberOFlogFILES{} log files, so AWStats
parsed only 17.15\% of the input lines, ignoring the remaining 82.85\%.

The graphs it produces give an overview of mails received for the last
calendar month, showing:

\begin{itemize}

    \item The number of mails accepted from each host.

    \item How many mails were received by each recipient.

    \item The average number of mails accepted by the server per-day and
        per-hour.

    \item The top ten \SMTP{} error codes (XXX SENDING, RECEIVING, OR
        BOTH\@?), e.g.\ the first entry in the list is: \texttt{Requested
        mail action not taken: relaying not allowed, unknown recipient
        user, \ldots}

\end{itemize}

% Parsed lines in file: 10416129
%  Found 4 dropped records,
%  Found 1176050 corrupted records,
%  Found 0 old records,
%  Found 9240075 new qualified records.

\subsection{Anteater}

\begin{quotation}

    The Anteater project is a Mail Traffic Analyser. Anteater supports
    currently the logformat produced by Sendmail and by Postfix. The tool
    is written in 100\% C++ and is very easy to customize. Input, output,
    and the analysis are modular class objects with a clear interface.
    There are eight useful analyse modules, writing the result in plain
    ASCII or \HTML{}, to stdout or to files.

\end{quotation}

\noindent{}\url{http://anteater.drzoom.ch/} \newline{}
(Last checked 2007/08/13.)

Anteater does not have any English documentation so it is impossible for
this author to accurately comment on the analysis it performs.  It did not
run successfully when tested, and its parsing would certainly be out of
date as Postfix has evolved considerably since this tool was last updated
(November 2003).  As it neither ran successfully nor has documentation the
author can read a detailed review cannot be provided.

The Debian project (\url{http://www.debian.org/}) provides a manual page
with the copy of anteater it distributes, so the author was at least able
to run anteater with the correct arguments; sadly anteater produced zero
for every statistic, presumably because it was unsuccessful in parsing the
log lines.

\subsection{Yet Another Advanced Logfile Analyser}

\begin{quotation}

    yaala is a very flexible analyser for all kinds of logfiles. It uses
    parsers to extract information from a logfile, an SQL-like query
    language to relate the information to each other and an output-module
    to format the information appropriately.

\end{quotation}

\noindent{}\url{http://yaala.org/} \newline{}
(Last checked 2007/10/09.)

YAALA uses a plugin based system to analyse log files and produce \HTML{}
output reports, with all the parsing and report generation handled by
modules.  Using YAALA as a base would be only slightly less work than
starting from scratch, as both input and output modules would need to be
written specially; it may even be more work to implement the parser within
the constraints of YAALA\@.  YAALA supports storing previously gathered
data using Perl's Storable module~\cite{perl-storable}, so other Perl
programs can use Storable to load, examine, and optionally modify the data;
\parsername{} uses a well documented database which is accessible from the
majority of programming languages.  This information was gleaned from the
source code, as the documentation is sadly lacking.

YAALA provides a Postfix parser that extracts the following two types of
fields from specific log lines:

\begin{eqlist}

    \item [Aggregations:] count (not explained), bytes (sum of bytes
        transferred).

    \item [Keyfields:] incoming\_host, outgoing\_host, date, hour, sender,
        recipient, defer\_count, delay.

\end{eqlist}

YAALA extracts most of the fields \parsername{} does, but unlike
\parsername{} it does not maintain separate counters for each restriction;
this rules out the possibility of using the collected data for
optimisation, testing or understanding of restrictions.  YAALA's Postfix
parser does not deal with the complications explained in
\sectionref{complications}, though it does correlate log lines by queueid.

YAALA provides a mini-language based on \SQL{} that is used when generating
reports; sample reports can be seen
at~\url{http://www.yaala.org/samples.html}.  Example query for HTTP proxy
servers: \newline{} \tab{} \texttt{requests BY file WHERE host =\~{}
Google} \newline{} The mini-language is quite limited and cannot be used to
extract data for external use, merely to create reports.  Only data
selected by the query will be saved in the data store; other data will be
discarded, and removed from the data store if already present.

Testing YAALA was unsuccessful because all the select clauses tried
produced a similar error message:
\newline{}\tab{}\texttt{lib/Yaala/Data/Core.pm: Unavailable aggregation
requested:} \newline{}\tab{}\tab{}\texttt{``bytes''. Returning 0.}
\newline{}  The underlying reason for this is that YAALA only parsed 408
(0.11\%) of 360632 log lines in the first log file; it was not tested with
the remainder of the \numberOFlogFILES{} log files.

In summary YAALA provides a Postfix parser that tries to parse the most
common Postfix log lines only, provides reasonably flexible report
generation from the limited data extracted, but has no facilities to
extract data for use in other tools.

\subsection{Lire}

\begin{quotation}

    As any good system administrator knows, there's a lot more to keep
    track of in an active network than just webservers. Lire is hands down
    the most versatile log analysis software available today. Lire not only
    keeps you informed about your HTTP, FTP, and mail traffic, it also
    reports on your firewalls, your print servers, and your DNS activity.
    The ever growing list of Lire-supported services clearly outstrips any
    other software, in large part thanks to the numerous volunteers who
    have pioneered many new services and features. Lire is a total solution
    for your log analysis needs.

    The email servers' reports will show you the number of deliveries and
    the volume of email delivered by day, the domains from which you
    receive or send the most emails, the relays most used, etc.

\end{quotation}

\noindent{}\url{http://logreport.org/lire.html} \newline{}
(Last checked 2008/04/29.)

Lire is a general purpose log parser supporting many different types of log
file.  The data extracted by its Postfix parser is shown in the quote
above; notably rejections are not mentioned or dealt with, and unlike
\parsername{} there is no facility to extend the parser.  It supports
multiple output formats for generated reports (text, HTML, PDF, and Excel
95) but the reports do not appear to be customisable; \parsername{} does
not produce any reports.  Lire produces a report with less detail than
Pflogsumm, and is considerable harder to configure.  Lire supports saving
extracted data for later report generation, but accessing this data from
another application is undocumented; given the source code it should be
possible, with enough time and effort, to understand the format.
\parsername{} uses an \SQL{} database to make accessing the extracted data
as easy as possible.  

Like AWStats and Logrep, Lire attempts to correlate log lines by queueid,
but not by \pid{}, so the complete list of recipients for a mail should be
available; however its parser extracts only part of the available data and
makes no attempt to deal with the other complications described in
\sectionref{complications}.  When testing Lire on the \numberOFlogFILES{}
test log files it performed reasonably well, producing summaries of: 

\begin{itemize}

    \item Delivery status and failed deliveries.

    \item Sender and recipient domains and servers.

    \item Number of deliveries and bytes per-day and per-hour.

    \item Recipients by domain.

    \item Deliveries by relays, by size, and by delay.

    \item Delays by server and by domain.

    \item Which pair of correspondents exchanged the highest number of
        emails.

\end{itemize}

The numbers it reports appear reasonable, and the subset verified by the
author were correct.  

\subsection{Logrep}

\begin{quotation}

    Logrep is a secure multi-platform framework for the collection,
    extraction, and presentation of information from various log files. It
    features HTML reports, multi dimensional analysis, overview pages, SSH
    communication, and graphs, and supports over 30 popular systems
    including Snort, Squid, Postfix, Apache, Sendmail, syslog, ipchains,
    iptables, NT event logs, Firewall-1, wtmp, xferlog, Oracle listener and
    Pix.

\end{quotation}

\noindent{}\url{http://www.itefix.no/phpws/index.php} \newline{}
(Last checked 2007/11/18.)

Logrep extracts roughly half the fields \parsername{} does:

XXX WHAT FIELDS ARE MISSING\@?

\begin{itemize}

    \item For mail sent and received: from address, size, and time and
        date.

    \item For mail sent: to addresses, \SMTP{} code, and delay.

    \item For mail received: the hostname of the sender.

\end{itemize}

It also counts the number of log lines parsed and skipped.  Log lines are
correlated based on the queueid (referred to as sessionname [sic] within
Logrep), but not by \pid{}.  The parsing is error prone: empty fields are
saved when the log line does not match the \regex{}, though it appears that
they will not overwrite existing data.  Most notably rejections are
completely ignored, making it unsuitable for the purposes of this project.
It does not try to address any of the complications in
\sectionref{complications} except for correlating by queueid.

Logrep does not come with any documentation, though some scant
documentation is available on its website (\parsername{} provides copious
documentation).  It requires a web browser to interact with it, so
automated log processing will be difficult, whereas automated processing is
a key part of \parsernames{} design.  Sadly all the author's attempts to
use Logrep failed, as it was unable to access the log files selected; this
appears to be a bug rather than operator error.  If it was caused by
operator error, the interface needs improvement as the (minimal)
instructions were followed as closely as possible, and multiple attempts
were made.  As parsing failed it was not possible to review the reports
Logrep can generate (available in \HTML{} only), or to examine the
(undocumented) format in which it can save extracted data for subsequent
reuse.

Logrep extracts far less data from Postfix log files than \parsername{},
completely ignores rejections, is effectively undocumented, does not deal
with the more complicated aspects of Postfix log files, and at the time of
writing does not work properly.

\subsection{Summary}

There are other programs available which perform basic Postfix log parsing
(some to a greater level of detail than others), but few attempt to
correlate log lines by queueid (none correlate by \pid{}) to produce an
overall record of the journey of each mail through Postfix.  None of the
reviewed parsers collect the breadth of information gathered by
\parsername{}, or make it as easy to extend the parser to handle new log
lines.  Most of the parsers generate a report and immediately discard the
data extracted from the log files; those that do not discard the data
typically retain it in a format inaccessible to other tools.  Nearly all of
the parsers reviewed can produce a report of greater or lesser detail and
complexity, unlike \parsername{}.  The quality of the documentation offered
by the subset of parsers that provide some varies from unusable to good;
none of the parsers provide any documentation on the format of their data
stores (if they have one).  Fewer than half of the parsers were capable of
parsing the \numberOFlogFILES{} test log files, and improving or extending
parsing would have been quite a difficult task for any of the parsers.
Table \refwithpage{Summary of parsers' features} provides a summary of the
parsers' features.

The overriding difference between \parsername{} and the other parsers
reviewed herein is that none of them aim for the high level of
understanding of Postfix log files achieved by \parsername{}.


\begin{table}[htb]
    \caption{Summary of parsers' features}
    \empty{}\label{Summary of parsers' features}
    \begin{tabular}{llllll}
        Parser          & Parsed test   & Data              & Custom            & Documentation  & Source \\
                        & log files?    & store?            & reports?          & quality?       & code?  \\
        \tableline{}%
        \LMA{}          & No            & Yes               & No                & Poor           & Yes    \\ 
        Pflogsumm       & Yes           & No                & Partial \dag{}    & Good           & Yes    \\
        Sawmill         & Yes           & Yes               & Searches          & Reasonable     & No     \\
        Splunk          & Yes           & Yes               & Searches          & Abundant       & No     \\
                        &               &                   & \& reports        & but poor       &        \\
        Isoqlog         & No            & Yes               & No                & Poor           & Yes    \\
        AWStats         & Partially     & Yes               & Partial \dag{}    & Good           & Yes    \\
        Anteater        & No            & No                & No                & Poor           & Yes    \\
        YAALA           & No            & Yes \ddag{}       & Searches          & Poor           & Yes    \\
        Lire            & Yes           & Yes               & Yes               & Reasonable     & Yes    \\
        Logrep          & No            & Yes               & No                & Poor           & Yes    \\
        \parsername{}   & Yes           & Yes \nialpha{}    & No \nibeta{}      & XXX            & Yes    \\
        \tableline{}
    \end{tabular}

    \begin{eqlist}

        \item [\dag{}] Sections can be omitted from a report, but extra
            sections can not be added.

        \item [\ddag{}] YAALA only stores the data required to produce the
            latest report; other data will be discarded.

        \item [\nialpha{}] \parsername{} is the only parser with
            documentation for its data store.

        \item [\nibeta{}] \parsername{} defers report generation to
            subsequent programs, but all the necessary data and
            documentation to produce reports is provided.

    \end{eqlist}

\end{table}

