\chapter{State of the Art Review}

\label{state of the art review}

At the start of this project ten Postfix log file parsers were tested, with
the hope of finding a suitable parser to build upon, rather than starting
from scratch.  There are not many Postfix log file parsers available ---
indeed it was quite difficult to find ten parsers to review for this
project --- and the functionality offered ranges from quite basic to much
more mature, depending on the needs of the author of the parser.  None of
those parsers were suitable for this project, so the decision was taken to
write a new parser.  The first parser reviewed is the only previously
published research in this area that the author is aware of; it parses
Postfix log files, but aim of the research is to show that presenting
extracted data in a more easily accessible format is useful to systems
administrators, rather than to improve anti-spam techniques..

The same ten parsers have been reviewed and compared to this project's
finished parser, to show how much effort would have been required to fulfil
the aims and requirements of this project.  It is important to compare and
contrast newly developed algorithms and parsers against those already
available, to accurately judge what improvements, if any, are delivered by
the newcomers.

XXX SHOULD THIS BE IN THE CONCLUSION INSTEAD\@?

There are some important differences between \parsername{} and the parsers
reviewed here:

\begin{enumerate}

    \item None of the parsers reviewed perform the kind of advanced parsing
        required for this project or deal with the complications described
        in \sectionref{complications}.

    \item Only \parsername{} enables parsing of new log lines without
        extensive and intrusive modifications to the parser; \parsernames{}
        architecture is described in \sectionref{why separate rules,
        actions, and framework?}.

    \item The parsers reviewed all produce a report of varying complexity
        and detail, whereas \gls{PLP} does not; it extracts data and leaves
        generation of reports from the data to other programs.  Using an
        \gls{SQL} database simplifies the process of generating such
        reports (discussed in \sectionref{database as API}); some sample
        queries are given in \sectionref{motivation}.  The parser developed
        for this project is designed to enable much more detailed log file
        analysis by providing a stable platform for subsequent programs to
        develop upon.

    \item Most of the reviewed parsers silently ignore log lines they
        cannot handle, whereas \parsername{} complains loudly about every
        single log line it fails to parse.  The exception is AWStats, which
        outputs the percentage of input lines it was unable to parse, but
        does not output the lines themselves.

    \item A minor difference is that most parsers do not handle compressed
        files; both \parsername{} and Splunk handle them transparently,
        without user intervention; Sawmill and Lire can be configured to
        support compressed files, but Sawmill exhibits a dramatic increase
        in parsing time when doing so.  Although this is a minor
        disparity, support for reading compressed log files is quite
        helpful, as it dramatically reduces the disk space required to
        store historical log files.

    \item Some of the parsers reviewed save the extracted data to a data
        store, but the majority discard all data once they have finished
        generating their report, making historical analysis impossible
        without parsing all log files every time.

\end{enumerate}

Each of the reviewed parsers was tested with the \numberOFlogFILES{} test
log files described in \sectionref{parser efficiency}.  The data extracted
by \parsername{} is documented in \sectionref{connections table} and
\sectionref{results table}; for convenience that list is repeated here:
server \gls{IP}, server hostname, client \gls{IP}, client hostname, helo
hostname, queueid, start time, end time, \gls{SMTP} code, sender,
recipient, size, message ID\@.

\section{Log Mail Analyser}

\label{prior art}

There only appears to be one prior published paper about parsing Postfix
log files: \textit{Log Mail Analyzer: Architecture and Practical
Utilizations\/}~\cite{log-mail-analyser}.  The aim of \gls{LMA} is quite
different from \parsername{}: it attempts to present correlated data from
log files in a form suitable for a systems administrator to search using
the myriad of standard Unix text processing utilities already available.
It produces a \gls{CSV} file and either a MySQL
(\url{http://www.mysql.com/}) or Berkeley DB
(\url{http://www.oracle.com/database/berkeley-db/index.html}) database.
The decision to support both \gls{CSV} and Berkeley DB appears to have been
a serious limitation: XXX EXTEND\@: LIMITATIONS WILL BE EXPLAINED LATER OR
SOMETHING\@.  Very little documentation is provided with \gls{LMA}, though
some documentation is available in~\cite{log-mail-analyser}.  Studying the
source code is informative, though this author had difficulty as the
authors of \gls{LMA} wrote in Italian.

\gls{CSV} is a very simple format where each record is stored in a single
line, with fields separated by a comma or other punctuation symbol.
Problems with \gls{CSV} files include the need to escape separators in the
data stored, providing multiple values for a field (e.g.\ multiple
recipients), and adding new fields.  There is no standard mechanism to
document the fields or the separator, unlike \gls{SQL} databases where
every database includes a schema naming the fields and the type of data
they store (integer, text, timestamp, etc.).  The \gls{CSV} record format
is not documented, but the output file contains a comment giving the
format:\newline{} \texttt{\# Timestamp|Nome Client|IP Client|IP
Server|From|To|Status|Size} \newline{}\gls{LMA} treats lines starting with
\texttt{\#} as comments, but not all \gls{CSV} parsers will.

Berkeley DB only supports storing simple \textbf{(key, value)} pairs,
unlike \gls{SQL} databases that store arbitrary tuples.  In \gls{LMA}'s
main table the key is an integer referred to by secondary tables, and the
value is a \gls{CSV} line containing all of the data for that row.  The
secondary by-sender, by-recipient, by-date, and by-\gls{IP} tables use the
sender/recipient/date/\gls{IP} as the key, and the value is a \gls{CSV}
list of integers referring to the main table.  This effectively
re-implements \gls{SQL} foreign keys, but without the functionality offered
by even the most basic of \gls{SQL} databases (joins, ordering, searches,
etc.).  It also requires custom code to search on some combination of the
above, though the authors of \gls{LMA} did provide some queries: IP-STORY,
FROM-STORY, DAILY-EMAIL, and DAILY-REJECT\@.  Berkeley DB appears to be the
least useful of the three output formats: it does not provide the
functionality of a basic \gls{SQL} database, and unlike \gls{CSV} files it
can not be used with standard Unix text processing tools.

The schema used with the MySQL database is undocumented, but at least it is
possible to discover the schema with an existing \gls{SQL} database, unlike with
Berkeley DB\@; all \gls{SQL} databases embed the schema into the database
and provide commands for displaying it.  Berkeley DB does not embed a
schema, as there is neither requirement nor benefit; it only provides
\textbf{(key, value)} pairs, so any additional structuring of the data is
imposed by the application, thus the application must document this
structure.  MySQL support was not tested because there is no documentation
on the schema required.

Whether a MySQL database or Berkeley DB table is chosen in addition to the
\gls{CSV} output, \gls{LMA} stores the following data: time and date,
client hostname and \gls{IP} address, server \gls{IP} address, sender and
recipient addresses, \gls{SMTP} code, and size (for accepted mails only).
It is unclear which time and date is stored: start time, end time, or
delivery time?  Unlike \parsername{} it does not store the server hostname,
helo hostname, queueid, start and end times, timestamps for each log line,
or message id (for accepted mails only).  Handling of multiple recipients,
\gls{SMTP} codes, or remote servers\footnote{A single mail may be sent to
multiple remote servers if it was addressed to recipients in different
domains, or Postfix needs to try multiple servers for one or more
recipients.} is not explained; experimental observation shows that multiple
records are added when there are multiple recipients (sadly the records are
not associated or linked in any way), and presumably the same approach is
taken when there are multiple destination servers.

\gls{LMA} requires major changes to the parser code to parse new log lines
or to extract additional data.  The code is structured as a long series of
blocks that each handle all log lines matching a single regex, so parsing
new log lines requires modifying an existing regex or carefully inserting a
new block in the correct place; extracting extra data will require
modifying multiple blocks, regexes, or both.

\gls{LMA} does not deal with any of the complications discussed in
\sectionref{complications}, except for correlating log lines by queueid;
not correlating log lines by pid means it cannot correlate most rejections.
It does not differentiate between different types of rejections, so it is
not suitable for the purposes of this project; the data about which
restriction caused the rejection is discarded, whereas the main goal of
this project is to retain that data to aid optimisation and evaluation of
anti-spam techniques.  \gls{LMA} fails to parse Postfix log files generated
on Solaris hosts because the fields automatically prepended to each log
line differ from those added on Linux hosts; log files from Solaris hosts
(and possibly other operating systems) thus require preprocessing before
parsing by \gls{LMA}.  Once the preprocessing has been performed on the
\numberOFlogFILES{} test log files \gls{LMA} parses the log files without
complaint, although it produced 32 entries in its output file for every
rejection in the input log file; it also missed some 40\% of delivered
mail.  Once these glaring deficiencies were discovered the author did not
waste any more time checking the results.

\gls{LMA} does provide some simple reports: IP-STORY, FROM-STORY,
DAILY-EMAIL and DAILY-REJECT\@.  These reports search the Berkeley DB files
for matching records: the first three extract \gls{CSV} lines for the
specified client \gls{IP} address, sender address, or date respectively.
DAILY-REJECT initially failed with an error message from the Perl
interpreter;\footnote{The error messages were: \newline{}\texttt{Undefined
subroutine \&main::LIST called at queryDB.pl line
372.}\newline{}\texttt{Undefined subroutine \&main::EXTRACT\_FROM\_DB
called at queryDB.pl line 379.}} after making corrections to the code it
worked, extracting the \gls{CSV} lines for the specified day where the
\gls{SMTP} code signifies a rejection.  All of these reports are trivially
simple to produce from the \gls{CSV} file using the standard Unix tool
\texttt{awk}\glsadd{awk}; the most complicated, DAILY-REJECT, is merely:

% perl queryDB.pl -dayreject 2007-01-26 > lma-query

\begin{verbatim}
awk -F\| 'BEGIN { previous = "" };
    $1 ~ /2007-01-26/ && $7 != "250" && $0 != previous 
    { print $0; print " "; previous = $0; }' lma_output.txt
\end{verbatim}

Notes about the command above:

\begin{itemize}

    \item It outputs a line containing only a single space after each
        matching record, to accurately replicate the output of
        DAILY-REJECT\@.

    \item DAILY-REJECT considers all \gls{SMTP} codes except \texttt{250}
        to be rejections; this includes invalid \gls{SMTP} codes such as
        \texttt{0} and \texttt{deferred}, so the awk command does too.
        These invalid \gls{SMTP} codes are most likely present because of
        incorrect parsing by \gls{LMA}.

    \item \gls{LMA} produces 32 output lines in its \gls{CSV} file for
        every single line it should have produced; the command above
        suppresses duplicate sequential lines.

\end{itemize}

There are some differences between the output from DAILY-REJECT and the
\texttt{awk} command; the author did not spend substantial time attempting
to explain these differences.

\begin{enumerate}

    \item The output from DAILY-REJECT is missing some records which are
        present in the \gls{CSV} file; this may be because it uses the
        Berkeley DB files instead, and there may be differences between the
        contents.

    \item Some records output by DAILY-REJECT are truncated: they are
        missing the last | separating fields and the newline following it,
        so the line containing only a single space is concatenated with the
        record.

\end{enumerate}

In summary, \gls{LMA} appears to be a proof of concept, written to
demonstrate the point of their paper (that having this information in an
accessible fashion is useful to systems administrators), rather than a
program designed to be useful in a production environment.

% Literature review notes:
%
% Hard-coded parsing, requiring code changes to add more.  Attempts to
% correlate log lines, saves data to database for data mining purposes.
% Hard to extend/expand/understand.  Appears to only save: date and hour,
% DNS name and \gls{IP} address host, mail server \gls{IP} address, sender,
% receiver and e-mail status (sent, rejected).  Undocumented schema.
% Design decision to use \gls{CSV} as an intermediate format between the
% log file and the database seems to have been restrictive.  Appears to
% require a queueid but majority of log lines (e.g.\ rejections) lack a
% queueid.  Supports whitelisting \gls{IP} addresses when parsing logs, but
% whitelisting when generating reports/data mining would be preferable.
% Supporting Berkeley DB is probably limiting the software - an example is
% the difficulty in searching a pipe-delimited string, so they have
% re-implemented foreign keys with tables keyed by ip address etc.\
% pointing at the main table - this also will not scale well.  There does
% not appear to be any attempt to deal with the complications I have
% encountered: their parsing is not detailed enough to encounter them.  It
% does not run properly; does not create any output; throws up errors.

\section{Pflogsumm}

\begin{quotation}

    pflogsumm is designed to provide an over-view of Postfix activity, with
    just enough detail to give the administrator a ``heads up'' for
    potential trouble spots.

\end{quotation}

\noindent{}\url{http://jimsun.linxnet.com/postfix_contrib.html} \newline{}
(Last checked 2008/11/23.)

Pflogsumm produces a report designed for troubleshooting rather than
in-depth analysis.  It does not support saving any data, and it does not
extract any data than is not required for producing its report.  Both the
parsing and reporting are difficult to extend as it is a specialised tool,
unlike the easily extensible design of \parsername{}.  It does not
correlate log lines by queueid or \gls{pid}, and does not need to deal with
the complications encountered during this project.  Pflogsumm produces a
useful report, and successfully parsed the \numberOFlogFILES{} log files
tested with,\footnote{The results it reported were not verified in detail,
but it did not report any errors, and has a very good reputation amongst
Postfix users.} but it is not a suitable base for this project.  It does
not extract the helo hostname, queueid, start and end times, timestamps for
each log line, or message id.

Pflogsumm has many options are available to include or exclude certain
sections of the report; by default it includes the following:

\begin{itemize}

    \item Total number of mails accepted, delivered, and rejected.  Total
        size of mails accepted and delivered.  Total number of sender and
        recipient addresses and domains.

    \item Per-hour averages and per-day summaries of the number of mails
        received, delivered, deferred, bounced, and rejected.

    \item For received mail: per-domain totals for mails sent, deferred,
        average delay, maximum delay, and bytes delivered.  For received
        mail: per-domain totals for size and number of mails received.

    \item Number and size of mails sent and received for each address.

    \item Summary of why mail delivery was deferred or failed, why mails
        were bounced, why mails were rejected, and warning messages.

\end{itemize}

\section{Sawmill Universal Log File Analysis and Reporting}

\begin{quotation}

    Sawmill is a Postfix log analyzer (it also support 818 other log
    formats). It can process log files in Postfix format, and generate
    dynamic statistics from them, analyzing and reporting events. Sawmill
    can parse Postfix logs, import them into a SQL database (or its own
    built-in database), aggregate them, and generate dynamically filtered
    reports, all through a web interface. Sawmill can perform Postfix
    analysis on any platform, including Window, Linux, FreeBSD, OpenBSD,
    Mac OS, Solaris, other UNIX, and more.

\end{quotation}

\noindent{}\url{http://www.thesawmill.co.uk/formats/postfix.html}
\newline{} \url{http://www.thesawmill.co.uk/formats/postfix_ii.html}
\newline{} \url{http://www.thesawmill.co.uk/formats/beta_postfix.html}
\newline{} (Last checked 2008/11/23.)

Sawmill is a general purpose commercial product that parses 818 log file
formats (as of 2008/11/23) and produces reports from the extracted data.
Its data extraction facilities (described later) are too limited to save
sufficient data for the purposes of this project: although it can extract
three different sets of data from Postfix log files, they are not
interlinked in any way.  The documentation does not suggest that any
attempt is made to correlate log lines or deal with the difficulties
documented in \sectionref{complications}.

Sawmill has three different Postfix log file parsers, extracting three
different sets to data:

\begin{enumerate}

    \item \url{http://www.thesawmill.co.uk/formats/postfix.html} \newline{}
        Fields extracted: from, to, server, UID, relay, status, number of
        recipients, origin hostname, origin \gls{IP}, and virus.  It also
        counts the number of and total size of all mails delivered.  The
        fields \texttt{server}, \texttt{uid}, \texttt{relay}, and
        \texttt{virus} are not explained in the documentation:
        \texttt{server} is probably the server the mail is delivered to,
        and \texttt{uid} might be the uid of the user submitting mail
        locally.  Postfix does not perform any form of virus checking
        (though it has many options for cooperating with an external virus
        scanner), so the \texttt{virus} field is a mystery.

    \item \url{http://www.thesawmill.co.uk/formats/postfix_ii.html}
        \newline{} Fields extracted: from, to, RBL list, client hostname,
        and client \gls{IP}\@.  It also counts the number of and total size
        of all mails delivered.  

    \item \url{http://www.thesawmill.co.uk/formats/beta_postfix.html}
        \newline{} Fields extracted: from, to, client hostname, client
        \gls{IP}, relay hostname, relay \gls{IP}, status, response code,
        RBL list, and message id.  It also counts the number and size of
        all mails delivered, processed, blocked, expired, and bounced.

\end{enumerate}

Even if the three data sets were combined Sawmill would extract less data
than \parsername{}: it omits the helo hostname, queueid, and start and end
times.  Sawmill does not extract any data about rejections except when the
rejection is caused by a \gls{DNSBL} check (\texttt{RBL list} in the list
of fields).

The source code is available in an obfuscated form, and the product is
quite expensive, requiring a \euros{100} + VAT licence per report
(discounts are available when buying multiple licences); in contrast
\parsername{} is free to use and the code is freely available.  Sawmill is
supplied with thorough and well written documentation; everything the
author searched for was documented, except the MySQL database schema.  A
commercial version of MySQL is required due to MySQL licensing
restrictions, but Sawmill's documentation explains why and includes
instructions on how to compile Sawmill so that it can use a non-commercial
version of MySQL (this was not attempted during the review process).

Sawmill's web interface supports searching on any combination of the fields
it extracts, and the results obtained from searches were accurate.  The
interface for searching is neither as simple to use nor as informative as
the interface provided by Splunk.  The administrative interface is much
easier to use than Splunk's: it took only five minutes to start parsing a
whole directory of log files.  Searches performed using the web interface
produced reasonable results, which were not verified in detail.

When tested with the \numberOFlogFILES{} test log files it performed
adequately, though the rate it processed log files at did slow down
noticeably as it progressed.  Sawmill supports reading compressed log files
but it exhibits a dramatic slow down when doing so: it took six hours to
parse the first half of the log files, and twelve hours to parse the next
third; after twenty four hours parsing the remaining sixth it crashed due
to lack of disk space.  On the second parsing attempt the log files were
uncompressed beforehand and parsing took eight hours.

In summary Sawmill suffers from being a general purpose product; it is
probably much more useful when parsing log files where each log line is
self-contained, rather than linked to other log lines (e.g.\ web server log
files).  It is not suitable as a base for this parser, as the source code
made available is obfuscated and not intended for modification; in addition
the architecture would probably need to be overhauled to deal with
correlating log lines.

\section{Splunk}

\begin{quotation}

    Splunk is IT Search.

    Search and navigate IT data from applications, servers and network
    devices in real-time. Logs, configurations, messages, traps and alerts,
    scripts, code, metrics and more. If a machine can generate it ---
    Splunk can eat it. It's easy to download and use and it's very
    powerful.

\end{quotation}

\noindent{}\url{http://www.splunk.com/} \newline{}
(Last checked 2008/11/23.)

XXX THE FIRST PARAGRAPH NEEDS TO BE REWRITTEN\@; PICK POINTS, THEN COMPARE
AND CONTRAST\@.

Splunk aims to index all of an organisation's log files, providing a
centralised view capable of searching and correlating diverse log sources.
The web interface supports complicated searches, providing statistics and
graphs in real time, a facility not provided by \parsername{} (report
generation has been deferred to a subsequent program).  Saved searches can
be run periodically and the results emailed to a recipient or sent to an
external program for further processing (though maybe without the graphs
and detailed statistics); the author was unable to save searches, though
that may have been due to limitations in the free version.  The database is
not available for use by external programs, whereas \parsername{} provides
the database and leaves it to the user to utilise it without limit or
restriction.  The interface is optimised for interactive use rather than
automated queries and it does not appear to be possible to write
independent tools to utilise the Splunk database; there are additional
reports available on \url{http://www.splunkbase.com/}, but the author was
unable to find any: every category was empty, even those that the interface
claimed had reports available.  Many types of reports are bundled with the
software, though most are variations of a bar or pie chart, except bubble
and heatmap graphs.  It is easy to drill down through the graphs to extract
a portion of the data (e.g.\ select the hour with the largest number of
events, then select a particular host, and finally a specific address),
though it is not possible to search on partial words.  All searches
performed using the indexed data returned reasonable results.  The web
interface is quite attractive and simple to use when searching, but as an
administrator it seems unnecessarily difficult to perform simple tasks.

When testing Splunk it took roughly 30 minutes to figure out how to add a
single log file to be indexed so that it could be searched, with the
downside that the log file was copied into a spool directory before
indexing, doubling the disk space usage.  The next test was to index all
the log files in a particular directory, but after three hours, numerous
futile attempts, and reading all the available documentation, the author
admitted defeat.  Using the \gls{CLI} rather than the web interface was
more successful: invoking the command \newline{} \tab{} \texttt{splunk find
logs }\textit{log-directory\/}\newline{} added 40 of the
\numberOFlogFILES{} log files to the queue for indexing.  Further attempts
enqueued the same 40 log files, without explaining why the others were
excluded.\footnote{The log files appear to have been indexed once only;
presumably Splunk keeps track of the files it has indexed and discards
requests to index files for a second time.  This may or may not be a useful
feature for \parsername{}.} There did not appear to be an option to ensure
the log files would be processed in the order they were created, though
this may be neither necessary nor beneficial with Splunk.  Subsequently the
author was successful in adding a single file at a time using the
\gls{CLI}; a simple loop to add all desired log files was sufficient to
index all files.  Splunk will periodically check all those files for
updates unless they are manually removed from its list; this may or may not
be useful behaviour.  Splunk did not appear to have any difficulty in
indexing the log files, once they had been successfully added to its queue.
\parsername{} parses the logs it is instructed to parse, in the order
given; periodic parsing of logs is a task an administrator can easily
achieve with \texttt{cron(8)} and \texttt{logrotate(8)}.

Copious documentation is made available on \url{http://www.splunk.com/},
but poor organisation and sheer abundance makes it extremely hard to find
useful information.  Searching using the website's interface confusingly
tends to return results from old documentation rather than new.  In general
the documentation appears to have been written by someone intimately
acquainted with the software who has difficulty understanding how a
newcomer would approach tasks or the questions they would ask.

Splunk supports reading compressed log files without any configuration by
the user.  The free version of Splunk limits the volume of data indexed per
day to 500MB, though a trial Enterprise licence is available that allows
indexing of up to 5GB of data per day.  In 2007 the cheapest licenced
version cost \$5000 plus \$1000 support, and limited the volume of data
indexed per day to 500MB\@.  Prices were removed from the Splunk website
during 2008; now Splunk's sales team must be contacted for a quote.
Typical log file sizes for a small scale mail server are given in
\sectionref{parser efficiency}.

When parsing Postfix log files Splunk extracts some standard fields: to and
from addresses, HELO hostname, time and date, the host the log files were
generated on, and protocol (\gls{SMTP} or \gls{ESMTP}).  It parses the
standard syslog\glsadd{syslog} fields at the beginning of the log line, and
extracts any \texttt{key=<value>} pairs occurring after the standard syslog
prologue; these pairs are the fields listed above.  Searches can be based
on the extracted fields or the full text of the log line.  \parsername{}
extracts noticeably more data (client and server \gls{IP} and hostname,
queueid, start and end times, timestamps for each log line, \gls{SMTP}
code, and message ID), though it does not make the full text of the line
available (this could be trivially added if desired, but would greatly
increase the size of the resulting database).  The full power of \gls{SQL}
is available when searching the data extracted by \parsername{}, allowing
the user to search on arbitrarily complicated conditions.

Splunk is a generic tool, so it lacks any Postfix specific support over and
above extracting the \texttt{key=<value>} fields from a log line; most
importantly it makes no attempt to correlate log lines by queueid or
\gls{pid}, or to handle any of the myriad complications discussed in
\sectionref{complications}.  Some additional Postfix reports are supposedly
available at \url{http://www.splunkbase.com/}, but the author was unable to
find Postfix reports, or additional reports for any other log file types.

\section{Isoqlog}

\begin{quotation}

    Isoqlog is an MTA log analysis program written in C. It designed to
    scan qmail, postfix, sendmail and exim logfile and produce usage
    statistics in HTML format for viewing through a browser. It produces
    Top domains output according to Sender, Receiver, Total mails and
    bytes; it keeps your main domain mail statistics with regard to Days
    Top Domain, Top Users values for per day, per month and years.

\end{quotation}

\noindent{}\url{http://www.enderunix.org/isoqlog/} \newline{}
(Last checked 2009/01/11.)

Isoqlog's report misses most of the information gathered by \parsername{},
and it only reports on the domains listed in its configuration file, making
it impossible to produce reports for every sender domain.  It ignores all
log lines except those for today, so it is impossible to analyse historical
logs, and testing with the \numberOFlogFILES{} test log files is pointless.
It does maintain a record of data previously extracted, which the newly
extracted data is merged into (no information is provided on the format of
the data store).  The data extracted is limited to the number of mails sent
by each sender, unlike the copious amounts of data extracted by
\parsername{}.  It does not utilise rejection log lines in any way, so is
unsuitable for the purposes of this project.  Its parsing is completely
inextensible, indeed is almost incomprehensible, relying on
\texttt{scanf(3)}, unexplained fixed offsets, and low level string
manipulation; it is the opposite end of the spectrum to \parsernames{}
parsing.  It does not handle any of the complications discussed in
\sectionref{complications}, does not gather the breadth of data required
for this project, and ignores the majority of log lines produced by
Postfix.

\section{AWStats}

\begin{quotation}

    AWStats is a free powerful and featureful tool that generates advanced
    web, streaming, ftp or mail server statistics, graphically. This log
    analyzer works as a CGI or from command line and shows you all possible
    information your log contains, in few graphical web pages. It uses a
    partial information file to be able to process large log files, often
    and quickly. It can analyze log files from all major server tools like
    Apache log files (NCSA combined/XLF/ELF log format or common/CLF log
    format), WebStar, IIS (W3C log format) and a lot of other web, proxy,
    wap, streaming servers, mail servers and some ftp servers.

\end{quotation}

\noindent{}\url{http://awstats.sourceforge.net/} \newline{}
\url{http://awstats.sourceforge.net/awstats.mail.html} \newline{}
\url{http://awstats.sourceforge.net/docs/awstats_faq.html#MAIL} \newline{}
(Last checked 2009/01/11.)

AWStats will produce simple graphs for many different services, but
supporting many different services without special purpose code limits its
functionality.  The data it will extract from an \gls{MTA} log file is
limited in comparison to \parsername{}: time2, email, email\_r, host,
host\_r, method, url, code, and bytesd.  There is no explanation for any of
those fields in the documentation (\parsername{} provides copious
documentation), so the author could not understand the extracted data, nor
determine what data is missing in comparison to \parsername{}.  AWStats
coerces Postfix log files into Apache\footnote{The Apache web server is the
most popular HTTP server in use over the past 10 years; more information is
available at \url{http://httpd.apache.org/}.} format log files, for
analysis by AWStats' HTTP log file parser.  The converting parser only
deals with a small portion of the log lines generated by Postfix, silently
skipping those it cannot deal with, and does not distinguish between
different types of rejection; it would be a lot of work to extend it to
handle new log lines.  Although it does correlate log lines by queueid (not
by pid), it does not deal with any of the other complications described in
\sectionref{complications}.  AWStats supports saving data but the format of
the saved data is not documented, as far as the author could tell.  It also
supports reading compressed log files, but that functionality was not
tested.

When tested with the \numberOFlogFILES{} test log files AWStats' reported
that it parsed 9,240,075 (88.70\%) of 10,416,129 log lines, skipping
1,176,050 (11.29\%) corrupt log lines; however there are
\numberOFlogLINES{} lines in the \numberOFlogFILES{} log files, so AWStats
parsed only 17.15\% of the input lines, ignoring the remaining 82.85\%.

The graphs it produces give an overview of mails received for the last
calendar month, showing:

\begin{itemize}

    \item The number of mails accepted from each host.

    \item How many mails were received by each recipient.

    \item The average number of mails accepted by the server per-day and
        per-hour.

    \item A summary of the \gls{SMTP} codes used when rejecting delivery
        attempts.

\end{itemize}

AWStats was not a suitable base for this project, because it assumes that
all log files can be rewritten to be compatible with web server log files,
and will contain similar data; coercing Postfix log files into web server
log files, without substantial data loss, would require fully parsing the
Postfix log files, so AWStats would not be required.  It may be possible to
use AWStats graphing capabilities to generate reports, by generating log
files to use as input to AWStats from the data extracted by \parsername{}.

\section{Anteater}

\begin{quotation}

    The Anteater project is a Mail Traffic Analyser. Anteater supports
    currently the logformat produced by Sendmail and by Postfix. The tool
    is written in 100\% C++ and is very easy to customize. Input, output,
    and the analysis are modular class objects with a clear interface.
    There are eight useful analyse modules, writing the result in plain
    ASCII or HTML, to stdout or to files.

\end{quotation}

\noindent{}\url{http://anteater.drzoom.ch/} \newline{}
(Last checked 2009/01/11.)

Anteater does not have any English documentation so it is impossible for
this author to accurately comment on the analysis it performs.  It did not
run successfully when tested, and its parsing would certainly be out of
date as Postfix has evolved considerably since this tool was last updated
(2003/11/06).  As it neither ran successfully nor has documentation the
author can read a detailed review cannot be provided.

The Debian project (\url{http://www.debian.org/}) provides a manual page
with the copy of anteater it distributes, so the author was at least able
to run anteater with the correct arguments; sadly anteater produced zero
for every statistic, presumably because it was unsuccessful in parsing the
log lines.

\section{Yet Another Advanced Logfile Analyser}

\begin{quotation}

    yaala is a very flexible analyser for all kinds of logfiles. It uses
    parsers to extract information from a logfile, an SQL-like query
    language to relate the information to each other and an output-module
    to format the information appropriately.

\end{quotation}

\noindent{}\url{http://yaala.org/} \newline{}
(Last checked 2009/01/11.)

YAALA uses a plugin-based system to analyse log files and produce HTML
output reports, with all the parsing and report generation handled by
modules.  Using YAALA as a base would be only slightly less work than
starting from scratch, as both the input and output modules would need to
be written specially; it may even be more work to implement the parser
within the constraints of YAALA\@.  YAALA supports storing previously
gathered data using Perl's Storable module~\cite{perl-storable}, so other
Perl programs can use Storable to load, examine, and optionally modify the
data; \parsername{} uses a well documented database which is accessible
from the majority of programming languages.  This information was gleaned
from the source code, as the documentation is sadly lacking.

YAALA provides a Postfix parser that extracts the following two types of
fields from specific log lines:

\begin{eqlist}

    \item [Aggregations:] count (not explained), bytes (sum of bytes
        transferred).

    \item [Keyfields:] incoming\_host, outgoing\_host, date, hour, sender,
        recipient, defer\_count, delay.  Which date and hour are stored is
        not documented: start time, end time, delivery time, or another
        time?

\end{eqlist}

\noindent{}YAALA's Postfix parser extracts some of the fields \parsername{}
does: it stores either the \gls{IP} or the hostname for client and server,
not both; it omits the helo hostname, queueid, \gls{SMTP} code, size of
each accepted mail, start and end times, timestamps for each log line, and
message ID\@.  It extracts some data that \parsername{} does not: the delay
in delivering each mail, and how many times delivery was deferred for each
mail; these could easily be extracted by \parsername{} if desired.  XXX
EXTRACT DELAY AND DEFER ONCE AUTOMATIC EXTRACTION HAS BEEN FINISHED\@.
Unlike \parsername{}, YAALA does not maintain separate counters for each
restriction; this rules out the possibility of using the collected data for
optimisation, testing or understanding of restrictions.  YAALA's Postfix
parser does not deal with the complications explained in
\sectionref{complications}, though it does correlate log lines by queueid.

YAALA provides a mini-language based on \gls{SQL} that is used when
generating reports; sample reports can be seen
at~\url{http://www.yaala.org/samples.html}.  Example query for HTTP proxy
servers: \newline{} \tab{} \texttt{requests BY file WHERE host =\~{}
Google} \newline{} The mini-language is quite limited and cannot be used to
extract data for external use, merely to create reports.  Only data
selected by the query will be saved in the data store; other data will be
discarded, and removed from the data store if already present.

Testing YAALA was unsuccessful because all the select clauses tried
produced a similar error message:
\newline{}\tab{}\texttt{lib/Yaala/Data/Core.pm: Unavailable aggregation
requested:} \newline{}\tab{}\tab{}\texttt{``bytes''. Returning 0.}
\newline{}  The underlying reason for this is that YAALA only parsed 408
(0.11\%) of 360632 log lines in the first log file; it was not tested with
the remainder of the \numberOFlogFILES{} log files.

In summary YAALA provides a Postfix parser that tries to parse the most
common Postfix log lines only, provides reasonably flexible report
generation from the limited data extracted, but has no facilities to
extract data for use in other tools.

\section{Lire}

\begin{quotation}

    As any good system administrator knows, there's a lot more to keep
    track of in an active network than just webservers. Lire is hands down
    the most versatile log analysis software available today. Lire not only
    keeps you informed about your HTTP, FTP, and mail traffic, it also
    reports on your firewalls, your print servers, and your DNS activity.
    The ever growing list of Lire-supported services clearly outstrips any
    other software, in large part thanks to the numerous volunteers who
    have pioneered many new services and features. Lire is a total solution
    for your log analysis needs.

\end{quotation}

\noindent{}\url{http://logreport.org/lire.html} \newline{}
(Last checked 2009/01/11.)

Lire is a general purpose log file parser supporting many different types
of log file.  It takes a similar approach to YAALA, using plugins to parse
different log file types.  The data extracted by its Postfix parser is not
clearly documented; the manual says only:

\begin{quotation}

    The email servers' reports will show you the number of deliveries and
    the volume of email delivered by day, the domains from which you
    receive or send the most emails, the relays most used, etc.

\end{quotation}

\noindent{}Examining the source code reveals that the parser looks for
\texttt{<key>=<value>} pairs in each log line, extracts them, and
correlates the data by queueid.  This approach will find the following
fields: helo hostname, queueid, \gls{SMTP} code, sender and recipient
addresses, and size of accepted mails.  It is unclear if the parser will
extract any further data.  Lire misses the following fields extracted by
\parsername{}: client and server \gls{IP} and hostname, start and end
times, timestamps of each log line, and message ID\@. 

Lire supports multiple output formats for generated reports (text, HTML,
PDF, and Excel 95) but the reports do not appear to be customisable;
\parsername{} does not produce any reports.  Lire's report contains less
detail than Pflogsumm, and is considerable harder to configure.  Lire
supports saving extracted data for later report generation, but accessing
this data from another application is undocumented; given the source code
it should be possible, with enough time and effort, to understand the
format.  \parsername{} uses an \gls{SQL} database to make accessing the
extracted data as easy as possible.  

Like AWStats and Logrep, Lire attempts to correlate log lines by queueid,
but not by \gls{pid}, so the complete list of recipients for a mail should
be available; however its parser extracts only part of the available data
and makes no attempt to deal with the other complications described in
\sectionref{complications}.  When testing Lire on the \numberOFlogFILES{}
test log files it performed reasonably well: the numbers it reports appear
reasonable, and the subset verified by the author were correct.  Its report
provided summaries of: 

\begin{itemize}

    \item Delivery status and failed deliveries.

    \item Sender and recipient domains and servers.

    \item Number of deliveries and bytes per-day and per-hour.

    \item Recipients by domain.

    \item Deliveries by relays, by size, and by delay.

    \item Delays by server and by domain.

    \item Which pair of correspondents exchanged the highest number of
        emails.

\end{itemize}

Lire would not be a suitable base for this project: it does not extract
enough data; does not deal with rejections in any way; does not make the
extracted data easily available to other programs.  Its parser is
in-extensible but could easily be replaced, however that would require
writing a parser from scratch, so would not be any easier.

\section{Logrep}

\begin{quotation}

    Logrep is a secure multi-platform framework for the collection,
    extraction, and presentation of information from various log files. It
    features HTML reports, multi dimensional analysis, overview pages, SSH
    communication, and graphs, and supports over 30 popular systems
    including Snort, Squid, Postfix, Apache, Sendmail, syslog, ipchains,
    iptables, NT event logs, Firewall-1, wtmp, xferlog, Oracle listener and
    Pix.

\end{quotation}

\noindent{}\url{http://www.itefix.no/i2/index.php} \newline{}
(Last checked 2009/01/11.)

Logrep extracts less than half the fields \parsername{} does:

\begin{itemize}

    \item For mail sent and received: from address, size, and time and
        date.  Which date and hour are stored is not documented: start
        time, end time, delivery time, or another time?

    \item For mail sent: to addresses, \gls{SMTP} code, and delay.

    \item For mail received: the hostname of the sender.

\end{itemize}

It also counts the number of log lines parsed and skipped.  It omits client
\gls{IP} and hostname, server \gls{IP}, helo hostname, queueid, and message
ID\@.  It extracts the delay for delivered mails, which \parsername{} does
not.  Log lines are correlated based on the queueid (referred to as
sessionname [sic] within Logrep), but not by \gls{pid}.  The parsing is
error prone: empty fields are saved when the log line does not match the
regex, though it appears that they will not overwrite existing data.  Most
notably rejections are completely ignored, making it unsuitable for the
purposes of this project.  It does not try to address any of the
complications in \sectionref{complications} except for correlating by
queueid.

Logrep does not come with any documentation, though some scant
documentation is available on its website (\parsername{} provides copious
documentation).  It requires a web browser to interact with it, so
automated log file processing will be difficult, whereas enabling automated
processing is a key part of \parsernames{} design.  Sadly all the author's
attempts to use Logrep failed, as it was unable to access the log files
selected; this appears to be a bug rather than operator error.  If it was
caused by operator error, the interface needs improvement as the (minimal)
instructions were followed as closely as possible, and multiple attempts
were made.  As parsing failed it was not possible to review the reports
Logrep can generate (available in HTML only), or to examine the
(undocumented) format in which it can save extracted data for subsequent
reuse.

Logrep extracts far less data from Postfix log files than \parsername{},
completely ignores rejections, is effectively undocumented, does not deal
with the more complicated aspects of Postfix log files, and at the time of
writing does not work properly.

\section{Summary}

There are other programs available which perform basic Postfix log file
parsing (some to a greater level of detail than others), but few attempt to
correlate log lines by queueid (none correlate by \gls{pid}) to produce an
overall record of the journey of each mail through Postfix.  None of the
reviewed parsers collect the breadth of information gathered by
\parsername{}, or make it as easy to extend the parser to handle new log
lines.  Most of the parsers generate a report and immediately discard the
data extracted from the log files; those that do not discard the data
typically retain it in a format inaccessible to other tools.  Nearly all of
the parsers reviewed can produce a report of greater or lesser detail and
complexity, unlike \parsername{}.  The quality of the documentation offered
by the subset of parsers that provide some varies from unusable to good;
none of the parsers provide any documentation on the format of their data
stores (if they have one).  Fewer than half of the parsers were capable of
parsing the \numberOFlogFILES{} test log files, and improving or extending
parsing would have been quite a difficult task for any of the parsers.
Table \refwithpage{Summary of parsers' features} provides a summary of the
parsers' features.

The overriding difference between \parsername{} and the other parsers
reviewed herein is that none of them aim for the high level of
understanding of Postfix log files achieved by \parsername{}.


\begin{table}[htb]
    \caption{Summary of parsers' features}
    \empty{}\label{Summary of parsers' features}
    \begin{tabular}{llllll}
        \tabletopline{}%
        Parser          & Parsed test   & Data              & Custom            & Documentation  & Source       \\
                        & log files?    & store?            & reports?          & quality?       & code?        \\
        \tablemiddleline{}%
        \gls{LMA}       & No            & Yes               & No                & Poor           & Yes          \\ 
        Pflogsumm       & Yes           & No                & Partial \dag{}    & Good           & Yes          \\
        Sawmill         & Yes           & Yes               & Searches          & Very good      & \nialpha{}   \\
        Splunk          & Yes           & Yes               & Searches          & Abundant       & No           \\
                        &               &                   & \& reports        & but poor       &              \\
        Isoqlog         & No            & Yes               & No                & Poor           & Yes          \\
        AWStats         & Partially     & Yes               & Partial \dag{}    & Good           & Yes          \\
        Anteater        & No            & No                & No                & Poor           & Yes          \\
        YAALA           & No            & Yes \ddag{}       & Searches          & Poor           & Yes          \\
        Lire            & Yes           & Yes               & Yes               & Reasonable     & Yes          \\
        Logrep          & No            & Yes               & No                & Poor           & Yes          \\
        \parsername{}   & Yes           & Yes \nibeta{}     & No \nichi{}       & XXX            & Yes          \\
        \tablebottomline{}%
    \end{tabular}

    \begin{eqlist}

        \item [\dag{}] Sections can be omitted from a report, but extra
            sections can not be added.

        \item [\ddag{}] YAALA only stores the data required to produce the
            latest report; other data will be discarded.

        \item [\nialpha{}] Sawmill's source code is available in an
            obfuscated form, so that customers can compile it on platforms
            that pre-compiled binaries are not available for.

        \item [\nibeta{}] \parsername{} is the only parser with
            documentation for its data store.

        \item [\nichi{}] \parsername{} defers report generation to
            subsequent programs, but all the necessary data and
            documentation to produce reports is provided.

    \end{eqlist}

\end{table}

\clearpage{}
