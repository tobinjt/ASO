\chapter{State Of The Art Review}

\label{state of the art review}

Ten Postfix log file parsers were tested at the start of this project, with
the hope of finding a suitable parser to build upon, rather than writing
one from scratch.  It was quite difficult to find ten parsers to review,
and the functionality offered by those parsers ranges from quite basic to
more developed, depending on the needs of the parser's author.

It was hoped to reuse an existing parser rather than writing one from
scratch, but the existing parsers considered were rejected for one or more
reasons.  The effort required to adapt and improve an existing parser was
judged to be greater than the effort to write a new one, because the
techniques used by the existing parsers severely limited their potential:
some ignored the majority of log lines, parsing specific log lines
accurately, but without any provision for parsing new or similar log lines;
others sloppily parsed the majority of log lines, but were incapable of
distinguishing between log lines within one category, e.g.\ not
distinguishing between different anti-spam techniques causing delivery
attempts to be rejected.  The first parser
reviewed~\cite{log-mail-analyser} is the only previously published research
in this area that the author could find; that research aims to show that
providing the data from log files in a more accessible form is helpful to
systems administrators.

The ten parsers have been compared and contrasted with \parsername{}, this
project's parser, to show how much effort would have been required to use
one of those parsers to fulfil the aims and requirements of this project.
It is important to compare and contrast newly developed algorithms and
parsers against those already available, to accurately judge what
improvements, if any, are delivered by the newcomers.  \Tableref{Summary of
parsers' features} summarises the results of this review.

Some important differences exist between \parsername{} and most or all of
the parsers reviewed in this chapter:

\begin{enumerate}

    \item None of the reviewed parsers perform the kind of advanced parsing
        required for this project, or deal with the complications described
        in \sectionref{complications}.  Some correlate log lines by
        queueid, but none correlate log lines by \acronym{pid}
        (\sectionref{queueid vs pid}).

    \item Only \parsername{} enables parsing of new log lines without
        extensive and intrusive modifications to the parser; the
        architecture enabling this is documented in \sectionref{parser
        architecture}.

    \item The reviewed parsers all produce a report of varying complexity
        and detail, whereas \parsername{} does not: it extracts data and
        leaves generation of reports to other programs.  Using a
        \acronym{SQL} database simplifies the process of generating such
        reports (discussed in \sectionref{database as API}); some sample
        queries are given in \sectionref{motivation}.  The parser developed
        for this project is designed to enable much more detailed log file
        analysis than other parsers by providing a stable platform for
        subsequent programs to build upon.

    \item Most of the reviewed parsers silently ignore log lines they
        cannot handle, whereas \parsername{} warns about every single log
        line it fails to recognise.  The exception is AWStats, which
        outputs the percentage of log lines it was unable to parse, but
        does not output the log lines themselves.

    \item A minor difference is that most parsers do not handle compressed
        log files; both \parsername{} and Splunk handle them transparently,
        without user intervention; Sawmill and Lire can be configured to
        support compressed log files, but Sawmill exhibits a dramatic
        increase in parsing time when doing so.  Support for reading
        compressed log files is quite helpful, because it dramatically
        reduces the disk space required to store historical log files.

    \item Most of the reviewed parsers do not distinguish between different
        delivery attempt rejections, so they cannot be used to determine
        the success rate of different anti-spam techniques.  The exception
        is Pflogsumm, which provides a summary of why delivery attempts
        were rejected.

\end{enumerate}

Each of the reviewed parsers was tested with the three months
(\numberOFlogFILES{} days) of contiguous log files described in
\sectionref{parser efficiency}.

The data extracted by \parsername{} is documented in
\sectionref{connections table} and \sectionref{results table}, but for
convenience the list is repeated here: client and server IP address and
hostname, HELO hostname, queueid, start time, end time, \acronym{SMTP}
code, enhanced status code, sender, recipient, size, message-id, delay, and
delays.  Note: \parsername{} correlates log lines by both queueid and
\acronym{pid} (\sectionref{queueid vs pid}), and it stores each mail's
queueid, but it does not store each connection's \acronym{pid}, because a
use has not been identified for that data.

\section{Log Mail Analyser}

\label{log mail analyser}

There appears to be only one prior published paper about parsing Postfix
log files: \textit{Log Mail Analyzer: Architecture and Practical
Utilizations\/}~\cite{log-mail-analyser}.  The aim of \acronym{LMA} is
quite different from \parsername{}: it attempts to present correlated data
from log files in a form suitable for a systems administrator to search
using the myriad of standard Unix text processing utilities already
available.  It produces a \acronym{CSV} file and either a MySQL or Berkeley
DB database.  Hardly any documentation is provided with \acronym{LMA}, but
some documentation is available in~\cite{log-mail-analyser}.  Studying the
source code is informative, though this author had some difficulty because
the authors of \acronym{LMA} wrote in Italian.

\begin{description}

    \item [CSV] \acronym{CSV} is a very simple format where each record is
        stored in a single line, with fields separated by a comma or other
        punctuation symbol.  Problems with \acronym{CSV} include the need
        to escape separators in the data stored, providing multiple values
        for a field (e.g.\ multiple recipients), and adding new fields.
        \acronym{CSV} does not have a standard mechanism to document the
        fields or the separator, unlike \acronym{SQL} where every database
        includes a schema naming the fields and the type of data they store
        (e.g.\ integer, text, timestamp).  The \acronym{CSV} record format
        used by \acronym{LMA} is not documented
        in~\cite{log-mail-analyser}, but the output file contains a comment
        in Italian giving the format:\newline{} \tab{} \texttt{\#
        Timestamp|Nome Client|IP Client|IP
        Server|From|To\newline{}\tab{}\tab{}|Status|Size}
        \newline{}\acronym{LMA} treats \acronym{CSV} lines starting with
        \texttt{\#} as comments, but not all \acronym{CSV} parsers will.

    \item [Berkeley DB] Berkeley DB only supports storing simple
        \textbf{(key, value)} pairs, unlike \acronym{SQL} databases that
        store arbitrary tuples.  In \acronym{LMA}'s main table the key is
        an integer referred to by secondary tables, and the value is a
        \acronym{CSV} line containing all of the data for that row.  The
        secondary by-sender, by-recipient, by-date, and by-IP tables use
        the sender/recipient/date/IP address as the key, and the value is a
        \acronym{CSV} list of integers referring to the keys in the main
        table.  This effectively re-implements \acronym{SQL} foreign keys,
        but without the functionality offered by even the most basic of
        \acronym{SQL} databases, e.g.\ joins, ordering, searches.  It also
        requires custom code to search on a combination of attributes, and
        the authors of \acronym{LMA} did provide some simple reports:
        IP-STORY, FROM-STORY, DAILY-EMAIL, and DAILY-REJECT (all described
        later).  Berkeley DB appears to be the least useful of the three
        output formats: it does not provide the functionality of a basic
        \acronym{SQL} database, and unlike \acronym{CSV} files it cannot be
        used with standard Unix text processing tools.

    \item [MySQL] The schema used with the MySQL database is undocumented,
        but at least it is possible to discover the schema if one has an
        existing \acronym{SQL} database, unlike with Berkeley DB\@; all
        \acronym{SQL} databases embed the schema into the database and
        provide commands for displaying it.  Berkeley DB does not embed a
        schema in its database files, because it does not need one, and
        embedding one would not provide any benefit; it only provides
        \textbf{(key, value)} pairs, so any additional structuring of the
        data is imposed by the application, and it is the responsibility of
        the author of the application to document this structure.
        \acronym['s]{LMA} MySQL support was not tested because the schema
        is not documented, and no existing database was available to
        extract the schema from, so the required database could not be
        created.

\end{description}

Whether a MySQL or Berkeley DB database is chosen in addition to the
\acronym{CSV} output, \acronym{LMA} stores the following data: time and
date of the log line, client hostname and IP address, server IP address,
sender and recipient addresses, \acronym{SMTP} code, and size (for accepted
mails only).  Unlike \parsername{} it does not store the server hostname,
HELO hostname, queueid, start and end times, timestamps for each log line,
enhanced status code, delivery delays, or message-id (for accepted mails
only).  Handling of multiple recipients, \acronym{SMTP} codes, or remote
servers\footnote{A single mail may be sent to multiple remote servers if it
was addressed to recipients in different domains, or Postfix needs to try
multiple servers for one or more recipients.} is not explained;
experimental observation shows that multiple records were added when a mail
had multiple recipients, but the records are not associated or linked in
any way, and presumably the same approach was taken when there were
multiple destination servers.

\acronym{LMA} requires major changes to the parser code to parse new log lines
or to extract additional data.  The code is structured as a long series of
blocks that each handle all log lines matching a single regex, so parsing
new log lines requires modifying an existing regex or carefully inserting a
new block in the correct place; extracting extra data would require
modifying multiple blocks, regexes, or both.

\acronym{LMA} does not deal with any of the complications discussed in
\sectionref{complications}, except for correlating log lines by queueid; it
cannot correlate most rejected delivery attempts because it does not
correlate log lines by \acronym{pid}.  It does not differentiate between
different types of rejections, so it is not suitable for the purposes of
this project; the data about which restriction caused the rejection is
discarded, whereas the main goal of this project is to retain that
information to aid optimisation and evaluation of anti-spam techniques.
\acronym{LMA} fails to parse Postfix log files generated on Solaris hosts
because the fields automatically prepended to each log line differ from
those prepended on Linux hosts; log files from Solaris hosts (and possibly
other operating systems) thus require pre-processing before parsing by
\acronym{LMA}.  The \numberOFlogFILES{} pre-processed test log files were
parsed without complaint by \acronym{LMA}, although it produced 32 entries
in the output \acronym{CSV} file for every rejection in the input log
files; it also missed some 40\% of delivered mail.  Once these deficiencies
were discovered the author did not spend any more time checking the
results.

\acronym{LMA} does provide some simple reports: IP-STORY, FROM-STORY,
DAILY-EMAIL and DAILY-REJECT\@.  These reports search the Berkeley DB files
for matching records: the first three produce \acronym{CSV} lines for the
specified client IP address, sender address, or date respectively.
DAILY-REJECT initially failed with an error message from the Perl
interpreter;\footnote{The error messages were: \newline{}\texttt{Undefined
subroutine \&main::LIST called at queryDB.pl line
372.}\newline{}\texttt{Undefined subroutine \&main::EXTRACT\_FROM\_DB
called at queryDB.pl line 379.}} after correcting the errors in the code it
worked, producing \acronym{CSV} lines for the specified day where the
\acronym{SMTP} code signifies a rejection.  All of these reports are
extremely simple to produce from the \acronym{CSV} file using the standard
Unix tool \texttt{awk}\glsadd{awk}; the most complicated, DAILY-REJECT, is
merely:

% perl queryDB.pl -dayreject 2007-01-26 > lma-query

\begin{verbatim}
awk -F\| 'BEGIN { previous = "" };
    $1 ~ /2007-01-26/ && $7 != "250" && $0 != previous
    { print $0; print " "; previous = $0; }' lma_output.txt
\end{verbatim}

Notes about the command above:

\begin{itemize}

    \item It outputs a line containing only a single space after each
        matching record, to accurately replicate the output of
        DAILY-REJECT\@.

    \item DAILY-REJECT considers all \acronym{SMTP} codes except ``250'' to
        be rejections; this includes invalid \acronym{SMTP} codes such as
        \texttt{0} and \texttt{deferred}, so the awk command does too.
        These invalid \acronym{SMTP} codes are most likely present because
        of incorrect parsing by \acronym{LMA}.

    \item \acronym{LMA} produces 32 lines in its \acronym{CSV} file for
        every single line it should have produced; the command above
        suppresses duplicate sequential lines.  DAILY-REJECT produces the
        correct number of output lines, probably because it searches the
        Berkeley DB database rather than the \acronym{CSV} file.

\end{itemize}

The output from DAILY-REJECT and the \texttt{awk} command is not exactly
the same; the author did not spend substantial time attempting to rectify
these differences.

\begin{enumerate}

    \item The output from DAILY-REJECT is missing some records that are
        present in the \acronym{CSV} file; this may indicate differences
        between the data stored in the \acronym{CSV} and Berkeley DB files.

    \item Some records output by DAILY-REJECT are truncated: they are
        missing the last ``$|$'' that separates fields and also the newline
        following it, so the line containing only a single space is
        concatenated with the record.

\end{enumerate}

In summary, \acronym{LMA} appears to be a proof of concept, written to
demonstrate the point of their paper, that making the information contained
in log files available in an accessible fashion is useful to systems
administrators, rather than a program intended to be useful in a production
environment.

% Literature review notes:
%
% Hard-coded parsing, requiring code changes to add more.  Attempts to
% correlate log lines, saves data to database for data mining purposes.
% Hard to extend/expand/understand.  Appears to only save: date and hour,
% DNS name and IP address host, mail server IP address, sender, receiver
% and e-mail status (sent, rejected).  Undocumented schema.  Design
% decision to use \acronym{CSV} as an intermediate format between the log
% file and the database seems to have been restrictive.  Appears to require
% a queueid but majority of log lines (e.g.\ rejections) lack a queueid.
% Supports whitelisting IP addresses when parsing log filess, but
% whitelisting when generating reports/data mining would be preferable.
% Supporting Berkeley DB is probably limiting the software; an example is
% the difficulty in searching a pipe-delimited string, so they have
% re-implemented foreign keys with tables keyed by ip address and so on
% pointing at the main table - this also will not scale well.  There does
% not appear to be any attempt to deal with the complications I have
% encountered: their parsing is not detailed enough to encounter them.  It
% does not run properly; does not create any output; throws up errors.

\section{Pflogsumm}

\parserblurb{%
    pflogsumm is designed to provide an over-view of Postfix activity, with
    just enough detail to give the administrator a ``heads up'' for
    potential trouble spots.
}
{http://jimsun.linxnet.com/postfix_contrib.html}
{2008/11/23}

Pflogsumm produces a report designed for troubleshooting rather than
in-depth analysis.  It does not support saving any of the data it extracts
from log files, and it does not extract any data that it does not require
to produce its report: the HELO hostname, queueid, start and end times,
timestamps for each log line, or message-id.  Both the parsing and
reporting are difficult to extend because it is a specialised tool, unlike
the easily extensible design of \parsername{}.  It does not correlate log
lines by queueid or \acronym{pid}, and does not need to deal with the
complications encountered during this project (\sectionref{complications}).
Pflogsumm produces a useful report, and successfully parsed the
\numberOFlogFILES{} log files it was tested with.  The results it reported
were not verified in detail, but it did not report any errors, and has an
excellent reputation amongst Postfix users.  Pflogsumm has many options to
include or exclude certain sections of the report, all clearly documented;
by default its report includes the following:

\begin{itemize}

    \item Total number of mails accepted, delivered, and rejected.  Total
        size of mails accepted and delivered.  Total number of sender and
        recipient addresses and domains.

    \item Per-hour averages and per-day summaries of the number of mails
        received, delivered, deferred, bounced, and rejected.

    \item For received mail: per-domain totals for the number of mails
        sent, number of mails deferred, average delay, maximum delay, and
        bytes delivered.  For received mail: per-domain totals for size and
        number of mails received.

    \item Number and size of mails sent and received for each address.

    \item Summary of why mail delivery was deferred or failed, why mails
        were bounced, why mails were rejected, and warning messages from
        Postfix.

\end{itemize}

Pflogsumm is the only reviewed parser that distinguishes between different
rejected delivery attempts, an important requirement for this project.

\section{Sawmill Universal Log File Analysis And Reporting}

\parserblurb{%
    Sawmill is a Postfix log analyzer (it also support 818 other log
    formats).  It can process log files in Postfix format, and generate
    dynamic statistics from them, analyzing and reporting events.  Sawmill
    can parse Postfix logs, import them into a SQL database (or its own
    built-in database), aggregate them, and generate dynamically filtered
    reports, all through a web interface.  Sawmill can perform Postfix
    analysis on any platform, including Window, Linux, FreeBSD, OpenBSD,
    Mac OS, Solaris, other UNIX, and more.
}
{http://www.thesawmill.co.uk/formats/postfix.html}
{2008/11/23}

Sawmill is a general purpose commercial product that parses 818 log file
formats (as of 2008/11/23) and produces reports from the extracted data.
Its data extraction facilities (described later) are too limited to save
enough data for the purposes of this project: although it can extract three
different sets of data from Postfix log files, they are not linked in any
way.  The documentation does not suggest that Sawmill correlates log lines
by either queueid or \acronym{pid}, or deals with the other difficulties
documented in \sectionref{complications}.

Sawmill has three different Postfix log file parsers, extracting three
different sets of data:

\begin{enumerate}

    \item
        \urlLastChecked[\hfill{}\newline{}]{http://www.thesawmill.co.uk/formats/postfix.html}{2008/11/23}.
        Fields extracted: from, to, server, uid, relay, status, number of
        recipients, origin hostname, origin IP address, and virus.  It
        also counts the number of and total size of all mails delivered.
        The fields \texttt{server}, \texttt{uid}, \texttt{relay}, and
        \texttt{virus} are not explained in the documentation:
        \texttt{server} is probably the hostname or IP address of the
        server the mail is delivered to; \texttt{relay} might be the
        delivery method: \acronym{SMTP}, local delivery, or \acronym{LMTP};
        \texttt{uid} might be the uid of the user submitting mail locally.
        Postfix does not do any form of virus checking itself, so the
        \texttt{virus} field is a mystery.

    \item
        \urlLastChecked[\hfill{}\newline{}]{http://www.thesawmill.co.uk/formats/postfix_ii.html}{2008/11/23}.
        Fields extracted: from, to, RBL list, client hostname, and client
        IP address.  It also counts the number and total size of all mails
        delivered.

    \item
        \urlLastChecked[\hfill{}\newline{}]{http://www.thesawmill.co.uk/formats/beta_postfix.html}{2008/11/23}.
        Fields extracted: from, to, client hostname, client IP address,
        relay hostname, relay IP address, status, response code, RBL list,
        and message-id.  It also counts the number and size of all mails
        delivered, processed, blocked, expired, and bounced.

\end{enumerate}

Even if the three data sets were combined Sawmill would extract less data
than \parsername{}: it omits the HELO hostname, queueid, enhanced status
code, delivery delays, and start and end times.  Sawmill does not extract
any data about rejections except when the rejection is caused by a
\acronym{DNSBL} check (\texttt{RBL list} in the list of fields).

The source code is only available in an encrypted form, to support people
who wish to use Sawmill on operating systems or machine architectures the
company do not provide executables for.  Sawmill is quite expensive,
requiring a \euros{100} + VAT licence per report, with discounts available
when buying multiple licences (correct as of 2008/11/23); in contrast,
\parsername{} is free to use and the code is freely available.  Sawmill is
supplied with thorough and well written documentation; everything the
author looked for was documented, except for the MySQL database schema and
some details of the data extracted by the parser, e.g.\ what the virus
field stores.  A commercial version of MySQL is required because of MySQL
licensing restrictions, but Sawmill's documentation explains why and
includes instructions on how to compile Sawmill so that it can use a
non-commercial version of MySQL (this was not attempted during the review
process).

Sawmill's web interface supports searching on any combination of the fields
it extracts, and all searches produced accurate results.  The interface for
searching is neither as simple to use nor as informative as the interface
provided by Splunk (see \sectionref{Splunk review}).  However, the
administrative interface is much easier to use than Splunk's: it took only
five minutes to start parsing all of the log files in a directory.

When tested with the \numberOFlogFILES{} test log files it performed
adequately, though the rate it processed log files at did slowdown
noticeably as it progressed.  Sawmill supports reading compressed log files
but it exhibits a dramatic slow down when doing so: it took six hours to
parse the first half of the log files, and twelve hours to parse the next
third; after twenty four hours spent parsing the remaining sixth it crashed
due to lack of disk space.  On the second parsing attempt the log files
were uncompressed beforehand and parsing took eight hours.

In summary, Sawmill suffers from supporting so many types of log files: it
is probably much more useful when parsing log files where each log line is
self-contained (e.g.\ web server log files), rather than log files where
data is spread across multiple log lines.  It is not suitable as a base for
this parser, because the source code made available is encrypted and not
intended for modification; in addition the architecture would probably need
to be overhauled or replaced to deal with correlating log lines.

\section{Splunk}

\label{Splunk review}

\parserblurb{%
    Splunk is IT Search.  Search and navigate IT data from applications,
    servers and network devices in real-time.  Logs, configurations,
    messages, traps and alerts, scripts, code, metrics and more.  If a
    machine can generate it --- Splunk can eat it.  It\empty{}'s easy to
    download and use and it\empty{}'s very powerful.
}
{http://www.splunk.com/}
{2008/11/23}

Splunk aims to index all of an organisation's log files, providing a
centralised view capable of searching and correlating diverse log sources.
The web interface provides search functionality, generating statistics and
graphs in real time, a facility not provided by \parsername{}.  Splunk
allows quite complicated searches, based on the fields it extracts
(described later) or the full text of the log line, though it is not
possible to search on partial words.  Searches can be saved for reuse;
saved searches can be run periodically and the results mailed to a
recipient or sent to an external program for further processing.  The
author was unable to save searches, possibly because of limitations in the
free version, and so was unable to examine the format of the data.  The web
interface is optimised for interactive use rather than automated queries,
and it does not appear to be possible to write independent tools to utilise
the Splunk database, whereas \parsername{} provides the database and leaves
it to the user to utilise it without limit or restriction.  Some additional
Postfix reports are supposedly available on
\urlLastChecked{http://www.splunkbase.com/}{2009/04/21}, but the author was
unable to find any Postfix reports, or indeed reports for any other log
file types: every category was empty, even those that the web site claimed
had numerous reports available.  Many types of graphs can be generated,
though all except the bubble and heatmap graphs are variations of a bar or
pie chart.  Drilling down through the graphs to select a portion of the
data is simple and intuitive, e.g.\ select the hour with the largest number
of events, then select a particular host, and finally a specific sender
address.  All searches performed using the indexed data returned reasonable
results.  The full power of \acronym{SQL} is available when searching the
data extracted by \parsername{}, allowing the user to search on arbitrarily
complicated conditions.

The web interface is quite attractive and simple to use when searching, but
as an administrator it seems unnecessarily difficult to perform simple
tasks.  When testing Splunk it took roughly 30 minutes to figure out how to
add a single log file to be indexed for later searching, with the added
downside that the log file was copied into a spool directory before
indexing, doubling the disk space usage.  The next test was to index all
the log files in a particular directory, but after three hours, reading all
the available documentation, and numerous futile attempts, the author was
still unable to index all the log files in a directory using the web
interface.  Using Splunk's command line interface rather than the web
interface was more successful: the command ``\texttt{splunk find logs
}\textit{log-directory\/}'' added 40 of the \numberOFlogFILES{} log files
to the queue for indexing.  Further attempts enqueued the same 40 log
files, without explaining why the others were excluded.\footnote{The log
files that were added multiple times appear to have been indexed once only;
presumably Splunk keeps track of the log files it has indexed and discards
requests to index log files for a second time.  This may or may not be a
useful feature for \parsername{}.}  The command did not have an option to
ensure the log files would be processed in the order they were created,
though such an option may be neither necessary nor beneficial with Splunk.
Subsequently the author was successful in adding a single log file at a
time using the command ``\texttt{splunk add tail }\textit{filename\/}'',
and then a simple loop using that command was enough to add all the desired
log files.  Splunk will periodically check all indexed log files for
updates unless they are manually removed from its list; this may or may not
be useful behaviour.  Splunk did not appear to have any difficulty in
indexing the log files once they had been successfully added to its queue.
\parsername{} parses the log files it is instructed to parse, in the order
they are given; periodic parsing of log files is a task an administrator
can easily achieve with \texttt{cron(8)} and \texttt{logrotate(8)}.

Copious documentation is made available on
\urlLastChecked{http://www.splunk.com/}{2009/04/21}, but the abundance of
material and lack of organisation makes it hard to find the topic being
sought, and searches confusingly tended to return results from old
documentation rather than new.  In general the documentation appears to
have been written by someone intimately acquainted with the software, who
has difficulty understanding how a newcomer would approach tasks or the
questions they would ask.

Splunk supports reading compressed log files without any extra
configuration by the user, like \parsername{}.  The free version of Splunk
limits the volume of data indexed per day to 500MB, though a trial
Enterprise licence is available that allows indexing of up to 5GB of data
per day.  In 2007, the cheapest licenced version cost \$5000 plus \$1000
support per annum, and limited the volume of data indexed per day to
500MB\@.  Prices were removed from the Splunk website during 2008; now
Splunk's sales team must be asked for a quote.  Median log file size for
the \numberOFlogFILES{} log files used when evaluating \parsername{} is
\input{build/include-median-file-size}.

When parsing Postfix log files Splunk parses the standard
syslog\glsadd{syslog} fields at the beginning of the log line, and extracts
any \texttt{key=value} pairs occurring after the syslog prologue: to and
from addresses, HELO hostname, and protocol (\acronym{SMTP},
\acronym{LMTP}, or \acronym{ESMTP}).  \parsername{} extracts noticeably
more data: client and server IP address and hostname, queueid, start and
end times, timestamps for each log line, \acronym{SMTP} and enhanced status
codes, delivery delays, and message-id.  \parsername{} does not make the full
text of the log line available; a few minutes work could add this to
\parsername{} if desired, but it would greatly increase the size of the
database.

Splunk is a generic tool, so it lacks any Postfix-specific support over and
above extracting the \texttt{key=value} fields from each log line; it makes
no attempt to correlate log lines by queueid or \acronym{pid}, or to handle
any of the other myriad complications discussed in
\sectionref{complications}.  Its source code is unavailable, so it could
not be used as a base for this project, even if it fulfilled all the other
requirements.

\section{Isoqlog}

\parserblurb{%
    Isoqlog is an MTA log analysis program written in C.  It designed to
    scan qmail, postfix, sendmail and exim logfile and produce usage
    statistics in HTML format for viewing through a browser.  It produces
    Top domains output according to Sender, Receiver, Total mails and
    bytes; it keeps your main domain mail statistics with regard to Days
    Top Domain, Top Users values for per day, per month and years.
}
{http://www.enderunix.org/isoqlog/}
{2009/01/11}

Isoqlog's report lacks most of the information gathered by \parsername{}:
the data it extracts is limited to the number of mails sent by each sender,
and it only reports on senders from the domains listed in its configuration
file, making it impossible to produce complete reports.  It ignores all log
lines except those with today's date, so it is impossible to analyse
historical log files, and testing with the \numberOFlogFILES{} test log
files was pointless.  It does maintain a record of data previously
extracted and newly extracted data is added to it; the format of the data
store is undocumented.  Almost no documentation is provided with Isoqlog,
little more than installation instructions.  It does not utilise rejection
log lines in any way, so is unsuitable for the purposes of this project.
Its parsing is completely inextensible, indeed is almost incomprehensible,
relying on \texttt{scanf(3)}, unexplained fixed offsets, and low level
string manipulation; it is the opposite end of the spectrum to
\parsernames{} parsing.  It does not handle any of the complications
discussed in \sectionref{complications}, does not gather the breadth of
data required for this project, and ignores most of the log lines produced
by Postfix.

\section{AWStats}

\parserblurb{%
    AWStats is a free powerful and featureful tool that generates advanced
    web, streaming, ftp or mail server statistics, graphically.  This log
    analyzer works as a CGI or from command line and shows you all possible
    information your log contains, in few graphical web pages.  It uses a
    partial information file to be able to process large log files, often
    and quickly.  It can analyze log files from all major server tools like
    Apache log files (NCSA combined/XLF/ELF log format or common/CLF log
    format), WebStar, IIS (W3C log format) and a lot of other web, proxy,
    wap, streaming servers, mail servers and some ftp servers.
}
{http://awstats.sourceforge.net/awstats.mail.html}
{2009/01/11}

AWStats can produce simple graphs from many different services' log files,
but supporting numerous log file formats without special purpose code
limits its functionality.  The data it can extract from Postfix log files
is limited in comparison to \parsername{}: time2, email, email\_r, host,
host\_r, method, url, code, and bytesd.  No explanation for any of those
fields is provided in the documentation at
\urlLastChecked{http://awstats.sourceforge.net/docs/awstats_faq.html\#MAIL}{2009/04/21},
so the author could neither understand what data is extracted, nor
determine what data is missing in comparison to \parsername{}, which fully
documents all the data it extracts.  AWStats coerces Postfix log files into
the log file format used by the Apache web server, for analysis by AWStats'
HTTP log file parser.  The converting parser only deals with a small
portion of the log lines generated by Postfix, silently skipping those it
cannot parse, and does not distinguish between different delivery attempt
rejections; extending it to parse all log lines would be at least as much
work as writing a new parser.  It does correlate log lines by queueid (not
by pid), but it does not deal with any of the other complications described
in \sectionref{complications}.  AWStats supports saving data extracted from
log files, but the format of the data store is not documented.  It also
supports reading compressed log files, but that was not tested.

When tested with the \numberOFlogFILES{} test log files, AWStats reported
that it parsed 9,240,075 (88.709\%) of 10,416,129 log lines, skipping
1,176,050 (11.290\%) corrupt log lines.  However, the \numberOFlogFILES{}
test log files contain \numberOFlogLINES{} log lines, so AWStats parsed
only 15.217\% of the log lines, declared that 1.936\% were corrupt, and
ignored the remaining 82.846\%.  The parsing results were not examined in
detail or verified.

The graphs it produces give an overview of mails received for the last
calendar month, showing:

\begin{itemize}

    \squeezeitems{}

    \item The number of mails accepted from each host.

    \item How many mails were received by each recipient.

    \item The average number of mails accepted by the server per-day and
        per-hour.

    \item A summary of the \acronym{SMTP} codes used when rejecting
        delivery attempts.

\end{itemize}

AWStats was not a suitable base for this project because it assumes that
all log files can be rewritten to be compatible with web server log files,
and will contain similar data; coercing Postfix log files into web server
log files, without substantial data loss, would require fully parsing the
Postfix log files without using AWStats, i.e.\ would require writing a
separate parser anyway.  It may be possible to use AWStats' graphing
capabilities to generate reports, by generating input for AWStats from the
database populated by \parsername{}, but the author has not attempted that.

\section{Anteater}

\parserblurb{%
    The Anteater project is a Mail Traffic Analyser.  Anteater supports
    currently the logformat produced by Sendmail and by Postfix.  The tool
    is written in 100\% C++ and is very easy to customize.  Input, output,
    and the analysis are modular class objects with a clear interface.
    There are eight useful analyse modules, writing the result in plain
    ASCII or HTML, to stdout or to files.
}
{http://anteater.drzoom.ch/}
{2009/01/11}

Anteater does not have any English documentation except for the quote above
so it is impossible for this author to accurately comment on the analysis
it performs.  It did not run successfully when tested, and its parsing
would certainly be out of date because Postfix has evolved considerably
since this tool was last updated (2003/11/06).  Because it neither ran
successfully nor has documentation the author can read, a detailed review
cannot be provided.

The Debian Linux distribution provides a translated manual page with the
copy of anteater it distributes, so the author was at least able to run
anteater with the correct arguments; sadly anteater produced zero for every
statistic, presumably because it was unsuccessful in parsing the log lines.

\section{Yet Another Advanced Logfile Analyser}

\parserblurb{%
    yaala is a very flexible analyser for all kinds of logfiles.  It uses
    parsers to extract information from a logfile, an SQL-like query
    language to relate the information to each other and an output-module
    to format the information appropriately.
}
{http://yaala.org/}
{2009/01/11}

YAALA uses plugins to analyse log files and produce reports in HTML format.
Using YAALA as a base for this project would have been as much work as
starting from scratch, because both the input and output modules would need
to be written specially; it might be more work to implement a parser within
the constraints of YAALA rather than independently.  YAALA supports storing
previously gathered data using Perl's Storable module, so other Perl
programs could use Storable to load, examine, and optionally modify the
data; \parsername{} uses a well documented database that is accessible from
most common programming languages.  Information about how YAALA stores data
was gleaned from the source code, because the format is undocumented and
differs amongst plugins.

YAALA provides a Postfix parser that extracts the following fields from
specific log lines:

\begin{boldeqlist}

    \item [Aggregations:] count (not explained), bytes (sum of bytes
        transferred).

    \item [Keyfields:] incoming\_host, outgoing\_host, date, hour, sender,
        recipient, defer\_count, delay.  Which date and hour are stored is
        not documented: start time, end time, delivery time, or another
        time?

\end{boldeqlist}

YAALA's Postfix log file parser extracts some of the fields \parsername{}
does: for client and server it stores either the IP address or the
hostname, not both; it omits the HELO hostname, queueid, \acronym{SMTP} and
enhanced status codes, size of each accepted mail, start and end times,
timestamps for each log line, and message-id\@.  It extracts one piece of
data that \parsername{} does not: how many times delivery was deferred for
each mail; this information can be calculated from the database populated
by \parsername{} if desired.  Unlike \parsername{}, YAALA does not maintain
separate counters for different delivery attempt rejections, precluding the
possibility of using the collected data for optimisation, testing, or
understanding of restrictions.  YAALA's Postfix log file parser does not
deal with the complications explained in \sectionref{complications}, except
that it does correlate log lines by queueid (but not by \acronym{pid}).

YAALA provides a mini-language based on \acronym{SQL} that is used when
generating reports; sample reports can be seen at
\urlLastChecked{http://www.yaala.org/samples.html}{2009/04/21}.  Example
query for HTTP proxy servers: \newline{} \tab{} \texttt{requests BY file
WHERE host =\~{} Google} \newline{} The mini-language is quite limited and
cannot be used to extract data for external use, merely to create reports.
Only data selected by the query will be saved in the data store; other data
will be discarded, and removed from the data store if already present.

Testing YAALA was unsuccessful because all the queries produced a similar
error message:
\newline{}\tab{}\texttt{lib/Yaala/Data/Core.pm: Unavailable aggregation
requested:} \newline{}\tab{}\tab{}\texttt{``bytes''.  Returning 0.}
\newline{}  The underlying reason for this is that YAALA only parsed 408
(0.11\%) of 360,632 log lines in the first log file; it was not tested with
the remainder of the \numberOFlogFILES{} log files.

It might be possible to use \parsername{} as a plugin with YAALA, perhaps
with an intermediate plugin interfacing between the two, but YAALA's data
store is insufficient for \parsernames{} needs: \parsername{} uses two
separate tables, whereas YAALA assumes all data will reside in one
structure; YAALA's querying mini-language might not deal successfully with
data in separate structures.  This approach has not been attempted by the
author.

In summary, YAALA provides a Postfix log file parser that unsuccessfully
attempts to parse only the most common Postfix log lines, provides
reasonably flexible report generation from the limited data extracted, but
has no facilities to extract data for use in other tools.

\section{Lire}

\sloppy{}%
\parserblurb{%
    As any good system administrator knows, there's a lot more to keep
    track of in an active network than just webservers.  Lire is hands down
    the most versatile log analysis software available today.  Lire not only
    keeps you informed about your HTTP, FTP, and mail traffic, it also
    reports on your firewalls, your print servers, and your DNS activity.
    The ever growing list of Lire-supported services clearly outstrips any
    other software, in large part thanks to the numerous volunteers who
    have pioneered many new services and features.  Lire is a total solution
    for your log analysis needs.
}
{http://logreport.org/lire.html}
{2009/01/11}
\fussy{}

Lire is a general purpose log file parser supporting many different types
of log file.  It takes a similar approach to YAALA, using plugins to parse
different log file types.  The data extracted by its Postfix log file
parser is not clearly documented: \textit{The email servers' reports will
show you the number of deliveries and the volume of email delivered by day,
the domains from which you receive or send the most emails, the relays most
used, etc\empty{}.\/}

Examining the source code reveals that Lire looks for
\texttt{<key>=<value>} pairs in each log line, extracts them, and
correlates the data by queueid (but not by \acronym{pid}).  This approach
will extract the following data: HELO hostname, queueid, \acronym{SMTP}
code, sender and recipient addresses, and size of accepted mails.  Lire
misses the following fields extracted by \parsername{}: client and server
IP address and hostname, start and end times, enhanced status code,
delivery delays, timestamps of each log line, and message-id\@.

Lire supports multiple output formats for the reports it generates (text,
HTML, PDF, and Excel 95) but the reports do not appear to be customisable,
and are not as detailed as Pflogsumm's; \parsername{} does not produce any
reports.  Lire supports saving extracted data for later report generation,
but the format of this data store is undocumented.  \parsername{} uses an
\acronym{SQL} database to make accessing the extracted data as effortless
as possible.  In general, Lire has poor documentation.

Similar to AWStats and Logrep, Lire attempts to correlate log lines by
queueid, but not by \acronym{pid}, so the complete list of recipients for
each delivered mail should be available; it does not attempt to deal with
the other complications described in \sectionref{complications}.  When
testing Lire on the \numberOFlogFILES{} test log files it performed
reasonably well: the numbers it reports appear accurate, and the subset
verified by the author were correct.  Its report provides summaries of:

\begin{itemize}

    \squeezeitems{}

    \item Delivery status and failed deliveries.

    \item Sender and recipient domains and servers.

    \item Number of deliveries and bytes per-day and per-hour.

    \item Recipients by domain.

    \item Deliveries by relays, by size, and by delay.

    \item Delays by server and by domain.

    \item The pair of correspondents that exchanged the highest number of
        mails.

\end{itemize}

Lire would not be a suitable base for this project: it does not extract
enough data; does not deal with rejected delivery attempts in any way; does
not make the extracted data easily available to other programs.  Its parser
is not extensible; it could easily be replaced, but that would require
writing a parser from scratch, so would not be any less work than writing
\parsername{}.  \parsername{} could possibly be used to parse Postfix log
files for Lire, but the difficulty may outweigh the benefits, e.g.\ Lire's
data store may not be capable of storing the data extracted by
\parsername{}, but the lack of documentation hinders any evaluation.  As
with YAALA this approach has not been attempted by the author.

\section{Logrep}

\parserblurb{%
    Logrep is a secure multi-platform framework for the collection,
    extraction, and presentation of information from various log files.  It
    features HTML reports, multi dimensional analysis, overview pages, SSH
    communication, and graphs, and supports over 30 popular systems
    including Snort, Squid, Postfix, Apache, Sendmail, syslog, ipchains,
    iptables, NT event logs, Firewall-1, wtmp, xferlog, Oracle listener and
    Pix.
}
{http://www.itefix.no/i2/index.php}
{2009/01/11}

Logrep extracts fewer than half the fields \parsername{} does:

\begin{itemize}

    \item For mail sent and received: from address, size, and time and
        date.  Which date and hour are stored is not documented: start
        time, end time, delivery time, or another time?

    \item For mail sent: to addresses, \acronym{SMTP} code, and delay.

    \item For mail received: the hostname of the sender.

\end{itemize}

It also counts the number of log lines parsed and skipped.  It omits client
IP address and hostname, server IP address, HELO hostname, queueid,
timestamps of each log line, enhanced status code, and message-id\@.  Log
lines are correlated based on the queueid (called sessionname [sic] within
Logrep), but not by \acronym{pid}.  The parsing is error prone: empty
fields are saved when the log line does not match the regex, though it
appears that they will not overwrite existing data.  Most notably, rejected
delivery attempts are completely ignored, making it unsuitable for the
purposes of this project.  It does not try to address any of the
complications in \sectionref{complications} except for correlating log
lines by queueid.

Logrep does not come with any documentation, though some scant
documentation is available on its website (\parsername{} provides copious
documentation).  It requires a web browser to interact with it, so
automated log file processing would be difficult, whereas enabling
automated processing is a key part of \parsernames{} design.  Sadly, all
the author's attempts to use Logrep failed, because it was unable to access
the log files selected for parsing; this appears to be a bug rather than
operator error.  If the problem was caused by operator error, the interface
needs improvement because the (minimal) instructions were followed as
closely as possible, and multiple attempts were made.  Because parsing
failed it was not possible to review the reports Logrep can generate
(available in HTML only), or to examine the (undocumented) format it uses
to save extracted data for subsequent reuse.

Logrep extracts far less data from Postfix log files than \parsername{},
completely ignores rejected delivery attempts, is effectively undocumented,
does not deal with the more complicated aspects of Postfix log files, and
does not work properly.

\section{Summary}

This chapter has reviewed ten programs that perform basic Postfix log file
parsing, some to a greater level of detail than others.  None of the
reviewed parsers collect the breadth of information gathered by
\parsername{}, and none are designed to be extensible to handle new log
lines.  Some correlate log lines by queueid, but none correlate by pid;
none deal with any of the other complications described in
\sectionref{complications}.  All of the reviewed parsers generate a
report, and some provide a greater or lesser degree of customisation.  Most
have a data store, but only \acronym{LMA} provides any documentation on its
format; some deliberately make the data store inaccessible to other tools.
Most but not all of the parsers provide documentation, with the quality
ranging from unusable to excellent.  Fewer than half of the parsers were
capable of parsing the \numberOFlogFILES{} test log files; improving or
extending parsing would have been quite a difficult task for any of the
parsers, and one that the author did not have the time to attempt.
\Tableref{Summary of parsers' features} provides a summary of the parsers'
features.  The overriding difference between \parsername{} and the other
parsers reviewed herein is that none of them aim for the high level of
understanding of Postfix log files achieved by \parsername{}.


\begin{table}[thbp]
    \caption{Summary of reviewed parsers' features}
    \empty{}\label{Summary of parsers' features}
    \begin{tabular}{llllll}
        \tabletopline{}%
        Parser          & Parsed test   & Data              & Custom            & Documentation  & Source           \\
                        & log files?    & store?            & reports?          & quality?       & code?            \\
        \tablemiddleline{}%
        \acronym{LMA}   & No            & Yes               & No                & Poor           & Yes              \\
        Pflogsumm       & Yes           & No                & Partial \dag{}    & Good           & Yes              \\
        Sawmill         & Yes           & Yes               & Searches          & Excellent      & Yes \nialpha{}   \\
        Splunk          & Yes           & Yes               & Searches          & Abundant       & No               \\
                        &               &                   & \& reports        & but poor       &                  \\
        Isoqlog         & No            & Yes               & No                & No English     & Yes              \\
                        &               &                   &                   & documentation  &                  \\
        AWStats         & Partially     & Yes               & Partial \dag{}    & Good           & Yes              \\
        Anteater        & No            & No                & No                & None           & Yes              \\
        YAALA           & No            & Yes \ddag{}       & Searches          & Poor           & Yes              \\
                        &               &                   & \& reports        &                &                  \\
        Lire            & Yes           & Yes               & Yes               & Poor           & Yes              \\
        Logrep          & No            & Yes               & No                & None           & Yes              \\
        \parsername{}   & Yes           & Yes \nibeta{}     & No \nichi{}       & \niepsilon{}   & Yes              \\
        \tablebottomline{}%
    \end{tabular}

    \begin{eqlist}

        \item [\dag{}] Sections can be omitted from a report, but extra
            sections cannot be added.

        \item [\ddag{}] YAALA only stores the data required to produce the
            latest report; other data will be discarded.

        \item [\nialpha{}] Sawmill's source code is available, but in an
            encrypted or obfuscated form.

        \item [\nibeta{}] \parsername{} is the only parser with
            documentation for the format of its data store.

        \item [\nichi{}] \parsername{} defers report generation to
            subsequent programs, but all the necessary documentation to
            produce reports is provided.

        \item [\niepsilon{}] \parsername{} aims to have thorough and
            complete documentation, but the author cannot provide an
            unbiased review.

    \end{eqlist}

\end{table}

\clearpage{}
