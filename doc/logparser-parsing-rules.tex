\section{Parsing Rules}

\label{rules}

This section discusses the rules used in parsing Postfix log files,
starting with rule characteristics, followed by the problems caused by
overlapping rules, and techniques to detect and deal with such overlaps.
An example rule and a log line it would match are provided, plus a
description of how the fields in the rule\footnote{The table containing the
rules has already been described in \sectionref{rule attributes}.} are used
when matching a log line and subsequently performing the requested action.
The section continues with a discussion of rule efficiency concerns, with
reference to the graphs in \sectionref{graphs}, and finishes with an
explanation of the algorithm used to generate new \regexes{} from unparsed
log lines.

Please refer to \sectionref{parser design} for a discussion of why the
rules and actions have been separated in the parser's design.

\subsection{Rule efficiency}

\label{rule efficiency}

XXX ADD TIMING INFORMATION\@.

Parsing efficiency is an obvious concern when the parser routinely needs to
deal with 75 MB log files containing 300,000 log lines (generated daily on
a mail server handling mail for approximately 700 users --- large scale
mail servers would have much larger log files on a daily basis).  When
generating the data for the graphs included in \sectionref{graphs},
\numberOFlogFILES{} log files (totaling 10.08 GB, \numberOFlogLINEShuman{}
log lines) were each parsed 10 times, the first run discarded, and the
execution time for the remaining 9 runs averaged.  The first run is
discarded for two reasons:

\begin{enumerate}

    \item The execution time will be higher because the log file must be
        read from disk, whereas for subsequent runs the log file will be
        cached in memory by the operating system.

    \item The execution time will also be higher because the rule ordering
        will be sub-optimal compared to subsequent runs.

\end{enumerate}

Saving results to the database was disabled for the test runs, as that
dominates the run time of the program, and the tests are aimed at measuring
the speed of the parser rather than the speed of the database and the disks
the database is stored on.

\subsubsection{Algorithmic complexity}

An important property of a parser is how execution time scales relative to
input size: does it scale linearly, polynomially, or exponentially?
\Graphref{execution time vs file size vs number of lines graph}
shows the execution time in seconds, file size in MB and tens of thousands
of log lines per log file.  All three lines run roughly in parallel, giving
a visual impression that the algorithm scales linearly with input size.
This impression is borne out by \graphref{execution time vs file
size vs number lines factor} which plots the ratio of file size vs
execution time and ratio of number of log lines vs execution time (higher
is better); \tableref{execution time vs file size vs number lines
factor table} shows the ratios for different breakdowns of the log files.
As the reader can see the ratios are quite tightly banded, showing that the
algorithm scales linearly: the much larger log files between points 60 and
70 on the X axis in \graphref{execution time vs file size vs
number of lines graph} actually cause the ratio to increase (i.e.\
improve), rather than decrease.  The strange behaviour where larger log
files are parsed more efficiently is explained fully in \sectionref{Why are
there dips in the graphs?}.  

\subsubsection{Rule ordering for efficiency}

\label{rule ordering for efficiency}

Rule ordering was mentioned in \sectionref{rule attributes} and will be
covered in greater detail in this section.  At the time of writing there
are \numberOFrules{} different rules: the top 10\% match the vast majority
of log lines, with the remaining log lines split across the other 90\% of
the rules (as shown in \graphref{rule hits graph}).  Assuming that
the distribution of log lines is reasonably steady over time, program
efficiency should benefit from trying more frequently matching rules before
those which match less frequently.  To test this hypothesis three full test
runs were performed with different rule orderings:

\begin{description}

    \item [normal]  The most optimal order, according to the hypothesis:
        rules which match most often will be tried first.

    \item [shuffle] This is intended to represent a randomly ordered rule
        set.  The rules will be shuffled once before use and will retain
        that ordering for the entirety of the log file.  Note that the
        ordering will change every time the parser is executed, so 10
        different orderings will be generated for each log file in the test
        run.  

    \item [reverse] Hypothetically the worst order: the most frequently
        matching rules will be tried last.

\end{description}

Graphs~\refwithpage{percentage increase of shuffled over normal}
and~\refwithpage{percentage increase of reversed over normal} show the
percentage increase of execution times, with \tableref{Execution
time increase for different rule orderings} showing the mean increases for
different groupings of log files.  Overall this provides a modest but
worthwhile performance increase of approximately 9\%, for a small
investment in time and programming.

\subsubsection{Caching each regex}

\label{Caching each regex}

Perl compiles the original \regex{} into an internal representation,
optimising the \regex{} to improve the speed of matching, but this
compilation and optimisation takes CPU time; far more CPU time, in fact,
than the actual matching takes.  Perl automatically caches static
\regexes{}, but dynamic \regexes{} need to be explicitly compiled and
cached.  \Graphref{normal regex vs discard regex} shows execution
times with and without caching the \regex{}.  Caching the compiled
\regexes{} is obviously far more efficient; \graphref{normal regex
vs discarded regex factor} shows the percentage execution time increase
when not caching each \regex{}.

Caching the compiled \regexes{} is quite simple, and is the single most
effective optimisation implemented in the parser.

XXX ADD A TABLE\@.

\subsection{Summary}

This section dealt with the rules used in parsing Postfix log files:

\begin{itemize}

    \item The characteristics of the rules were described.

    \item Detecting overlapping rules and dealing with the problems they
        can cause was covered, including a discussion of why overlapping
        rules can be helpful as well as harmful.

    \item An example log line and the rule matching it illustrated a
        description of how the fields in the rule are used both in the
        matching phase and the action that is subsequently executed.  XXX
        REWRITE ONCE EXAMPLE HAS CHANGED\@; MERGE WITH NEXT ITEM\@.

    \item The structure of the database table containing the rules is dealt
        with in \sectionref{rule attributes}, and is not duplicated in this
        section.

    \item The topic of rule efficiency was discussed, covering the effects
        of caching compiled \regexes{} and optimal ordering of rules, with
        reference to the graphs in \sectionref{graphs}.

    \item This section finished with an explanation of the algorithm used
        to generate a new \regex{} from unparsed log lines.

\end{itemize}

