\section{Results}

XXX NEED AN INTRODUCTION\@.

XXX COMPARE SAVING RESULTS TO RAM DISK VERSUS SAVING TO HARD DISK\@.

XXX COMPARE PARSING + ACTIONS TO JUST PARSING\@.

XXX USE THE DATA COLLECTED ABOUT NUMBER OF RULES TRIED\@.  MAYBE INCLUDE IN
ORDERINGS SECTION\@?

\subsection{Parser Efficiency}

\label{parser efficiency}

Parsing efficiency is an obvious concern when the parser routinely needs to
parse large log files.  The server that generated the log files used
in testing this parser accepts approximately 10,000 mails for 700 users a
day; \graphref{Mails received per day} shows that as expected there are far
more mails received on weekdays than at weekends.   Median log file size is
50MB, containing 285,000 log lines --- large scale mail servers would have
much larger log files.  Note that \graphref{Mails received per day} and
\tableref{Number of mails received per day: statistics} show the number of
mails received by \SMTP{} only; in particular the mail loops noticeable in
other graphs do not contribute to these figures.

\showgraph{build/graph-mails-received}{Number of mails received via SMTP per
day}{Mails received per day}

\showtable{build/include-mails-received-table}{Number of mails received via
SMTP per day}{Number of mails received per day: statistics}

When generating the timing data used in this section, \numberOFlogFILES{}
log files (totaling 10.08 GB, \numberOFlogLINEShuman{} log lines) were each
parsed 10 times and the parsing times averaged.  Saving results to the
database was disabled for the test runs, as that dominates the run time of
the program, and the tests are aimed at measuring the speed of the parser
rather than the speed of the database and the disks the database is stored
on.  The computer used for test runs was a Dell Optiplex 745, equipped
with: 

\begin{tabular}[]{ll}

    CPU         & One dual core 2.40GHz Intel\textregistered{}
                    Core\texttrademark{}2 CPU, \\
                & with 32KB L1 cache and 4MB L2 cache. \\
    RAM         & 2GB 667 MHz DDR RAM\@. \\
    Hard disk   & One Seagate Barracuda 7200 RPM 250GB SATA disk. \\

\end{tabular}

Parsing all \numberOFlogFILES{} log files in one run took
\input{build/include-timing-run-duration}.  The computer was dedicated to
the task of gathering statistics from test runs, and was not used for any
other purpose while test runs were ongoing; any services not necessary for
running the tests were disabled.

\subsubsection{Architecture Scalability: Input Size}

An important property of a parser is how parsing time scales relative to
input size: does it scale linearly, polynomially, or exponentially?
\Graphref{parsing time vs file size vs number of lines graph} shows the
parsing time in seconds, file size in MB, and tens of thousands of log
lines for each of the \numberOFlogFILES{} log files.  All three lines run
roughly in parallel, giving the impression that the algorithm scales
linearly with input size.  This impression is borne out by
\graphref{parsing time vs file size vs number lines factor} which plots
both the ratio of file size vs parsing time, and the ratio of number of log
lines vs parsing time (higher is better in both cases); \tableref{parsing
time vs file size vs number lines factor table} shows the same ratios for
different groups of log files.  The ratios are quite tightly banded,
providing empirical evidence that the algorithm scales linearly.  The ratio
increases (i.e.\ improves) for log files 22 and 62--68 despite their larger
than average size (shown in \graphref{parsing time vs file size vs number
of lines graph}); this is explained fully in \sectionref{Explaining the
peaks in log file size}.  

\showtable{build/include-file-size-and-number-of-lines-vs-parsing-time}{Ratio
of file size and number of lines to parsing time}{parsing time vs file size
vs number lines factor table}

\showgraph{build/graph-input-size-vs-parsing-time}{Parsing time, file size,
and number of lines}{parsing time vs file size vs number of lines graph}

\showgraph{build/graph-input-size-vs-parsing-time-ratio}{Ratio of file size
and number of lines to parsing time}{parsing time vs file size vs number
lines factor}

\subsubsection{Explaining the peaks in log file size}

\label{Explaining the peaks in log file size}

\Graphref{parsing time vs file size vs number of lines graph} shows log
file size varying between 30--100MB, except for log files 22 (200MB) and
62--68 (400--900MB).  These large log files were caused by a user
unintentionally creating a mail forwarding loop, resulting in a huge number
of mail deliveries and up to an order of magnitude more log lines than is
usual.  The user specified his mail forwarding as: \newline{}
\tab{}\texttt{$\backslash$username, username@domain} \newline{} This
instructs Postfix to deliver the mail to the local user, and also forward a
copy to the remote address; this is generally not a problem except that the
remote address in this instance is the address the mail was originally sent
to, creating an infinite loop.  To prevent this happening Postfix examines
the Delivered-To header in the mail, and if the mail has already been
delivered to the current address it is bounced back to the sender with the
error message \newline{} \tab{} \texttt{mail forwarding loop for
username@domain}\newline{}  Ordinarily this works well, but unfortunately
in this instance the user noticed they had not received any mail in a while
and opted to send a test mail to themselves, causing a loop not caught by
Postfix:

\begin{enumerate}

    \item Postfix accepts a mail from username@domain, for username@domain.

    \item Postfix delivers the mail to the local mailbox and
        username@domain, as instructed by the user's forwarding
        instructions. The forwarded mail has a
        \texttt{Delivered-To:~username@domain} header added, and the
        envelope sender address is username@domain.

    \item Postfix accepts the mail for username@domain, but during delivery
        it notices that the \texttt{Delivered-To} header already contains
        the address it is currently delivering to, and therefore sends a
        bounce notification with sender address \texttt{<>} to the original
        sender: username@domain.

    \item Postfix accepts the bounce notification and delivers it to both
        the local mailbox and to username@domain, as instructed by the
        user's forwarding instructions.  A
        \texttt{Delivered-To:~username@domain} header is added to the
        forwarded bounce notification, which now has an envelope sender
        address of username@domain.

    \item Postfix accepts the forwarded bounce notification but during
        deliver the mail it notices that the \texttt{Delivered-To}
        header already contains the address currently being delivered to,
        and sends a bounce notification to the sender: username@domain.

    \item Repeat from step two; this will continue indefinitely unless an
        administrator intervenes and deletes the appropriate mails from the
        queue.

\end{enumerate}

The sequence described above occurs extremely rapidly because Postfix does
not have to deliver the mail to an external system, so mails are delivered,
bounced and generated as fast as the disks can keep up, resulting in a huge
volume of log lines.

The different distribution of log lines during a mail forwarding loop has
unexpected effects on parsing time, described in detail in
\sectionref{rule ordering for efficiency}, \sectionref{scalability as the
number of rules rises}, and \sectionref{Caching compiled regexes}.

\subsubsection{Rule ordering for efficiency}

\label{rule ordering for efficiency}

At the time of writing there are \numberOFrules{} different rules: the top
10\% match the vast majority of log lines, with the remaining log lines
split across the other 90\% of the rules (as shown in \graphref{rule hits
graph}).  Assuming that the distribution of log lines is reasonably
consistent over time, parser efficiency should benefit from trying more
frequently matching rules before those that match less frequently.  To
test this hypothesis three full test runs were performed with different
rule orderings:

\begin{description}

    \item [optimal]  The most optimal order, according to the hypothesis:
        rules that match most often will be tried first.

    \item [shuffle] This is intended to represent a randomly ordered rule
        set.  The rules will be shuffled once before use and will retain
        that ordering until the parser exits.  Note that the ordering will
        change every time the parser is executed, so 10 different rule
        orderings will be generated for each log file in the test run.  

    \item [reverse] Hypothetically the worst order: the most frequently
        matching rules will be tried last.

\end{description}

\Graphref{Parsing time relative to shuffled ordering graph} shows the
parsing times of optimal and reverse orderings relative to shuffled
ordering; the mean relative parsing times for different groupings of log
files are given in \tableref{Parsing time relative to shuffled ordering
table}.  Overall this optimisation provides a modest but worthwhile
performance increase of approximately 10\%, for a small investment in time
and programming.

\showgraph{build/graph-optimal-and-reverse-vs-shuffle}{Parsing time relative
to shuffled ordering}{Parsing time relative to shuffled ordering graph}

\showtable{build/include-optimal-and-reverse-vs-shuffle}{Parsing time
relative to shuffled ordering}{Parsing time relative to shuffled ordering
table}

Differences in rule ordering have less effect on parsing time when parsing
log files 22 and 62--68, due to the different distribution of log lines in
those log files (see \sectionref{Explaining the peaks in log file size}).
When a mail loop occurs the vast majority of log lines are from Postfix
components that have few rules associated with them, whereas in general the
most log lines are from Postfix components that have a high number of
associated rules; this results in the average number of rules required to
parse a log line from log files 22 or 62--68 being considerably lower than
usual.  The ratio of number of lines versus parsing time in
\graphref{parsing time vs file size vs number lines factor} shows a
noticeable improvement at log files 22 and 62--68, where a higher ratio
means more log lines parsed per unit of time; similarly \graphref{Parsing
time relative to shuffled ordering graph} shows a much smaller difference
between the orderings at log files 22 and 62--68.

\subsubsection{Scalability as the number of rules rises}

\label{scalability as the number of rules rises}

How any architecture scales as the number of rules increases is important,
but it is particularly important in this architecture because it is
expected that the typical parser will have a large number of rules.  There
are \numberOFrules{} rules in the full Postfix ruleset, whereas the minimum
number of rules required to parse the \numberOFlogFILES{} log files used
when generating the results in this document is \numberOFrulesMINIMUM{},
\numberOFrulesMINIMUMpercentage{} of the full ruleset.  The full ruleset is
larger because the Postfix parser is tested with \numberOFlogFILESall{} log
files, to improve its parsing and ensure it correctly parses logs generated
by later Postfix versions.  A second set of statistics was generated using
the minimum ruleset and compared to the statistics generated using the full
ruleset: the percentage parsing time increase when using the full ruleset
instead of the minimal ruleset for optimal, shuffled and reversed orderings
is shown in \graphref{Percentage parsing time increase of maximum ruleset
over minimum ruleset}.

\showgraph{build/graph-full-ruleset-vs-minimum-ruleset}{Percentage parsing
time increase of maximum ruleset over minimum ruleset}{Percentage parsing
time increase of maximum ruleset over minimum ruleset}

It is clear from \graphref{Percentage parsing time increase of maximum
ruleset over minimum ruleset} that the increased number of rules noticeably
increases parsing time with reverse ordering, and to a lesser extent with
shuffled ordering.  The optimal ordering (where the most frequently
matching rules are tried first) shows a mean increase of
\input{build/include-full-ruleset-vs-minimum-ruleset} in parsing time for a
\numberOFrulesMAXIMUMpercentage{} increase in the number of rules.  Log
files 22 and 62--68 show much smaller increases for shuffled and reversed
ordering than other log files do, because the majority of log lines in
those log files are parsed by Postfix components with a small number of
parsing rules, so removing unnecessary rules has little effect on the total
number of rules used.  These results show that the architecture scales very
well as the number of rules increases, and that optimally sorting the rules
is an important optimisation contributing to this scalability.

\subsubsection{Caching compiled regexs}

\label{Caching compiled regexes}

Before the perl interpreter attempts to match a \regex{} against a piece of
text, the \regex{} is compiled into an internal representation and
optimised to improve the speed of matching.  This compilation and
optimisation takes CPU time ---in many cases it takes far more CPU time
than the actual matching.  If the interpreter is certain that a \regex{}
will not change it will automatically cache the compilation results for
later use.   The results of compiling a dynamically generated \regex{} can
be cached and used in preference to the original \regex{}, but it is the
responsibility of the programmer to do this.  The Postfix parser loads the
rules it uses from a database, and thus each rule's \regex{} is compiled
and the result cached to reduce parsing time.

\Graphref{Increase in parsing time when not caching compiled regexes graph}
shows the impact that not caching compiled \regexes{} has on parser
performance: on typical log files the parsing time when not caching
compiled \regexes{} is 400--600\% of the parsing time when caching.
Caching compiled \regexes{} is probably the single most effective
optimisation possible in the parser's implementation, and was quite simple
to implement.

\showgraph{build/graph-cached-regexes-vs-discarded-regexes}{Increase in
parsing time when not caching compiled regexes}{Increase in parsing time
when not caching compiled regexes graph}

Two large dips can be seen in \graphref{Increase in parsing time when not
caching compiled regexes graph} at log files 22 and 62--68, corresponding
to the spikes in log file size in \graphref{parsing time vs file size vs
number of lines graph}.  The distribution of log lines across rules when
there is a mail loop is much different to the norm, and the average number
of rules consulted per log line is much lower: this results in far fewer
\regex{} compilations per line when there is a mail loop, and a
corresponding decrease in the average parsing time for a single log line.
The outcome is that caching compiled \regexes{} is proportionally less
important when the log file contents were created by a mail loop.  The
increases in parsing time when not caching compiled \regexes{} for
different groups of log files are summarised in \tableref{Increase in
parsing time when not caching compiled regexes table}.

\showtable{build/include-cached-regexes-vs-discarded-regexes} {Increase in
parsing time when not caching compiled regexes} {Increase in parsing time
when not caching compiled regexes table}


\subsubsection{Summary}

This section began with parser scalability, showing the linear relationship
between parsing time and input size.  Demonstrates the effect of rule
ordering on parsing time, and the unexpected consequences of specific
inputs.  The necessity of caching compiled \regexes{} is attested to by the
third group of graphs, where the difference between caching and not caching
is staggering.  The penultimate section contains a breakdown of the rule
hits accumulated during a single test run of the parser.  Miscellaneous
graphs expected to be useful are collected in the final section.  

\subsection{Coverage}

I THINK THIS SECTION NEEDS SUBSTANTIAL WORK, IF NOT A RE-WRITE\@.

\label{parsing coverage}

The discussion of the parser's coverage of Postfix log files is separated
into two parts: log lines covered and mails covered.  The first is
important because the parser should handle all (relevant) log lines it is
given; the second is equally important because the parser must properly
deal with every mail if it is to be useful.  Improving the former is
less intrusive, as it just requires new rules to be written; improving the
latter is much more intrusive as it requires changes to the parser
algorithm, and it can also be much harder to notice a deficiency.

\subsubsection{Log lines covered}

\label{log-lines-covered}

Parsing a log line is a three stage process:

\begin{enumerate}

    \item Check if there are any rules for the Postfix component that
        produced the log line; if not then skip the log line.

    \item Try each rule until a matching rule is found; if no match is
        issue a warning and move to the next log line.

    \item Execute the action specified by the rule.

\end{enumerate}

Full coverage of log lines requires the following:

\begin{enumerate}

    \item Each Postfix component whose log lines are of interest must have
        at least one rule or its log lines will be silently skipped; in the
        extreme case of zero rules the parser would happily skip every log
        line.  There may be any number of log lines from other programs
        intermingled in the log file, and there are some Postfix programs
        that do not produce any log lines of interest.

    \item There must be a rule to match each different log line produced by
        each program; if a log line is not successfully matched the parser
        will issue a warning.  Rules should be as specific and tightly
        bound as possible to ensure accurate parsing:\footnote{A rule that
        matches zero or more of any character will successfully parse every
        log line, but not in a meaningful way.} most log lines contain
        fixed strings and have a rigid pattern, so this is not a problem.

    \item The appropriate action to take --- discussed in
        \sectionref{mails-covered}.

\end{enumerate}

Full coverage of log lines is easy to achieve yet hard to maintain.  It is
easy to achieve full coverage for a limited set of log files (at the time
of writing the parser is tested with \numberOFrules{} rules, fully parsing
\numberOFlogFILESall{} contiguous log files from Postfix 2.2 through to
Postfix 2.5), and new rules are easy to add.  Maintaining full coverage is
hard because other servers have different restrictions with custom
messages, \DNSBL{} messages change over time, major releases of Postfix
change warning messages (usually adding more information), etc.,\ so over
time the log lines drift and change.  \Graphref{rule hits graph} shows the
number of hits for each rule over all \numberOFlogFILES{} log files.  The
number of hits per rule is quite unevenly spread, resembling a Power Law
distribution~\cite{powerlaw} --- it is obvious that a small number of rules
match the vast majority of the lines, and more than half the rules match
fewer than 100 times.

\showgraph{build/graph-hits}{Hits per rule}{rule hits graph}

Warnings are issued for any log lines that are not parsed; no warnings are
issued for unparsed log lines while testing with the \numberOFlogFILES{}
test log files, so it can be safely concluded that there are zero false
negatives.  False positives are harder to quantify: short of examining each
of the 60,721,709 log lines and determining which \regex{} parsed it, there
is no way to be sure that every line was parsed by the correct \regex{},
making it impossible to quantify the false positive rate; however a random
sample of 6,039 log lines was parsed and the results checked manually to
ensure that the correct \regex{} parsed each log line.\footnote{Each log
line was examined and the correct \regex{} identified from the
\numberOFrules{} rules in the database; the correct \regex{} was then
compared to the \regex{} that was used by the parser.}  The sample was
generated by running the following command:

\verb!    perl -e 'print if (rand 1 < 0.0001)' -n LOG_FILES!

\noindent{}to randomly extract roughly one line in every 10,000.  Although
on initial appearances exercising only 36 rules (from a total of
\numberOFrules{}) when parsing 6039 log lines seems quite low, after
examining \graphref{rule hits graph} it becomes apparent that such
a low hit rate is to be expected; the reader should also bear in mind that
even when parsing all \numberOFlogFILES{} log files not all the rules are
exercised (some of the rules are for parsing log lines that only appear in
other log files).

\subsubsection{Mails covered}

\label{mails-covered}

Coverage of mails is much more difficult to determine accurately than
coverage of log lines.  The parser can dump its state tables in a human
readable form; examining these tables with reference to the log files is
the best way to detect mails that were not handled properly (many of the
complications discussed in \sectionref{additional complications} were
detected in this way).  The parser issues warnings when it detects any
errors, some of which may alert the user to a problem, e.g.\ when a queueid
is reused before the previous mail is fully dealt with, when a queueid or
\pid{} is not found in the state tables,\footnote{There will often be
warnings about a missing queueid or \pid{} in the first few hundred or
thousand log lines because the earlier log lines for those connections or
mails are in the previous log file; loading the saved state from the
previous log file will solve this problem.} or when there are problems
tracking a child mail (see \sectionref{tracking re-injected mail}).  There
should be few or no warnings when parsing, and when finished parsing the
state table should only contain entries for mails that had yet to be
delivered when the log files ended, or were accepted before the log files
began.

At the time of writing the parser is being tested with \numberOFlogFILES{}
log files.  There are 5 warnings produced, but because the parser errs on
the side of producing more warnings rather than fewer, those 5 warnings
represent 3 instances of 1 problem: 3 connections started before the first
log file, so their initial log entries are missing, leading to warnings
when their log lines are parsed.

The state tables contain entries for mails not yet delivered when the
parser finishes execution.  Ideally all they should contain are mails that
are awaiting delivery after the period covered by the log files, though
they may also contain mails whose initial entries are not contained in the
log files.  Any other entries are evidence of a failure in parsing or an
aberration in the log files.  After parsing the \numberOFlogFILES{} test
log files the state tables contain 18 entries, breaking down into:

\begin{itemize}

    \item 1 connection that started only seconds before the log files
        ended and had not yet been fully transferred from client to server.

    \item 1 mail that had been accepted only seconds before the log files
        ended and had not yet been delivered.

    \item 9 mails whose initial log entries were not present in the log
        files.

    \item 7 mails that had yet to be delivered due to repeated failures.

\end{itemize}

There are no mails in the state tables which should not be present, thus it
can be concluded that there are zero false negatives.  Once again,
determining the false positive rate is much harder, as manually checking
the results of parsing 13,850,793 connections and mails accepted, rejected,
bounced or delivered is infeasible.  There is considerable circumstantial
evidence that the false positive rate is quite low:

\begin{itemize}

    \item The parser is quite verbose when complaining about known problems
        (e.g.\ if a mail is tracked twice as described in
        \sectionref{tracking re-injected mail}), and no such warnings are
        produced during the test runs.

    \item Queueids and \pids{} naturally group together log lines belonging
        to one mail or connection respectively; it is extremely unlikely
        that a log line would be associated with the wrong connection.

    \item When dealing with the complications described in
        \sectionref{complications} and \sectionref{additional
        complications} the solutions are as specific and restrictive as
        possible, with the goal of minimising the number of false
        positives.  In addition the solution to the \textit{Out of order
        log files\/} complication described in \sectionref{out of order log
        lines} imposes conditions that each reassembled mail must comply
        with to be acceptable.

    \item Every effort has been made while developing to make the parser as
        precise, demanding and particular as possible.

\end{itemize}

Whereas verifying by inspection that the parser correctly deals with all
60,721,709 lines in the test log files is infeasible, verifying a subset of
those log files is a tractable, if extremely time consuming, task.  A
sample of log lines was obtained by randomly selecting a log file:

\verb!    perl -Mstrict -Mwarnings -MList::Util=shuffle \!\newline{}
\verb!            -e 'print [shuffle(@ARGV)]->[0];'!

The first 6000 lines of this log file (roughly 0.01\% of the total number
of log lines used in testing) was extracted:

\verb!    sed -n -e '1,6000p' logfile > test-log-segment!

It is important that the log lines used are contiguous so that all log
entries are present for as many of the connections as possible.  This log
segment was parsed with all debugging options enabled, resulting in 167,448
lines of output.\footnote{A mean of 27.908 lines of output per line of
input; each connection has 30 debugging lines, plus 21 debugging lines per
result.  Connections which have been cloned will have the cloned connection
in their debugging output, plus another 33 debugging lines.  Those numbers
are approximate, and may vary $\pm{}$ 2.  There is a linear relationship
between the number of log lines and debugging lines: $33(connections) +
30(accepted~~mails) + 21(results)$.  This formula is an approximation only,
and has not been rigorously verified.}  All 167,448 lines were examined in
conjunction with the input log and a dump of the resulting database,
verifying that for each of the input lines the parser used the correct rule
and executed the correct action, that in turn produced the correct result
and inserted the correct data in the database.  The log segment produced 4
warnings, 10 mails remaining in the state tables, and 1625 connections
correctly entered in the database.

Given the circumstantial and experimental evidence detailed above, the
author is confident that the false positive rate when reconstructing a mail
is exceedingly low, if not approaching zero.

\subsection{Summary}

XXX EXTEND TO SUMMARISE PARSER EFFICIENCY TOO\@.

XXX REWRITE COVERAGE SUMARY IF NECESSARY\@.

Parser coverage is divided into two topics in this section: log lines
covered and mails covered.  The former is initially more important, as the
parser must successfully parse every line if it is to be complete, but
subsequently the latter takes precedence because reproducing the path a
mail takes through Postfix is the aim of the parser.  Increasing the
percentage of log lines parsed is relatively simple and non-intrusive:
adding new rules or modifying existing rules is simplified by the
separation of rules, actions and framework.  Improving the logical coverage
is harder, as the actions taken by Postfix must be reconstructed by the
author, and the new sequence of actions integrated into the existing model
without breaking the existing parsing.  Detecting a deficiency in the
parsing algorithm is also significantly harder than detecting unparsed log
lines, as the parser will warn about any unparsed line, whereas discovering
a flaw in the parser requires understanding of the warnings produced and
the mails remaining in the state table.  Rectifying a flaw in the parser
requires an understanding of both the parser and Postfix's log files, and
investigative work to determine the cause of the deficiency, followed by
further examination of the log files in developing a solution.

