\chapter{Evaluation}

\label{Evaluation}

\renewcommand{\figurename}{Graph}

% Reduce the gap between columns in tables.
\addtolength{\tabcolsep}{-2pt}

This chapter evaluates \parsername{} on two criteria: efficiency and
coverage.  \parsername{} is intended to be used in a production
environment, and must be capable of parsing log files generated by a high
volume mail server in a reasonable time period.  Accurate but slow parsing
is preferable to sloppy but quick parsing, so the goal of better parser
efficiency must be balanced against the requirement for precise and correct
processing of log files.

The performance evaluation begins by describing the characteristics of the
mail server that produced the log files used to test and evaluate the
parser's performance, and also the computer that the tests were run on.
How the parser scales as the size of log files increases is described next,
with an explanation of why \parsername{} has better performance with the
larger log files in the group.  The effect of using different rule
orderings is explored, and the use of optimal rule ordering is compared to
use of an oracle that allows the parser to use only one rule when
recognising each log line.  When \parsername{} is used by other mail
administrators they will need to extend the ruleset to parse their own log
lines, so the next section addresses the question of how parser performance
is affected as the number of rules in the ruleset rises.  The simple
optimisation of caching the results of compiling regexes, and the huge
effect it has on parser efficiency, is the penultimate topic to be covered.
The performance evaluation concludes by examining where the parsing time is
spent: recognition of log lines or their subsequent processing?

The second criterion the parser is evaluated on is its coverage of Postfix
log files.  This topic consists of two sections: what proportion of log
lines are correctly recognised by the ruleset, and what proportion of mail
delivery attempts are correctly understood and reconstructed by the
actions?  The former is a requirement before the latter can be achieved,
and the latter is important because the data provided by \parsername{} must
be both complete and correct for it to be of use to others.

\section{Parser Efficiency}

\label{parser efficiency}

Parsing efficiency is an obvious concern when the parser routinely needs to
parse large log files.  The server that generated the log files used in
testing this parser accepts approximately 10,000 mails for 700 users each
weekday; \graphref{Mails received per day} and \tableref{Number of mails
received per day: statistics} show that, as expected, far more mails are
received on weekdays than at weekends.  Note that these figures count mails
received by \acronym{SMTP} only, and the mail loops noticeable in later
graphs are not included.  Median log file size is 50MB, containing 285,000
log lines; large scale mail servers would have much larger log files.  The
mail server in question is a production mail server handling mail for a
university department; the benefit of using this server is that its log
files exhibit the idiosyncrasies and peculiarities a mail server in the
wild must deal with, but the downside is that significantly altering the
configuration to accommodate this project is not an option.

\showgraph{build/graph-mails-received}{Number of mails received via SMTP
per day}{Mails received per day}

\showtable{build/include-mails-received-table}{Number of mails received via
SMTP per day}{Number of mails received per day: statistics}

\begin{table}[thbp]
    \caption{Details of the computer used to generate statistics}
    \empty{}\label{Details of the computer used to generate statistics}
    \centering{}
    \begin{tabular}[]{ll}
        \tabletopline{}%
        Component  & Component in use                                   \\
        \tablemiddleline{}%
        CPU         & One dual core 2.40GHz Intel\textregistered{}
                        Core\texttrademark{}~2 CPU,                     \\
                    & with 32KB L1 cache and 4MB L2 cache.              \\
        RAM         & 2GB 667 MHz DDR RAM\@.                            \\
        Hard disk   & One Seagate Barracuda 7200 RPM 250GB SATA disk.   \\
        \tablebottomline{}%
    \end{tabular}
\end{table}

When generating the timing data used in this section, \numberOFlogFILES{}
log files (totaling 10.08 GB, \numberOFlogLINEShuman{} log lines) were each
parsed 10 times, and the mean parsing time used.  The computer used for
test runs was a Dell Optiplex 745, shown in \tableref{Details of the
computer used to generate statistics}; it was dedicated to the task of
gathering statistics from test runs, and did not run any other programs
while test runs were in progress.  Saving results to the database was
disabled for the test runs, because that dominates the run time of the
parser, and the tests are aimed at measuring the speed of \parsername{}
rather than the speed of the database and the disks the database is stored
on.  Parsing all \numberOFlogFILES{} log files in one run without saving
results to the database took \input{build/include-full-run-duration}, with
mean throughput of \input{build/include-full-run-throughput}.  In contrast,
when saving results to the database, parsing all \numberOFlogFILES{} log
files took \input{build/include-insert-results-duration}, with mean
throughput of \input{build/include-insert-results-throughput} --- parsing
makes up only
\input{build/include-skip-inserting-as-percentage-of-inserting} of the
execution time when saving results to the database.

\subsection{Architecture Scalability: Input Size}

XXX THIS SECTION DOES NOT SHOW ANY VARIETY IN INPUT SIZE, SO DOES NOT SHOW
SCALABILITY\@.  MAYBE I SHOULD RENAME IT TO ``Characteristics Of The Input
Log Files'' OR SOMETHING\@?  THE GRAPHS SHOW THAT LOGS HAVE CONSISTENT
CONTENT EXCEPT WHEN THERE IS A MAIL LOOP\@.

An important property of a parser is how parsing time scales relative to
input size: does it scale linearly, polynomially, or exponentially?
\Graphref{parsing time vs file size vs number of log lines graph} shows the
parsing time in seconds, file size in MB, and number of log lines in tens
of thousands, for each of the \numberOFlogFILES{} log files.  The three
lines run roughly in parallel, giving the impression that the algorithm
scales linearly with input size.  This impression is borne out by
\graphref{parsing time vs file size vs number of log lines factor}, which
plots both the ratio of file size vs parsing time, and the ratio of number
of log lines vs parsing time (higher is better in both cases);
\tableref{parsing time vs file size vs number of log lines factor table}
shows the same ratios for different groups of log files.  The ratios are
quite tightly banded, showing that the algorithm scales linearly; the
ratios increase (i.e.\ improve) for log files 22 \& 62--68, despite their
larger than usual size.  Both groups of log files are much larger than
usual because of a mail loop caused by a user who set up mail forwarding
incorrectly, resulting in a very different distribution of log lines:
normally most log lines are generated by mail delivery attempts from other
hosts, but when the mail loops occurred most of the log lines resulted from
failed delivery of mail generated on the server itself.  The Postfix
components that generated most of the log lines during the mail loop have
fewer associated rules than the Postfix components whose log lines normally
make up the bulk of each log file, so the mean number of rules tried per
log line reduces and so does the mean parsing time per log line.
\Graphref{Number of rules per Postfix component} shows the number of rules
per Postfix component, \graphref{Mean number of rules tried per log line}
shows the drop in the mean number of rules tried per log line for log files
containing a mail loop, and \graphref{Mean number of rules tried per log
line for each Postfix component} shows the mean number of rules tried per
log line for each Postfix component.

\showtable{build/include-file-size-and-number-of-log-lines-vs-parsing-time}{Ratio
of file size and number of log lines to parsing time}{parsing time vs file
size vs number of log lines factor table}

\showgraph{build/graph-input-size-vs-parsing-time}{Parsing time, file size,
and number of log lines for \numberOFlogFILES{} log files}{parsing time vs
file size vs number of log lines graph}

\showgraph{build/graph-input-size-vs-parsing-time-ratio}{Ratio of file size
and number of log lines to parsing time (higher is better)}{parsing time vs
file size vs number of log lines factor}

\showgraph{build/graph-average-number-of-rules-tried-per-log-line}{Mean
number of rules tried per log line}{Mean number of rules tried per log
line}

\showgraph{build/graph-average-number-of-rules-tried-per-program}{Mean
number of rules tried per log line for each Postfix component}{Mean number
of rules tried per log line for each Postfix component}

\begin{table}[thbp]
    \caption{Number of rules per Postfix component}
    \empty{}\label{Number of rules per Postfix component}
    \centering{}
    \begin{tabular}{lr}
        \tabletopline{}%
        Postfix component & Number of rules \\
        \tablemiddleline{}%
        \input{build/include-number-of-rules-per-program}
        \tablebottomline{}%
    \end{tabular}
\end{table}

\FloatBarrier{}

\subsection{Rule Ordering For Efficiency}

\label{rule ordering for efficiency}

\parsername{} has \numberOFrules{} rules: the top 10\% recognise
\input{build/include-top-ten-hits}\% of the log lines in the
\numberOFlogFILES{} log files, with the remaining log lines split across
the other 90\% of the rules, as shown in \graphref{rule hits graph}.
Assuming that the distribution of log lines is reasonably consistent over
time, \parsernames{} efficiency should benefit from trying rules that
recognise log lines more frequently before those rules that recognise log
lines less frequently.  To test this hypothesis, three full test runs were
performed with different rule orderings:

\begin{eqlist}

    \item [Optimal]  The most optimal order, according to the hypothesis:
        rules that recognise log lines most often will be tried first.

    \item [Shuffle] This ordering is intended to represent a randomly
        ordered rule set.  The rules will be shuffled once before use and
        will retain that ordering until the parser exits.  Note that the
        ordering will change every time the parser is executed, so 10
        different rule orderings will be generated for each log file in the
        test run.

    \item [Reverse] Hypothetically the worst order: rules that recognise
        log lines most frequently will be tried last.

\end{eqlist}

\Graphref{Parsing time relative to shuffled ordering graph} shows the
parsing times of optimal and reverse orderings relative to shuffled
ordering; the mean relative parsing times for different groupings of log
files are given in \tableref{Parsing time relative to shuffled ordering
table}.  This optimisation provides a mean reduction in parsing time of
\input{build/include-optimal-ordering-parsing-time-reduction-other-logs}\%
with normal log files,
\input{build/include-optimal-ordering-parsing-time-reduction-logs-22-62-68}\%
when a mail loop occurs and the distribution of log lines is unusual.
\Tableref{Parsing time relative to shuffled ordering table} shows that
differences in rule ordering have less effect on parsing time when parsing
log files 22 \& 62--68, because of the different distribution of log lines
in those log files.  A careful examination of \graphref{Parsing time
relative to shuffled ordering graph} shows that, for the first log file,
optimal and reverse orderings perform identically: this is because the hits
field of each rule is zero for the first log file, so optimal and reverse
orderings produce identical rule orderings.  For the first log file,
shuffled ordering is the most efficient of the three, but that is
accidental and cannot be relied upon.

\showgraph{build/graph-hits}{Log lines recognised per rule}{rule hits graph}

\showgraph{build/graph-optimal-and-reverse-vs-shuffle}{Parsing time
relative to shuffled ordering}{Parsing time relative to shuffled ordering
graph}

\showtable{build/include-optimal-and-reverse-vs-shuffle}{Parsing time
relative to shuffled ordering}{Parsing time relative to shuffled ordering
table}

\FloatBarrier{}

\subsection{Comparing Optimal Ordering Against A Perfect Oracle}

\label{perfect rule ordering}

Optimal rule ordering, as described in \sectionref{rule ordering for
efficiency}, is the best rule ordering it is possible to achieve without
having an oracle that magically divines which rule should be used to
recognise each log line.  Such an oracle would give perfect performance,
because only one rule would need to be used to recognise each log line.
\parsername{} can save a list showing which rule recognised each log line,
and use that list to simulate an oracle and improve parsing speed the
\textit{second\/} time a log file is parsed.  This does not provide a
practical benefit, but it does provide a means to evaluate the performance
of optimal rule ordering in comparison to an oracle.

\Graphref{Parsing time of oracle and optimal ordering relative to shuffled
ordering graph} shows the parsing times of the oracle and the optimal
ordering, relative to shuffled ordering; \tableref{Parsing time of oracle
and optimal ordering relative to shuffled ordering table} shows mean and
standard deviation.  As expected, the oracle is more efficient than optimal
ordering, but not by much.  \Graphref{Percentage increase in parsing time
when using optimal ordering instead of oracle graph} shows the percentage
increase in parsing time when using optimal ordering instead of the oracle,
with mean and standard deviation in \tableref{Percentage increase in
parsing time when using optimal ordering instead of oracle table}.

Once again, the difference between the oracle and optimal ordering is at
its lowest when parsing log files resulting from a mail loop (log files 22
\& 62--68), because the mean number of rules tried per log line is lower
(see \graphref{Mean number of rules tried per log line}).  When parsing the
first log file, the performance of optimal ordering relative to the oracle
is much worse than for the remainder of the log files, because for the
first log file the hits field of every rule is zero, so optimal ordering
does not provide any benefit for that log file; the oracle, in contrast, is
flawless for every log file.

Optimal ordering proves to be quite efficient: \tableref{Percentage
increase in parsing time when using optimal ordering instead of oracle
table} shows that optimal ordering is less than
\input{build/include-perfect-best-vs-optimal-mean} slower than parsing
using a magical oracle that divines the correct rule to use for each log
line.


\showgraph{build/graph-perfect-best-and-optimal-vs-shuffled}{Parsing time
of oracle and optimal ordering relative to shuffled ordering}{Parsing time
of oracle and optimal ordering relative to shuffled ordering graph}

\showtable{build/include-perfect-best-and-optimal-vs-shuffle}{Parsing time
of oracle and optimal ordering relative to shuffled ordering}{Parsing time
of oracle and optimal ordering relative to shuffled ordering table}

\showgraph{build/graph-perfect-best-vs-optimal}{Percentage increase in
parsing time when using optimal ordering instead of oracle}{Percentage
increase in parsing time when using optimal ordering instead of oracle
graph}

\showtable{build/include-perfect-best-vs-optimal-stddev}{Percentage
increase in parsing time when using optimal ordering instead of
oracle}{Percentage increase in parsing time when using optimal ordering
instead of oracle table}

\FloatBarrier{}

\subsection{Scalability As The Number Of Rules Rises}

\label{scalability as the number of rules rises}

How any architecture scales as the number of rules increases is important,
but it is particularly important in this architecture because it is
expected that the typical parser will have a large ruleset.  The full
\parsername{} ruleset has \numberOFrules{} rules, whereas the minimum
number of rules required to parse the \numberOFlogFILES{} log files is
\numberOFrulesMINIMUM{}, \numberOFrulesMINIMUMpercentage{} of the full
ruleset.  The full ruleset is larger because \parsername{} is tested with
\numberOFlogFILESall{} log files; testing with more log files increases the
chance of finding bugs in the parser or new complications to be overcome.
The \numberOFlogFILES{} log files were each parsed 10 times using the
minimum ruleset, and the parsing times compared to those generated using
the full ruleset: the percentage parsing time increase when using the full
ruleset instead of the minimal ruleset for optimal, shuffled, and reverse
orderings is shown in \graphref{Percentage parsing time increase when using
the maximum ruleset instead of the minimum ruleset}, with mean and standard
deviation in \tableref{Percentage parsing time increase when using the
maximum ruleset instead of the minimum ruleset table}.

Clearly the increased number of rules causes a noticeable performance
decrease with reverse ordering, and a lesser decrease with shuffled
ordering, whereas optimal ordering shows scant change.  Log files 22 \&
62--68 show much smaller increases in parsing time than other log files do,
because most of the log lines in those log files are produced by Postfix
components with few rules, so removing unnecessary rules has little effect
on the total number of rules used; \tableref{Number of rules per Postfix
component in the maximum and minimum rulesets} shows the number of rules
per Postfix component for each ruleset.  Once again, for the first log
file, optimal and reverse orderings have identical performance, because the
hits field of every rule is zero.

The optimal ordering has a mean increase of just
\input{build/include-full-ruleset-vs-minimum-ruleset-mean} in parsing time
for a \numberOFrulesMAXIMUMpercentage{} increase in the number of rules.
These results show that both the architecture and \parsername{} scale
extremely well as the number of rules increases, and that optimally sorting
the rules is an important optimisation contributing to this scalability.

\showgraph{build/graph-full-ruleset-vs-minimum-ruleset}{Percentage parsing
time increase when using the maximum ruleset instead of the minimum
ruleset}{Percentage parsing time increase when using the maximum ruleset
instead of the minimum ruleset}

\showtable{build/include-full-ruleset-vs-minimum-ruleset}{Percentage
parsing time increase when using the maximum ruleset instead of the minimum
ruleset}{Percentage parsing time increase when using the maximum ruleset
instead of the minimum ruleset table}

\begin{table}[thbp]
    \caption{Number of rules per Postfix component in the maximum and
    minimum rulesets}
    \empty{}\label{Number of rules per Postfix component in the maximum and
    minimum rulesets}
    \centering{}
    \begin{tabular}{lrr}
        \tabletopline{}%
        Postfix component & Maximum ruleset & Minimum ruleset \\
        \tablemiddleline{}%
        \input{build/include-number-of-rules-per-program-minimum-ruleset}
        \tablebottomline{}%
    \end{tabular}
\end{table}

\FloatBarrier{}

\subsection{Caching Compiled Regexes}

\label{Caching compiled regexes}

Before the Perl interpreter attempts to match a regex against a piece of
text, the regex is compiled into an internal representation and optimised
to improve the speed of matching.  This compilation and optimisation takes
CPU time: in many cases it takes far more CPU time than the actual
matching.  If the interpreter is certain that a regex will not change while
the program is running, it will automatically cache the results of
compiling and optimising the regex for later use.  The results of compiling
a dynamically generated regex can be cached and used in preference to the
original regex, but it is the responsibility of the programmer to do this;
\parsername{} does this with every rule's regex when the rules are loaded
from the database.

\Graphref{Increase in parsing time when not caching compiled regexes graph}
shows the effect that not caching compiled regexes has on parser
performance, with mean and standard deviation in \tableref{Increase in
parsing time when not caching compiled regexes table}.  For typical log
files, the mean increase in parsing time when not caching compiled regexes
is \input{build/include-cached-regexes-vs-discarded-regexes-mean}.  Caching
compiled regexes is probably the single most effective optimisation
possible in \parsername{}, and was quite simple to implement.  As with
previous optimisations, log files 22 \& 62--68 do not suffer such a large
increase in parsing time when not caching compiled regexes; this is
because, on average, fewer regexes are compiled per log line for those log
files.  The increase in parsing time when parsing the first log file is
much greater than for the other log files; again, this is because every
rule's hits field is zero, so optimal ordering is less efficient than
usual, and the mean number of rules tried for each log line will be higher
than usual.

\showgraph{build/graph-cached-regexes-vs-discarded-regexes}{Increase in
parsing time when not caching compiled regexes}{Increase in parsing time
when not caching compiled regexes graph}

\showtable{build/include-cached-regexes-vs-discarded-regexes}{Increase in
parsing time when not caching compiled regexes}{Increase in parsing time
when not caching compiled regexes table}

\FloatBarrier{}

\subsection{Where Is The Time Spent: Recognising Or Processing Log Lines?}

\label{recognising vs processing}

The optimisations described in this chapter have optimised the process of
recognising log lines, but have not optimised actions at all.  Optimisation
efforts have concentrated on recognition of log lines for two reasons:

\begin{enumerate}

    \item With the optimisations described in this chapter enabled,
        recognising log lines still dominates the execution time of the
        parser.  \Graphref{Percentage of parsing time spent recognising log
        lines graph} shows the percentage of parsing time spent recognising
        log lines for each of the \numberOFlogFILES{} log files, with mean
        and standard deviation shown in \tableref{Percentage of parsing
        time spent recognising log lines table}; for normal log files,
        \input{build/include-percentage-time-spent-recognising-log-lines-mean}
        of parsing time is spent recognising log lines.  Optimal ordering
        reduces parsing time by
        \input{build/include-optimal-ordering-parsing-time-reduction-other-logs}\%
        relative to shuffled ordering; processing recognised log lines
        occupies
        \input{build/include-percentage-time-spent-processing-log-lines-mean}
        of parsing time, less if using shuffled ordering, so optimising
        actions could not possibly provide as big a performance increase as
        optimally ordering rules.  Similarly, caching compiled regexes
        provides an
        \input{build/include-cached-regexes-vs-discarded-regexes-mean-reduction}
        reduction in parsing time; not invoking actions at all reduces the
        most optimised parsing time by less than half that.  If processing
        of recognised log lines was optimised to 1\% of its original
        parsing time, it would be slightly better than optimal ordering,
        but it would be vastly harder to implement; optimising actions
        would not provide enough reduction in parsing time to justify the
        amount of effort required.

    \item The process of recognising log lines is not parser-specific
        (excluding evaluation of rule conditions), so the optimisations
        described in this chapter are applicable to all parsers based on
        this architecture.  Actions are parser-specific, so it is unlikely
        that any optimisations made to actions would be portable to other
        parsers.

\end{enumerate}

Individual actions or the framework could and have been optimised, but
plenty of existing literature is available on the topic of optimising
programs, so the subject will not be dealt with here.

\showgraph{build/graph-percentage-time-spent-recognising-log-lines}{Percentage
of parsing time spent recognising log lines}{Percentage of parsing time
spent recognising log lines graph}

\showtable{build/include-percentage-time-spent-recognising-log-lines-table}{Percentage
of parsing time spent recognising log lines}{Percentage of parsing time
spent recognising log lines table}

\FloatBarrier{}

\section{Coverage}

\label{parsing coverage}

The discussion of \parsernames{} coverage of Postfix log files is separated
into two parts: log lines correctly recognised, and mail delivery attempts
correctly understood --- the former is a requirement for the latter to be
achieved.  Correctly understanding and reconstructing every mail delivery
attempt, whether it was successful or not, is important so that the
information in the database is correct and complete.  Improving the
proportion of log lines correctly recognised is the less difficult of the
two, because usually it just requires new rules to be written or existing
rules to be changed.  Improving the proportion of correctly understood and
reconstructed mail delivery attempts is more difficult and intrusive,
because it requires adding or changing actions, and it can be much harder
to realise that a deficiency exists and needs to be addressed.

\subsection{Log Lines Correctly Recognised}

\label{log-lines-covered}

Parsing a log line is a three stage process:

\begin{enumerate}

    \squeezeitems{}

    \item Skip the log line if the ruleset does not contain any rules for
        the Postfix component that produced the log line.

    \item Try each rule until a recognising rule is found; if the log line
        is not recognised, issue a warning and move on to the next log
        line.

    \item Invoke the action specified by the recognising rule.

\end{enumerate}

Correctly recognising all log lines requires that each Postfix component
whose log lines are of interest must have at least one rule, or its log
lines will be silently skipped; in the extreme case of an empty ruleset the
parser would skip every log line.  \parsername{} skips those log lines
because there may be any number of log lines from other programs
intermingled in the log file, and some Postfix components that do not
produce any log lines of interest.  There must be a rule to recognise each
log line variant produced by each Postfix component; if a log line is not
recognised the parser will issue a warning, to inform the user that they
need to extend their ruleset.  \parsername{} does not parse log lines from
non-Postfix programs, e.g.\ Amavisd-new or SpamAssassin; it could easily be
extended to do so, if a method could be developed to correctly associate
such log lines with existing state table entries.  Each rule's regex should
be as specific and precise as possible, to ensure accurate parsing: a rule
with a regex that matches zero or more of any character will recognise
every log line, but not in a meaningful way.  Most log lines contain fixed
strings, so this is not a problem in practice.

Full coverage of log lines can be achieved without undue effort, yet it is
hard to maintain.  Maintaining full coverage is hard because log lines
change over time, e.g.\ administrators add restrictions with custom
messages, \acronym{DNSBL} messages change, or major releases of Postfix
change log lines (usually adding more information).  Warnings are issued
for any log lines that are not recognised; no warnings are issued for
unrecognised log lines while parsing the \numberOFlogFILES{} log files, so
it can be safely concluded that zero false negatives arise.  False
positives are harder to quantify, short of examining each of the 60,721,709
log lines and ensuring that the correct rule recognised it.  However, a
random sample of 6039 log lines was parsed, and the results manually
verified by inspection to ensure that the correct rule recognised each log
line.  The sample was generated by running the command: \texttt{perl -n -e
\singlequote{}print if (rand 1 < 0.0001)\singlequote{} LOG\_FILES} to
randomly extract roughly one log line in every 10,000 (it actually
extracted 0.00994\% instead of 0.01\%).  Each log line was examined and the
correct rule identified from the \numberOFrules{} rules in the database;
the correct rule was then compared to the rule that recognised the log line
when parsing.  The sample results contained zero false positives, and this
check has been automated to ensure continued accuracy.  Based on this, the
author is confident that zero false positives occur when parsing the
\numberOFlogFILES{} log files.  On initial appearances, exercising only 36
rules from a total of \numberOFrules{} when parsing 6039 log lines seems
low, but after examining \graphref{rule hits graph} it becomes apparent
that parsing using such a low number of rules is to be expected.  The
reader should also bear in mind that even when parsing all
\numberOFlogFILES{} log files, only \numberOFrulesMINIMUM{} are used,
because some of the rules are for recognising log lines that only appear in
other log files.

\subsection{Mail Delivery Attempts Correctly Understood And Reconstructed}

\label{mails-covered}

The proportion of mail delivery attempts that are correctly understood and
reconstructed is much more difficult to determine accurately than the
proportion of log lines that are correctly recognised.  The parser can dump
its state tables in a human readable form; examining those tables with
reference to the log files is the best way to detect mails that were not
handled properly (many of the complications discussed in
\sectionref{complications} were detected in this way).  \parsername{}
issues warnings when it detects any errors or discrepancies, alerting the
user to the problem, e.g.\ when a queueid is reused but the previous mail
remains in the state tables, when a queueid or \acronym{pid} is not found
in the state tables, or when a mail does not include sufficient data to
satisfy the database schema.  The parser should produce few or no warnings
during parsing, and when finished parsing the state tables should only
contain entries for mails that have log lines in subsequent log files.
There will often be warnings about a missing queueid or \acronym{pid} in
the first few thousand log lines, because the earlier log lines for those
connections or mails are in a previous log file; loading saved state tables
from the previous log file will solve this problem.

The data used in this chapter is generated by parsing \numberOFlogFILES{}
log files.  5 are warnings produced, but because \parsername{} errs on the
side of producing more warnings rather than fewer, those 5 warnings
represent 3 instances of 1 problem: 3 connections that started before the
first log file, so their initial log lines are missing, leading to warnings
when their remaining log lines are parsed.  None of the warnings are false
positives.

The state tables will contain entries for mails not yet delivered when the
parser finishes execution.  Ideally, that is all they will contain, though
they may also contain mails whose initial log lines are not contained in
the log files.  Any other entries in the state tables are evidence of
either a failure in parsing, or an aberration in the log files.  After
parsing the \numberOFlogFILES{} log files, the state tables contain 18
entries, breaking down into:

\begin{itemize}

    \squeezeitems{}

    \item 1 connection that started only seconds before the log files
        ended and had not yet been fully transferred from client to server.

    \item 1 mail that had been accepted only seconds before the log files
        ended and had not yet been delivered.

    \item 9 mails whose initial log lines were not present in the log
        files.  These mails did not produce warnings because they resemble
        child mails waiting to be tracked with a parent; see
        \sectionref{Re-injected mails} for details.

    \item 7 mails that had yet to be delivered because of repeated
        failures.

\end{itemize}

None of the mails in the state tables should not be present, thus it can be
concluded that zero false negatives occur when parsing the
\numberOFlogFILES{} log files.  Once again, determining the false positive
rate is much harder, as manually checking the results of parsing 13,850,793
connections and mails accepted, rejected, bounced, or delivered is
infeasible.  Considerable evidence exists that the false positive rate is
extremely low:

\begin{itemize}

    \item The parser is quite verbose when complaining about known
        problems, e.g.\ if a mail is missing required data, and no such
        warnings are produced during the test runs.

    \item Queueids and \glspl{pid} naturally identify log lines belonging
        to one mail or connection respectively; it is extremely unlikely
        that a log line would be associated with the wrong connection.

    \item When dealing with the complications described in
        \sectionref{complications}, the solutions are as specific and
        restrictive as possible, with the goal of minimising the number of
        false positives.  In addition, the solution to the complication
        described in \sectionref{out of order log lines} imposes conditions
        that each reconstructed mail must comply with to be acceptable.

    \item Every effort has been made to make \parsername{} as precise,
        demanding, and particular as possible.

\end{itemize}

%\verb!perl -Mstrict -Mwarnings -MList::Util=shuffle -e! \newline{}
%\verb!     '@ARGV = [shuffle(@ARGV)]->[0];!             \newline{}
%\verb!      while (<>) {!                               \newline{}
%\verb!          if (rand 1 < 0.0001) {!                 \newline{}
%\verb!              foreach my $i (1 .!.  6000) {!        \newline{}
%\verb!                  print scalar <>;!               \newline{}
%\verb!              }!                                  \newline{}
%\verb!              exit;!                              \newline{}
%\verb!          }!                                      \newline{}
%\verb!      }' LOG_FILES!

Verifying by inspection that the parser correctly deals with all 60,721,709
log lines in the \numberOFlogFILES{} log files is infeasible, but verifying
the parsing of a sample from those log lines is a tractable, if extremely
time consuming, task.  A sample of log lines was obtained by randomly
selecting a log file, and then randomly selecting a block of 6000
contiguous log lines from it (0.00988\% of the total number of log lines).
It is important that the log lines are contiguous, so that all log lines
are present for as many of the mail delivery attempts contained in the
block as possible.  This log segment was parsed with all debugging options
enabled, resulting in 167,448 lines of output.\footnote{A mean of 27.908
lines of output per log line; each connection has 30 debugging lines, plus
21 debugging lines per result.  Connections which have been cloned will
have the cloned connection in their debugging output, plus another 33
debugging lines.  Those numbers are approximate, and may vary $\pm{}$ 2.
An approximate linear relationship between the number of log lines and
debugging lines is: $33(connections) + 30(accepted~~mails) + 21(results)$.}
All 167,448 lines were examined in conjunction with the log file segment
and a dump of the resulting database, verifying that for each of the log
lines \parsername{} used the correct rule and invoked the correct action,
which in turn produced the correct result, and the correct data was
inserted in the database.  The log file segment produced 4 warnings, 10
mails remaining in the state tables, 1625 mail delivery attempts correctly
entered in the database, zero false positives, and zero false negatives.

Given the evidence detailed above, the author is confident that the false
positive rate when reconstructing a mail delivery attempt from the
\numberOFlogFILES{} log files is exceedingly low, if not zero.

\section{Summary}

This chapter evaluated \parsername{} on two criteria: efficiency, and
coverage of Postfix log files.  The former began by describing the mail
server the log files were taken from, the computer used to generate the
statistics in this chapter, and how the parser scales as the size of log
files increases, including why performance is better on the larger log
files.  The framework optimises the order in which rules are used when
trying to recognise each log line, and the effect that optimisation has on
parsing time is explored; this is followed by the effect that adding more
rules to the ruleset has on parser performance.  The simplest and most
effective of the optimisations, caching compiled regexes, is described
next, and the efficiency evaluation concludes with an examination of where
parsing time is spent: recognising log lines or processing them?

Coverage of Postfix log files is divided into two topics in this chapter:
log lines correctly recognised, and mail delivery attempts correctly
understood and reconstructed.  The former is initially more important,
because the parser must correctly recognise every log line if it is to be
complete, but subsequently the latter takes precedence because correctly
reconstructing the journey a mail delivery attempt takes through Postfix is
the aim of the parser.  Increasing the proportion of log lines correctly
recognised is relatively simple and non-intrusive: adding new rules or
modifying existing rules is very easy because of the separation of rules,
actions, and framework.  Improving the understanding and reconstruction of
mail delivery attempts is harder, because Postfix's behaviour must be
analysed and figured out, and the new behaviour integrated into the actions
without breaking the existing parsing.  Detecting a deficiency in the
parser is also significantly harder, because the parser will warn about
unrecognised log lines, whereas discovering a flaw in the parser requires
careful study of any warnings produced and the entries remaining in the
state table.  Rectifying a flaw in the parser requires a deep understanding
of both the parser and Postfix's log files, investigative work to determine
the cause of the deficiency, further examination of the log files to aid in
developing a solution, and finally implementation, integration, and testing
of the solution.

This chapter shows that it is possible to balance the conflicting goals of
efficient and accurate parsing, and that one does not have to be sacrificed
to achieve the other.
