\chapter{Results And Evaluation}

\label{Results}

\renewcommand{\figurename}{Graph}

% Reduce the gap between columns in tables.
\addtolength{\tabcolsep}{-1pt}

XXX NEED AN INTRODUCTION\@.

\section{Parser Efficiency}

\label{parser efficiency}

Parsing efficiency is an obvious concern when the parser routinely needs to
parse large log files.  The server that generated the log files used in
testing this parser accepts approximately 10,000 mails for 700 users per
day; \graphref{Mails received per day} and \tableref{Number of mails
received per day: statistics} show that, as expected, far more mails are
received on weekdays than at weekends.  The mail server in question is a
production mail server handling mail for a university department; the
benefit of using this server is that its log files exhibit the
idiosyncrasies and peculiarities a mail server in the wild must deal with,
but the downside is that significantly altering the configuration to
accommodate this project is not an option.  Median log file size is 50MB,
containing 285,000 log lines; large scale mail servers would have much
larger log files.  Note that \graphref{Mails received per day} and
\tableref{Number of mails received per day: statistics} show the number of
mails received by \acronym{SMTP} only, and the mail loops noticeable in
later graphs do not contribute to these figures.

\showgraph{build/graph-mails-received}{Number of mails received via SMTP
per day}{Mails received per day}

\showtable{build/include-mails-received-table}{Number of mails received via
SMTP per day}{Number of mails received per day: statistics}

\begin{table}[thbp]
    \caption{Details of the computer used to generate statistics}
    \empty{}\label{Details of the computer used to generate statistics}
    \centering{}
    \begin{tabular}[]{ll}
        \tabletopline{}%
        Component  & Component in use                                   \\
        \tablemiddleline{}%
        CPU         & One dual core 2.40GHz Intel\textregistered{}
                        Core\texttrademark{}2 CPU,                      \\
                    & with 32KB L1 cache and 4MB L2 cache.              \\
        RAM         & 2GB 667 MHz DDR RAM\@.                            \\
        Hard disk   & One Seagate Barracuda 7200 RPM 250GB SATA disk.   \\
        \tablebottomline{}%
    \end{tabular}
\end{table}

When generating the timing data used in this section, \numberOFlogFILES{}
log files (totaling 10.08 GB, \numberOFlogLINEShuman{} log lines) were each
parsed 10 times, and the times from the 10 runs averaged.  The computer
used for test runs was a Dell Optiplex 745, shown in \tableref{Details of
the computer used to generate statistics}; it was dedicated to the task of
gathering statistics from test runs, and was not used for any other purpose
while test runs were ongoing.  Saving results to the database was disabled
for the test runs, because that dominates the run time of the parser, and
the tests are aimed at measuring the speed of \parsername{} rather than the
speed of the database and the disks the database is stored on.  Parsing all
\numberOFlogFILES{} log files in one run without saving results to the
database took \input{build/include-full-run-duration}, mean throughput was
\input{build/include-full-run-throughput}; median throughput when parsing
files separately was \input{build/include-median-throughput-MB}
(\input{build/include-median-throughput-log-lines}) parsed per minute.  In
contrast, when saving results to the database, parsing all
\numberOFlogFILES{} log files took
\input{build/include-insert-results-duration}, with mean throughput of
\input{build/include-insert-results-throughput}.  Parsing makes up only
\input{build/include-skip-inserting-as-percentage-of-inserting} of the
execution time when saving results to the database, so a 50\% improvement
in the speed of parsing would result in only a
\input{build/include-skip-inserting-with-50-percent-improvement--as-percentage-of-inserting}
improvement in speed overall.

\subsection{Architecture Scalability: Input Size}

An important property of a parser is how parsing time scales relative to
input size: does it scale linearly, polynomially, or exponentially?
\Graphref{parsing time vs file size vs number of log lines graph} shows the
parsing time in seconds, file size in MB, and number of log lines in tens
of thousands, for each of the \numberOFlogFILES{} log files.  The three
lines run roughly in parallel, giving the impression that the algorithm
scales linearly with input size.  This impression is borne out by
\graphref{parsing time vs file size vs number of log lines factor}, which
plots both the ratio of file size vs parsing time, and the ratio of number
of log lines vs parsing time (higher is better in both cases);
\tableref{parsing time vs file size vs number of log lines factor table}
shows the same ratios for different groups of log files.  The ratios are
quite tightly banded, showing that the algorithm scales linearly; the
ratios increase (i.e.\ improve) for log files 22 \& 62--68, despite their
larger than average size.  Both groups of log files are much larger than
usual due to a mail loop caused by a user who set up mail forwarding
incorrectly, resulting in a very different distribution of log lines:
normally most log lines are generated by mail delivery attempts from other
hosts, but when the mail loops occurred most of the log lines resulted from
failed delivery of mail generated on the server itself.  The Postfix
components that generated most of the log lines during the mail loop have
fewer associated rules than the Postfix components whose log lines normally
make up the bulk of each log file, so the average number of rules tried per
log line reduces and so does the average parsing time per log line.
\Graphref{Number of rules per Postfix component} shows the number of rules
per Postfix component, \graphref{Average number of rules tried per log
line} shows the drop in the average number of rules tried per log line for
log files containing a mail loop, and \graphref{Average number of rules
tried per log line for each Postfix component} shows the average number of
rules tried per log line for each Postfix component.

\showtable{build/include-file-size-and-number-of-log-lines-vs-parsing-time}{Ratio
of file size and number of log lines to parsing time}{parsing time vs file
size vs number of log lines factor table}

\showgraph{build/graph-input-size-vs-parsing-time}{Parsing time, file size,
and number of log lines for \numberOFlogFILES{} log files}{parsing time vs
file size vs number of log lines graph}

\showgraph{build/graph-input-size-vs-parsing-time-ratio}{Ratio of file size
and number of log lines to parsing time}{parsing time vs file size vs
number of log lines factor}

\showgraph{build/graph-average-number-of-rules-tried-per-log-line}{Average
number of rules tried per log line}{Average number of rules tried per log
line}

\showgraph{build/graph-average-number-of-rules-tried-per-program}{Average
number of rules tried per log line for each Postfix component}{Average
number of rules tried per log line for each Postfix component}

\begin{table}[thbp]
    \caption{Number of rules per Postfix component}
    \empty{}\label{Number of rules per Postfix component}
    \centering{}
    \begin{tabular}{lr}
        \tabletopline{}%
        Postfix component & Number of rules \\
        \tablemiddleline{}%
        \input{build/include-number-of-rules-per-program}
        \tablebottomline{}%
    \end{tabular}
\end{table}

\FloatBarrier{}

\subsection{Rule Ordering For Efficiency}

\label{rule ordering for efficiency}

\parsername{} has \numberOFrules{} rules: the top 10\% recognise
\input{build/include-top-ten-hits}\% of the log lines, with the remaining
log lines split across the other 90\% of the rules (as shown in
\graphref{rule hits graph}).  Assuming that the distribution of log lines
is reasonably consistent over time, \parsernames{} efficiency should
benefit from trying rules that recognise log lines more frequently before
those rules that recognise log lines less frequently.  To test this
hypothesis, three full test runs were performed with different rule
orderings:

\begin{eqlist}

    \item [Optimal]  The most optimal order, according to the hypothesis:
        rules that recognise log lines most often will be tried first.

    \item [Shuffle] This ordering is intended to represent a randomly
        ordered rule set.  The rules will be shuffled once before use and
        will retain that ordering until the parser exits.  Note that the
        ordering will change every time the parser is executed, so 10
        different rule orderings will be generated for each log file in the
        test run.

    \item [Reverse] Hypothetically the worst order: the rules that
        recognise log lines most frequently will be tried last.

\end{eqlist}

\Graphref{Parsing time relative to shuffled ordering graph} shows the
parsing times of optimal and reverse orderings relative to shuffled
ordering; the mean relative parsing times for different groupings of log
files are given in \tableref{Parsing time relative to shuffled ordering
table}.  This optimisation provides a mean reduction in parsing time of
\input{build/include-optimal-ordering-parsing-time-reduction-other-logs}\%
with normal log files,
\input{build/include-optimal-ordering-parsing-time-reduction-logs-22-62-68}\%
when a mail loop occurs and the distribution of log lines is unusual.
\Tableref{Parsing time relative to shuffled ordering table} shows that
differences in rule ordering have less effect on parsing time when parsing
log files 22 \& 62--68, due to the different distribution of log lines in
those log files.  A careful examination of \graphref{Parsing time relative
to shuffled ordering graph} shows that, for the first log file, optimal and
reverse orderings perform identically: this is because the hits field of
each rule is zero for the first log file, so optimal and reverse orderings
are identical.  For the first log file, shuffled ordering is most
efficient, but that is accidental and cannot be relied upon.

\showgraph{build/graph-hits}{Log lines recognised per rule}{rule hits graph}

\showgraph{build/graph-optimal-and-reverse-vs-shuffle}{Parsing time
relative to shuffled ordering}{Parsing time relative to shuffled ordering
graph}

\showtable{build/include-optimal-and-reverse-vs-shuffle}{Parsing time
relative to shuffled ordering}{Parsing time relative to shuffled ordering
table}

\FloatBarrier{}

\subsection{Comparing Optimal Ordering Against A Perfect Oracle}

\label{perfect rule ordering}

Optimal rule ordering, as described in \sectionref{rule ordering for
efficiency}, is the best rule ordering it is possible to achieve without
having an oracle that magically divines which rule should be used to
recognise each log line.  Such an oracle would give perfect performance,
because only one rule needs to be used to recognise each log line.
\parsername{} can save a list showing which rule recognised each log line,
and use that list to simulate an oracle and improve parsing speed the
second time a log file is parsed.  This does not provide a practical
benefit, but it does provide a means to evaluate the performance of optimal
rule ordering in comparison to an oracle.

\Graphref{Parsing time of perfect best oracle and optimal ordering relative
to shuffled ordering graph} shows the parsing times of the perfect best
oracle and the optimal ordering, relative to shuffled ordering;
\tableref{Parsing time of perfect best oracle and optimal ordering relative
to shuffled ordering table} shows mean and standard deviation.  As
expected, the perfect best oracle is more efficient than optimal ordering.
\Graphref{Percentage increase in parsing time when using optimal
ordering instead of perfect best oracle graph} shows the percentage
increase in parsing time when using optimal ordering instead of the perfect
best oracle; mean and standard deviation are given for different groups of
log files in \tableref{Percentage increase in parsing time when using
optimal ordering instead of perfect best oracle table}.

Once again, the difference between the perfect best oracle and optimal
ordering is at its lowest when parsing log files resulting from a mail loop
(log files 22 \& 62--68), because the average number of rules tried per log
line is lower (see \graphref{Average number of rules tried per log line}).
The performance of optimal ordering when parsing the first log file is much
worse than the remainder of the log files, because the hits field of every
rule is zero, so optimal ordering does not provide any benefit for that log
file.

Optimal ordering proves to be quite efficient: \tableref{Percentage
increase in parsing time when using optimal ordering instead of perfect
best oracle table} shows that optimal ordering is less than
\input{build/include-perfect-best-vs-optimal-mean} slower than parsing
using a magical oracle that divines the correct rule to use for each log
line.


\showgraph{build/graph-perfect-best-and-optimal-vs-shuffled}{Parsing time
of perfect best oracle and optimal ordering relative to shuffled
ordering}{Parsing time of perfect best oracle and optimal ordering relative
to shuffled ordering graph}

\showtable{build/include-perfect-best-and-optimal-vs-shuffle}{Parsing time
of perfect best oracle and optimal ordering relative to shuffled
ordering}{Parsing time of perfect best oracle and optimal ordering relative
to shuffled ordering table}

\showgraph{build/graph-perfect-best-vs-optimal}{Percentage increase in
parsing time when using optimal ordering instead of perfect best
oracle}{Percentage increase in parsing time when using optimal ordering
instead of perfect best oracle graph}

\showtable{build/include-perfect-best-vs-optimal-stddev}{Percentage
increase in parsing time when using optimal ordering instead of perfect
best oracle}{Percentage increase in parsing time when using optimal
ordering instead of perfect best oracle table}

\FloatBarrier{}

\subsection{Scalability When The Number Of Rules Rises}

\label{scalability as the number of rules rises}

How any architecture scales as the number of rules increases is important,
but it is particularly important in this architecture because it is
expected that the typical parser will have a large number of rules.  There
are \numberOFrules{} rules in the full \parsername{} ruleset, whereas the
minimum number of rules required to parse the \numberOFlogFILES{} log files
used when generating the data in this chapter is \numberOFrulesMINIMUM{}
rules, \numberOFrulesMINIMUMpercentage{} of the full ruleset.  The full
ruleset is larger because \parsername{} is tested with
\numberOFlogFILESall{} log files; testing with more log files increases the
chance of finding bugs in the parser or new complications to be overcome.
The \numberOFlogFILES{} log files were each parsed 10 times using the
minimum ruleset, and the parsing times compared to those generated using
the full ruleset: the percentage parsing time increase when using the full
ruleset instead of the minimal ruleset for optimal, shuffled and reverse
orderings is shown in \graphref{Percentage parsing time increase of maximum
ruleset over minimum ruleset}; mean and standard deviation are shown in
\tableref{Percentage parsing time increase of maximum ruleset over minimum
ruleset table}.

Clearly the increased number of rules has a noticeable performance impact
with reverse ordering, and a lesser impact with shuffled ordering, whereas
optimal ordering shows very little change.  Log files 22 \& 62--68 show
much smaller increases than other log files do, because the majority of log
lines in those log files are produced by Postfix components with a small
number of rules, so removing unnecessary rules has little effect on the
total number of rules used; \tableref{Number of rules per Postfix component
in the maximum and minimum rulesets} shows the number of rules per Postfix
component for each ruleset.  Once again, for the first log file, optimal
and reverse orderings have identical performance, because the hits field of
every rule is zero.

The optimal ordering shows a mean increase of just
\input{build/include-full-ruleset-vs-minimum-ruleset-mean} in parsing time
for a \numberOFrulesMAXIMUMpercentage{} increase in the number of rules.
These results show that the architecture scales very well as the number of
rules increases, and that optimally sorting the rules is an important
optimisation contributing to this scalability.

\showgraph{build/graph-full-ruleset-vs-minimum-ruleset}{Percentage parsing
time increase of maximum ruleset over minimum ruleset}{Percentage parsing
time increase of maximum ruleset over minimum ruleset}

\showtable{build/include-full-ruleset-vs-minimum-ruleset}{Percentage parsing
time increase of maximum ruleset over minimum ruleset}{Percentage parsing
time increase of maximum ruleset over minimum ruleset table}

\begin{table}[thbp]
    \caption{Number of rules per Postfix component in the maximum and
    minimum rulesets}
    \empty{}\label{Number of rules per Postfix component in the maximum and
    minimum rulesets}
    \centering{}
    \begin{tabular}{lrr}
        \tabletopline{}%
        Postfix component & Maximum ruleset & Minimum ruleset \\
        \tablemiddleline{}%
        \input{build/include-number-of-rules-per-program-minimum-ruleset}
        \tablebottomline{}%
    \end{tabular}
\end{table}

\FloatBarrier{}

\subsection{Caching Compiled Regexes}

\label{Caching compiled regexes}

Before the Perl interpreter attempts to match a regex against a piece of
text, the regex is compiled into an internal representation and optimised
to improve the speed of matching.  This compilation and optimisation takes
CPU time: in many cases it takes far more CPU time than the actual
matching.  If the interpreter is certain that a regex will not change while
the program is running, it will automatically cache the results of
compiling and optimising the regex for later use.  The results of compiling
a dynamically generated regex can be cached and used in preference to the
original regex, but it is the responsibility of the programmer to do this;
\parsername{} does this with every rule's regex when the rules are loaded
from the database.

\Graphref{Increase in parsing time when not caching compiled regexes graph}
shows the impact that not caching compiled regexes has on parser
performance, with mean and standard deviation in \tableref{Increase in
parsing time when not caching compiled regexes table}.  For typical log
files, the mean increase in parsing time when not caching compiled regexes
is \input{build/include-cached-regexes-vs-discarded-regexes-mean}.  Caching
compiled regexes is probably the single most effective optimisation
possible in \parsername{}, and was quite simple to implement.  As with
previous optimisations, log files 22 \& 62--68 do not suffer such a large
increase in parsing time when not caching compiled regexes; this is
because, on average, fewer regexes are compiled per log line for those log
files.  The increase in parsing time when parsing the first log file is
much greater than for the other log files; again, this is because every
rule's hits field is zero, so optimal ordering is less efficient than
usual, and the average number of rules tried for each log line will be
higher than usual.

\showgraph{build/graph-cached-regexes-vs-discarded-regexes}{Increase in
parsing time when not caching compiled regexes}{Increase in parsing time
when not caching compiled regexes graph}

\showtable{build/include-cached-regexes-vs-discarded-regexes}{Increase in
parsing time when not caching compiled regexes}{Increase in parsing time
when not caching compiled regexes table}

\FloatBarrier{}

\subsection{Where Is The Time Spent: Recognising Or Processing Log Lines?}

\label{recognising vs processing}

XXX COMMENT THAT THE OPTIMISATION TIME HAS BEEN SPENT ON RULES, AND THAT IS
GOOD BECAUSE THAT IS WHERE THE MAJORITY OF EXECUTION TIME IS SPENT\@.
ALSO, RULE OPTIMISATION IS GENERIC, WHEREAS ACTION OPTIMISATION IS
PARSER-SPECIFIC\@.

XXX EXAMINE THE TIME SPENT RECOGNISING LOG LINES VS THE TIME SPENT
PROCESSING THEM\@.

\showgraph{build/graph-percentage-time-spent-recognising-log-lines}{XXX}{XXX}

\FloatBarrier{}

\subsection{Summary}

XXX REWRITE WHEN THIS CHAPTER IS FINISHED\@.

This section began with parser scalability, showing the linear relationship
between parsing time and input size.  Demonstrates the effect of rule
ordering on parsing time, and the unexpected consequences of specific
inputs.  The necessity of caching compiled regexes is attested to by the
third group of graphs, where the difference between caching and not caching
is staggering.  The penultimate section contains a breakdown of the rule
hits accumulated during a single test run of the parser.  Miscellaneous
graphs expected to be useful are collected in the final section.

\section{Coverage}

XXX I THINK THIS SECTION NEEDS SUBSTANTIAL WORK, IF NOT A RE-WRITE\@; START
WITH THE PAPER'S CONTENT\@.

\label{parsing coverage}

The discussion of the parser's coverage of Postfix log files is separated
into two parts: log lines covered and mails covered.  The first is
important because the parser should handle all (relevant) log lines it is
given; the second is equally important because the parser must properly
deal with every mail if it is to be useful.  Improving the former is
less intrusive, as it just requires new rules to be written; improving the
latter is much more intrusive as it requires changes to the parser
algorithm, and it can also be much harder to notice a deficiency.

\newpage{} % XXX

\subsection{Log Lines Covered}

\label{log-lines-covered}

Parsing a log line is a three stage process:

\begin{enumerate}

    \item Check if there are any rules for the Postfix component that
        produced the log line; if not then skip the log line.

    \item Try each rule until a recognising rule is found; if the log line
        is not recognised, issue a warning and move to the next log line.

    \item Execute the action specified by the rule.

\end{enumerate}

Full coverage of log lines requires the following:

\begin{enumerate}

    \item Each Postfix component whose log lines are of interest must have
        at least one rule or its log lines will be silently skipped; in the
        extreme case of zero rules the parser would happily skip every log
        line.  There may be any number of log lines from other programs
        intermingled in the log file, and there are some Postfix programs
        that do not produce any log lines of interest.

    \item There must be a rule to recognise each different log line
        produced by each program; if a log line is not successfully
        recognised the parser will issue a warning.  Rules should be as
        specific and tightly bound as possible to ensure accurate
        parsing:\footnote{A rule with a regex that matches zero or more of
        any character will successfully recognise every log line, but not
        in a meaningful way.} most log lines contain fixed strings and have
        a rigid pattern, so this is not a problem.

    \item The appropriate action to take --- discussed in
        \sectionref{mails-covered}.

\end{enumerate}

Full coverage of log lines is easy to achieve yet hard to maintain.  It is
easy to achieve full coverage for a limited set of log files (at the time
of writing the parser is tested with \numberOFrules{} rules, fully parsing
\numberOFlogFILESall{} contiguous log files from Postfix 2.2 through to
Postfix 2.5), and new rules are easy to add.  Maintaining full coverage is
hard because other servers have different restrictions with custom
messages, \acronym{DNSBL} messages change over time, major releases of
Postfix change warning messages (usually adding more information), etc.,\
so over time the log lines drift and change.  \Graphref{rule hits graph}
shows the number of hits for each rule over all \numberOFlogFILES{} log
files.  The number of hits per rule is quite unevenly spread, resembling a
Power Law distribution --- it is obvious that a small number of rules
recognise the vast majority of the log lines, and more than half the rules
recognise fewer than 100 log lines.

Warnings are issued for any log lines that are not parsed; no warnings are
issued for unparsed log lines while testing with the \numberOFlogFILES{}
test log files, so it can be safely concluded that there are zero false
negatives.  False positives are harder to quantify: short of examining each
of the 60,721,709 log lines and determining which regex parsed it, there
is no way to be sure that every line was parsed by the correct regex,
making it impossible to quantify the false positive rate; however a random
sample of 6,039 log lines was parsed and the results checked manually to
ensure that the correct regex parsed each log line.\footnote{Each log
line was examined and the correct regex identified from the
\numberOFrules{} rules in the database; the correct regex was then
compared to the regex that was used by the parser.}  The sample was
generated by running the following command:

\verb!    perl -e 'print if (rand 1 < 0.0001)' -n LOG_FILES!

\noindent{}to randomly extract roughly one line in every 10,000.  Although
on initial appearances exercising only 36 rules (from a total of
\numberOFrules{}) when parsing 6039 log lines seems quite low, after
examining \graphref{rule hits graph} it becomes apparent that such
a low hit rate is to be expected; the reader should also bear in mind that
even when parsing all \numberOFlogFILES{} log files not all the rules are
exercised (some of the rules are for parsing log lines that only appear in
other log files).

\newpage{} % XXX

\subsection{Mails Covered}

\label{mails-covered}

Coverage of mails is much more difficult to determine accurately than
coverage of log lines.  The parser can dump its state tables in a human
readable form; examining these tables with reference to the log files is
the best way to detect mails that were not handled properly (many of the
complications discussed in \sectionref{complications} were detected in this
way).  The parser issues warnings when it detects any errors, some of which
may alert the user to a problem, e.g.\ when a queueid is reused before the
previous mail is fully dealt with, when a queueid or \acronym{pid} is not
found in the state tables,\footnote{There will often be warnings about a
missing queueid or \acronym{pid} in the first few hundred or thousand log
lines because the earlier log lines for those connections or mails are in
the previous log file; loading the saved state from the previous log file
will solve this problem.} or when there are problems tracking a child mail
(see \sectionref{tracking re-injected mail}).  There should be few or no
warnings when parsing, and when finished parsing the state table should
only contain entries for mails that had yet to be delivered when the log
files ended, or were accepted before the log files began.

At the time of writing the parser is being tested with \numberOFlogFILES{}
log files.  There are 5 warnings produced, but because the parser errs on
the side of producing more warnings rather than fewer, those 5 warnings
represent 3 instances of 1 problem: 3 connections started before the first
log file, so their initial log lines are missing, leading to warnings when
their remaining log lines are parsed.

The state tables contain entries for mails not yet delivered when the
parser finishes execution.  Ideally all they should contain are mails that
are awaiting delivery after the period covered by the log files, though
they may also contain mails whose initial entries are not contained in the
log files.  Any other entries are evidence of a failure in parsing or an
aberration in the log files.  After parsing the \numberOFlogFILES{} test
log files the state tables contain 18 entries, breaking down into:

\begin{itemize}

    \item 1 connection that started only seconds before the log files
        ended and had not yet been fully transferred from client to server.

    \item 1 mail that had been accepted only seconds before the log files
        ended and had not yet been delivered.

    \item 9 mails whose initial log lines were not present in the log
        files.

    \item 7 mails that had yet to be delivered due to repeated failures.

\end{itemize}

There are no mails in the state tables which should not be present, thus it
can be concluded that there are zero false negatives.  Once again,
determining the false positive rate is much harder, as manually checking
the results of parsing 13,850,793 connections and mails accepted, rejected,
bounced or delivered is infeasible.  There is considerable circumstantial
evidence that the false positive rate is quite low:

\begin{itemize}

    \item The parser is quite verbose when complaining about known problems
        (e.g.\ if a mail is tracked twice as described in
        \sectionref{tracking re-injected mail}), and no such warnings are
        produced during the test runs.

    \item Queueids and \glspl{pid} naturally group together log lines
        belonging to one mail or connection respectively; it is extremely
        unlikely that a log line would be associated with the wrong
        connection.

    \item When dealing with the complications described in
        \sectionref{complications} the solutions are as specific and
        restrictive as possible, with the goal of minimising the number of
        false positives.  In addition the solution to the \textit{Out of
        order log files\/} complication described in \sectionref{out of
        order log lines} imposes conditions that each reassembled mail must
        comply with to be acceptable.

    \item Every effort has been made while developing to make the parser as
        precise, demanding, and particular as possible.

\end{itemize}

Whereas verifying by inspection that the parser correctly deals with all
60,721,709 log lines in the test log files is infeasible, verifying a
subset of those log files is a tractable, if extremely time consuming,
task.  A sample of log lines was obtained by randomly selecting a log file:

\verb!    perl -Mstrict -Mwarnings -MList::Util=shuffle \!\newline{}
\verb!            -e 'print [shuffle(@ARGV)]->[0];'!

The first 6000 log lines of this log file (roughly 0.01\% of the total
number of log lines used in testing) was extracted:

\verb!    sed -n -e '1,6000p' logfile > test-log-segment!

It is important that the log lines used are contiguous so that all log
entries are present for as many of the connections as possible.  This log
segment was parsed with all debugging options enabled, resulting in 167,448
lines of output.\footnote{A mean of 27.908 lines of output per line of
input; each connection has 30 debugging lines, plus 21 debugging lines per
result.  Connections which have been cloned will have the cloned connection
in their debugging output, plus another 33 debugging lines.  Those numbers
are approximate, and may vary $\pm{}$ 2.  There is a linear relationship
between the number of log lines and debugging lines: $33(connections) +
30(accepted~~mails) + 21(results)$.  This formula is an approximation only,
and has not been rigorously verified.}  All 167,448 lines were examined in
conjunction with the input log file and a dump of the resulting database,
verifying that for each of the log lines the parser used the correct rule
and executed the correct action, which in turn produced the correct result
and inserted the correct data in the database.  The log file segment
produced 4 warnings, 10 mails remaining in the state tables, and 1625
connections correctly entered in the database.

Given the circumstantial and experimental evidence detailed above, the
author is confident that the false positive rate when reconstructing a mail
is exceedingly low, if not approaching zero.

\section{Summary}

XXX EXTEND TO SUMMARISE PARSER EFFICIENCY TOO\@.

XXX REWRITE COVERAGE SUMMARY IF NECESSARY\@.

Parser coverage is divided into two topics in this section: log lines
covered and mails covered.  The former is initially more important, as the
parser must successfully parse every line if it is to be complete, but
subsequently the latter takes precedence because reproducing the path a
mail takes through Postfix is the aim of the parser.  Increasing the
percentage of log lines parsed is relatively simple and non-intrusive:
adding new rules or modifying existing rules is simplified by the
separation of rules, actions, and framework.  Improving the logical
coverage is harder, as the actions taken by Postfix must be reconstructed
by the author, and the new sequence of actions integrated into the existing
model without breaking the existing parsing.  Detecting a deficiency in the
parsing algorithm is also significantly harder than detecting unparsed log
lines, as the parser will warn about any unparsed line, whereas discovering
a flaw in the parser requires understanding of the warnings produced and
the mails remaining in the state table.  Rectifying a flaw in the parser
requires an understanding of both the parser and Postfix's log files, and
investigative work to determine the cause of the deficiency, followed by
further examination of the log files in developing a solution.

