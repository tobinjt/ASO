\chapter{Evaluation}

\label{Evaluation}

\renewcommand{\figurename}{Graph}

% Reduce the gap between columns in tables.
\addtolength{\tabcolsep}{-2pt}

This chapter evaluates \parsername{} on two criteria: efficiency, and
coverage of  Postfix log files.  Efficiency is important because
\parsername{} is intended to be used in a production environment, and must
be capable of parsing log files generated by a high volume mail server in a
reasonable period of time.  Full coverage of Postfix log files is important
because the data gathered by the parser must be accurate and complete for
it to be useful and reliable.  Progress in achieving one goal usually comes
at the expense of the other: every extra measure implemented to improve
accuracy slows down parsing.  Some of the complications described in
\sectionref{complications} occur fewer than ten times when parsing the
\numberOFlogFILES{} log files used to evaluate \parsername{} in this
chapter, yet their solutions slow down parsing of every log file.  However,
those solutions are retained in \parsername{}, because accurate but slow
parsing is preferable to sloppy but quick parsing.

The performance evaluation begins by describing the characteristics of the
mail server that produced the \numberOFlogFILES{} log files used to
evaluate the parser's performance and coverage; the computer that the tests
were run on is also described.  The characteristics of the
\numberOFlogFILES{} log files are described next, with an explanation of
why \parsername{} has better performance when parsing the larger log files
in the group.  The effect of using different rule orderings is explored,
and optimal rule ordering is compared to an oracle that allows the parser
to use only one rule when recognising each log line.  When \parsername{} is
used by other mail administrators, they will need to extend the ruleset to
parse their own log lines, so the next section addresses the question of
how parser performance is affected by an increase in the number of rules in
the ruleset.  The penultimate topic is the simple optimisation of caching
the results of compiling each rule's \acronym{regex}, and the huge effect
it has on parser efficiency.  The performance evaluation concludes by
examining where parsing time is spent: recognition of log lines or their
subsequent processing by actions?

The second criterion the parser is evaluated on is its coverage of Postfix
log files.  Two kinds of coverage need to be evaluated: what proportion of
log lines are correctly recognised by the ruleset, and what proportion of
mail delivery attempts are correctly understood and reconstructed by the
actions?  The former is a requirement for the latter to be achieved, and
the latter is important because the data provided by \parsername{} must be
both complete and correct for it to be of use to others.

\section{Parser Efficiency}

\label{parser efficiency}

Efficiency is an obvious concern when the parser routinely needs to parse
large log files.  The mail server that generated the \numberOFlogFILES{}
log files used in this chapter is a production mail server handling mail
for a university department; the benefit of using this mail server is that
its log files exhibit the idiosyncrasies and peculiarities that a mail
server in the wild must deal with, but the downside is that significantly
altering its Postfix configuration to accommodate this project is not an
option.  \Graphref{Mails received per day} shows the number of mails
accepted over \acronym{SMTP}, with mean and standard deviation in
\tableref{Number of mails received per day: statistics}; as expected, far
more mails are accepted on weekdays than at weekends.  Note that mail
generated on the server (e.g.\ bounce notifications, mail re-injected for
forwarding, mail sent from mailing lists) does not contribute to these
figures: in particular, the vast amounts of mail resulting from the mail
loops noticeable in later graphs are not reflected in these figures.

The standard deviation for mails received on weekend days is quite high,
because approximately four times the usual number of mails were received on
the weekend contained in log files 16 \& 17.  The standard deviation for
all days is high because of the unusually high number of mails received
during log files 16, 17, 35, 36, 39 \& 40.  Interestingly, the extra mails
received during log files 35, 36, 39 \& 40 stopped during log files 37 \&
38, which may indicate that the source was a virus infected office machine
that was turned off over the weekend.  Unfortunately, the source cannot be
investigated properly because all of the mails in question were relayed via
the department's secondary \acronym{MTA} (a backup mail server, hosted
elsewhere), so the log files showing the original source were unavailable.
Relaying spam mail through a secondary \acronym{MTA} is a common practice
amongst spam senders, because historically, primary \acronyms{MTA} had
better anti-spam defences than secondary \acronyms{MTA}.

\showgraph{build/graph-mails-received}{Number of mails received via SMTP
per day}{Mails received per day}

\showtable{build/include-mails-received-table}{Number of mails received via
SMTP per day}{Number of mails received per day: statistics}

\begin{table}[thbp]
    \caption{Details of the computer used to generate statistics}
    \empty{}\label{Details of the computer used to generate statistics}
    \centering{}
    \begin{tabular}[]{ll}
        \tabletopline{}%
        Component  & Component in use                                   \\
        \tablemiddleline{}%
        CPU         & One dual core 2.40GHz Intel\textregistered{}
                        Core$^{\textsc{TM}}$2 CPU,                      \\
                    & with 32KB L1 cache and 4MB L2 cache.              \\
        RAM         & 2GB 667 MHz DDR RAM\@.                            \\
        Hard disk   & One Seagate Barracuda 7200 RPM 250GB SATA disk.   \\
        \tablebottomline{}%
    \end{tabular}
\end{table}

When generating the timing data used in this section, \numberOFlogFILES{}
log files were used, each containing one day's log lines; the three months
of contiguous log files contain \numberOFlogLINEShuman{} log lines and
total \input{build/include-total-size-of-93-log-files}.  Each log file was
parsed 10 times, and the mean parsing time calculated for each log file.
The computer used for test runs was a Dell Optiplex 745, with components
shown in \tableref{Details of the computer used to generate statistics}; it
was dedicated to the task of gathering statistics from test runs, and did
not run any other programs while test runs were in progress.  Saving
results to the database was disabled for the test runs, because that
dominates the run time of the parser, and the tests are aimed at measuring
the speed of \parsername{} rather than the speed of the database and the
disks it is stored on.  Parsing all \numberOFlogFILES{} log files in one
run without saving results to the database took
\input{build/include-full-run-duration}, with mean throughput of
\input{build/include-full-run-throughput}.  In contrast, when saving
results to the database, parsing all \numberOFlogFILES{} log files took
\input{build/include-insert-results-duration}, with mean throughput of
\input{build/include-insert-results-throughput} --- parsing takes up only
\input{build/include-skip-inserting-as-percentage-of-inserting} of
execution time when saving results to the database.

\subsection{Characteristics of the \numberOFlogFILES{} Log Files}

\label{Characteristics of the 93 log files}

The \numberOFlogFILES{} log files used in this chapter were generated on
the School of Computer Science and Statistics' mail server from 2007/01/26
to 2007/04/28.  The \numberOFlogFILES{} log files have fairly consistent
sizes and contents, except for two groups of log files: 22 \& 62--68.
Median log file size is \input{build/include-median-file-size}, containing
\input{build/include-median-number-of-log-lines} log lines, with a median
of \input{build/include-median-mails-accepted-over-smtp} mails accepted
each weekday; large scale mail servers would accept many more mails and
consequently generate much larger log files.  \Graphref{parsing time vs log
file size vs number of log lines graph} shows the parsing time in seconds,
log file size in MB, and number of log lines in tens of thousands, for each
of the \numberOFlogFILES{} log files.  Parsing time is plotted against
number of log lines in \graphref{Parsing time plotted against number of log
lines}, and against log file size in \graphref{Parsing time plotted against
log file size}; the points on both graphs appear to form straight lines,
but they actually curve downwards slightly as they move to the right.

\Graphref{parsing time vs log file size vs number of log lines factor}
plots both the ratio of log file size to parsing time, and the ratio of
number of log lines to parsing time (higher is more efficient in both
cases); \tableref{parsing time vs log file size vs number of log lines
factor table} shows mean and standard deviation.  The ratios are quite
tightly banded, except they increase (i.e.\ improve) for log files 22 \&
62--68, despite their larger than usual size.  The log files in both groups
are much larger than usual because of mail loops caused by users who set up
mail forwarding incorrectly, resulting in a very different distribution of
log lines: normally most log lines are generated by mail delivery attempts
from other hosts, but during the mail loops most of the log lines resulted
from failed delivery of bounce notifications re-injected for forwarding.
The Postfix components that generated most of the log lines during the mail
loops have fewer associated rules than the Postfix components whose log
lines normally make up the bulk of each log file, so the mean number of
rules used per log line is lower for these log files, as is the mean
parsing time per log line.  \Tableref{Number of rules per Postfix
component} shows the number of rules for each Postfix component;
\graphref{Mean number of rules used per log line} shows the drop in the
mean number of rules used per log line for log files containing a mail
loop, and \graphref{Mean number of rules used per log line for each Postfix
component} shows the mean number of rules used per log line for each
Postfix component for \numberOFlogFILES{} log files.

The three plots on \graphref{parsing time vs log file size vs number of log
lines graph} are quite close together, and the graph can be difficult to
read.  Log files 62--68 show a large gap between log file size and number
of log lines, whereas those plots appear to have a small gap for all the
other log files, suggesting that the ratio of number of log lines to log
file size is lower for log files 62--68 (i.e.\ fewer log lines per MB).
This appearance is misleading: \graphref{Mean log line size in bytes} shows
the mean log line size for all \numberOFlogFILES{} log files, and it
reduces for log files 22 \& 62--68, i.e.\ there are more log lines per MB
in those log files.

\showgraph{build/graph-input-size-vs-parsing-time}{Parsing time, log file
size, and number of log lines for \numberOFlogFILES{} log files}{parsing
time vs log file size vs number of log lines graph}

\showgraph{build/graph-parsing-time-vs-file-size}{Parsing time plotted
against log file size}{Parsing time plotted against log file size}

\showgraph{build/graph-parsing-time-vs-number-of-log-lines}{Parsing time
plotted against number of log lines}{Parsing time plotted against number of
log lines}

\showgraph{build/graph-input-size-vs-parsing-time-ratio}{Ratio of log file
size and number of log lines to parsing time (higher is more
efficient)}{parsing time vs log file size vs number of log lines factor}

\showtable{build/include-file-size-and-number-of-log-lines-vs-parsing-time}{Ratio
of log file size and number of log lines to parsing time}{parsing time vs
log file size vs number of log lines factor table}

\begin{table}[thbp]
    \caption{Number of rules per Postfix component}
    \empty{}\label{Number of rules per Postfix component}
    \centering{}
    \begin{tabular}{lr}
        \tabletopline{}%
        Postfix component & Number of rules \\
        \tablemiddleline{}%
        \input{build/include-number-of-rules-per-program}
        \tablebottomline{}%
    \end{tabular}
\end{table}

\showgraph{build/graph-average-number-of-rules-tried-per-log-line}{Mean
number of rules used per log line}{Mean number of rules used per log line}

\showgraph{build/graph-average-number-of-rules-tried-per-program}{Mean
number of rules used per log line for each Postfix component over
\numberOFlogFILES{} log files}{Mean number of rules used per log line for
each Postfix component}

\showgraph{build/graph-mean-bytes-per-log-line}{Mean log line size in
bytes}{Mean log line size in bytes}

\FloatBarrier{}

\subsection{Rule Ordering for Efficiency}

\label{rule ordering for efficiency}

\parsernames{} ruleset has \numberOFrules{} rules: the top 10 rules
recognise \input{build/include-top-ten-hits}\% of the log lines in the
\numberOFlogFILES{} log files, with the remaining log lines split across
the other \numberOFrulesMINUSten{} of the rules, as shown in \graphref{rule
hits graph}.  Assuming that the distribution of log lines is reasonably
consistent across log files, \parsernames{} efficiency should benefit from
using rules that recognise log lines more frequently before it uses rules
that recognise log lines less frequently.  To test this hypothesis, three
full test runs were performed with different rule orderings:

\begin{boldeqlist}

    \item [Optimal]  The optimal order, according to the hypothesis: rules
        that recognise log lines most frequently will be used first.

    \item [Shuffled] This ordering is intended to represent a randomly
        ordered ruleset.  The rules will be shuffled once before use and
        will retain that ordering until the parser exits.  Note that the
        ordering will change every time the parser is executed, so 10
        different rule orderings will be generated for each log file in the
        test run.

    \item [Reverse] Hypothetically the worst order: rules that recognise
        log lines most frequently will be used last.

\end{boldeqlist}

The parsing times of the three orderings are plotted against log file size
in \graphref{Parsing time plotted against log file size for optimal,
shuffled, and reverse orderings}; \graphref{Parsing time of optimal and
reverse orderings relative to shuffled ordering graph} shows the parsing
times of optimal and reverse orderings relative to shuffled ordering, with
mean and standard deviation in \tableref{Parsing time of optimal and
reverse orderings relative to shuffled ordering table}.  \Graphref{Number
of rules used by optimal, shuffled, and reverse orderings} shows the number
of rules used by each ordering while parsing the \numberOFlogFILES{} log
files.  This optimisation provides a mean reduction in parsing time of
\input{build/include-optimal-ordering-parsing-time-reduction-other-logs}\%
with normal log files, making it a very worthwhile and effective
optimisation.

\Tableref{Parsing time of optimal and reverse orderings relative to
shuffled ordering table} shows that differences in rule ordering have less
effect on parsing time when parsing log files 22 \& 62--68, because of the
different distribution of log lines in those log files.  A careful
examination of \graphref{Parsing time of optimal and reverse orderings
relative to shuffled ordering graph} shows that, for the first log file
only, optimal and reverse orderings perform identically: this is because
the hits field of each rule is zero for the first log file, so optimal and
reverse orderings produce identical rule orderings.  For the first log
file, shuffled ordering is the most efficient of the three, but that is
accidental and cannot be relied upon.

\showgraph{build/graph-hits}{Log lines recognised per rule}{rule hits graph}

\showgraph{build/graph-file-size-optimal-shuffle-reverse}{Parsing time
plotted against log file size for optimal, shuffled, and reverse
orderings}{Parsing time plotted against log file size for optimal,
shuffled, and reverse orderings}

\showgraph{build/graph-optimal-and-reverse-vs-shuffle}{Parsing time of
optimal and reverse orderings relative to shuffled ordering}{Parsing time
of optimal and reverse orderings relative to shuffled ordering graph}

\showtable{build/include-optimal-and-reverse-vs-shuffle}{Parsing time of
optimal and reverse orderings relative to shuffled ordering}{Parsing time
of optimal and reverse orderings relative to shuffled ordering table}

\showgraph{build/graph-number-or-rules-used-by-different-orderings}{Number
of rules used by optimal, shuffled, and reverse orderings}{Number of rules
used by optimal, shuffled, and reverse orderings}

\FloatBarrier{}

\subsection{Comparing Optimal Ordering Against an Oracle}

\label{perfect rule ordering}

Optimal rule ordering, described in \sectionref{rule ordering for
efficiency}, is the best rule ordering it is possible to achieve without
having an oracle that magically divines which rule should be used to
recognise each log line.  Such an oracle would give perfect performance,
because only one rule would need to be used to recognise each log line.
\parsername{} can save a list showing which rule recognised each log line,
and use that list to simulate an oracle and improve parsing speed the
\textit{second\/} time a log file is parsed.  This does not provide a
practical benefit, but it does provide a means to evaluate the performance
of optimal rule ordering in comparison to an oracle.

\Graphref{Parsing time of the oracle and optimal ordering relative to
shuffled ordering graph} shows how the oracle and optimal ordering perform
relative to shuffled ordering, with mean and standard deviation in
\tableref{Parsing time of the oracle and optimal ordering relative to
shuffled ordering table}; \graphref{Parsing time plotted against log file
size for the oracle, optimal ordering, and shuffled ordering} shows parsing
time plotted against log file size for the oracle, optimal ordering, and
shuffled ordering.  As expected, the oracle is more efficient than optimal
ordering, but not by much, particularly when parsing the larger log files.
\Graphref{Percentage increase in parsing time when using optimal ordering
instead of the oracle graph} shows the percentage increase in parsing time
when using optimal ordering instead of the oracle, with mean and standard
deviation in \tableref{Percentage increase in parsing time when using
optimal ordering instead of the oracle table}.

Once again, the difference between the oracle and optimal ordering is at
its lowest when parsing log files resulting from a mail loop (log files 22
\& 62--68), because the mean number of rules used per log line is lower
when parsing these log files (see \graphref{Mean number of rules used per
log line}).  The performance of optimal ordering relative to the oracle is
much worse when parsing the first log file than for the remainder of the
log files, because the hits field of every rule starts at zero, so optimal
ordering does not provide any benefit for the first log file; the oracle,
in contrast, is flawless for every log file.

Optimal ordering proves to be quite efficient: \tableref{Percentage
increase in parsing time when using optimal ordering instead of the oracle
table} shows that optimal ordering is less than
\input{build/include-perfect-best-vs-optimal-mean} slower than parsing
using an oracle that magically divines the correct rule to use for each log
line.

\showgraph{build/graph-perfect-best-and-optimal-vs-shuffled}{Parsing time
of the oracle and optimal ordering relative to shuffled ordering}{Parsing
time of the oracle and optimal ordering relative to shuffled ordering
graph}

\showtable{build/include-perfect-best-and-optimal-vs-shuffle}{Parsing time
of the oracle and optimal ordering relative to shuffled ordering}{Parsing
time of the oracle and optimal ordering relative to shuffled ordering
table}

\showgraph{build/graph-file-size-oracle-optimal-shuffle}{Parsing time
plotted against log file size for the oracle, optimal ordering, and
shuffled ordering}{Parsing time plotted against log file size for the
oracle, optimal ordering, and shuffled ordering}

\showgraph{build/graph-perfect-best-vs-optimal}{Percentage increase in
parsing time when using optimal ordering instead of the oracle}{Percentage
increase in parsing time when using optimal ordering instead of the oracle
graph}

\showtable{build/include-perfect-best-vs-optimal-stddev}{Percentage
increase in parsing time when using optimal ordering instead of the
oracle}{Percentage increase in parsing time when using optimal ordering
instead of the oracle table}

\FloatBarrier{}

\subsection{Scalability as the Ruleset Grows}

\label{scalability as the number of rules rises}

How any architecture scales as the number of rules increases is important,
but it is particularly important for this architecture because it is
anticipated that the typical parser will have a large ruleset.  The full
\parsername{} ruleset has \numberOFrules{} rules, whereas the minimum
ruleset required to parse the \numberOFlogFILES{} log files has
\numberOFrulesMINIMUM{} rules, \numberOFrulesMINIMUMpercentage{} of the
full ruleset.  The full ruleset is larger because \parsername{} is tested
with \numberOFlogFILESall{} log files (\numberOFlogFILESallYEARS{} of log
files); testing with more log files increases the chance of finding bugs in
the parser or new complications to be overcome.  The \numberOFlogFILES{}
log files were each parsed 10 times using the minimum ruleset, and the mean
parsing times compared to those generated using the full ruleset: the
percentage parsing time increase when using the full ruleset instead of the
minimal ruleset for optimal, shuffled, and reverse orderings is shown in
\graphref{Percentage parsing time increase when using the maximum ruleset
instead of the minimum ruleset}, with mean and standard deviation in
\tableref{Percentage parsing time increase when using the maximum ruleset
instead of the minimum ruleset table}.  For each ordering, the parsing time
using the maximum ruleset is compared to the parsing time using the minimum
ruleset: the three orderings are not compared to a common baseline.

Clearly, the increased number of rules causes a noticeable performance
decrease with reverse ordering, and a lesser decrease with shuffled
ordering, whereas optimal ordering shows scant change.  Log files 22 \&
62--68 show much smaller increases in parsing time than other log files do,
because most of the log lines in those log files are produced by Postfix
components with few rules, so removing unnecessary rules has little effect
on the total number of rules used; \tableref{Number of rules per Postfix
component in the maximum and minimum rulesets} shows the number of rules
per Postfix component for each ruleset.  Once again, \graphref{Percentage
parsing time increase when using the maximum ruleset instead of the minimum
ruleset} shows that optimal and reverse orderings have identical
performance for the first log file, because the hits field of every rule
starts at zero, and so the two rule orderings are identical for the first
log file.

The optimal ordering shows a mean increase of just
\input{build/include-full-ruleset-vs-minimum-ruleset-mean} in parsing time
for a \numberOFrulesMAXIMUMpercentage{} increase in the number of rules.
These results show that both the architecture and \parsername{} scale
extremely well as the ruleset increases in size, and that optimally
ordering the rules makes an important contribution to this scalability.

\showgraph{build/graph-full-ruleset-vs-minimum-ruleset}{Percentage parsing
time increase when using the maximum ruleset instead of the minimum
ruleset}{Percentage parsing time increase when using the maximum ruleset
instead of the minimum ruleset}

\showtable{build/include-full-ruleset-vs-minimum-ruleset}{Percentage
parsing time increase when using the maximum ruleset instead of the minimum
ruleset}{Percentage parsing time increase when using the maximum ruleset
instead of the minimum ruleset table}

\begin{table}[thbp]
    \caption{Number of rules per Postfix component in the maximum and
    minimum rulesets}
    \empty{}\label{Number of rules per Postfix component in the maximum and
    minimum rulesets}
    \centering{}
    \begin{tabular}{lrrr}
        \tabletopline{}%
        Postfix component & Maximum ruleset & Minimum ruleset & Difference \\
        \tablemiddleline{}%
        \input{build/include-number-of-rules-per-program-minimum-ruleset}
        \tablebottomline{}%
    \end{tabular}
\end{table}

\FloatBarrier{}

\subsection{Caching Compiled Regexes}

\label{Caching compiled regexes}

Before the Perl interpreter attempts to match a \acronym{regex} against a
piece of text, the \acronym{regex} is compiled into an internal
representation and optimised to improve the speed of matching.  This
compilation and optimisation takes CPU time: for most \regexes{} it takes
far more CPU time than the actual matching does.  If the interpreter is
certain that a \acronym{regex} will not change while the program is
running, it will automatically cache the results of compiling and
optimising the \acronym{regex} for later use.  The results of compiling a
dynamically generated \acronym{regex} can be cached and used in preference
to the original \acronym{regex}, but it is the responsibility of the
programmer to do so; \parsername{} does this with every rule's
\acronym{regex} when the rules are loaded from the database.

A test run using optimal ordering was performed as described in
\sectionref{parser efficiency}, with one difference: \regexes{} were not
compiled and cached when the ruleset was loaded from the database, so the
Perl interpreter had to compile each \acronym{regex} each time it was used
when trying to recognise a log line.  \Graphref{Percentage increase in
parsing time when not caching compiled regexes graph} shows the effect that
not caching compiled \regexes{} has on parser performance, with mean and
standard deviation in \tableref{Percentage increase in parsing time when
not caching compiled regexes table}.  For typical log files, the mean
increase in parsing time when not caching compiled \regexes{} is
\input{build/include-cached-regexes-vs-discarded-regexes-mean}; looking at
it from the opposite direction, caching compiled \regexes{} reduces parsing
time by
\input{build/include-cached-regexes-vs-discarded-regexes-mean-reduction}.
Caching compiled \regexes{} is probably the single most effective
optimisation possible in \parsername{}, and was quite simple to implement:
the framework compiles each rule's \acronym{regex} when the ruleset is
loaded, and uses the compiled \acronym{regex} instead of the source
\acronym{regex} when recognising log lines.  As seen previously, log files
22 \& 62--68 do not suffer such a large increase in parsing time when the
optimisation is disabled; this is because, on average, fewer rules are used
per log line, so fewer \regexes{} are compiled per log line for those log
files.

The increase in parsing time when parsing the first log file is much
greater than for the other log files (see \graphref{Percentage increase in
parsing time when not caching compiled regexes graph}); again, this is
because every rule's hits field starts at zero, so optimal ordering is less
efficient than usual, the mean number of rules used for each log line is
higher than usual, and more \regexes{} will need to be compiled when
recognising each log line.  Parsing time is plotted against log file size
for caching and not caching compiled \regexes{} in \graphref{Parsing time
plotted against log file size when caching and not caching compiled
regexes}; the plot for not caching compiled \regexes{} is quite uneven
compared to caching compiled \regexes{} --- some log files are particularly
slow, whereas others are not as badly affected, but these differences have
not been investigated.

\showgraph{build/graph-cached-regexes-vs-discarded-regexes}{Percentage
increase in parsing time when not caching compiled regexes}{Percentage
increase in parsing time when not caching compiled regexes graph}

\showtable{build/include-cached-regexes-vs-discarded-regexes}{Percentage
increase in parsing time when not caching compiled regexes}{Percentage
increase in parsing time when not caching compiled regexes table}

\showgraph{build/graph-file-size-optimal-discarded}{Parsing time plotted
against log file size when caching and not caching compiled
regexes}{Parsing time plotted against log file size when caching and not
caching compiled regexes}

\FloatBarrier{}

\subsection{Where is Parsing Time Spent: Recognising or Processing Log
Lines?}

\label{recognising vs processing}

The optimisations described in this chapter have addressed the process of
recognising log lines, but have not optimised actions at all.  Optimisation
efforts have concentrated on recognition of log lines for two reasons:

\begin{enumerate}

    \item Even with the optimisations described in this chapter enabled,
        recognising log lines still accounts for two thirds of the parser's
        execution time.  \Graphref{Percentage of parsing time spent
        recognising log lines graph} shows the percentage of parsing time
        spent recognising log lines for each of the \numberOFlogFILES{} log
        files, with mean and standard deviation shown in
        \tableref{Percentage of parsing time spent recognising log lines
        table}; for normal log files,
        \input{build/include-percentage-time-spent-recognising-log-lines-mean}
        of parsing time is spent recognising log lines.  To measure how
        long recognition of log lines takes, a full test run using optimal
        ordering and caching compiled \regexes{} was performed as
        previously described in \sectionref{parser efficiency}, but with
        one difference: actions were not invoked when a log line was
        recognised, so the parsing time did not include the time normally
        taken by invocation of actions; the mean parsing times for each log
        file were subtracted from the mean parsing times from a normal test
        run to obtain the parsing time taken by actions.  The parsing times
        exclude the time taken for framework initialisation, loading and
        saving state tables, loading the ruleset, and other housekeeping
        tasks: in as far as possible, the times are just for recognising
        log lines or recognising log lines plus invoking actions.

        Optimal ordering reduces parsing time by
        \input{build/include-optimal-ordering-parsing-time-reduction-other-logs}\%
        relative to shuffled ordering; actions occupy
        \input{build/include-percentage-time-spent-processing-log-lines-mean}
        of parsing time, less if any optimisations are disabled, so
        optimising actions could not possibly provide as big a performance
        increase as optimally ordering rules.  Similarly, caching compiled
        \regexes{} provides an
        \input{build/include-cached-regexes-vs-discarded-regexes-mean-reduction}
        reduction in parsing time; not invoking actions at all reduces the
        most optimised parsing time by less than half that.  Optimising
        actions to 1\% of their original parsing time would be only
        slightly more effective than optimal ordering is, but would be
        vastly more difficult to implement; optimising actions would not
        reduce parsing time enough to justify the amount of effort
        required.

    \item The process of recognising log lines is not parser-specific
        (excluding evaluation of rule conditions), so the optimisations
        described in this chapter are applicable to all parsers based on
        this architecture.  Actions are parser-specific, so it is unlikely
        that any optimisations made to actions would be portable to other
        parsers.

\end{enumerate}

Individual actions and the framework could and have been optimised, but
plenty of existing literature is available on the topic of optimising
programs, so the subject is not dealt with here.

\showgraph{build/graph-percentage-time-spent-recognising-log-lines}{Percentage
of parsing time spent recognising log lines}{Percentage of parsing time
spent recognising log lines graph}

\showtable{build/include-percentage-time-spent-recognising-log-lines-table}{Percentage
of parsing time spent recognising log lines}{Percentage of parsing time
spent recognising log lines table}

\FloatBarrier{}

\newpage{}

\section{Coverage}

\label{parsing coverage}

The discussion of \parsernames{} coverage of Postfix log files is separated
into two parts: log lines correctly recognised, and mail delivery attempts
correctly understood --- the former is a requirement for the latter to be
achieved.  Correctly understanding and reconstructing every mail delivery
attempt is important so that the information in the database is accurate
and complete.  Improving the proportion of log lines correctly recognised
is the less difficult of the two, because it just requires new rules to be
written or existing rules to be extended.  Improving the proportion of
correctly understood and reconstructed mail delivery attempts is more
difficult and intrusive, because it requires adding or changing actions,
and it can be much harder to realise that a deficiency exists and needs to
be addressed.

\subsection{Log Lines Correctly Recognised}

\label{log-lines-covered}

Parsing a log line is a three step process:

\begin{enumerate}

    \squeezeitems{}

    \item Skip the log line if the ruleset does not contain any rules for
        the Postfix component that produced it.

    \item Try each rule that recognises log lines from that Postfix
        component, then any generic rules, until a recognising rule is
        found; if the log line is not recognised, issue a warning and move
        on to the next log line.

    \item Invoke the action specified by the recognising rule.

\end{enumerate}

Each Postfix component whose log lines are of interest must have at least
one rule that recognises its log lines, or all of its log lines will be
silently skipped; in the extreme case of an empty ruleset the parser would
skip every log line.  \parsername{} skips log lines from programs that do
not have any associated rules, because there may be any number of log lines
from other programs in the log file, and some Postfix components do not
produce any log lines of interest.  \parsername{} does not parse log lines
from non-Postfix programs, e.g.\ Amavisd-new or SpamAssassin; it could
easily be extended to do so, if a method could be developed to correctly
associate such log lines with existing state table entries.  To correctly
recognise all log lines that are not skipped, there must be a rule to
recognise each log line variant produced by each Postfix component; if a
log line is not recognised the parser will issue a warning, to inform the
user that they need to extend their ruleset.  Each rule's \acronym{regex}
should be as specific and precise as possible, to ensure accurate parsing:
a rule with a \acronym{regex} that matches zero or more of any character
will recognise every log line, but not in a meaningful way; most log lines
contain fixed strings, so this is not a problem in practice.

Full coverage of log lines can be achieved without undue effort, yet once
achieved it requires maintenance.  Maintaining full coverage is an ongoing
task because the set of log line variants changes over time, e.g.\
administrators add restrictions with custom messages, \acronym{DNSBL}
messages change, or major releases of Postfix change log lines (usually by
adding more information).  Warnings are issued for any log lines that are
not recognised; no warnings are issued for unrecognised log lines while
parsing the \numberOFlogFILES{} log files, so it can be safely concluded
that zero false negatives arise.  False positives are harder to quantify,
short of examining each of the 60,721,709 log lines and checking that the
correct rule recognised it.  However, a random sample of 6039 log lines was
parsed, and the results manually verified by inspection to ensure that the
correct rule recognised each log line.  The sample was generated by running
the command: \newline{} \tab{} \texttt{perl -n -e \singlequote{}print if
(rand 1 < 0.0001)\singlequote{} LOG\_FILES} \newline{} to randomly extract
roughly one log line in every 10,000 (it actually extracted 0.00994\%
instead of 0.010\%).  Each log line was examined and the correct rule
identified from the \numberOFrules{} rules in the database; the correct
rule was then compared to the rule that recognised the log line when
parsing.  The sample results contained zero false positives, and this check
has been automated to ensure continued accuracy.  Based on these results,
and how precise each rule's \acronym{regex} is, the author is confident
that zero false positives occur when parsing the \numberOFlogFILES{} log
files.

\subsection{Mail Delivery Attempts Correctly Understood and Reconstructed}

\label{mails-covered}

The proportion of mail delivery attempts that are correctly understood and
reconstructed is more difficult to determine accurately than the proportion
of log lines that are correctly recognised.  The parser can dump its state
tables in a human readable form; examining those tables with reference to
the log files and the database is the best way to detect mails that were
not reconstructed properly; many of the complications documented in
\sectionref{complications} were detected in this way.  \parsername{} issues
warnings when it detects any errors or discrepancies, e.g.\ when a queueid
is reused but the previous mail remains in the state tables, when a queueid
or \acronym{pid} is not found in the state tables, or when an entry in the
state tables does not include sufficient data to satisfy the database
schema.  The parser should produce few or no warnings during parsing, and
when finished parsing the state tables should only contain entries for
mails that have log lines in subsequent log files.  There will often be
warnings about a missing queueid or \acronym{pid} when parsing the first
few thousand log lines, because some earlier log lines for those
connections or mails are in a previous log file; loading state tables saved
when parsing the log file containing those log lines will solve this
problem.

5 warnings are produced when parsing the \numberOFlogFILES{} log files to
generate the data used in this chapter, but because \parsername{} errs on
the side of producing more warnings rather than fewer, those 5 warnings
represent 3 instances of 1 problem: 3 connections that started before the
first log file, so their initial log lines are missing, leading to warnings
when their remaining log lines are parsed.  None of the warnings are false
positives.

The state tables will contain entries for mails not yet delivered when the
parser finishes parsing the log file.  Ideally, those are the only entries
the state tables will contain, though they may also contain mails whose
initial log lines are not contained in the log files.  Any other entries in
the state tables are evidence of either a failure in parsing, or an
aberration in the log files.  After parsing the \numberOFlogFILES{} log
files, the state tables contain 18 entries:

\begin{itemize}

    \squeezeitems{}

    \item 1 connection that started only seconds before the log files ended
        and the mail had not yet been fully transferred from client to
        server.

    \item 1 mail that had been accepted only seconds before the log files
        ended and had not yet been delivered.

    \item 9 mails whose initial log lines were not present in the log
        files.  Six of those mails did not produce warnings because they
        resemble child mails waiting to be tracked with a parent; see
        \sectionref{Re-injected mails} for details.  Three of these mails
        were missing more log lines, and so did five produce warnings, as
        documented previously.

    \item 7 mails that had yet to be delivered because of repeated
        delivery failures.

\end{itemize}

All of the mails remaining in the state tables have valid reasons for being
present, so it can be concluded that zero false negatives occur when
parsing the \numberOFlogFILES{} log files.  Once again, determining the
false positive rate is much harder, because manually checking the results
of parsing \numberOFconnectionsINlogFILES{} connections and mails accepted,
rejected, bounced, or delivered is infeasible.  Considerable evidence
exists that the false positive rate is extremely low, if not zero:

\begin{itemize}

    \item \parsername{} performs many checks to detect known problems,
        e.g.\ a queueid missing from the state tables.  No such warnings
        are produced during the test runs other than the five described
        previously.

    \item Queueids and \glspl{pid} naturally identify log lines belonging
        to one mail or connection respectively; it is extremely unlikely
        that a log line would not be associated with the right connection.

    \item When dealing with the complications described in
        \sectionref{complications}, the solutions are as specific and
        restrictive as possible, with the goal of minimising the number of
        false positives.  In addition, the solution to the complication
        described in \sectionref{out of order log lines} imposes conditions
        that every reconstructed mail must comply with to be acceptable,
        not just the five mails exhibiting that complication.

    \item Every effort has been made to make \parsername{} as precise,
        demanding, and particular as possible.

\end{itemize}

\renewcommand{\figurename}{Figure}

\begin{figure}[thbp]
    \caption{The command used to extract the log segment used to verify
        \parsernameshort{'s} parsing}
    \empty{}\label{Command to extract log segment used to verify parsing}

\begin{verbatim}
perl -Mstrict -Mwarnings -e '
    while (<>) {
        if (rand 1 < 0.0001) {
            my $count = 0;
            while ($count < 6000) {
                print scalar <>;
                $count++;
            }
            exit;
        }
    }' LOG_FILES
\end{verbatim}

\end{figure}

Verifying by inspection that the parser correctly processes all
\numberOFconnectionsINlogFILES{} mail delivery attempts in the
\numberOFlogFILES{} log files is infeasible, but verifying the parsing of a
sample from those log files is a tractable albeit extremely time consuming
task.  A sample of log lines was obtained by randomly selecting a block of
6000 contiguous log lines from the \numberOFlogFILES{} log files (0.00988\%
of the total number of log lines), using the command shown in
\figureref{Command to extract log segment used to verify parsing}.  It is
important that the log lines are contiguous, so that all log lines are
present for as many of the mail delivery attempts contained in the block as
possible.  This log segment was parsed with all debugging options enabled,
resulting in 167,448 lines of output.\footnote{A mean of 27.908 lines of
output per log line; each connection has 30 debugging lines, plus 21
debugging lines per result.  Connections which have been cloned will have
the cloned connection in their debugging output, plus another 33 debugging
lines.  Those numbers are approximate, and may vary $\pm{}$ 2.  An
approximate linear relationship between the number of log lines and
debugging lines is: $33(connections) + 30(accepted~~mails) + 21(results)$.}
All 167,448 lines were examined in conjunction with the log file segment
and a dump of the resulting database, verifying that \parsername{}
recognised each of the log lines with the correct rule and invoked the
correct action, which in turn correctly processed the log line and saved
the correct data to be inserted into the database.  The log file segment
produced 4 warnings, 10 mails correctly remaining in the state tables, 1625
mail delivery attempts correctly entered in the database, 0 false
positives, and 0 false negatives.

Given the evidence detailed above, the author is confident that the false
positive rate when reconstructing a mail delivery attempt from the
\numberOFlogFILES{} log files is exceedingly low, if not zero.

\section{Summary}

This chapter evaluated \parsername{} on two criteria: efficiency, and
coverage of Postfix log files.  The former began by describing the mail
server the log files were taken from, the computer used to generate the
statistics in this chapter, and the characteristics of the
\numberOFlogFILES{} log files used to generate statistics and test
coverage, including why performance is better when parsing the larger log
files.  The framework optimises the order in which rules are used when
trying to recognise each log line, and the effect that optimisation has on
parsing time is explored; this is followed by comparing optimal ordering
with an oracle.  How \parsername{} scales as the size of the ruleset
increases is addressed, followed by a description and analysis of the
simplest and most effective of the optimisations, caching compiled
\regexes{}, and the efficiency evaluation concludes with an examination of
where parsing time is spent: recognising log lines or processing them?

Coverage of Postfix log files is divided into two topics in this chapter:
log lines correctly recognised, and mail delivery attempts correctly
understood and reconstructed.  The former is initially more important,
because the parser must correctly recognise every log line if it is to be
complete, but subsequently the latter takes precedence because correctly
reconstructing the journey a mail delivery attempt takes through Postfix is
the purpose of the parser.  Increasing the proportion of log lines
correctly recognised is relatively simple and non-intrusive: adding new
rules or modifying existing rules is very easy because of the separation of
rules, actions, and framework in both the architecture and \parsername{}.
Improving the understanding and reconstruction of mail delivery attempts is
harder, because Postfix's behaviour must be analysed and figured out, and
support for the newly understood behaviour integrated into the actions
without breaking the existing parsing.  Detecting a deficiency in the
parser's understanding of mail delivery attempts requires careful study of
any warnings produced and the entries remaining in the state tables.
Rectifying a flaw in the parser requires a deep understanding of Postfix's
log files, and a working knowledge of the framework, actions, and rules;
investigative work will be needed to determine the cause of the deficiency,
followed by further examination of the log files to aid in developing a
solution, and finally implementation, integration, and testing of the
solution.

This chapter shows that it is possible to balance the conflicting goals of
efficient and accurate parsing, and that one does not have to be sacrificed
to achieve the other.
