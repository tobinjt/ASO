\section{Results}

\subsection{Efficiency}

\subsubsection{Rule efficiency}

\label{rule efficiency}

XXX ADD TIMING INFORMATION\@.

Parsing efficiency is an obvious concern when the parser routinely needs to
deal with 75 MB log files containing 300,000 log lines (generated daily on
a mail server handling mail for approximately 700 users --- large scale
mail servers would have much larger log files on a daily basis).  When
generating the data for the graphs included in \sectionref{graphs},
\numberOFlogFILES{} log files (totaling 10.08 GB, \numberOFlogLINEShuman{}
log lines) were each parsed 10 times, the first run discarded, and the
execution time for the remaining 9 runs averaged.  The first run is
discarded for two reasons:

\begin{enumerate}

    \item The execution time will be higher because the log file must be
        read from disk, whereas for subsequent runs the log file will be
        cached in memory by the operating system.

    \item The execution time will also be higher because the rule ordering
        will be sub-optimal compared to subsequent runs.

\end{enumerate}

Saving results to the database was disabled for the test runs, as that
dominates the run time of the program, and the tests are aimed at measuring
the speed of the parser rather than the speed of the database and the disks
the database is stored on.

\subsubsection{Algorithmic complexity}

An important property of a parser is how execution time scales relative to
input size: does it scale linearly, polynomially, or exponentially?
\Graphref{execution time vs file size vs number of lines graph}
shows the execution time in seconds, file size in MB and tens of thousands
of log lines per log file.  All three lines run roughly in parallel, giving
a visual impression that the algorithm scales linearly with input size.
This impression is borne out by \graphref{execution time vs file
size vs number lines factor} which plots the ratio of file size vs
execution time and ratio of number of log lines vs execution time (higher
is better); \tableref{execution time vs file size vs number lines
factor table} shows the ratios for different breakdowns of the log files.
As the reader can see the ratios are quite tightly banded, showing that the
algorithm scales linearly: the much larger log files between points 60 and
70 on the X axis in \graphref{execution time vs file size vs
number of lines graph} actually cause the ratio to increase (i.e.\
improve), rather than decrease.  The strange behaviour where larger log
files are parsed more efficiently is explained fully in \sectionref{Why are
there dips in the graphs?}.  

\subsubsection{Rule ordering for efficiency}

\label{rule ordering for efficiency}

Rule ordering was mentioned in \sectionref{rule attributes} and will be
covered in greater detail in this section.  At the time of writing there
are \numberOFrules{} different rules: the top 10\% match the vast majority
of log lines, with the remaining log lines split across the other 90\% of
the rules (as shown in \graphref{rule hits graph}).  Assuming that
the distribution of log lines is reasonably steady over time, program
efficiency should benefit from trying more frequently matching rules before
those which match less frequently.  To test this hypothesis three full test
runs were performed with different rule orderings:

\begin{description}

    \item [normal]  The most optimal order, according to the hypothesis:
        rules which match most often will be tried first.

    \item [shuffle] This is intended to represent a randomly ordered rule
        set.  The rules will be shuffled once before use and will retain
        that ordering for the entirety of the log file.  Note that the
        ordering will change every time the parser is executed, so 10
        different orderings will be generated for each log file in the test
        run.  

    \item [reverse] Hypothetically the worst order: the most frequently
        matching rules will be tried last.

\end{description}

Graphs~\refwithpage{percentage increase of shuffled over normal}
and~\refwithpage{percentage increase of reversed over normal} show the
percentage increase of execution times, with \tableref{Execution
time increase for different rule orderings} showing the mean increases for
different groupings of log files.  Overall this provides a modest but
worthwhile performance increase of approximately 9\%, for a small
investment in time and programming.

\subsubsection{Caching each regex}

\label{Caching each regex}

Perl compiles the original \regex{} into an internal representation,
optimising the \regex{} to improve the speed of matching, but this
compilation and optimisation takes CPU time; far more CPU time, in fact,
than the actual matching takes.  Perl automatically caches static
\regexes{}, but dynamic \regexes{} need to be explicitly compiled and
cached.  \Graphref{normal regex vs discard regex} shows execution
times with and without caching the \regex{}.  Caching the compiled
\regexes{} is obviously far more efficient; \graphref{normal regex
vs discarded regex factor} shows the percentage execution time increase
when not caching each \regex{}.

Caching the compiled \regexes{} is quite simple, and is the single most
effective optimisation implemented in the parser.

XXX ADD A TABLE\@.

\subsection{Coverage}

\label{parsing coverage}

The discussion of the parser's coverage of Postfix log files is separated
into two parts: log lines covered and mails covered.  The first is
important because the parser should handle all (relevant) log lines it is
given; the second is equally important because the parser must properly
deal with every mail if it is to be useful.  Improving the former is
less intrusive, as it just requires new rules to be written; improving the
latter is much more intrusive as it requires changes to the parser
algorithm, and it can also be much harder to notice a deficiency.

\subsubsection{Log lines covered}

\label{log-lines-covered}

Parsing a log line is a three stage process:

\begin{enumerate}

    \item Check if there are any rules for the Postfix component which
        produced the log line; if not then skip the log line.

    \item Try each rule until a matching rule is found; if no match is
        issue a warning and move to the next log line.

    \item Execute the action specified by the rule.

\end{enumerate}

Full coverage of log lines requires the following:

\begin{enumerate}

    \item Each Postfix component whose log lines are of interest must have
        at least one rule or its log lines will be silently skipped; in the
        extreme case of zero rules the parser would happily skip every log
        line.  There may be any number of log lines from other programs
        intermingled in the log file, and there are some Postfix programs
        which do not produce any log lines of interest.

    \item There must be a rule to match each different log line produced by
        each program; if a log line is not successfully matched the parser
        will issue a warning.  Rules should be as specific and tightly
        bound as possible to ensure accurate parsing:\footnote{A rule which
        matches zero or more of any character will successfully parse every
        log line, but not in a meaningful way.} most log lines contain
        fixed strings and have a rigid pattern, so this is not a problem.

    \item The appropriate action to take --- discussed in
        \sectionref{mails-covered}.

\end{enumerate}

Full coverage of log lines is easy to achieve yet hard to maintain.  It is
easy to achieve full coverage for a limited set of log files (at the time
of writing the parser is tested with \numberOFrules{} rules, fully parsing
\numberOFlogFILES{} contiguous log files from Postfix 2.2 and 2.3), and new
rules are easy to add.  Maintaining full coverage is hard because other
servers have different restrictions with custom messages, \DNSBL{} messages
change over time, major releases of Postfix change warning messages
(usually adding more information), etc.,\ so over time the log lines drift
and change.  \Graphref{rule hits graph} shows the number of hits
for each rule over all \numberOFlogFILES{} log files; it is obvious that a
small number of rules match the vast majority of the lines, and more than
half the rules match fewer than 100 times.

Warnings are issued for any log lines which are not parsed; no warnings are
issued for unparsed log lines while testing with the \numberOFlogFILES{}
test log files, so it can be safely concluded that there are zero false
negatives.  False positives are harder to quantify: short of examining each
of the 60,721,709 log lines and determining which \regex{} parsed it, there
is no way to be sure that every line was parsed by the correct \regex{},
making it impossible to quantify the false positive rate; however a random
sample of 6,039 log lines was parsed and the results checked manually to
ensure that the correct \regex{} parsed each log line.\footnote{Each log
line was examined and the correct \regex{} identified from the
\numberOFrules{} rules in the database; the correct \regex{} was then
compared to the \regex{} which was used by the parser.}  The sample was
generated by running the following command:

\verb!    perl -e 'print if (rand 1 < 0.0001)' -n LOG_FILES!

\noindent{}to randomly extract roughly one line in every 10,000.  Although
on initial appearances exercising only 36 rules (from a total of
\numberOFrules{}) when parsing 6039 log lines seems quite low, after
examining \graphref{rule hits graph} it becomes apparent that such
a low hit rate is to be expected; the reader should also bear in mind that
even when parsing all \numberOFlogFILES{} log files not all the rules are
exercised (some of the rules are for parsing log lines which only appear in
other log files).

\subsubsection{Mails covered}

\label{mails-covered}

Coverage of mails is much more difficult to determine accurately than
coverage of log lines.  The parser can dump its state tables in a human
readable form; examining these tables with reference to the log files is
the best way to detect mails which were not handled properly (many of the
complications discussed in \sectionref{additional complications} were
detected in this way).  The parser issues warnings when it detects any
errors, some of which may alert the user to a problem, e.g.\ when a queueid
is reused before the previous mail is fully dealt with, when a queueid or
\pid{} is not found in the state tables,\footnote{There will often be
warnings about a missing queueid or \pid{} in the first few hundred or
thousand log lines because the earlier log lines for those connections or
mails are in the previous log file; loading the saved state from the
previous log file will solve this problem.} or when there are problems
tracking a child mail (see \sectionref{tracking re-injected mail}).  There
should be few or no warnings when parsing, and when finished parsing the
state table should only contain entries for mails which had yet to be
delivered when the log files ended, or were accepted before the log files
began.

At the time of writing the parser is being tested with \numberOFlogFILES{}
log files.  There are 5 warnings produced, but because the parser errs on
the side of producing more warnings rather than fewer, those 5 warnings
represent 3 instances of 1 problem: 3 connections started before the first
log file, so their initial log entries are missing, leading to warnings
when their log lines are parsed.

The state tables contain entries for mails not yet delivered when the
parser finishes execution.  Ideally all they should contain are mails which
are awaiting delivery after the period covered by the log files, though
they may also contain mails whose initial entries are not contained in the
log files.  Any other entries are evidence of a failure in parsing or an
aberration in the log files.  After parsing the \numberOFlogFILES{} test
log files the state tables contain 18 entries, breaking down into:

\begin{itemize}

    \item 1 connection which started only seconds before the log files
        ended and had not yet been fully transferred from client to server.

    \item 1 mail which had been accepted only seconds before the log files
        ended and had not yet been delivered.

    \item 9 mails whose initial log entries were not present in the log
        files.

    \item 7 mails which had yet to be delivered due to repeated failures.

\end{itemize}

There are no mails in the state tables which should not be present, thus it
can be concluded that there are zero false negatives.  Once again,
determining the false positive rate is much harder, as manually checking
the results of parsing 13,850,793 connections and mails accepted, rejected,
bounced or delivered is infeasible.  There is considerable circumstantial
evidence that the false positive rate is quite low:

\begin{itemize}

    \item The parser is quite verbose when complaining about known problems
        (e.g.\ if a mail is tracked twice as described in
        \sectionref{tracking re-injected mail}), and no such warnings are
        produced during the test runs.

    \item Queueids and \pids{} naturally group together log lines belonging
        to one mail or connection respectively; it is extremely unlikely
        that a log line would be associated with the wrong connection.

    \item When dealing with the complications described in
        \sectionref{complications} and \sectionref{additional
        complications} the solutions are as specific and restrictive as
        possible, with the goal of minimising the number of false
        positives.  In addition the solution to the \textit{Out of order
        log files\/} complication described in \sectionref{out of order log
        lines} imposes conditions which each reassembled mail must comply
        with to be acceptable.

    \item Every effort has been made while developing to make the parser as
        precise, demanding and particular as possible.

\end{itemize}

Whereas verifying by inspection that the parser correctly deals with all
60,721,709 lines in the test log files is infeasible, verifying a subset of
those log files is a tractable, if extremely time consuming, task.  A
sample of log lines was obtained by randomly selecting a log file:

\verb!    perl -Mstrict -Mwarnings -MList::Util=shuffle \!\newline
\verb!            -e 'print [shuffle(@ARGV)]->[0];'!

The first 6000 lines of this log file (roughly 0.01\% of the total number
of log lines used in testing) was extracted:

\verb!    sed -n -e '1,6000p' logfile > test-log-segment!

It is important that the log lines used are contiguous so that all log
entries are present for as many of the connections as possible.  This log
segment was parsed with all debugging options enabled, resulting in 167,448
lines of output.\footnote{A mean of 27.908 lines of output per line of
input; each connection has 30 debugging lines, plus 21 debugging lines per
result.  Connections which have been cloned will have the cloned connection
in their debugging output, plus another 33 debugging lines.  Those numbers
are approximate, and may vary $\pm{}$ 2.  There is a linear relationship
between the number of log lines and debugging lines: $33(connections) +
30(accepted~~mails) + 21(results)$.  This formula is an approximation only,
and has not been rigorously verified.}  All 167,448 lines were examined in
conjunction with the input log and a dump of the resulting database,
verifying that for each of the input lines the parser used the correct rule
and executed the correct action, which in turn produced the correct result
and inserted the correct data in the database.  The log segment produced 4
warnings, 10 mails remaining in the state tables, and 1625 connections
correctly entered in the database.

Given the circumstantial and experimental evidence detailed above, the
author is confident that the false positive rate when reconstructing a mail
is exceedingly low, if not approaching zero.

\subsubsection{Summary}

Parser coverage is divided into two topics in this section: log lines
covered and mails covered.  The former is initially more important, as the
parser must successfully parse every line if it is to be complete, but
subsequently the latter takes precedence because reproducing the path a
mail takes through Postfix is the aim of the parser.  Increasing the
percentage of log lines parsed is relatively simple and non-intrusive:
adding new rules or modifying existing rules is simplified by the
separation of rules, actions and framework.  Improving the logical coverage
is harder, as the actions taken by Postfix must be reconstructed by the
author, and the new sequence of actions integrated into the existing model
without breaking the existing parsing.  Detecting a deficiency in the
parsing algorithm is also significantly harder than detecting unparsed log
lines, as the parser will warn about any unparsed line, whereas discovering
a flaw in the parser requires understanding of the warnings produced and
the mails remaining in the state table.  Rectifying a flaw in the parser
requires an understanding of both the parser and Postfix's log files, and
investigative work to determine the cause of the deficiency, followed by
further examination of the log files in developing a solution.

\subsection{Graphs}

\label{graphs}

XXX COMPARE SAVING RESULTS TO RAM DISK VERSUS SAVING TO HARD DISK\@.

XXX DO SOMETHING WITH NORMAL VS SMALLER GRAPH\@.

XXX EXPLAIN THE X-AXES ON THE GRAPHS\@.

\showgraph{build/plot-normal-vs-smaller}{NORMAL VS SMALLER}{NORMAL VS
SMALLER}

\renewcommand{\figurename}{Graph}

\subsubsection{Introduction}

Graphs are an excellent means of displaying data, transforming a
meaningless stream of numbers into an easily comprehensible form, where
anomalies and patterns are immediately obvious.  These graphs are used to
illustrate the topics discussed in \sectionref{rule efficiency}.  The
graphs in the first section cover parser scalability, demonstrating that
performance scales linearly with input size.  The impact of rule ordering
is shown in \sectionref{rule ordering graphs}, and the anomalous dips and
peaks apparent in some graphs are explained.  The third group of graphs
vividly shows the huge impact that caching compiled \regexes{} has on
parser performance.  Miscellaneous graphs are presented in the final group
of graphs; these graphs are referenced from various places in the test.
This section concludes with a breakdown of the rule hits accumulated during
a single test run of the parser.

\newpage

\subsubsection{Parser scalability}

\showgraph{build/plot-normal-filesize-numlines}{Execution time vs file
size vs number of lines}{execution time vs file size vs number of lines
graph}

The Y axis in \graphref{execution time vs file size vs number of
lines graph} represents the following:

\begin{enumerate}

    \item The time required, in seconds, to parse the log file.

    \item The size of the log file, in megabytes.

    \item The number of lines in the log file, divided by 10000.

\end{enumerate}

\Graphref{execution time vs file size vs number lines factor}
shows the ratio of execution time vs file size and number of lines (higher
is better, it means more bytes or lines processed per second).  The ratios
are quite tightly banded with the exception of log files 22 and 62--68,
where they are noticeably higher; \graphref{execution time vs file
size vs number lines factor} and \tableref{execution time vs file
size vs number lines factor table} show that the parser's execution time
scales linearly with input size.

\showgraph{build/plot-normal-filesize-numlines-factor}{Ratio of file
size and number of lines to execution time}{execution time vs file size vs
number lines factor}

\showtable{build/stats-normal-filesize-line-count-include}{Ratio of file
size \& number of lines to execution time: statistics}{execution time vs
file size vs number lines factor table}

\clearpage

\subsubsection{Rule ordering}

\label{rule ordering graphs}

\showgraph{build/plot-normal-shuffle-factor}{Percentage increase of
shuffled over normal}{percentage increase of shuffled over normal}

\showgraph{build/plot-normal-reverse-factor}{Percentage increase of
reversed over normal}{percentage increase of reversed over normal}

\showtable{build/stats-normal-shuffle-reverse-include}{Execution time
increase for different rule orderings}{Execution time increase for
different rule orderings}

\subsubsection{Why are there dips in the graphs?}
\label{Why are there dips in the graphs?}

The dips at log files 22 and 62--68 correspond to peaks in log file size in
\graphref{execution time vs file size vs number of lines graph},
and peaks in \graphref{execution time vs file size vs number lines
factor} (where a peak means that more lines are processed per second, i.e.\
performance is better).  The explanation for this took some time to arrive
at, but it turns out to be reasonably simple.  The large log files in
question were caused by a mail forwarding loop, where the distribution of
log lines is quite different to normal, resulting in different performance
characteristics.

The mail loop was set up by a user modifying his mail forwarding to:
\newline \tab{}\texttt{$\backslash$username, username@domain} \newline This
instructs Postfix to deliver the mail to the local user, and also forward
it to the remote address; this is generally not a problem except that the
remote address in this case is the address the mail was originally sent to,
creating an infinite loop.  To prevent this happening Postfix examines the
Delivered-To header in the mail, and if the mail has already been delivered
to the current address it is bounced back to the sender with the error
message \texttt{mail forwarding loop for username@domain}.  Ordinarily this
works well, but unfortunately in this case the user noticed they had not
received any mail in a while and opted to send a test mail to themselves,
causing a loop not caught by Postfix:

\begin{enumerate}

    \item Postfix accepts a mail from username@domain, for username@domain.

    \item Postfix delivers the mail to the local mailbox and
        username@domain, as instructed by the user's forwarding
        instructions. The forwarded mail has a
        \texttt{Delivered-To:~username@domain} header added, and the
        envelope sender address is username@domain.  Log lines are added by
        \daemon{local}, \daemon{qmgr} (twice), \daemon{cleanup} and finally
        \daemon{pickup}.

    \item Postfix accepts the mail for username@domain, but while
        delivering it notices that the \texttt{Delivered-To} header already
        contains the address it is currently delivering to, and therefore
        sends a bounce notification with sender address \texttt{<>} to
        the original sender: username@domain.  Log lines are added by
        \daemon{local}, \daemon{qmgr} (twice) and \daemon{cleanup}.

    \item Postfix accepts the bounce notification and delivers it to both
        the local mailbox and to username@domain, as instructed by the
        user's forwarding instructions.  A
        \texttt{Delivered-To:~username@domain} header is added to the
        forwarded bounce notification, which now has an envelope sender
        address of username@domain.  Log lines are added by \daemon{local},
        \daemon{qmgr} (twice), \daemon{cleanup}, and finally
        \daemon{pickup}.

    \item Postfix accepts the forwarded bounce notification but while
        delivering the mail it notices that the \texttt{Delivered-To}
        header already contains the address currently being delivered to,
        and sends a bounce notification to the sender: username@domain.
        Log lines are added by \daemon{local}, \daemon{qmgr} (twice) and
        \daemon{cleanup}.

    \item Repeat from step two; this will continue indefinitely unless an
        administrator intervenes and deletes the appropriate mails from the
        queue.

\end{enumerate}

The sequence described above occurs extremely rapidly because Postfix does
not have to deliver the mail to an external system, so mails are delivered,
bounced and generated as fast as the disks can keep up, resulting in a huge
volume of log lines.

The vast majority of log lines when a mail loop occurs are from Postfix
components which have a small number of rules associated with them, whereas
in general \daemon{smtpd} adds the majority of log lines, and also has the
highest number of rules.  \daemon{smtpd} log lines are distributed across
rules much more evenly than the log lines of \daemon{qmgr}, \daemon{local},
\daemon{cleanup} or \daemon{pickup}, so the average number of rules
required to parse a \daemon{smtpd} log line is much higher that the average
number required to parse other log lines.

These two characteristics combine to reduce the average number or rules
required to parse a log line when there is a mail loop, as shown by the
peaks in \graphref{execution time vs file size vs number lines factor}.
When the rule ordering is reversed the majority of log lines generated by a
mail loop will be parsed with very few rules, whereas without a mail loop
the majority of log lines require a large number of rules; this leads to a
noticeable drop in the average time required to parse a log line, as shown
in \graphref{percentage increase of reversed over normal}.  The number of
rules which need to be consulted when the ordering is shuffled varies
between the optimal and the pessimal, and the performance varies
proportionally.

The difference between log files with a mail loop and log files without can
be seen in \tableref{Execution time increase for different rule
orderings} showing the increases for the different rule orderings and
combinations of log files:



\subsubsection{Caching regexes}

\label{Caching regexes}

The following graphs show the impact that not caching compiled \regexes{}
has on parser performance: on typical log files the execution time when not
caching compiled \regexes{} is 500--600\% of the execution time when
caching; reversing the perspective shows that cached execution time is
merely 17--20\% of non-cached execution time.  Caching compiled \regexes{}
is probably the single most effective optimisation possible in the parser's
implementation; given that it only required sixteen extra lines of
reasonably simple code,\footnote{The sixteen lines of code breaks down as
follows: three lines to add caching, eight lines of error checking and
reporting, three lines to optionally disable caching for debugging and
performance measurement, and two lines of comments.} the investment in time
was minimal.

\showgraph{build/plot-cached-discarded}{Regex: cached vs
discarded}{normal regex vs discard regex}

\showgraph{build/plot-cached-discarded-factor}{Regex caching: percentage
execution time increase}{normal regex vs discarded regex factor}

Two large dips can be seen in \graphref{normal regex vs discarded
regex factor} at log files 22 and 62--68, corresponding to the spikes in
log file size in graphs~\refwithpage{execution time vs file size vs number
of lines graph} and~\refwithpage{normal regex vs discard regex}.  The
reason for the anomalous log files has already been explained in
\sectionref{Why are there dips in the graphs?}.

The distribution of log lines across rules when there is a mail loop is
much different to the norm and the average number of rules consulted per
log line is much lower; this results in far fewer \regex{} compilations per
line than when there is not a mail loop, and a correspondingly decreased
execution time.  The unexpected outcome is that caching \regexes{} is
proportionally less important when the log file contents were created by a
mail loop.  The increases in execution time when not caching \regexes{} for
different combinations of log files are summarised in the table below:

\showtable{build/stats-cached-discarded-include-for-graph}
{Regex caching/discarding with different groups of log files}
{Regex caching/discarding with different groups of log files}


\subsubsection{Rule hits}
\label{rule hits}

The number of hits per rule is quite unevenly spread, resembling a Power
Law distribution~\cite{powerlaw}.

\showgraph{build/plot-hits}{Hits per rule}{rule hits graph}

As \graphref{rule hits graph} is quite difficult to read it has
been separated into three sections: low, middle and high.

\showgraph{build/plot-hits-low}{Hits per rule (low)}{hits per rule low}

It is apparent from the low graph (\graphref{hits per rule low})
that some rules have few or no hits; those with zero hits are rules which
were written to parse log files used during development of the parser but
not utilised in the test runs performed for this document.

\showgraph{build/plot-hits-middle}{Hits per rule (middle)}{hits per rule
middle}

\showgraph{build/plot-hits-high}{Hits per rule (high)}{hits per rule
high}

\clearpage



\subsubsection{Miscellaneous graphs}

\label{Miscellaneous graphs}

\showgraph{build/plot-mails-received}{Mails received per day}{Mails
received per day}

As expected there are far more mails received on weekdays than at weekends.
Note that this graph and the table below show the number of mails received
by \SMTP{} only; in particular the mail loops noticeable in other graphs
do not contribute to these figures.

\showtable{build/mails-received-include-for-graph}{Number of mails received
per day: statistics}{Number of mails received per day: statistics}

\showgraph{build/plot-action-distribution}{Distribution of rules per
action}{Distribution of rules per action}

\clearpage

\subsubsection{Summary}

The graphs presented in this section illustrate the topics discussed in
\sectionref{rule efficiency}.  The first collection of graphs are about
parser scalability, showing the linear relationship between execution time
and input size.  \sectionref{rule ordering graphs} demonstrates the effect
of rule ordering on execution time, and the unexpected consequences of
specific inputs.  The necessity of caching compiled \regexes{} is attested
to by the third group of graphs, where the difference between caching and
not caching is staggering.  The penultimate section contains a breakdown of
the rule hits accumulated during a single test run of the parser.
Miscellaneous graphs expected to be useful are collected in the final
section.  
