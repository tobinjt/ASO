% $Id$
\documentclass[a4paper,12pt,draft]{article}

% Useful stuff for math mode.
\usepackage{amstext}
% Wrapping of URLs.  This is used in the bibliography instead of \url{}.
\usepackage{url}
\DeclareUrlCommand\urlwrap{}
\newcommand{\urlwrapfill}[1]{\urlwrap{#1}\hfill}
% Include images
\usepackage[final]{graphicx}
% Change how nested enumerate environments are labelled.
\renewcommand{\labelenumii}{\roman{enumii}:}
\renewcommand{\refname}{Bibliography}
% Add the bibliography into the table of contents.
\usepackage[section,numbib,nottoc]{tocbibind}
% When creating a PDF make the table of contents into links to the pages
% (without horrible red borders) and include bookmarks.  The title and
% author don't work - I think either gnuplot or graphviz clobbers it.
\usepackage{hyperref}
\hypersetup{%
    pdftitle    = {Parsing Postfix log files},
    pdfauthor   = {John Tobin},
    final       = true,
    pdfborder   = {0 0 0},
%    breaklinks  = true,
}
\usepackage{breakurl}
    
\begin{document}

\title{Parsing Postfix log files}
\author{John Tobin \\ School of Computer Science and Statistics \\ 
Trinity College \\ Dublin 2 \\ Ireland \\ tobinjt@cs.tcd.ie}
\date{}
\maketitle

\begin{abstract}

    Parsing Postfix logs is much more difficult than it first appears but
    it is possible to achieve a high degree of accuracy in understanding
    the logs, and thus reconstructing the actions taken by Postfix to
    generate the logs.  This paper describes the creation of a parser,
    documenting the parsing algorithm and rules, explaining the
    difficulties encountered and the solutions developed, with reference to
    an implementation which stores data gleaned from the logs in an SQL
    database.  The gathered data can then be used to optimise current
    anti-spam measures, provide a baseline to test new anti-spam measures
    against, or to produce statistics showing how effective those measures
    are.

\end{abstract}

XXX WHAT IS THE CONCLUSION???  FIRST THING IS TO COMPARE TO OTHER PARSERS
REVIEWED

\newpage
\tableofcontents

\newpage
\section{Introduction}

\label{introduction}

Most mail server administrators will have performed some basic processing
of Postfix logs at one time or another, whether it was to debug a problem,
explain to a user why their mail is being rejected, or check whether their
new anti-spam measures are working.  The more adventurous will have
generated statistics to show how many hits each of their anti-spam measures
has gotten in the last week, and possibly even generated some graphs to
clearly illustrate the point to management or users.\footnote{This was the
author's first real foray into processing Postfix logs.}  Very few will
have performed in-depth parsing and analysis of their logs, where the
parsing must correlate the log lines per-connection or per-queueid rather
than processing lines independently.  One of the barriers to this type of
processing is the unstructured nature of Postfix logs, where each log line
was added on an ad hoc basis as a requirement was discovered or new
functionality was added.  Further complication arises because the set of
rejection messages is not fixed: new messages can be added by the
administrator with custom checks; every Real-time Black List returns a
different explanatory message; policy servers may log different messages
depending on the characteristics of the connection; there are many ways in
which the log lines may differ between servers.  This paper documents the
difficult process of parsing Postfix logs, presenting a program which
parses logs and places the resulting data into a database.  The gathered
data can then be used to optimise current anti-spam measures, provide a
baseline to test new anti-spam measures against, or to produce statistics
showing how effective those measures are.  There are numerous other uses
for such data: improving server performance by identifying troublesome
destinations and reconfiguring appropriately or dedicating transports to
those destinations; identifying regular high volume uses (e.g.\ customer
newsletters) and restricting those uses to off-peak times; detecting virus
outbreaks which propagate via email; as a base for billing customers on a
shared server.  Preserving the raw data enables users to develop a
multitude of uses far beyond those conceived of by the author.

\vspace{1em}\noindent\textbf{Layout of the paper:}

Section~\ref{background} provides background information useful in
understanding the paper, parser and algorithm.

Section~\ref{rules} discusses the parsing rules in detail, explaining their
structure, using an example rule and sample data it matches successfully
against, followed by a discussion of rule efficiency concerns.

Section~\ref{parsing-algorithm} contains the meat of the paper, describing
a naive parsing algorithm and the complications encountered which shaped
the full algorithm, followed by a comprehensive explanation of the
different stages of the algorithm, the actions taken during the execution
of the algorithm, and further complications whose solution completes the
algorithm.

Section~\ref{limitations-improvements} lists the limitations of the program
and/or the algorithm, then suggests some ways of dealing with the
limitations, with the goal of improving parsing and reproduction of the
journey a mail takes through Postfix.

Section~\ref{conclusion} describes the results of the project to date and
contains the paper's conclusion.

Section~\ref{other-parsers} discusses other parsers and why they were
deemed unsuitable for the task.

The bibliography contains references to the resources used in developing
the algorithm, writing the program and preparing this paper.  Also listed
are some additional resources expected to be helpful in understanding SMTP,
Postfix, anti-spam techniques, or the paper.

Section~\ref{graphs} contains graphs illustrating the topics discussed in
section~\ref{rule efficiency}, rule efficiency.

\section{Background}

\label{background}

\subsection{Motivation}

This paper and the program it describes are part of a larger project to
optimise a server's Postfix restrictions, generate statistics and graphs,
and provide a platform on which new restrictions can be trialled and
evaluated to see if they are beneficial in the fight against spam.  The
program parses Postfix logs and populates a database with the data gleaned
from those logs, providing a consistent and simple view of the logs which
future tools can utilise.  The gathered data can then be used to optimise
current anti-spam measures, provide a baseline to test new anti-spam
measures against, or to produce statistics showing how effective those
measures are.\footnote{Section~\ref{introduction} lists some more example
uses.}

A short example of the optimisation possible using data from the database
is which Postfix restrictions reject the highest number of mails:

\begin{verbatim}
SELECT name, description, restriction_name, hits_total
    FROM rules
    WHERE postfix_action = 'REJECTED'
    ORDER BY hits_total DESC;
\end{verbatim}

If the database supports sub-selects percentages can be
obtained:

\begin{verbatim}
SELECT name, description, restriction_name, hits_total,
        (hits_total * 100.0 /
            (SELECT SUM(hits_total)
                FROM rules
                WHERE postfix_action = 'REJECTED'
            )
        ) || '%' AS percentage
    FROM rules
    WHERE postfix_action = 'REJECTED'
    ORDER BY hits_total DESC;
\end{verbatim}

SQL note: $||$ is the concatenation operator in SQLite3; if the database
containing the extracted data does not support this syntax, then simply
remove `` $||$ '$\%$'\hspace{1ex}'' from the query --- the results will be
the same, just slightly less visually pleasing.

Another example is determining which restrictions are not effective: this
example shows which rules had less than 100 hits on the last log file
parsed.

\begin{verbatim}
SELECT name, description, restriction_name, hits,
        (hits * 100.0 /
            (SELECT SUM(hits)
                FROM rules
                WHERE postfix_action = 'REJECTED'
            )
        ) || '%' AS percentage
    FROM rules
    WHERE postfix_action = 'REJECTED'
        AND hits < 100
    ORDER BY hits ASC;
\end{verbatim}

\subsection{Database as Application Programming Interface}

The database populated by this program provides a simple interface to
Postfix logs.  Although the interface is a database schema, it is in effect
quite similar to the API provided by a library: it insulates both user and
provider of the API from changes in the implementation of the API\@.  The
algorithm implemented by the parser can be improved; support can be added
for earlier or later releases of Postfix; bugs can be fixed or limitations
removed from the parser.  Statistics and/or graphs can be generated from
the database; new restrictions can be tested and the results inspected;
trends in the fight against spam can emerge from historical data saved in
the database.  Using a database simplifies writing programs which need to
interact with the data in several ways:

\begin{enumerate}

    \item Libraries exist for the majority of programming languages
        allowing access to databases, whereas if this parser was written as
        a shared library a new interface layer would need to be written for
        every programming language using the API\@.

    \item Databases provide complex querying and sorting functionality to
        the user without requiring large amounts of programming.  All
        databases provide a program, of varying complexity and
        sophistication, which can be used for ad hoc queries with minimal
        investment of time.

    \item Databases are easily extensible, e.g.:
        
        \begin{itemize}

            \item Other tables can be added to the database, e.g.\ to cache
                historical data.

            \item New columns can be added to the tables used by the
                program, with sufficient DEFAULT clauses or a clever
                TRIGGER or two.

            \item A VIEW gives a custom arrangement of data with very
                little effort.

            \item If the database supports it, access can be granted on a
                fine-grained basis so that the finance department can
                produce invoices, the helpdesk can run limited queries as
                part of dealing with support calls, and the administrators
                have full access to the data.

            \item Triggers can be written to perform actions when certain
                events occur.  In pseudo-SQL\@:

\begin{verbatim}
CREATE TRIGGER ON INSERT INTO results
    WHERE sender = 'boss@example.com'
        AND postfix_action = 'REJECTED'
    SEND PANIC EMAIL TO 'postmaster@example.com';
\end{verbatim}

        \end{itemize}


    \item SQL is reasonably standard and many people will already be
        familiar with it; for those unfamiliar with it there are already
        lots of readily available resources from which to learn.  Although
        every vendor implements a different dialect of SQL, the basics are
        the same everywhere.

\end{enumerate}




\subsection{Database schema}
\label{database schema}

XXX SHOULD THIS SECTION BE MOVED ELSEWHERE\@?

The database schema can be conceptually divided in two: the rules which are
used to parse log files, and the data saved from the parsing of log files.
Rules have the fields required to parse the log lines, extract data to be
saved, and the action to be executed; they also have several fields which
aid the user in understanding what each rule parses.  The rules are
described in detail in section~\ref{rules} and the fields in particular in
section~\ref{rule attributes}, and that discussion won't be repeated here.
The data saved from parsing the logs is divided into two tables described
below: connections and results.

\subsubsection{Connections table}

Every connection and/or mail will have a single entry in the connections
table containing the following fields:

\begin{description}

    \item [id] This field uniquely identifies the row.

    \item [server\_ip] The IP address (IPv4 or IPv6) of the server: the
        local server when receiving mail, the remote server when sending
        mail.

    \item [server\_hostname] The hostname of the server, it will be
        \textit{unknown\/} if the IP address could not be resolved to a
        hostname via DNS\@.

    \item [client\_ip] The client IP address (IPv4 or IPv6): the remote
        server when receiving mail, the local server when sending mail.

    \item [client\_hostname] The hostname of the client, it will be
        \textit{unknown\/} if the IP address could not be resolved to a
        hostname via DNS\@.

    \item [helo] The hostname used in the HELO command.  The HELO
        occasionally changes during a connection, presumably because spam
        or virus senders think it's a good idea.  By default Postfix only
        logs the HELO when it rejects an SMTP command, but it is quite easy
        to rectify this: 

\label{logging helo}

        \begin{enumerate}

            \item Create \texttt{/etc/postfix/log\_helo.pcre}
                containing:\newline
                \hspace*{2em}\texttt{/./~~~~WARN~Logging~HELO}

            \item Modify \texttt{smtpd\_helo\_restrictions} in
                \texttt{/etc/postfix/main.cf} to contain\newline
                \hspace*{2em}\texttt{check\_helo\_access~/etc/postfix/log\_helo.pcre}

        \end{enumerate}

        The purpose of logging the HELO in
        \texttt{smtpd\_helo\_restrictions} is to log only once per mail; it
        also seems like the natural place.

    \item [queueid] The queueid of the mail if the conection represents an
        accepted mail, or \textit{NOQUEUE\/} otherwise.

    \item [start] The timestamp of the first log line, in seconds since the
        epoch.

    \item [end] The timestamp of the last log line, in seconds since the
        epoch.

\end{description}

\subsubsection{Results table}

Every log line corresponding to Postfix performing an action other than
INFO or IGNORED will have an entry in the results table, e.g.\ rejecting an
SMTP command, delivering a mail, or bouncing a mail.  Any log line where
the Postfix action is INFO is not interesting in and of itself, but
provides additional information which will be saved with other results;
IGNORED log lines don't even provide useful information.

\begin{description}

    \item [connection\_id] A reference to the entry in the connections
        table this results table entry belongs to.

    \item [rule\_id] A reference to the entry in the rules table which
        matched the log line and created this result.

    \item [warning] Postfix can be configured to log a warning instead of
        enforcing a restriction that would reject an SMTP command --- a
        facility that is quite useful for testing new restrictions.  This
        field will be 1 if the log line parsed was a warning rather than a
        real rejection, or 0 for a real rejection or any other result.

    \item [smtp\_code] The SMTP code associated with the log line.  In
        general an SMTP code is only present for a rejection or final
        delivery; results missing an SMTP code will duplicate the SMTP code
        of other results in the connection.  Some final delivery log lines
        don't contain an SMTP code: in those cases the code is faked based
        on the success or failure represented by the log line.

    \item [sender] The sender's email address.  This can change during one
        single connection, when the connection is reused to send multiple
        mails.

    \item [recipient] The recipient address.

    \item [message\_id] The message-id of the accepted mail, or NULL if no
        mail was accepted.

    \item [data] A field available for anything not covered by other
        fields, e.g.\ the rejection message from an RBL\@.

    \item [timestamp] The time the line was logged, in seconds since the
        epoch.

\end{description}

\subsection{SMTP background}
\label{smtp background}

The Simple Mail Transfer Protocol, originally defined in RFC
821~\cite{RFC821} and updated in RFC 2821~\cite{RFC2821}, is used for
transferring mail between Mail Transport Agents (MTAs).  It is a simple,
human readable, plain text protocol, making it quite easy to test and debug
problems with it.  Despite the simplicity many virus and/or spam sending
programs fail to implement it properly, so requiring strict adherence to
the protocol specification is beneficial in protecting against spam and
viruses.\footnote{This violates the principle of \textit{Be liberal in what
you accept, and conservative in what you send\/} from RFC
760~\cite{rfc760}, but unfortunately that principle was written in a
friendlier time.}  A typical SMTP conversation resembles the following (the
lines starting with a three digit number are sent by the server, all other
lines are sent by the client):

\begin{verbatim}
220 smtp.example.com ESMTP
HELO client.example.com
250 smtp.example.com
MAIL FROM: <alice@example.com>
250 2.1.0 Ok
RCPT TO: <bob@example.com>
250 2.1.5 Ok
DATA
354 End data with <CR><LF>.<CR><LF>
Message headers and body sent here.
.
250 2.0.0 Ok: queued as D7AFA38BA
QUIT
221 2.0.0 Bye
\end{verbatim}

A detailed description of SMTP is beyond the scope of this document:
introductory guides can be found at~\cite{smtp-intro-01, smtp-intro-02}.

\subsection{Postfix background}

Postfix is a highly configurable, high performance, secure and scalable
Mail Transport Agent.  It features extensive and extensible optional
anti-spam restrictions, allowing an administrator to deploy those
restrictions which they judge suitable for their site's needs, rather than
a fixed set chosen by Postfix's author.  These restrictions can be
selectively applied, combined and bypassed on a per-client, per-recipient
or per-sender basis, allowing varying levels of severity and/or
permissiveness.  Postfix leverages simple lookup tables to support
arbitrarily complicated user-defined sequences of restrictions and
exceptions, with policy servers\footnote{Policy servers will be explained
shortly.} as the ultimate in flexibility.  Administrators can also supply
their own rejection messages to make it clear to senders why exactly their
mail was rejected.  Unfortunately this flexibility has a cost: complexity
in the logs generated.  While it is easy to use \texttt{grep(1)} to
determine the fate of an individual email, following the journey an email
takes through Postfix can be quite difficult.  The logs tend to follow a
90\%-10\% pattern: 90\% of the time the journey is simple, but the other
10\% of the time requires 90\% of the code.\footnote{These numbers don't
have a solid scientific basis, they're based on gut feeling from writing
and debugging the software.}

\subsubsection{Mixing and matching Postfix restrictions}

Postfix restrictions are documented fully in~\cite{smtpd_access_readme,
smtpd_per_user_control, policy-servers}, the following is a brief overview.

Postfix uses several restriction lists (each containing zero or more
restrictions) to decide the fate of an SMTP command, one list for each
stage of the SMTP conversation: client connection, HELO command, MAIL FROM
command, RCPT TO command (possibly multiple times), DATA command, and end
of data.  By default restriction lists will not be evaluated until the
first RCPT TO command is received, because some clients don't deal with
earlier rejections properly; a side benefit of this delay is that Postfix
has more information available when logging the rejection.\footnote{The
\texttt{smtpd\_delay\_reject} parameter controls this behaviour.}  Postfix
uses simple lookup tables as the deciding factor for some restrictions,
e.g.\ \texttt{check\_client\_access~cidr:/etc/postfix/client\_access}
\footnote{\texttt{cidr} is the type of the lookup table.} will check
whether the IP address of the connected client matches the left hand side
of each line in the file and will return the right hand side of the first
matching line.  Other restrictions determine their result in different
ways, e.g.\ \texttt{reject\_rbl\_client rbl.example.com} checks whether the
IP address of the client is present in the listed RBL\@.

Each restriction is evaluated to produce a result of \textit{reject},
\textit{permit}, \textit{dunno\/} or the name of another restriction to be
evaluated.\footnote{Other results are possible, for full details
see~\cite{smtpd_access_readme, smtpd_per_user_control, policy-servers}.}
The meaning of \textit{permit\/} and \textit{reject\/} is fairly obvious;
\textit{dunno\/} means to stop evaluating the current restriction and
continue with the next restriction in the list, allowing more specific
cases to be handled differently from more general cases.  When the result
is another restriction Postfix will evaluate the new restriction, allowing
restrictions to be chosen based on the client IP address, HELO hostname,
sender address, recipient address, etc.  The administrator can define new
restrictions as a list of existing restrictions, allowing arbitrarily long
and complex sequences of lookups and restrictions.

This description is necessarily brief, for further details
see~\cite{smtpd_access_readme, smtpd_per_user_control, policy-servers}.


\subsubsection{Policy servers}

A policy server~\cite{policy-servers} is an external program that accepts
state information from Postfix for each SMTP command not rejected by an
earlier restriction; the policy server can utilise that state information
to implement whatever logic is required.  E.g.\ some users can be
restricted to sending mail on the third Tuesday after pay day only --- this
example may actually be useful in a payroll system to prevent problems from
spam or (worse) phishing mails with faked sender addresses.  More commonly
encountered scenarios are:

\begin{itemize}

    \item Checking Sender Policy Framework records~\cite{openspf,
        wikipedia-spf}.  SPF records specify which mail servers are allowed
        to send mail claiming to be from a particular domain.  The
        intention is to reduce spam from faked sender addresses,
        backscatter~\cite{postfix-backscatter} and
        joe-jobs~\cite{wikipedia-joe-job}; however there has been a lot of
        resistance to the proposal because it breaks or vastly complicates
        some features of SMTP\@.

    \item Greylisting~\cite{greylisting} is a technique that temporarily
        rejects mail when the triple of (sender, recipient, remote IP
        address) is unknown; on the second and subsequent delivery attempts
        from that triple the mail will be accepted.  The assumption is that
        retrying after a temporary failure is uneconomical for a spammer,
        but that a legitimate mail server must retry.  Sadly spammers are
        using increasingly complex and well written programs to distribute
        spam, frequently using an ISP provided SMTP server from a
        compromised machine on the ISP's network.  Greylisting is slowly
        becoming less useful, but it does block a large percentage of spam
        mail at the moment.

    \item Using a scoring system such as
        Policyd-weight~\cite{policyd-weight} where tests accumulate points
        against the sending system --- if the eventual score is too high
        the mail is rejected.
        
    \item Rate limiting or throttling on a per-sender, per-client or
        per-recipient basis as performed by Policyd~\cite{policyd}.

\end{itemize}

Example attributes taken from~\cite{policy-servers}:

\begin{tabular}[]{ll}

    request                 & smtpd\_access\_policy     \\
    protocol\_state         & RCPT                      \\
    protocol\_name          & SMTP                      \\
    helo\_name              & some.domain.tld           \\
    queue\_id               & 8045F2AB23                \\
    sender                  & foo@bar.tld               \\
    recipient               & bar@foo.tld               \\
    recipient\_count        & 0                         \\
    client\_address         & 1.2.3.4                   \\
    client\_name            & another.domain.tld        \\
    reverse\_client\_name   & another.domain.tld        \\
    instance                & 123.456.7                 \\

\end{tabular}



\subsection{Parser background}

XXX REWRITE THIS

Before getting into detail about the parser a brief overview is in order.
The parser is split into two parts: the parsing algorithm and the rules
which are applied to the lines.  Rules can be thought of as the Lex part of
a Lex and YACC style parser: rules identify the line and return some data,
which the algorithm (the YACC part) receives and uses when performing the
requested action.  Rules are solely concerned with identifying a line and
extracting data from it, whereas the algorithm's task is to follow the
journey each mail takes through Postfix, piecing the data together into a
coherent whole, saving it in a useful and consistent form, and performing
housekeeping duties.

\subsection{Assumptions}

The algorithm described and the program implementing it make a small number
of (hopefully safe and reasonable) assumptions:

\begin{itemize}

    \item The logs are whole and complete: nothing has been removed, either
        deliberately or accidentally (e.g.\ log rotation gone awry, file
        system filling up, logging system unable to cope with the volume of
        logs).

    \item Postfix logs sufficient information to make it possible to
        accurately reconstruct the actions it has taken.

    \item The Postfix queue has not been tampered with, causing unexplained
        appearance or disappearance of mail.

\end{itemize}

In some ways this task is similar to reverse engineering or replicating a
black box program based solely on its inputs and outputs.  Although the
source code is available, reading and understanding it would require a
significant investment of time:

\begin{tabular}[]{llll}

    Postfix 2.2.11  & Postfix 2.3.8   & Postfix 2.4.0 &                   \\
    71548           & 82224           & 83965         & lines of code     \\
    60962           & 67146           & 68675         & lines of comments \\
    16117           & 17647           & 18069         & lines are blank   \\
    148627          & 167017          & 170709        & lines in total    \\

\end{tabular}


\subsection{Other parsers}

10 other parsers have been reviewed in Appendix~\ref{other-parsers} as part
of the background research for this project.  None of the reviewed parsers
perform the type of advanced parsing and log correlation described here;
all are intended to perform a specific parsing and reporting task, rather
than be a generic parser, extracting data and leaving generation of reports
from the data to other programs.  Some parsers save the data extracted to a
database but the majority discard all data once they have finished running,
making historical analysis impossible.  The other parsers reviewed all
produce a report of greater or lesser complexity and detail, whereas the
program described here doesn't attempt to produce a report at all; that
responsibility is deferred to a separate program, to be developed later.
The parsing algorithm and program described here are designed to enable
much more detailed log analysis by providing a stable platform for
subsequent programs to develop upon.



\subsection{Conventions used in the paper}

The words \textit{connection\/} and \textit{mail\/} are often used
interchangeably in this paper; in general the word used was chosen based on
the context it appears in.

\subsection{Previous research in this area}

There only appears to be one prior paper published about parsing Postfix
log files: \textit{Log Mail Analyzer: Architecture and Practical
Utilizations\/}~\cite{log-mail-analyser}.  The aim of Log Mail Analyzer is
quite different from this parser: it attempts to present correlated log
data in a form suitable for a systems administrator to search using
grep(1), producing a CSV file\footnote{Comma-Separated Value, a very simple
format where a record is stored in a single line, with fields separated by
a comma or other punctuation symbol.  Problems with CSV files include the
difficulty in escaping separators, providing multiple values for a field
(e.g.\ multiple recipients), adding new fields, and lack of a bundled
schema describing the fields.} which can subsequently be imported into a
MySQL or Berkeley DB database.  The data stored is limited to time and
date, hostname and IP address of client, sender and recipient addresses,
and the SMTP code.  The decision to support both CSV and Berkeley DB
appears to have been a limiting factor: the values which can be stored are
limited, it's difficult to link different records (foreign keys in SQL
enable this easily), and ad-hoc queries require extensive code to be
written.  Berkeley DB is not an SQL database, it only supports (key, value)
pairs; in the main table the value used is a CSV line, and the key is an
integer referred to by other tables.  The secondary by-sender,
by-recipient, by-date and by-IP tables use the sender/recipient/date/IP as
the key, and a CSV list of integers referring to the main table is used as
the value.  In effect this re-implements SQL foreign keys, but without the
full functionality offered by even the most basic of SQL databases.  It
also requires custom code to search on some combination of the above.

Handling of multiple recipients, SMTP codes or remote servers\footnote{A
single mail may be sent to multiple remote servers if it was addressed to
recipients in different domains, or Postfix needs to try multiple servers
for one or more recipients.} is not explained in the paper, but the options
appear to be:

\begin{enumerate}

    \item Discard some of the data; this option is obviously undesirable,
        and presumably was not chosen by the LMA authors.

    \item Combine all data into a CSV line, which is then embedded into the
        main CSV line for that mail, making searching even more difficult
        and requiring a second separating character.

\end{enumerate}

The schema used with the MySQL database is undocumented, but at least it's
possible to discover the schema with an SQL database, unlike with Berkeley
DB\@.

Parsing is hand-coded and requires major changes to the code to parse new
log lines or extract additional data.  It does not appear to deal with any
of the complications discussed in
sections~\ref{complications}~and~\ref{additional complications}.  The
program fails to parse Postfix 2.2.x or 2.3.x log files; indeed it produces
a number of warnings and no usable output.

In summary Log Mail Analyser appears to be a proof of concept, written to
demonstrate the point of the paper (that having this information in an
accessible fashion is useful to systems administrators), rather than a
program designed to be extensible and useful in a production environment.

XXX EXPLAIN WHY ASO IS MORE COMPREHENSIVE, WHAT IT DOES BETTER AND WHY\@.
MAKE THIS A COMPARISON INSTEAD OF A CRITIQUE\@.

% Literature review notes:
%
% Hard-coded parsing, requiring code changes to add more.  Attempts to
% correlate log lines, saves data to database for data mining purposes.
% Hard to extend/expand/understand.  Appears to only save: date and hour,
% DNS name and IP address host, mail server IP address, sender, receiver
% and e-mail status (sent, rejected).  Undocumented schema.  Design
% decision to use CSV as an intermediate format between the log file and
% the database seems to have been restrictive.  Appears to require a
% queueid but majority of log entries (e.g. rejections) lack a queueid.
% Supports whitelisting IP addresses when parsing logs, but whitelisting
% when generating reports/data mining would be preferable.  Supporting
% Berkeley DB is probably limiting the software - an example is the
% difficulty in searching a pipe-delimited string, so they have
% re-implemented foreign keys with tables keyed by ip address etc. pointing
% at the main table - this also won't scale well.  There doesn't appear to
% be any attempt to deal with the complications I've encountered: their
% parsing isn't detailed enough to encounter them.  It doesn't run
% properly; doesn't create any output; throws up errors.

\section{Adaptable parsing: user defined rules}

\label{rules}

The complexity and variation in Postfix's logs requires similar flexibility
in the parser; decoupling the parsing rules from the associated actions
allows new rules to be written and tested without requiring modifications
to the algorithm source code (significantly lowering the barrier to entry
for new or casual users), and greatly simplifies both algorithm and rules.
It also creates a clear separation of functionality: rules handle low level
details of identifying log lines and extracting data from a line, whereas
the algorithm handles the higher level details of following the path a mail
takes through Postfix, assembling the required data, etc.

Rule have certain characteristics which may help in understanding the
parser:

\begin{itemize}

    \item The first matching rule wins: no further rules are tried against
        that line, but there is a facility for specifying the order of
        rules so that more specific rules can be tried first.

    \item Rules are completely self-contained and can be understood in
        isolation, without reference to any other rules.

    \item There are no sub-rules, so rules have linear computational
        complexity.

\end{itemize}

\label{comparison against context-free grammars}

In context-free grammar terms the parser rules could be described as:

$\text{\textless{}log-line\textgreater{}} \mapsto \text{rule-1} |
\text{rule-2} | \text{rule-3} | \dots | \text{rule-n}$


\subsection{Rule attributes}

\label{rule attributes}

Each rule defines the following:

\begin{description}

    \item [name] A short name for the rule.

    \item [description] Something must have occurred to cause Postfix to
        log each line (e.g.\ a remote client connecting causes a connection
        line to be logged).  This field describes the action causing the
        log lines this rule matches.

    \item [restriction\_name] The restriction which caused the mail to be
        rejected.  Only applicable to rules which have a result of
        \texttt{rejected}, other rules will have an empty string.

    \item [postfix\_action] This is the action Postfix must have taken to
        generate this line, with two exceptions:

        \begin{itemize}

            \item [INFO] Represents an unspecified intermediate action that
                the parser is not interested in per se, but which does log
                useful information, supplementing other log lines.

            \item [IGNORED] An action which is not only uninteresting in
                itself, but which also provides no useful data.

        \end{itemize}

        Uninteresting lines are parsed so that any lines the parser isn't
        capable of handling become immediately obvious errors.

    \item [program] The program (\texttt{postfix/smtpd},
        \texttt{postfix/qmgr}, etc.) whose log lines the rule applies to.
        This avoids needlessly trying rules which won't match the line, or
        worse, might match unintentionally.  Rules whose program is
        \texttt{*} will be tried against any lines which aren't parsed by
        program specific rules.

    \item [regex] The regular expression\cite{wikipedia-regex, perlre} to
        match the log line against.  The regex will first have several
        keywords expanded: this simplifies reading and writing rules;
        avoids needless repetition of complex regex components; allows the
        components to be corrected and/or improved in one location; and
        makes each regex largely self-documenting.
        
        The following keywords are expanded (full explanations can be found
        in the source code):

        \_\_SENDER\_\_, \_\_RECIPIENT\_\_, \_\_MESSAGE\_ID\_\_,
        \_\_HELO\_\_, \newline \_\_EMAIL\_\_, \_\_HOSTNAME\_\_, \_\_IP\_\_,
        \_\_IPv4\_\_, \_\_IPv6\_\_, \newline \_\_SMTP\_CODE\_\_,
        \_\_RESTRICTION\_START\_\_, \_\_QUEUEID\_\_, \newline
        \_\_COMMAND\_\_, \_\_SHORT\_CMD\_\_, \_\_DELAYS\_\_, \_\_DELAY\_\_,
        \_\_DSN\_\_ and \_\_CONN\_USE\_\_.

        Additional fields are captured by \_\_RESTRICTION\_START\_\_, so
        rules using it will start the fields in result\_cols,
        connection\_cols, etc.\ at 5.

        For efficiency the keywords are expanded and every rule's regex is
        compiled before attempting to parse the log file --- otherwise each
        regex would be recompiled each time it was used, resulting in a
        large, data dependent slowdown.  Rule efficiency concerns are
        discussed in section~\ref{rule efficiency}.

    \item [result\_cols, connection\_cols] Specifies how the fields in the
        log line will be extracted.  The format is: \newline
        \texttt{smtp\_code = 1; recipient = 2, sender = 4;} \newline
        i.e.\ semi-colon or comma separated assignment statements, with the
        variable name on the left and the matching field from the regex on
        the right hand side.  The list of acceptable variable names is:

        \texttt{connection\_cols: client\_hostname, client\_ip, server\_ip,
        \newline \hspace*{2em} server\_hostname and helo.\newline}
        \texttt{result\_cols: sender, recipient, smtp\_code, message\_id,
        \newline \hspace*{2em} pid\_regex, data and child.}

    \item [result\_data, connection\_data] Sometimes rules need to supply a
        piece of data which isn't present in the log line: e.g.\ setting
        \texttt{smtp\_code} when mail is accepted.  The format and allowed
        variables are the same as for \texttt{result\_cols} and
        \texttt{connection\_cols}, except that arbitrary
        data\footnote{Commas and semi-colons cannot be escaped and thus
        cannot be used.  This is intended for use with small amounts of
        data rather than large, so dealing with escape sequences seemed
        unnecessary.} is permitted on the right hand side of the
        assignment.

    \item [action] The action the algorithm will take; a full list can be
        found in Section~\ref{actions-in-detail}.

    \item [queueid] Specifies the matching field from the regex which gives
        the queueid, or zero if the log line doesn't contain a queueid.

    \item [hits] is an efficiency measure.  This counter is maintained for
        every rule and incremented each time the rule successfully matches.
        At the start of each run the program sorts the rules in descending
        order of hits, and at the end of the run updates every rule's hits.
        Assuming that the distribution of log lines is reasonably
        consistent between log files, rules matching more commonly
        occurring log lines will be tried before rules matching less
        commonly occurring log lines, lowering the program's execution
        time.  Rule ordering for efficiency is discussed in
        section~\ref{rule ordering for efficiency}.

    \item [hits\_total] The total number of hits for this rule over all
        runs of the parser.

    \item [priority] This is the user-configurable companion to hits: rules
        will be tried in order of priority, overriding hits.  This allows
        more specific rules to take precedence over more general rules
        (described in section~\ref{overlapping rules}).

\end{description}


\subsection{Overlapping rules}

\label{overlapping rules}

The parser does not try to detect overlapping rules;\footnote{It may be
possible to parse each rule's regex, generate an NFA, and compare all NFAs
to determine if any overlap.  The author has not attempted to do this: such
a project would probably qualify for a PhD, and may involve solving the
Halting Problem and circumventing the Church-Turing Thesis.} that
responsibility is left to the author of the rules.  Unintentionally
overlapping rules lead to inconsistent parsing and data extraction because
the order in which rules are tried against each line determines which rule
matches.  Overlapping rules are frequently a requirement, allowing a more
specific rule to match some lines and a more general rule to match the
majority, e.g.\ separating smtp delivery to a filtering proxy from smtp
delivery to the rest of the world.  The algorithm provides a facility for
ordering overlapping rules: the priority field in each rule (defaults to
zero).  Rules are tried in order of priority, highest first, overriding the
hits field maintained by the parser.  Negative priorities may also be
useful for catchall rules.

Detecting overlapping rules is difficult, but the following may be helpful:

\begin{itemize}

    \item Sort the regexs and visually inspect the list, e.g.\ with SQL
        similar to: \textbf{select regex from rules order by regex;}

    \item Compare the results of parsing using sorted, shuffled and
        reversed rules.\footnote{See section~\ref{rule efficiency} for more
        details of sorting the rules.}  Parse a number of log files using
        normal sorting, dump a textual representation of the connections
        and results tables, and delete everything from those tables.
        Repeat with shuffled and reversed sorting.  If there are no
        overlapping rules the tables from each run will be identical;
        differences indicate overlapping rules, and the rule\_ids which
        differ determine which rules overlap.  Unfortunately this method
        cannot prove the absence of overlapping rules; it can detect
        overlapping rules --- but only if there are log lines in the input
        files which match more than one rule.

\end{itemize}

\subsection{Example rule}

This example rule matches the message logged by Postfix when it rejects
mail from a sender address where the domain has neither an MX record nor an
A record, i.e.\ mail could not be delivered to the sender's address --- for
full details see~\cite{reject-unknown-sender-domain}.

This rule would match the following log line:

\begin{verbatim}
NOQUEUE: reject: RCPT from example.com[10.1.1.1]: 
  550 <foo@example.com>: Sender address rejected:
  Domain not found; from=<foo@example.com>
  to=<info@example.net> proto=SMTP
  helo=<smtp.example.com>
\end{verbatim}

% Don't reformat this!
\begin{tabular}[]{ll}

\textbf{Field}      & \textbf{Value}                                    \\
name                & Unknown sender domain                             \\
description         & We do not accept mail from unknown domains        \\
restriction\_name   & reject\_unknown\_sender\_domain                   \\
postfix\_action     & REJECTED                                          \\
program             & postfix/smtpd                                     \\
regex               & \verb!^__RESTRICTION_START__ <(__SENDER__)>: !    \\
                    & \verb!Sender address rejected: Domain not found;! \\
                    & \verb!from=<\5> to=<(__RECIPIENT__)> !            \\
                    & \verb!proto=E?SMTP helo=<(__HELO__)>$!            \\
result\_cols        & recipient = 6; sender = 5                         \\
connection\_cols    & helo = 7                                          \\
result\_data        &                                                   \\
connection\_data    &                                                   \\
action              & REJECTION                                         \\
queueid             & 1                                                 \\
hits                & 0                                                 \\
hits\_total         & 0                                                 \\
priority            & 0                                                 \\

\end{tabular}

\vspace{1em}

The various fields are used as follows;

\begin{description}

    \item [name, description, restriction\_name and postfix\_action:] are
        not \newline used by the algorithm, they serve to document the rule
        for the user's benefit.

    \item [program and regex:] If the program in the rule equals the
        program which logged the line the regex will be matched against the
        line; if the match is successful the action will be executed, if not
        the next rule will be tried.  If the program-specific rules don't
        match the log line, the generic rules will be tried in the same
        way.

    \item [action:] will be executed if the regex matches successfully (see
        section~\ref{actions-in-detail} for full details).

    \item [result\_cols, connection\_cols, result\_data and
        connection\_data:] are \newline used by the action to extract and
        save data matched by the regex.

    \item [queueid:] is used the extract the queueid matched by the regex,
        so that the correct mail can be found by queueid and actions
        performed on it.

    \item [hits, hits\_total and priority:] hits and priority are used in
        ordering the rules (see section~\ref{rule ordering for
        efficiency}); hits\_total is updated at the end of each run but is
        otherwise unused.

\end{description}

Additional fields are captured by \_\_RESTRICTION\_START\_\_, hence the
fields in result\_cols and connection\_cols start at 5 in the example.

        
\subsection{Rule efficiency}

\label{rule efficiency}

Parsing efficiency is an obvious concern when the parser routinely needs to
deal with 75 MB log files containing 300,000 log lines (for a small mail
server --- large scale mail servers would have much larger log files on a
daily basis).  When generating the data for the graphs included in
section~\ref{graphs}, 93 log files (totaling 10.08 GB, 60.72 million lines)
were each parsed 10 times, the first run discarded and the remaining 9 runs
averaged.  The first run is discarded for two reasons:

\begin{enumerate}

    \item The execution time will be higher because the log file must be
        read from disk, whereas for subsequent runs the log file will be
        cached in memory by the operating system.

    \item The execution time will also be higher because the rule ordering
        will be sub-optimal compared to subsequent runs.

\end{enumerate}

Saving results to the database was disabled for the test runs, as that
dominates the run time of the program, and the tests are aimed at measuring
the speed of the parser rather than the speed of the database.

\subsubsection{Algorithmic complexity}

An important property of a parser is how execution time scales with input
size: does it scale linearly, polynomially, or exponentially?
Graph~\ref{execution time vs file size vs number lines} shows the execution
time in seconds, file size in MB and tens of thousands of lines per log
file per log file.  All three lines run roughly in parallel, giving a
visual impression that the algorithm scales linearly with input size.  This
impression is borne out by graph~\ref{execution time vs file size vs number
lines factor}, which plots the ratio of file size vs execution time and
number of lines vs execution time.  As the reader can see, the ratios are
quite tightly banded, showing that the algorithm scales linearly: the much
larger log files between points 60 and 70 on the X axis in
graph~\ref{execution time vs file size vs number lines} don't cause any
abnormality in the corresponding points in graph~\ref{execution time vs
file size vs number lines factor}.


\subsubsection{Rule ordering for efficiency}

\label{rule ordering for efficiency}

Rule ordering was mentioned in section~\ref{rule attributes} and will be
covered in greater detail in this section.  At the time of writing there
are 139 different rules, with the top 10\% matching the vast majority of
the log lines, and the remaining log lines split across the other 90\% of
the rules (as shown in graph~\ref{rule hits}).  Assuming that the
distribution of log lines is reasonably steady over time, program
efficiency should benefit from trying more frequently matching rules before
those which match less frequently.  To test this hypothesis three full test
runs were performed with different rule orderings:

\begin{description}

    \item [normal]  The most optimal order, according to the hypothesis:
        rules which match most often will be tried first.

    \item [shuffle] Random ordering --- the rules will be shuffled once
        before use and will retain that ordering for the entirety of the
        log file.  Note that the ordering will change every time the parser
        is executed, so 10 different orderings will be generated for each
        log file in the test run.  This is intended to represent an
        unsorted rule set.

    \item [reverse] Hypothetically the least optimal order: the most
        frequently matching rules will be tried last.

\end{description}


The resulting execution times are plotted in graph~\ref{normal vs shuffled
vs reversed ordering}, where the difference is not readily apparent, but
graph~\ref{normal vs shuffled vs reversed ordering factor} shows the
percentage increase of execution times:

\begin{description}

    \item [shuffled] 6--11\% increase; mean 8.34, standard deviation 2.44.

    \item [reversed] 15--22\% increase; mean 17.97, standard deviation
        2.94.

\end{description}

Overall this increases the parser's efficiency by 5.5--10\%; a modest but
worthwhile increase.

\subsubsection{Caching each regex}

Perl compiles the original regex into an internal representation,
optimising the regex to improve the speed of matching, but this compilation
and optimisation takes CPU time.  Perl automatically caches static regexs,
but dynamic regexs need to be explicitly compiled and cached.
Graph~\ref{normal regex vs discard regex} shows execution times with and
without caching the regex; also shown on the graph are the file sizes and
number of lines in each file.  Caching the compiled regexs is obviously far
more efficient; graph~\ref{normal regex vs disarded regex factor} shows the
percentage execution time increase when not caching each regex is between
350--570\% (mean 465.58\%, standard deviation 114.70\%).  


\section{Parsing algorithm}

\label{parsing-algorithm}

While the rules have more lines of code than the algorithm, the rules are
quite simple and each rule is completely independent of its fellows.  The
algorithm is significantly more complicated and highly internally
interdependent.


\subsection{A naive approach}

A high level view of the algorithm could be expressed as:

\begin{enumerate}

    \item Mail enters the system via SMTP or local submission.

    \item If the mail is rejected, log all data and finish.

    \item Follow the progress of the accepted mail until it's either
        delivered, bounced or deleted, log all data, and finish.

\end{enumerate}

Unfortunately it's not that straightforward.


\subsection{Complications encountered}

\label{complications}


\subsubsection{Queueid vs pid}

The mail lacks a queueid until it has been accepted, so log lines must
first be correlated by the smtpd pid, then transition to being correlated
by the queueid.  This is relatively minor, but does require:

\begin{itemize}

    \item Two versions of several functions: \texttt{by\_pid} and
        \texttt{by\_queueid}.

    \item Two state tables to hold the data structure for each connection.

    \item Most importantly: every section of code must know whether it
        needs to lookup the data structures by pid or queueid.

\end{itemize}

\subsubsection{Connection reuse}

Multiple independent mails may be delivered during one connection: this
requires cloning the current data as soon as a mail is accepted, so that
subsequent mails won't trample over each other's data.  This must be done
every time a mail is accepted, as it's impossible to tell in advance which
connections will accept multiple mails.  It is quite easy to overlook this
complication because only a small minority of connections accept more than
one mail. Happily once the mail has been accepted log entries won't be
correlated by pid for that mail any more (its queueid will be used
instead), so there isn't any ambiguity about which mail a given log line
belongs to.\footnote{Unfortunately this statement is not completely
accurate: see section~\ref{timeouts-during-data-phase} for details.
However in general there isn't any ambiguity about which data structure
should be used for a given log line.}  The original connection will be
discarded unsaved when the client disconnects if there haven't been any
rejections.  One unsolved difficulty is
distinguishing between different groups of rejections, e.g.\ when dealing
with the following sequence:

\begin{enumerate}

    \item The client attempts to deliver a mail, but it is rejected.

    \item The client issues the RSET command to reset the session.

    \item The client attempts to deliver another mail, likewise rejected.

\end{enumerate}

There should probably be two different entires in the database resulting
from the above sequence, but currently there will only be one.



\subsubsection{Re-injected mails}

The most difficult complication initially encountered is that locally
addressed mails are not always delivered directly to a mailbox: sometimes
they are accepted for a local address but need to be delivered to one or
more remote addresses due to aliases.  When this occurs a child mail will
be injected into the postfix queue, but without the explicit logging smtpd
or sendmail injected mails have.  Thus the source is not immediately
discernible from the log line in which the mail first appears; from a
strictly chronological reading of the logs it \textit{usually\/} appears as
if the child mail has appeared from thin air.  Subsequently the parent mail
will log the creation of the child mail:

\texttt{3FF7C4317: to=<username@example.com>, relay=local, \newline 
delay=0, status=sent (forwarded as 56F5B43FD)}

Unfortunately while all log lines from an individual process appear in
chronological order, the order in which log lines from different processes
are interleaved is subject to the vagaries of process scheduling.  In
addition the first log line belonging to the child mail (the log line cited
above properly belongs to the parent mail) is logged by qmgr,\footnote{Qmgr
is the Postfix daemon which manages the mail queue, determining which mails
will have delivery attempted next.} so the order also depends on how busy
qmgr is.\footnote{Postfix is quite paranoid about mail delivery, an
excellent characteristic for an MTA to possess, so it won't log that the
child has been created until it is absolutely certain that the mail has
been written to disk.}

Because of this the parser cannot complain when it encounters a log line
from qmgr for a previously unseen mail; it must flag the mail as coming
from an unknown origin, and subsequently clear the flag if and when the
origin of the mail becomes clear.  Obviously the parser could omit checking
of where mails originate from, but the author believes that it is better to
require an explicit source, as bugs are more likely to be exposed.

Process scheduling can have a still more confusing effect: quite often the
child mail will be created, delivered and entirely finished with
\textbf{before} the parent logs the creation line!  Thus mails flagged as
coming from an unknown origin cannot be entered into the database when
their final log line is parsed; instead they must be marked as ready for
entry and subsequently entered by the parent mail once it has been
identified.  Quite apart from identifying mail injected in an unknown
fashion, or bugs in the parser, unknown origin mails need to be marked as
such because they lack some data present in the parent mail; they must copy
that data from their parent before being entered in the database.  The
effect of this complication upon the algorithm is discussed in
section~\ref{tracking re-injected mail}.



\newpage
\subsection{Flow chart}

\label{flow-chart}

This flow chart shows the paths the data representing a mail/connection can
take through the parser algorithm.

\includegraphics{logparser-flow-chart.ps}

\subsection{Full algorithm}

\label{full-algorithm}

The intermingling of log entries from different mails immediately rules out
the possibility of handling each mail in isolation; the parser must be
capable of handling multiple mails in parallel, each potentially at a
different stage in its journey, without any interference between mails ---
except in the minority of cases where intra-mail interference is required.
The best way to implement this is to maintain state information for every
unfinished mail and manipulate the appropriate mail correctly for each log
line encountered.  The parser thus requires both a method of mapping log
lines to the correct mail and a method of specifying the action the log
line represents.  The former is achieved by using the smtpd process id to
identify the correct mail during the initial phase, then switching to the
queueid once the mail has been accepted.  The latter uses the action field
of the rule which matched the log line, executing the code in the function
named by the action.

Section~\ref{actions-in-detail} explains the actions in substantive detail;
this section will omit such detail because it would clutter and confuse the
algorithm description.  The data flow chart in section~\ref{flow-chart}
should also be consulted while reading this section.

\subsubsection{Mail enters the system}

\label{mail-enters-the-system}

Everything starts off with a mail entering the system, whether by local
submission via postdrop/sendmail, by SMTP, by re-injection due to
forwarding, or internally generated by Postfix.  Local submission is the
simplest case: a queueid is assigned immediately and the sender address is
logged (action: pickup; flowchart:~2).

SMTP is more complicated: 

\begin{enumerate}
        
    \item First there is a connection from the remote client
        (action: connect; flowchart:~1).

    \item This is followed by rejection of sender, recipients, client
        address, etc. (action: rejection; flowchart:~4); acceptance of one
        or more mails (action: clone; flowchart:~5); or some interleaving
        of both.
        
    \item The client disconnects (action: disconnect; flowchart:~6).  If
        Postfix has rejected any SMTP commands the data will be saved to
        the database; if not there won't be any data to save (any mails
        accepted will already have been cloned so their data is in another
        data structure).

    \item If one or more mails were accepted there will be more log entries
        for those mails later, see section~\ref{mail-delivery}.

\end{enumerate}

Re-injection due to forwarding sadly lacks explicit log lines of its
own;\footnote{Previously discussed in section~\ref{complications},
complication 3.} re-injection is somewhat awkward to explain because it
overlaps both the mail acceptance and mail delivery sections.  See
section~\ref{tracking re-injected mail} for a full discussion.

Internally generated mails lack any explicit origin (in Postfix 2.2.x) and
can only be detected using heuristics (see
section~\ref{identifying-bounce-notifications} for details).  Bounce
notifications are the primary example of internally generated mails, though
there may be other types.

\subsubsection{Mail delivery}

\label{mail-delivery}

The obvious counterpart to mail entering the system is mail leaving the
system, whether it is by deletion, bouncing, local or remote delivery.  All
four are handled in exactly the same way:

\begin{enumerate}

    \item The sender and recipient addresses will be logged separately
        (action: save\_by\_queueid; flowchart:~10).

    \item Sometimes mail is re-injected and the child mail needs to be
        tracked by the parent mail (action: track; flowchart:~11) ---
        section~\ref{tracking re-injected mail} discusses this in detail.

    \item Eventually the mail will be delivered, bounced, or deleted by the
        administrator (action: commit; flowchart:~12).  This is the last
        log line for this particular mail (though it may be indirectly
        referred to if it was re-injected).  If it is neither parent nor
        child of re-injection the data is cleaned up and entered in the
        database (flowchart:~14), then deleted from memory (flowchart:~15).
        Re-injected mails are described in section~\ref{tracking
        re-injected mail}.

\end{enumerate}

It should be reiterated that the actions above happen whether the mail is
delivered to a mailbox, piped to a command, delivered to a remote server,
bounced (due to a mail loop, delivery failure, or five day timeout), or
deleted by the administrator.  The exception is what happens after delivery
to the parent or children of mail re-injected due to forwarding, and that
is explained in section~\ref{tracking re-injected mail}.

\subsubsection{Tracking re-injected mail}

\label{tracking re-injected mail}

The crux of the problem is that re-injected mails appear in the logs
without explicit logging indicating their source.  There are two implicit
indications:

\begin{enumerate}

    \item The indicator which more commonly introduces re-injection is qmgr
        selecting a mail with a previously unseen queueid for delivery
        (action: mail\_picked\_for\_delivery; flowchart:~3), in which case
        a new data structure will be created.  The mail will be flagged as
        having unknown origins; this flag should be subsequently cleared
        once the origin has been established.  This may also be an
        indicator that the mail is a bounce notification, see
        section~\ref{identifying-bounce-notifications} for details.

    \item Local delivery re-injects the mail and logs a relayed delivery
        rather than delivering directly to a mailbox or program as it
        usually would (action: track; flowchart:~11).\footnote{Relayed
        delivery is performed by the SMTP client; local delivery means
        local to the server, i.e.\ an address the server is final
        destination for.} In this case the mail may already have been
        created (action: qmgr\_chooses\_mail; flowchart:~3; described
        above) and the unknown origin flag will be cleared; if not a new
        data structure will be created.  In both cases the new mail is
        marked as a child of the parent.  The log line in question is:

        \texttt{3FF7C4317: to=<username@example.com>, relay=local, \newline 
        delay=0, status=sent (forwarded as 56F5B43FD)}

        This second indicator always occurs for re-injected mail but
        typically it occurs after the first indicator explained above, and
        so rarely introduces re-injection.  This indicator is required to
        tie the parent and child mails together and so is central to the
        process of tracking re-injected mails.

\end{enumerate}

The algorithm for tracking and saving re-injected mail to the database can
finally be described:

\begin{itemize}

    \item If the mail is of unknown origin it is assumed to be a child mail
        whose parent hasn't yet been identified (action: commit;
        flowchart:~16).  Mark the mail as ready for entry in the database
        (flowchart:~17), and wait for the parent to deal with it
        (flowchart:~18).  The mail should not have subsequent log entries;
        only its parent should refer to it.

    \item If the mail is a child mail then it has already been tracked
        (action: commit; flowchart:~19): the data is cleaned up, missing
        data is copied from its parent if necessary, and the child is
        entered in the database (flowchart:~20), then deleted from the
        state tables (flowchart:~21).  The child mail will be removed from
        the parent mail's list of children (flowchart:~22); if this is the
        last child and the parent has also been entered in the database the
        parent will be deleted from the state tables.

    \item The last alternative is that the mail is a parent mail (action:
        commit; flowchart:~23).  Regardless of the state of its children
        the data is cleaned up and entered in the database (flowchart:~24).
        The parent may have children which are waiting to be entered in the
        database (flowchart:~25); for each of those children their data is
        cleaned up and entered in the database, then deleted from the state
        tables.  The parent may also have outstanding children which are
        not yet delivered, in which case (flowchart:~26) the parent must
        wait for those children to be finished with.  As soon as the last
        child is deleted from the state tables the parent will also be
        finished with (flowchart:~27), and deleted from the state tables.

\end{itemize}

A parent mail can have multiple children, which may be delivered before or
after the parent mail.  

\subsection{Actions in detail}

\label{actions-in-detail}

Each action is passed the same arguments: 

\begin{description}

    \item [\$line] The log line, with the standard syslog(3) fields parsed,
        removed and made available.
        
    \item [\$rule] The matching rule.
        
    \item [@matches] The fields in the line captured by \$rule's regex.

\end{description}

The actions:

\begin{description}

    \item [IGNORE] This rule just returns successfully; it is used when a
        line needs to be parsed for completeness but doesn't either provide
        any useful data or require anything to be done.

    \item [CONNECT] Handle a remote client connecting: create a new state
        table entry (indexed by smtpd pid) and save both the client
        hostname and IP address.

    \item [DISCONNECT] Deal with the remote client disconnecting: enter the
        connection in the database, perform any required cleanup, and
        delete the connection from the state tables.

    \item [SAVE\_BY\_QUEUEID] Find the correct mail based on the queueid in
        the log line, and save the data extracted by the regex to it.

    \item [COMMIT] Enter the data from the mail into the database. Entry
        will be postponed if the mail is a child waiting to be tracked.
        Once entered, the mail will be deleted from the state tables.

    \item [TRACK] Track a mail when it is re-injected for forwarding to
        another mail server; this happens when a local address is aliased
        to a remote address.  TRACK will be called when dealing with the
        parent mail, and will create the child mail if necessary. TRACK
        checks if the child has already been tracked, either by this parent
        or by another parent, and issues appropriate warnings in either
        case.

    \item [REJECTION] Deal with postfix rejecting an SMTP command from the
        remote client: log the rejection with a mail if there is a queueid
        in the log line, or with the connection if not.

    \item [MAIL\_PICKED\_FOR\_DELIVERY] This action represents Postfix
        picking a mail from the queue to deliver. This action is used for
        both qmgr and cleanup as it needs to deal with out of order log
        lines; see section~\ref{discarding cleanup lines} for details.

    \item [PICKUP] Pickup is the service which deals with mail submitted
        locally via /usr/sbin/sendmail. 

        Out of order log entries may have caused the state table entry to
        already exist (see section~\ref{pickup logging after cleanup});
        otherwise it is created.  The data from the log line is then saved
        to the state table entry.

    \item [CLONE] Multiple mails may be accepted on a single connection, so
        each time a mail is accepted the connection's state table entry
        must be cloned; if the original data structure was used the second
        and subsequent mails would overwrite one another's data.

    \item [MAIL\_TOO\_LARGE] When the client tries to send a message larger
        than the local server accepts the mail will be discarded.  See
        TIMEOUT for further discussion; the two are handled in exactly the
        same way.

    \item [TIMEOUT] The connection timed out so the mail currently being
        transferred must be discarded. The mail may have been accepted, in
        which case there's a data structure to dispose of, or it may not in
        which case there is not.  See
        section~\ref{timeouts-during-data-phase} for the gory details.

    \item [POSTFIX\_RELOAD] When Postfix stops or reloads its configuration
        it kills all smtpds,\footnote{Possibly other programs are killed
        also, but the parser is only affected by and interested in smtpds
        exiting.} requiring any active connections to be committed and
        deleted from the state tables.

    \item [SMTPD\_KILLED] Sometimes Postfix needs to forcefully kill an
        smtpd; the active connection for that smtpd must be committed and
        deleted from the state tables.

    \item [SMTPD\_DIED] Sometimes an smtpd dies or exits unsuccessfully;
        the active connection for that smtpd must be committed and deleted
        from the state tables.

    \item [SMTPD\_WATCHDOG] Smtpds have a watchdog timer to deal with
        unusual situations --- after five hours the timer will expire and
        the smtpd will exit.  This occurs very infrequently, as there are
        many other timeouts which should occur in the intervening hours:
        DNS timeouts, timeouts reading data, etc.  The active connection
        for that smtpd must be committed and deleted from the state tables.

    \item [BOUNCE] Postfix 2.3 (and hopefully subsequent versions) logs the
        creation of bounce messages.  This action creates a new mail if
        necessary, marking it as a bounce notification and removing the
        unknown origin flag if it already exists.

\end{description}

\subsection{Additional complications}

\label{additional complications}

\subsubsection{Identifying bounce notifications}

\label{identifying-bounce-notifications}

Postfix 2.2.x (and presumably previous versions) lacks explicit logging
when bounce notifications are generated; suddenly there will be log entries
for a mail which lacks an obvious source.  There are similarities to the
problem of re-injected mails discussed in
section~\ref{tracking re-injected mail}, but unlike the solution described
therein bounce notifications do not eventually have a log line which
identifies their source.  Heuristics must be used to identify bounce
notifications, and those heuristics are:

\begin{enumerate}

    \item The sender address is $<>$.

    \item Neither smtpd nor pickup have logged any messages associated with
        the mail, indicating it was generated internally by Postfix, not
        accepted via SMTP or submitted locally by sendmail.

    \item The message-id has a specific format: \newline
        \texttt{YYYYMMDDhhmmss.queueid@server.hostname} \newline
        e.g.\ \texttt{20070321125732.D168138A1@smtp.example.com}

    \item The queueid in the message-id must be the same as the queueid of
        the mail: this is what distinguishes bounce notifications generated
        locally from bounce notifications which are being re-injected as a
        result of aliasing.  In the later case the message-id will be
        unchanged from the original bounce notification, and so even if it
        happens to be in the correct format (e.g.\ if it was generated by
        Postfix on another server) the queueid in the message-id will not
        match the queueid of the mail.

\end{enumerate}

Once a mail has been identified as a bounce notification the unknown origin
flag is cleared and the mail can be entered in the database.

There is a small chance that a mail will be incorrectly identified as a
bounce notification, as the heuristics used may be too broad.  For this to
occur the following conditions would have to be met:

\begin{enumerate}

    \item The mail must have been generated internally by Postfix.

    \item The sender address must be $<>$.

    \item The message-id must have the correct format and match the queueid
        of the mail.  While a mail sent from elsewhere could easily have
        the correct message-id format, the chance that the queueid in the
        message-id would match the queueid of the mail is extremely small.

\end{enumerate}

The most likely cause of mis-identification is if a mail generated
internally by Postfix is identified as a bounce notification when it is a
different type of message; arguably this is a benefit rather than a
drawback, as future mails generated internally by Postfix will be handled
correctly.

Postfix 2.3 (and hopefully subsequent versions) log the creation of a
bounce message.

\subsubsection{Aborted delivery attempts}

\label{aborted-delivery-attempts}

Some mail clients appear to send the following sequence of commands during
the SMTP session:

\begin{verbatim}
    EHLO client.hostname
    MAIL FROM: <sender@address>
    RCPT TO: <recipient@address>
    RSET
    MAIL FROM: <sender@address>
    RCPT TO: <recipient@address>
    DATA
\end{verbatim}

The client aborts the first delivery attempt after the first recipient is
accepted, then makes a second delivery attempt which it continues with
until the delivery is complete.

Once again Postfix does not log a message making the client's behaviour
clear, so once again heuristics are required to identify when this
behaviour occurs.  In this case a list of all mails accepted during a
connection are saved in the connection state, and the accepted mails are
examined when the disconnection line is parsed and the disconnection action
executed.  Each mail is checked for the following characteristics:

\begin{itemize}

    \item Is the mail missing its cleanup log message?  Every mail which
        passes through Postfix will have a cleanup line; lack of a cleanup
        line is a sure sign the mail didn't make it too far.

    \item Were there exactly two smtpd log lines for the mail?  There
        should be a connection line and a mail accepted line.

\end{itemize}

If both checks are successful then the mail is assumed to be one of the
offending bogus mails and is discarded.  There will be no further entries
logged for such mails, so without identifying and discarding them they
accumulate in the state table and will cause clashes if the queueid is
reused.  The mail cannot be entered in the database as the only data
available is the client hostname and IP address, but the database schema
requires many more fields be populated (see section~\ref{database schema}).

\subsubsection{Further aborted delivery attempts}

Some mail clients disconnect abruptly if a second or subsequent recipient
is rejected; they may also disconnect after other errors, but such
disconnections are either unimportant or are handled elsewhere in the
algorithm (section~\ref{timeouts-during-data-phase}).  Sadly Postfix
doesn't log a message saying the mail has been discarded, as should be
expected by now.  The checks to identify this happening are:

\begin{itemize}

    \item Is the mail missing its cleanup log message?  Every mail which
        passes through Postfix will have a cleanup line; lack of a cleanup
        line is a sure sign the mail didn't make it too far.

    \item Were there three or more smtpd log lines for the mail?  There
        should be a connection line and a mail accepted line, followed by
        one or more rejection lines.

    \item Is the last smtpd log line a rejection line?

\end{itemize}

These checks are made during the DISCONNECT action: if all checks are
successful then the mail is assumed to have been discarded when the client
disconnected.  There will be no further entries logged for such mails, so
without identifying and discarding them they accumulate in the state table
and will cause clashes if the queueid is reused.  The mail cannot be
entered in the database as the only data available is the client hostname
and IP address, but the database schema (see section~\ref{database schema})
requires many more fields be populated.

\subsubsection{Timeouts during DATA phase}

\label{timeouts-during-data-phase}

The DATA phase of the SMTP conversation is where the headers and body of the
mail are transferred.  Sometimes there is a timeout or the connection is
lost\footnote{For brevity's sake timeout will be used throughout this
section, but everything applies equally to lost connections.} during the
DATA phase; when this occurs Postfix will discard the mail and the parser
needs to discard the data associated with that mail.  It seems more
intuitive to save the data to the database, but if a timeout occurs there
won't yet have been any data gathered for the mail, so there is none
available to save; the timeout is saved with the connection data instead.

To deal properly with timeouts the parsing algorithm needs to do the
following in the TIMEOUT action:

\begin{enumerate}

    \item Record the timeout and associated data in the connection's
        results.

    \item If no mails have been accepted yet nothing needs to be done; the
        timeout action ends.  The timeout action is dependant on the clone
        action keeping a list of all mails accepted on each connection.

    \item Because of ESMTP pipelining the client can send MAIL FROM, RCPT
        TO and DATA commands in one packet, instead of sending each command
        individually and waiting for the reply before sending subsequent
        commands.  If pipelining is used and Postfix rejects the sender
        address or any of the recipient addresses there is no way for the
        client to tell which command was rejected.  Some clients make no
        attempt to recover and disconnect uncleanly.

        A timeout may thus apply either to an accepted mail or a rejected
        mail.  To distinguish between the two cases the algorithm compares
        the timestamp of the last accepted mail against the timestamp of
        the last line logged by smtpd for that connection.  If the smtpd
        timestamp is later there was a rejection between the accepted mail
        and the timeout, therefore the timeout applies to a rejected mail;
        the timeout has already been recorded so the timeout action
        finishes.  If the mail acceptance timestamp is greater then the
        timeout applies to the just-accepted mail, which will be discarded.

\end{enumerate}

This complication is further complicated by out of order cleanup lines: see
section~\ref{discarding cleanup lines} for details.

\subsubsection{Discarding cleanup lines}

\label{discarding cleanup lines}

The author has only observed this complication occurring after a timeout,
though there may be other circumstances which trigger it.  Sometimes the
cleanup line is logged after the timeout line; parsing this line causes the
creation of a new state table entry for the queueid in the log line.  This
is incorrect because the line actually belongs to the mail which has just
been discarded, and the next log line for that queueid will be seen when
the queueid is reused, causing a queueid clash and the appropriate warning.

In the case where the cleanup line is still pending the algorithm updates a
global cache of queueids, adding the queueid and the timestamp from the
timeout line.  When the next cleanup line is parsed for that queueid the
cache will be checked, and the line will be deemed part of the discarded
mail and discarded if it meets the following requirements:

\begin{itemize}

    \item The queueid must not have been reused yet, i.e.\ there isn't an
        entry in the state tables for the queueid.

    \item The timestamp of the cleanup line must be within ten minutes of
        the mail acceptance timestamp.  Timeouts happen after five minutes,
        but some data may have been transferred slowly, and empirical
        evidence shows that ten minutes is not unreasonable; hopefully it
        is a good compromise between false positives and false negatives.

\end{itemize}

The next cleanup line must meet the criteria above for it to be discarded
because not every connection where a timeout occurs will have a cleanup
line logged for it; if the algorithm blindly discarded the next cleanup
line it would in some cases be mistaken.  Whether or not the next cleanup
line is discarded the queueid will be removed from the cache of timeout
queueids when the next pickup line containing that queueid is parsed.

\subsubsection{Pickup logging after cleanup}

\label{pickup logging after cleanup}

Occasionally the pickup line logged when mail is submitted locally via
sendmail appears later in the logfile than the cleanup line for that mail.
This seems to occur during periods of particularly heavy load, so is most
likely due to process scheduling vagaries.  Normally if the queueid given
in the pickup line exists a warning is generated by the pickup action, but
if the following conditions are met it is assumed that the lines are out of
order:

\begin{itemize}

    \item The only program which logged anything for the mail is cleanup.

    \item There is less than a five second difference in the timestamps of
        the cleanup and pickup lines.

\end{itemize}

As always with heuristics there may be circumstances in which these
heuristics incorrectly match, but none have been identified so far.

\subsubsection{Smtpd stops logging}

\label{smtpd stops logging}

Occasionally an smtpd will just stop logging, without an immediately
obvious reason.  After poring over logs for some time there are several
reasons for this infrequent occurrence:

\begin{enumerate}

    \item Postfix is stopped or its configuration is reloaded.  When this
        happens all smtpds exit, and all entries in the connections state
        table must be committed and deleted from the state table.  The
        queueid state table is untouched.

    \item Postfix sometimes forcibly kills an smtpd, so the active
        connection must be committed and deleted from the connections state
        table.

    \item Occasionally an smtpd will die or exit uncleanly, so the active
        connection must be committed and deleted from the connections state
        table.

\end{enumerate}

The above appear to cover all situations where an smtpd suddenly stops
logging.  In addition to removing an active connection the last accepted
mail may need to be discarded, as detailed in
section~\ref{timeouts-during-data-phase}.


\subsubsection{Out of order log lines}

\label{out of order log lines}

Occasionally a log file will have out of order log lines which cannot be
dealt with by the techniques described in sections~\ref{tracking
re-injected mail},~\ref{discarding cleanup lines}~and~\ref{pickup logging
after cleanup}.  In the 93 log files used for testing this occurs only five
times in 607,21,709 log lines, but for completeness of the algorithm it
should be dealt with.  The five occurrences in the test log files have the
same characteristics: the \texttt{postfix/local} log line showing delivery
to a local mailbox occurs after the \texttt{postfix/qmgr} log line showing
removal of the mail from the queue because delivery is completed.  This
causes problems: the mail is not complete, so entry into the database
fails; a new mail is created when the \texttt{postfix/local} line is parsed
and remains in the state tables; four warnings are issued per pair of out
of order log lines.

The solution to this problem is to examine the list of programs which have
logged messages for each mail, comparing the list against a table of
known-good combinations of programs.  If the mail's combination is found in
the valid list the mail can be entered in the database; if the combination
is not found entry must be postponed and the mail flagged.  The
SAVE\_BY\_QUEUEID action checks for the flag and retries entry if it's
found; if the additional log lines have caused the mail to reach a valid
combination entry will succeed, otherwise it must be postponed once more.

The list of valid combinations is given below.  Every mail will
additionally have log entries from \texttt{postfix/cleanup} and
\texttt{postfix/qmgr}; any mail may also have log entries from
\texttt{postfix/bounce}, \texttt{postfix/postsuper}, or both.

\begin{description}

    \item [postfix/local:] Local delivery of bounce notification, or local
        delivery of forwarded/tracked mail.

    \item [postfix/local, postfix/pickup:] Locally submitted mail, locally
        delivered.

    \item [postfix/local, postfix/pickup, postfix/smtp:] Locally submitted
        mail, \newline both local and remote delivery.

    \item [postfix/local, postfix/smtp, postfix/smtpd:] Mail accepted from
        a remote client, both local and remote delivery.

    \item [postfix/local, postfix/smtpd:] Mail accepted from a remote
        client, local delivery.

    \item [postfix/pickup, postfix/smtp:] Locally submitted mail, remote
        delivery.

    \item [postfix/smtp:] Remote delivery of forwarded/tracked mail.

    \item [postfix/smtp, postfix/smtpd:] Mail accepted from a remote
        client, remotely delivered (relaying mail for clients on the local
        network).

\end{description}

This applies to accepted mails only, not to rejected mails.

\subsubsection{Yet more aborted delivery attempts}

\label{yet-more-aborted-delivery-attempts}

The aborted delivery attempts described in
section~\ref{aborted-delivery-attempts} occur frequently, but the aborted
delivery attempts described in this section only occur four times in the 93
log files used for testing.  The symptoms are the same as in
section~\ref{aborted-delivery-attempts}, except that there \textit{is\/} a
\texttt{postfix/cleanup} log line; there does not appear to be anything in
the log file to explain why there are no further log messages.  The only
way to detect these mails is to periodically scan all mails in the state
tables, deleting any mails displaying the following characteristics:

\begin{itemize}

    \item The timestamp of the last log line for the mail must be 12 hours
        or more earlier than the last log line parsed.

    \item There must be exactly two \texttt{postfix/smtpd} and one
        \texttt{postfix/cleanup} log entries for the mail, with no
        additional log entries.

\end{itemize}

12 hours is a somewhat arbitrary time period, but it is far longer than
Postfix would delay delivery of a mail in the queue.\footnote{This may be a
problem if Postfix is not running for an extended period of time.}  The
state tables are scanned for mails matching the characteristics above each
time the end of a log file is reached, and matching mails are deleted.

\section{Coverage}

The discussion of the parser's coverage of Postfix log files is separated
into two parts: log lines covered and mails covered.  The first is
important because the parser should handle all (relevant) log lines it's
given; the second is equally important because the parser must properly
deal with every mail if it is to be useful.  Improving the former is
less intrusive, as it just requires new rules to be written; improving the
latter is much more intrusive as it requires changes to the parser
algorithm, and it can also be much harder to notice a deficiency.

\label{parsing-coverage}

\subsection{Log lines covered}

\label{log-lines-covered}

Parsing a log line is a three stage process:

\begin{enumerate}

    \item Check if there are any rules for the program which produced the
        log line; if not then skip the line.  

    \item Try each rule until a matching rule is found.

    \item Execute the action specified by the rule.

\end{enumerate}

Full coverage of log lines requires the following:

\begin{enumerate}

    \item Each program of interest must have at least one rule or its log
        lines will be silently skipped; in the extreme case of zero rules
        the parser would happily skip every log line.  There may be any
        number of log lines from other programs intermingled in the log
        file, and there are some Postfix programs which don't produce any
        log lines of interest.

    \item There must be a rule to match each different line produced by
        each program; if a line is not successfully matched the parser will
        issue a warning.  Rules should be as specific and tightly bound as
        possible to ensure accurate parsing:\footnote{A rule which matches
        zero or more of any character will successfully parse every log
        line, but not in a meaningful way.} most log lines contain fixed
        strings and have a rigid pattern, so this is not a problem.

    \item The appropriate action to take --- this is discussed in
        section~\ref{mails-covered}.

\end{enumerate}

Full coverage of log lines is easy to achieve yet hard to maintain.  It is
easy to achieve full coverage for a limited set of log files (at the time
of writing the parser has 139 rules, fully parsing 93 contiguous log files
from Postfix 2.2 and 2.3), and new rules are easy to add.  Maintaining full
coverage is hard because other servers have different restrictions with
custom messages, RBLs change their messages over time, major releases of
Postfix change warning messages (usually adding more information), etc.\ so
over time the log lines drift and change.  Graph~\ref{rule hits} shows the
number of hits for each rule over all 93 log files; it's obvious that a
small number of rules match the vast majority of the lines, and more than
half the rules match fewer than 100 times.

\subsection{Mails covered}

\label{mails-covered}

Coverage of mails is much more difficult to determine accurately than
coverage of lines.  The parser can dump its state tables in a human
readable form; examining these tables with reference to the log files is
the best way to detect mails which were not handled properly (many of the
complications discussed in section~\ref{additional complications} were
detected in this way).  The parser issues warnings when it detects any
errors, some of which may alert the user to a problem, e.g.\ when a queueid
is reused before the previous mail is fully dealt with, when a queueid or
pid is not found,\footnote{There will often be warnings about missing
queueids or pids in the first few hundred or thousand log lines because the
earlier log lines for those connections or mails are in the previous log
file; loading the saved state from the previous log file will solve this
problem.} or when there are problems tracking a child mail (see
section~\ref{tracking re-injected mail}).  There should be few or no
warnings, and the state table should only contain entries for mails which
had yet to be delivered when the logfiles ended, or started before the
logfiles began.

At the time of writing the parser is being tested with 93 log files.  There
are 5 warnings produced, but because the parser errs on the side of
producing more warnings rather than fewer those 5 warnings represent 3
instances of 1 problem: 3 connections started before the first log file, so
their initial log entries are missing, leading to warnings when their log
lines are parsed.

The state tables contain entries for mails not yet delivered when the
parser finishes execution.  Ideally all they should contain are mails which
are awaiting delivery after the period covered by the logs, though they may
also contain mails whose initial entries are not contained in the logs.
Any other entries are evidence of a failure in parsing or an aberration in
the logs.  After parsing the 93 test log files the state tables contain 18
entries, breaking down into:

\begin{itemize}

    \item 1 connection which started only seconds before the logs ended and
        had not yet completed.

    \item 1 mail which had been accepted only seconds before the logs ended
        and had not yet been delivered.

    \item 9 mails whose initial log entries were not present in the logs.

    \item 7 mails which had yet to be delivered due to repeated failures.

\end{itemize}

The 13,850,793 connections and mails accepted, rejected or delivered by
Postfix during this period are handled correctly by the parser.

\section{Limitations and possible improvements}

\label{limitations-improvements}

Every piece of software suffers from some limitations and there is almost
always room for improvement.

\subsection{Limitations}

\begin{itemize}

    \item Each new Postfix release requires new rules to be written to cope
        with the new log lines.  Similarly new RBLs, new policy servers and
        new administrator defined rejection messages require new rules.

    \item It appears that the hostname used in the HELO command is not
        logged if the mail is accepted.\footnote{Tested with Postfix 2.2.10
        and 2.3.11; this may possibly have changed in Postfix 2.4.}
        Rectifying this has already been described in section~\ref{logging
        helo}.

    \item The algorithm does not distinguish between mails where one or
        more mails are rejected and a subsequent mail is accepted; it will
        appear in the database as one mail with lots of rejections followed
        by acceptance.  I don't believe it's possible to make this
        distinction given the data Postfix logs, though it might be
        possible to write a policy server to provide additional
        logging.

    \item The program will not detect parsing the same log file twice,
        resulting in the database containing duplicate entries.

\end{itemize}

\subsection{Possible improvements}

\begin{itemize}

    \item Write the policy server referred to in the limitations above.

\end{itemize}

\section{Conclusion}

\label{conclusion}

XXX WRITE THIS


\appendix


\section{Other Postfix log parsers reviewed}

XXX THIS NEEDS TO BE EXPANDED\@; EXPLAIN WHY THEY'RE NOT GOOD ENOUGH

XXX ADD DATES CHECKED FOR ALL PARSERS

\label{other-parsers}

\begin{description}

    \item [pflogsumm.pl] \textit{pflogsumm.pl is designed to provide an
        over-view of postfix activity, with just enough detail to give the
        administrator a ``heads up'' for potential trouble spots.\/}
        \newline \url{http://jimsun.linxnet.com/postfix_contrib.html}

    \item [Sawmill Universal Log File Analysis and Reporting] is a
        general purpose commercial product which parses 687 log file
        formats (correct as of 2007/04/15) and produces reports.  I have not
        experimented with it due to its limited data extraction facilities;
        it does have three different sets of data for Postfix (as of
        2007/04/15, one is beta), but they do not appear to be interlinked,
        nor does it save sufficient data for the purposes of this project.
        The source code is available in an obfuscated form only (presumably
        for a fee), and the product is quite expensive, as it requires a
        license for each report which is to be generated. \newline
        \url{http://www.thesawmill.co.uk/formats/postfix.html} \newline
        \url{http://www.thesawmill.co.uk/formats/postfix_ii.html} \newline
        \url{http://www.thesawmill.co.uk/formats/beta_postfix.html}

    \item [Splunk] XXX INVESTIGATE FURTHER \url{http://www.splunk.com}

    \item [Isoqlog] \textit{Isoqlog is an MTA log analysis program written
        in C. It designed to scan qmail, postfix, sendmail and exim logfile
        and produce usage statistics in HTML format for viewing through a
        browser. It produces Top domains output according to Sender,
        Receiver, Total mails and bytes; it keeps your main domain mail
        statistics with regard to Days Top Domain, Top Users values for per
        day, per month and years.\/}  \newline
        \url{http://www.enderunix.org/isoqlog/}

    \item [AWStats] \textit{AWStats is a free powerful and featureful tool
        that generates advanced web, streaming, ftp or mail server
        statistics, graphically.\/}
        
        AWStats will produce simple graphs for many different services, but
        that restricts it to supporting the Lowest Common Denominator: the
        data it will extract from an MTA log file is:
        \texttt{time2, email, email\_r, host, host\_r, method, url, code,
        and bytesd.} \newline
        \url{http://awstats.sourceforge.net/} \newline
        \url{http://awstats.sourceforge.net/awstats.mail.html} \newline
        \url{http://awstats.sourceforge.net/docs/awstats_faq.html#MAIL}

    \item [Log analyser --- throughput monitor] This utility tracks the
        number of events which occurred over a particular time and warns if
        the frequency of events passes a certain threshold.  It's designed
        to provide real time alerts when dictionary attacks, mail loops or
        similar problems occur.  \newline
        \url{http://home.uninet.ee/~ragnar/throughput_monitor/}

    \item [Anteater] \textit{The Anteater project is a Mail Traffic
        Analyser. Anteater supports currently the logformat produced by
        Sendmail and by Postfix. The tool is written in 100\% C++ and is
        very easy to customize. Input, output, and the analysis are modular
        class objects with a clear interface. There are eight useful
        analyse modules, writing the result in plain ASCII or HTML, to
        stdout or to files.\/}

        Anteater hasn't been updated since November 2003, and doesn't have
        any English documentation.
        
        \url{http://anteater.drzoom.ch/}

    \item [Yet Another Advanced Logfile Analyser] uses a plugin based
        system to analyse log files and produce output reports.  The core
        code is merely 91 lines long, as all the parsing and report
        generation is handled by modules.  Using YAALA as a base would be
        only slightly less work as both input and output modules would need
        to be written; it may even be more work to implement the parser
        within the constraints of YAALA\@.

        \url{http://yaala.org/}

    \item [Logparser/Lire] Lire is another general purpose log parser, but
        is developed under the GNU GPL\@.  Like other general purpose log
        parsers it doesn't extract enough data from Postfix logs to be
        worthwhile.  Development also seems to have stalled, with only a
        minor update since October 2004.  \newline
        \url{http://logreport.org/lire/}

    \item [Logrep] \textit{Logrep is a secure multi-platform framework for
        the collection, extraction, and presentation of information from
        various log files.\/}

        The parsing Logrep performs is extremely basic, counting the number
        of different processes executed, from address, to addresses, number
        of recipients, bytes transferred and delay.  \newline
        \url{http://www.itefix.no/phpws/index.php}

    \item [Log Mail Analyser]~\cite{log-mail-analyser} attempts to
        correlate Postfix log lines by queueid and produce both a CSV file
        and either a Berkeley DB or MySQL database.  It aims to save
        \textit{date and hour, DNS name and IP address host, mail server IP
        address, sender, receiver and e-mail status (sent, rejected)}, so
        its data extraction is reasonably complete.  The parsing is
        hard-coded into the program; extending it requires familiarity with
        a substantial portion of the code.  Producing a CSV file appears to
        be quite limiting, but the intention is to provide a format
        suitable for use with grep(1), awk(1) and the myriad of other Unix
        utilities already available.  Supporting Berkeley DB is also
        limiting, requiring an understanding of the (partially documented)
        database structure and writing a program to extract the required
        data --- it appears to be the least useful of the three output
        formats, though the authors of LMA did provide some queries:
        IP-STORY, FROM-STORY, DAILY-EMAIL and DAILY-REJECT\@.
        Unfortunately when the author tested LMA it didn't work properly
        and after exploring the code for some time the author reluctantly
        gave up.

\end{description}


\bibliographystyle{logparser-bibliography-style}
\bibliography{logparser-bibliography}
\label{bibliography}

\section{Graphs}

\label{graphs}

\subsection{Parser scalability}

\subsubsection{Execution time vs file size vs number of lines.}
\label{execution time vs file size vs number lines}
\includegraphics{plot-normal-filesize-numlines.ps}

\subsubsection{Ratio of file size and number of lines to execution time}
\label{execution time vs file size vs number lines factor}
\includegraphics{plot-normal-filesize-numlines-factor.ps}



\subsection{Rule ordering}

\subsubsection{Rule hits}
\label{rule hits}
\includegraphics{plot-hits.ps}

\subsubsection{Normal vs shuffled vs reversed ordering}
\label{normal vs shuffled vs reversed ordering}
\includegraphics{plot-normal-shuffle-reverse.ps}

\subsubsection{Percentage increase of shuffled + reversed over normal}
\label{normal vs shuffled vs reversed ordering factor}
\includegraphics{plot-normal-shuffle-reverse-factor.ps}

The worst-case performance of shuffled rules can be seen at log file 30,
where it is equivalent to reversed rules.

The dips at logfiles 22 and 62--68 correspond to peaks in log file size in
graph~\ref{execution time vs file size vs number lines}.  The explanation
for this took some time to arrive at, but it turns out to be reasonably
simple.  Examining the database after parsing logfile~22 in isolation shows
64 rules matched during parsing, but as shown in graph~\ref{log22
distribution by rule} the distribution of matching rules is extremely
skewed.  Comparing the number of log lines per program versus rules per
program in graph~\ref{log22 line and rule distribution by program} shows
that \texttt{postfix/qmgr} has the highest number of log lines, however it
has a low number of rules.  This combination results in a low average
number of rules to try for \texttt{postfix/qmgr} log lines, reducing the
overall average number of rules to try per log line.  In turn this reduces
the average parsing time per log line, and thus the parsing time for the
log file.  While the parsing time is substantially greater than the time
for the log files on either side, the increase from optimal ordering to
reversed ordering is proportionally smaller, producing the dips in
graph~\ref{normal vs shuffled vs reversed ordering factor}.

\subsubsection{Log 22 distribution by rule}
\label{log22 distribution by rule}
\includegraphics{plot-log22-distribution-by-rule.ps}

The individual rules are not labelled on the x-axis because doing so makes
the graph illegible.

\subsubsection{Log 22 line and rule distribution by program}
\label{log22 line and rule distribution by program}
\includegraphics{plot-log22-distribution-versus-rule-count.ps}




\subsection{Caching regexs}

\subsubsection{Regex: cached vs discarded}
\label{normal regex vs discard regex}
\includegraphics{plot-cached-discarded.ps}

\subsubsection{Regex: percentage execution time increase}
\label{normal regex vs disarded regex factor}
\includegraphics{plot-cached-discarded-factor.ps}

Two large dips can be seen in graph~\ref{normal regex vs disarded regex
factor} at log files 22 and 62--68, corresponding to the spikes in log file
size in graph~\ref{execution time vs file size vs number lines}.  The
anomaly at files 62--68 is caused by a combination of a mail loop and a
delivery failure which resulted in a huge number of log lines, all of which
were matched by a single rule.  Sorting the rules by the number of
successful matches has the effect that this rule will be tried first, so
the average number of regexs compiled per line drops sharply for those
eight log files, resulting in a much lower increase in relative execution
time, as detailed below.

\begin{tabular}[]{lrr}
                                    & Mean      & Standard Deviation    \\
    All logs                        & 465.58\%  & 114.70\%              \\
    All logs except 22 and 62--68   & 497.43\%  & 50.58\%               \\
    Logs 22 and 62--68              & 127.16\%  & 21.28\%               \\

\end{tabular}



\end{document}
