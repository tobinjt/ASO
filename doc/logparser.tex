% $Id$
\documentclass[a4paper,12pt,draft]{article}

% Useful stuff for math mode.
\usepackage{amstext}
% Wrapping of URLs.  This is used in the bibliography instead of \url{}.
\usepackage{url}
\newcommand{\parsername}{ASO}
% Include images
\usepackage[final]{graphicx}
% Change how nested enumerate environments are labelled.
\renewcommand{\labelenumii}{\roman{enumii}:}
\renewcommand{\refname}{Bibliography}
% Add the bibliography into the table of contents.
\usepackage[section,numbib]{tocbibind}

\usepackage[acronym=true,style=altlist,number=none,toc=true]{glossary}
\makeglossary{}
\makeacronym{}

% When creating a PDF make the table of contents into links to the pages
% (without horrible red borders) and include bookmarks.  The title and
% author don't work - I think either gnuplot or graphviz clobbers it.
\usepackage{hyperref}
\hypersetup{
    pdftitle    = {Parsing Postfix log files},
    pdfauthor   = {John Tobin},
    final       = true,
    pdfborder   = {0 0 0},
%    breaklinks  = true,
}
\usepackage{breakurl}
\usepackage{lastpage}

% \showgraph{filename}{caption}{label}
\newcommand{\showgraph}[3]{
    \begin{figure}[hbt!]
        \caption{#2}\label{#3}
        \includegraphics{#1}
    \end{figure}
}

%\showtable{filename}{caption}{label}
\newcommand{\showtable}[3]{
    \begin{table}[ht]
        \caption{#2}\label{#3}
        \input{#1}
    \end{table}
}

% Replacement for \ref{}, adds the page number too.
\newcommand{\refwithpage}[1]{%
    \empty{}\ref{#1} [page~\pageref{#1}]%
}

% A command to format a Postfix daemon's name
\newcommand{\daemon}[1]{%
    \texttt{postfix/#1}%
}

% This is ridiculous, but I can't put @ in glossary entries, so . . .
\newcommand{\at}[0]{%
    @%
}

\begin{document}

% Pull in the acronyms early, so they can be used throughout the text.
\input{logparser-acronyms.tex}

\title{Parsing Postfix log files}
\author{John Tobin \\ School of Computer Science and Statistics \\ 
Trinity College \\ Dublin 2 \\ Ireland \\ tobinjt@cs.tcd.ie}
\maketitle

\begin{abstract}

    Parsing Postfix logs is much more difficult than it first appears, but
    it is possible to achieve a high degree of accuracy in understanding
    the logs, and thus reconstructing the actions taken by Postfix to
    generate the logs.  This paper describes the creation of a parser,
    documenting the parsing algorithm and rules, explaining the
    difficulties encountered and the solutions developed, with reference to
    an implementation which stores data gleaned from the logs in an SQL
    database.  The gathered data can then be used to optimise current
    anti-spam measures, provide a baseline to test new anti-spam measures
    against, or to produce statistics showing how effective those measures
    are.

\end{abstract}

XXX WHAT IS THE CONCLUSION???  FIRST THING IS TO COMPARE TO OTHER PARSERS
REVIEWED

\newpage
\tableofcontents
\listoffigures
\listoftables

\newpage
\section{Introduction}

\label{introduction}

Most mail server administrators will have performed some basic processing
of Postfix logs at one time or another, whether it was to debug a problem,
explain to a user why their mail is being rejected, or check whether their
new anti-spam measures are working.  The more adventurous will have
generated statistics to show how many hits each of their anti-spam measures
has gotten in the last week, and possibly even generated some graphs to
clearly illustrate the point to management or users.\footnote{This was the
author's first real foray into processing Postfix logs.}  Very few will
have performed in-depth parsing and analysis of their logs, where the
parsing must correlate the log lines per-connection or per-queueid rather
than processing lines independently.  One of the barriers to this type of
processing is the unstructured nature of Postfix logs, where each log line
was added on an ad hoc basis as a requirement was discovered or new
functionality was added.\footnote{A history of all changes made to Postfix
is distributed with the source code, available from
\url{http://www.postfix.org/}} Further complication arises because the set
of rejection messages is not fixed: new messages can be added by the
administrator with custom checks; every \RBL{} returns a different
explanatory message; policy servers may log different messages depending on
the characteristics of the connection; there are many ways in which the log
lines may differ between servers, even within the same organisation:
servers may be configured differently, or running different version of
Postfix.  This paper documents the difficult process of parsing Postfix
logs, presenting a program which parses logs and places the resulting data
into a database for later use.  The gathered data can then be used to
optimise current anti-spam measures, provide a baseline to test new
anti-spam measures against, or to produce statistics showing how effective
those measures are.  There are numerous other uses for such data: improving
server performance by identifying troublesome destinations and
reconfiguring appropriately or dedicating transports to those destinations;
identifying regular high volume uses (e.g.\ customer newsletters) and
restricting those uses to off-peak times; detecting virus outbreaks which
propagate via email; as a base for billing customers on a shared server.
Preserving the raw data enables users to develop a multitude of uses far
beyond those conceived of by the author.

\vspace{1em}\noindent\textbf{Layout of the paper:}

Section~\ref{background} provides background information useful in
understanding the paper, parser and algorithm.

Section~\ref{database schema} describes the database schema used by the
parser, explaining in detail the tables used for storing the data gleaned
from the logs and the table that stores the rules.

Section~\ref{rules} discusses the parsing rules in detail, describing the
table in the database used to hold the rules and explaining their
structure, referring to an example rule and sample data it matches
successfully against.  This section concludes with a discussion of rule
efficiency concerns.

Section~\ref{parsing-algorithm} contains the meat of the paper, describing
a naive parsing algorithm and the complications encountered which shaped
the full algorithm, followed by a comprehensive explanation of the
different stages of the algorithm, the actions taken during execution of
the algorithm, and further complications whose solution completes the
algorithm.

Section~\ref{parsing coverage} analyses the coverage the parser achieves
over a set of 93 log files taken from a mail server handling mail for over
1000 users.  Coverage is described both in terms of the fraction of lines
parsed and the fraction of mails and connections successfully reconstructed
by the parsing algorithm.

Section~\ref{limitations-improvements} lists the limitations of the
algorithm, then suggests some ways of dealing with the limitations, with
the goal of improving parsing and reproduction of the journey a mail takes
through Postfix.

Section~\ref{conclusion} is contains the paper's conclusion.

Appendix~\ref{other-parsers} discusses other parsers and why they were
deemed unsuitable for the task, including why they could not be improved or
expanded upon.

The bibliography contains references to the resources used in developing
the algorithm, writing the program and preparing this paper.  Also listed
are some additional resources expected to be helpful in understanding
\SMTP{}, Postfix, anti-spam techniques, or the paper.

Appendix~\ref{graphs} contains graphs illustrating the topics previously
discussed in section~\ref{rule efficiency}, rule efficiency, and explains
the irregularities observed in the graphs.

Appendix~\ref{Acronyms} provides a list of acronyms used in the paper.

Appendix~\ref{Glossary} provides a glossary of terms used in the paper.

\section{Background}

\label{background}

\subsection{Introduction}

This section provides background information helpful in understanding the
remainder of the paper.  It begins with a discussion of the motivation
underlying the project, followed by some technical information: the use of
a database as an \API{}\@; a brief introduction to
\ifacronymfirstuse{SMTP}{the}{} \SMTP{}\@; and a longer introduction to
Postfix, concentrating on the topics most relevant to this paper, namely
Postfix anti-spam restrictions and policy servers.  The assumptions made in
designing and implementing the parser are explained, as are the conventions
used in this paper.  Other projects which attempt to parse Postfix logs are
summarised (full details are available in
appendix~\refwithpage{other-parsers}), finishing with a review of
previously published research in this area.

\subsection{Motivation}

This paper and the program it describes are part of a larger project to
optimise a server's Postfix restrictions, generate statistics and graphs,
and provide a platform on which new restrictions can be trialled and
evaluated to see if they are beneficial in the fight against spam.  The
program parses Postfix logs and populates a database with the data gleaned
from those logs, providing a consistent and simple view of the logs which
future tools can utilise.  The gathered data can then be used to optimise
current anti-spam measures, provide a baseline to test new anti-spam
measures against, or to produce statistics showing how effective those
measures are.\footnote{Section~\refwithpage{introduction} lists some more
example uses.}

A short example of the optimisation possible using data from the database
is which Postfix restrictions reject the highest number of mails:

\begin{verbatim}
SELECT name, description, restriction_name, hits_total
    FROM rules
    WHERE postfix_action = 'REJECTED'
    ORDER BY hits_total DESC;
\end{verbatim}

If the database supports sub-selects percentages can be
obtained:

\begin{verbatim}
SELECT name, description, restriction_name, hits_total,
        (hits_total * 100.0 /
            (SELECT SUM(hits_total)
                FROM rules
                WHERE postfix_action = 'REJECTED'
            )
        ) || '%' AS percentage
    FROM rules
    WHERE postfix_action = 'REJECTED'
    ORDER BY hits_total DESC;
\end{verbatim}

\SQL{} note: $||$ is the concatenation operator in SQLite3; if the database
containing the extracted data does not support this syntax, then simply
remove `` $||$ '$\%$'\hspace{1ex}'' from the query --- the results will be
the same, just slightly less visually pleasing.

Another example is determining which restrictions are not effective: this
example shows which restrictions had fewer than 100 hits on the last log
file parsed.

\begin{verbatim}
SELECT name, description, restriction_name, hits,
        (hits * 100.0 /
            (SELECT SUM(hits)
                FROM rules
                WHERE postfix_action = 'REJECTED'
            )
        ) || '%' AS percentage
    FROM rules
    WHERE postfix_action = 'REJECTED'
        AND hits < 100
    ORDER BY hits ASC;
\end{verbatim}

These database queries yield summary statistics about the efficiency of
spam avoidance techniques that is far less feasible to assess directly from
log files without prior pre-processing into a database along the lines
proposed, implemented and tested herein.

\subsection{Database as Application Programming Interface}

The database populated by this program provides a simple interface to
Postfix logs.  Although the interface is a database schema, it is in effect
quite similar to any other \API{} provided by shared code: it insulates
both user and provider of the \API{} from changes in the implementation of
the \API{}\@.  The algorithm implemented by the parser can be improved;
support can be added for earlier or later releases of Postfix; bugs can be
fixed or limitations removed from the parser; these changes will not cause
the user to be negatively impacted.  Statistics and/or graphs can be
generated from the database; new restrictions can be tested and the results
inspected; trends in the fight against spam can emerge from historical data
saved in the database; the parser remains the same as the usage adapts.
Using a database simplifies writing programs which need to interact with
the data in several ways:

\begin{enumerate}

    \item The majority of programming languages have existing code
        available allowing access to databases, so the majority of
        languages will be able to access the results obtained by running
        the parser.  If this parser was written to be utilised as shared
        code there would be little or no difficulty in using it when
        writing in the programing language it is implemented in, but every
        other programing language wishing to utilise it would require an
        interface layer to be written.  This requirement would drastically
        reduce the viability of any project wishing to build upon the work
        of this project.

    \item Databases provide complex querying and sorting functionality to
        the user without requiring large amounts of programming.  All
        databases provide a program, of varying complexity and
        sophistication, which can be used for ad hoc queries with minimal
        investment of time.

    \item Databases are easily extensible, e.g.:

        \begin{itemize}

            \item Other tables can be added to the database, e.g.\ to cache
                historical data.

            \item New columns can be added to the tables used by the
                program, with sufficient DEFAULT clauses or a clever
                TRIGGER or two.\footnote{Please refer to an \SQL{} guide
                for explanations of these terms,
                e.g.~\cite{sql-for-web-nerds}}

            \item A VIEW gives a custom arrangement of data with very
                little effort.

            \item If the database supports it, access can be granted on a
                fine-grained basis so that the finance department can
                produce invoices, the helpdesk can run limited queries as
                part of dealing with support calls, and the administrators
                have full access to the data.

            \item Triggers can be written to perform actions when certain
                events occur.  In pseudo-\SQL{}\@:

\begin{verbatim}
CREATE TRIGGER ON INSERT INTO results
    WHERE sender = 'boss@example.com'
        AND postfix_action = 'REJECTED'
    SEND PANIC EMAIL TO 'postmaster@example.com';
\end{verbatim}

        \end{itemize}


    \item \SQL{} is reasonably standard and many people will already be
        familiar with it; for those unfamiliar with it there are lots of
        readily available resources from which to learn.  Although every
        vendor implements a different dialect of \SQL{}, the basics are the
        same everywhere (analogous to the overall similarities and minor
        differences between British English, American English and
        Australian English).  A good introduction to \SQL{} can be found
        at~\cite{sql-for-web-nerds}, others
        are~\cite{w3schools-sql-tutorial, sqlcourse.com}.

\end{enumerate}

Storing the results in a database will also increase the efficiency of
using those results, as the logs need only be parsed once; indeed the
results may be used by someone with no access to the original logs.



\subsection{SMTP background}
\label{SMTP background}

The \SMTPlong, originally defined in \RFC{}~821~\cite{RFC821} and updated
in \RFC{}~2821~\cite{RFC2821}, is used for transferring mail between the
sending and receiving \MTA{}\@.  It is a simple, human readable, plain text
protocol, making it quite easy to test and debug problems with it.  Despite
the simplicity many virus and/or spam sending programs fail to implement it
properly, so requiring strict adherence to the protocol specification is
beneficial in protecting against spam and viruses.\footnote{This violates
the principle of \textit{Be liberal in what you accept, and conservative in
what you send\/} from \RFC{}~760~\cite{rfc760}, but unfortunately that
principle was written in a friendlier time.}  A typical \SMTP{}
conversation resembles the following (the lines starting with a three digit
number are sent by the server, all other lines are sent by the client):

\begin{verbatim}
220 smtp.example.com ESMTP
HELO client.example.com
250 smtp.example.com
MAIL FROM: <alice@example.com>
250 2.1.0 Ok
RCPT TO: <bob@example.com>
250 2.1.5 Ok
DATA
354 End data with <CR><LF>.<CR><LF>
Message headers and body sent here.
.
250 2.0.0 Ok: queued as D7AFA38BA
QUIT
221 2.0.0 Bye
\end{verbatim}

A detailed description of \SMTP{} is beyond the scope of this document:
introductory guides can be found at~\cite{smtp-intro-01, smtp-intro-02}.

\subsection{Postfix background}

Postfix is a highly configurable, high performance, secure and scalable
Mail Transport Agent.  It features extensive and extensible optional
anti-spam restrictions, allowing an administrator to deploy those
restrictions which they judge suitable for their site's needs, rather than
a fixed set chosen by Postfix's author.  These restrictions can be
selectively applied, combined and bypassed on a per-client, per-recipient
or per-sender basis, allowing varying levels of severity and/or
permissiveness.  Postfix leverages simple lookup tables to support
arbitrarily complicated user-defined sequences of restrictions and
exceptions, with policy servers\footnote{Policy servers will be explained
in section~\refwithpage{policy servers}.} as the ultimate in flexibility.
Administrators can also supply their own rejection messages to make it
clear to senders why exactly their mail was rejected.  Unfortunately this
flexibility has a cost: complexity in the logs generated.  While it is easy
to use standard Unix text processing utilities to determine the fate of an
individual email, following the journey an email takes through Postfix can
be quite difficult.  The logs tend to follow a 90\%-10\% pattern: 90\% of
the time the journey is simple, but the other 10\% of the time requires
90\% of the code.\footnote{These numbers don't have a solid scientific
basis, they're based on gut feeling from writing and debugging the
software.}

Postfix's design follows the Unix philosophy of \textit{Write programs that
do one thing and do it well\/}~\cite{unix-philosophy}, and is separated
into various component programs to perform the tasks required of an
\MTA{}\@: receive mail, send mail, local delivery of mail, etc. --- full
details can be found at~\cite{postfix-overview}.  Each log line contains
the name of the Postfix program which produced it, and this information is
used when determining which rules should be used to parse each log line
(see section~\refwithpage{rule characteristics} for details.)

\subsubsection{Mixing and matching Postfix restrictions}

Postfix restrictions are documented fully in~\cite{smtpd_access_readme,
smtpd_per_user_control, policy-servers}, the following is a brief overview.

Postfix uses several restriction lists (each containing zero or more
restrictions) to decide the fate of an \SMTP{} command, one list for each
stage of the \SMTP{} conversation: client connection, HELO command, MAIL
FROM command, RCPT TO command (possibly multiple times), DATA command, and
end of data.  By default restriction lists will not be evaluated until the
first RCPT TO command is received, because some clients don't deal with
earlier rejections properly; a side benefit of this delay is that Postfix
has more information available when logging the rejection.\footnote{The
\texttt{smtpd\_delay\_reject} parameter in Postfix's configuration controls
this behaviour.}  Postfix uses simple lookup tables as the deciding factor
for some restrictions, e.g.\
\texttt{check\_client\_access~cidr:/etc/postfix/client\_access}
\footnote{\texttt{cidr}~\cite{cidr_table} is the type of the lookup table.}
will check whether the \IP{} address of the connected client matches the
left hand side of each line in the file and will return the right hand side
of the first matching line.  Other restrictions determine their result in
different ways, e.g.\ \texttt{reject\_rbl\_client rbl.example.com} checks
whether the \IP{} address of the client is present in the listed \RBL{}\@.

Each restriction is evaluated to produce a result of \textit{reject},
\textit{permit}, \textit{dunno\/} or the name of another restriction to be
evaluated.\footnote{Other results are possible, for full details
see~\cite{smtpd_access_readme, smtpd_per_user_control, policy-servers}.}
The meaning of \textit{permit\/} and \textit{reject\/} is fairly obvious;
\textit{dunno\/} means to stop evaluating the current restriction and
continue with the next restriction in the list, allowing more specific
cases to be used as exceptions to more general cases.  When the result is
the name of another restriction Postfix will evaluate the new restriction,
allowing restrictions to be chosen based on the client \IP{} address, HELO
hostname, sender address, recipient address, etc.  The administrator can
define new restrictions as a list of existing restrictions, allowing
arbitrarily long and complex sequences of lookups and restrictions.

This description is necessarily brief, for further details
see~\cite{smtpd_access_readme, smtpd_per_user_control, policy-servers}.


\subsubsection{Policy servers}

\label{policy servers}

A policy server~\cite{policy-servers} is an external program that accepts
state information from Postfix for each \SMTP{} command not rejected by an
earlier restriction; the policy server can utilise that state information
to implement whatever logic is required.  E.g.\ some users can be
restricted to sending mail on the third Tuesday after pay day only --- this
example may actually be useful in a payroll system to prevent problems from
spam or (worse) phishing mails with faked sender addresses.  More commonly
encountered scenarios are:

\begin{itemize}

    \item Checking \SPF{} records~\cite{openspf, Wikipedia-spf}.  \SPF{}
        records specify which mail servers are allowed to send mail
        claiming to be from a particular domain.  The intention is to
        reduce spam from faked sender addresses,
        backscatter~\cite{postfix-backscatter} and joe
        jobs~\cite{Wikipedia-joe-job}; however there has been a lot of
        resistance to the proposal because it breaks or vastly complicates
        some features of \SMTP{}\@.

    \item Greylisting~\cite{greylisting} is a technique that temporarily
        rejects mail when the triple of (sender, recipient, remote \IP{}
        address) is unknown; on second and subsequent delivery attempts
        from that triple the mail will be accepted.  The assumption is that
        maintaining a list of failed addresses and retrying after a
        temporary failure is uneconomical for a spammer, but that a
        legitimate mail server must retry.  Sadly spammers are using
        increasingly complex and well written programs to distribute spam,
        frequently using an \ISP{} provided \SMTP{} server from a
        compromised machine on the \ISP{}'s network.  Greylisting will
        slowly become less useful, but it does block a large percentage of
        spam mail at the moment; the most effective restrictions over the
        93 log files used in testing the parser are shown in
        table~\refwithpage{Summary of rejections}.  Greylisting is
        obviously worth using, at least at the moment, particularly when
        you factor in Greylisting's position as the final restriction which
        a mail must overcome: Greylisting only takes effect for mails which
        have passed every other restriction.

        \begin{table}[ht]
            \caption{Summary of rejections}\label{Summary of rejections}
            \begin{tabular}[]{lr}
                Restriction type    & Total rejections  \\
                Total rejections    & 5283739           \\
                Unknown recipient   & 2338000           \\
                Unknown sender      & 864148            \\
                \RBL{}s             & 807185            \\
                Greylisting         & 497244            \\
                Non-\FQDN{} HELO    & 403652            \\
            \end{tabular}
        \end{table}

    \item Using a scoring system such as
        Policyd-weight~\cite{policyd-weight} where tests accumulate points
        against the sending system --- if the eventual score is too high
        the mail is rejected.

    \item Rate limiting or throttling on a per-sender, per-client or
        per-recipient basis as performed by Policyd~\cite{policyd}.

\end{itemize}

Example attributes taken from~\cite{policy-servers}:

\begin{tabular}[]{ll}

    request                 & smtpd\_access\_policy     \\
    protocol\_state         & RCPT                      \\
    protocol\_name          & SMTP                      \\
    helo\_name              & some.domain.tld           \\
    queue\_id               & 8045F2AB23                \\
    sender                  & foo@bar.tld               \\
    recipient               & bar@foo.tld               \\
    recipient\_count        & 0                         \\
    client\_address         & 1.2.3.4                   \\
    client\_name            & another.domain.tld        \\
    reverse\_client\_name   & another.domain.tld        \\
    instance                & 123.456.7                 \\

\end{tabular}



\subsection{Assumptions}

The algorithm described and the program implementing it make a small number
of (hopefully safe and reasonable) assumptions:

\begin{itemize}

    \item The logs are whole and complete: nothing has been removed, either
        deliberately or accidentally (e.g.\ log rotation gone awry, file
        system filling up, logging system unable to cope with the volume of
        logs).  On a well run system it is extremely unlikely that any of
        these problems will arise, though it is of course possible,
        particularly when undergoing a deluge of spam.

    \item Postfix logs sufficient information to make it possible to
        accurately reconstruct the actions it has taken.

    \item The Postfix queue has not been tampered with, causing unexplained
        appearance or disappearance of mail.

\end{itemize}

In some ways this task is similar to reverse engineering or replicating a
black box program based solely on its inputs and outputs.  Although the
source code is available, reading and understanding it would require a
significant investment of time:

\begin{tabular}[]{llll}

    Postfix 2.2.11  & Postfix 2.3.8   & Postfix 2.4.0 &                   \\
    71548           & 82224           & 83965         & lines of code     \\
    60962           & 67146           & 68675         & lines of comments \\
    16117           & 17647           & 18069         & lines are blank   \\
    148627          & 167017          & 170709        & lines in total    \\

\end{tabular}


\subsection{Conventions used in the paper}

The words \textit{connection\/} and \textit{mail\/} are often used
interchangeably in this paper; in general the word used was chosen based on
the context it appears in.

\subsection{Other parsers}

Ten other parsers have been reviewed in
appendix~\refwithpage{other-parsers} as part of the background research for
this project.  None of the reviewed parsers perform the type of advanced
parsing and log correlation described here; all are intended to perform a
specific parsing and reporting task, rather than be a generic parser,
extracting data and leaving generation of reports from the data to other
programs.  Some parsers save the data extracted to a data store but the
majority discard all data once they have finished running, making
historical analysis impossible.  The other parsers reviewed all produce a
report of greater or lesser complexity and detail, whereas the program
described here doesn't attempt to produce a report at all; that
responsibility is deferred to a separate program, to be developed later.
The parsing algorithm and program described here are designed to enable
much more detailed log analysis by providing a stable platform for
subsequent programs to develop upon.



\subsection{Previous research in this area}

\label{prior art}

There only appears to be one prior paper published about parsing Postfix
log files: \textit{Log Mail Analyzer: Architecture and Practical
Utilizations\/}~\cite{log-mail-analyser}.  The aim of \LMA{} is quite
different from this parser: it attempts to present correlated log data in
a form suitable for a systems administrator to search using grep(1), awk(1)
and the myriad of other standard Unix text processing utilities already
available, producing a \CSV{} file which can subsequently be imported into
a MySQL or Berkeley DB database.  The decision to support both \CSV{} and
Berkeley DB appears to have been a limiting factor: \CSV{} is a very simple
format where a record is stored in a single line, with fields separated by
a comma or other punctuation symbol.  Problems with \CSV{} files include
the difficulty in escaping separators, providing multiple values for a
field (e.g.\ multiple recipients), adding new fields, and lack of a bundled
schema describing the fields.

Berkeley DB is not an \SQL{} database, supporting only \textbf{(key,
value)} pairs; in the main table the key is an integer referred to by
secondary tables and the value used is a \CSV{} line containing all the
data for that row.  The secondary by-sender, by-recipient, by-date and
by-\IP{} tables use the sender/recipient/date/\IP{} as the key, and a
\CSV{} list of integers referring to the main table is the value.  In
effect this re-implements \SQL{} foreign keys, but without the
functionality offered by even the most basic of \SQL{} databases (joins,
ordering, searches, etc.).  It also requires custom code to search on some
combination of the above, though the authors of \LMA{} did provide some
queries: IP-STORY, FROM-STORY, DAILY-EMAIL and DAILY-REJECT\@.  Berkeley DB
appears to be the least useful of the three output formats

The data stored is limited to time and date, hostname and \IP{} address of
client, sender and recipient addresses, and the \SMTP{} code.  Handling of
multiple recipients, \SMTP{} codes or remote servers\footnote{A single mail
may be sent to multiple remote servers if it was addressed to recipients in
different domains, or Postfix needs to try multiple servers for one or more
recipients.} is not explained in the paper, but the options appear to be:

\begin{enumerate}

    \item Discard some of the data; this option is obviously undesirable,
        and presumably was not chosen by the \LMA{} authors.

    \item Combine all data into a \CSV{} line, which is then embedded into
        the main \CSV{} line for that mail, making searching even more
        difficult and requiring a second separating character.

\end{enumerate}

The schema used with the MySQL database is undocumented, but at least it's
possible to discover the schema with an \SQL{} database, unlike with
Berkeley DB\@; all \SQL{} databases embed the database schema into the
database and provide commands for displaying it.  Berkeley DB does not
embed a schema, as there is neither requirement nor benefit; it provides
\textbf{(key, value)} pairs, so any additional structuring of the data is
imposed by the application, thus it behoves the application to document
this structure.

\parsername{} is bundled with extensive documentation, including an
in-depth explanation of the database schema.  It supports multiple values
for every field for which multiple values are reasonable (e.g.\ the sender
address should have one value only, whereas the recipient address can have
multiple values).  The default database it uses is SQLite3, which supports
most standard \SQL{} features (its lacks full foreign key support), has
libraries available for most common programming languages, and a simple
text based client allowing ad-hoc queries with minimal effort.

Parsing in \LMA{} is hand-coded and requires major changes to the code to
parse new log lines or extract additional data.  It does not appear to deal
with any of the complications discussed in
sections~\refwithpage{complications} and~\refwithpage{additional
complications}.  The program fails to parse Postfix 2.2.x or 2.3.x log
files; indeed it produces a number of warnings and no usable output.
Parsing in \parsername{} is divided in two (see
sections~\refwithpage{rules} and~\refwithpage{parsing-algorithm} for a more
detailed explanation): parsing a new log line is a trivial matter of adding
a new rule, but code changes may be required if new complications
(sections~\refwithpage{complications} and~\refwithpage{additional
complications}) must be dealt with or a new action is required
(section~\refwithpage{actions-in-detail}).

\LMA{} does provide some simple reports (IP-STORY, FROM-STORY, DAILY-EMAIL
and DAILY-REJECT); the author was unable to parse any logs with \LMA{}, so
didn't have an opportunity to review the reports.  \parsername{} does not
have any bundled reports, it delegates that task to a separate, dedicated
program.

In summary \LMA{} appears to be a proof of concept, written to demonstrate
the point of the paper (that having this information in an accessible
fashion is useful to systems administrators), rather than a program
designed to be extensible and useful in a production environment.

% Literature review notes:
%
% Hard-coded parsing, requiring code changes to add more.  Attempts to
% correlate log lines, saves data to database for data mining purposes.
% Hard to extend/expand/understand.  Appears to only save: date and hour,
% \DNS{} name and \IP{} address host, mail server \IP{} address, sender,
% receiver and e-mail status (sent, rejected).  Undocumented schema.
% Design decision to use \CSV{} as an intermediate format between the log
% file and the database seems to have been restrictive.  Appears to require
% a queueid but majority of log entries (e.g. rejections) lack a queueid.
% Supports whitelisting \IP{} addresses when parsing logs, but whitelisting
% when generating reports/data mining would be preferable.  Supporting
% Berkeley DB is probably limiting the software - an example is the
% difficulty in searching a pipe-delimited string, so they have
% re-implemented foreign keys with tables keyed by ip address etc. pointing
% at the main table - this also won't scale well.  There doesn't appear to
% be any attempt to deal with the complications I've encountered: their
% parsing isn't detailed enough to encounter them.  It doesn't run
% properly; doesn't create any output; throws up errors.

\subsection{Conclusion}

This section has provided background information on several topics relevant
to the remainder of the paper.  It started with the motivation behind the
project, continuing with explanations of:

\begin{itemize}

    \item Using a database as an \API{}.

    \item \SMTP{}.

    \item Postfix restrictions and policy servers.

\end{itemize}

The assumptions which must be satisfied for the parser to work correctly
were discussed; the conventions used in the paper were listed; a brief
comparison of this project against other Postfix parsers was provided,
followed by a review of the previously published literature.

\section{Database schema}
\label{database schema}

The database is an integral part of the parser: it stores the rules and the
data gleaned by applying those rules to Postfix log files.  Understanding
the database schema is important in understanding the actions of the
parser, and essential to developing further applications which utilise the
data gathered.

\subsection{Introduction}

The database schema can be conceptually divided in two: the rules which are
used to parse log files, and the data saved from the parsing of log files.
Rules have the fields required to parse the log lines, extract data to be
saved, and the action to be executed; they also have several fields which
aid the user in understanding what each rule parses.  The rules are
described in detail in section~\refwithpage{rules} but the fields are
covered here.

The data saved from parsing the logs is also divided into two tables as
described below: connections and results.  The connections table contains a
row for every mail accepted and every connection where there was a
rejection; the individual fields will be described in the forthcoming
section~\refwithpage{connections table}.  The results table has one or more
rows per row in the connections table, depending on the specifics of the
mail/connection; the fields will be covered in detail in
section~\refwithpage{results table}.

\subsection{Rules table}

\label{rule attributes}

Rules are discussed in detain in section~\refwithpage{rules}, but the rules
table is covered here.  Rules are created by the user, not the parser, and
will not be modified by the parser (except for the hits and hits\_total
fields).  Rules parse the individual lines, telling the parser which fields
to extract and what action to take for that line.

Each rule defines the following:

\begin{description}

    \item [name] A short name for the rule.

    \item [description] Something must have occurred to cause Postfix to
        log each line (e.g.\ a remote client connecting causes a connection
        line to be logged).  This field describes the action causing the
        log lines this rule matches.

    \item [restriction\_name] The restriction which caused the mail to be
        rejected.  Only applicable to rules which have a result of
        \texttt{rejected}, other rules will have an empty string.

    \item [postfix\_action] This is the action Postfix must have taken to
        generate this line, with two exceptions:

        \begin{description}

            \item [INFO] Represents an unspecified intermediate action that
                the parser is not interested in per se, but which does log
                useful information, supplementing other log lines.

            \item [IGNORED] An action which is not only uninteresting in
                itself, but which also provides no useful data.

        \end{description}

        Uninteresting lines are parsed so that any lines the parser isn't
        capable of handling become immediately obvious errors.

    \item [program] The program (\daemon{smtpd}, \daemon{qmgr}, etc.) whose
        log lines the rule applies to.  This avoids needlessly trying rules
        which won't match the line, or worse, might match unintentionally.
        Rules whose program is \texttt{*} will be tried against any lines
        which aren't parsed by program specific rules.

    \item [regex] The \regex{} to match the log line against.  The \regex{}
        will first have several keywords expanded: this simplifies reading
        and writing rules; avoids needless repetition of complex \regex{}
        components; allows the components to be corrected and/or improved
        in one location; and makes each \regex{} largely self-documenting.

        The following keywords are expanded (full explanations can be found
        in the source code):

        \_\_SENDER\_\_, \_\_RECIPIENT\_\_, \_\_MESSAGE\_ID\_\_,
        \_\_HELO\_\_, \newline \_\_EMAIL\_\_, \_\_HOSTNAME\_\_, \_\_IP\_\_,
        \_\_IPv4\_\_, \_\_IPv6\_\_, \newline \_\_SMTP\_CODE\_\_,
        \_\_RESTRICTION\_START\_\_, \_\_QUEUEID\_\_, \newline
        \_\_COMMAND\_\_, \_\_SHORT\_CMD\_\_, \_\_DELAYS\_\_, \_\_DELAY\_\_,
        \_\_DSN\_\_ and \_\_CONN\_USE\_\_.

        Additional fields are captured by \_\_RESTRICTION\_START\_\_, so
        rules using it will start the fields in result\_cols,
        connection\_cols, etc.\ at 5.

        For efficiency the keywords are expanded and every rule's \regex{}
        is compiled before attempting to parse the log file --- otherwise
        each \regex{} would be recompiled each time it was used, resulting
        in a large, data dependent slowdown.  Rule efficiency concerns are
        discussed in section~\refwithpage{rule efficiency}.

    \item [result\_cols, connection\_cols] Specifies how the fields in the
        log line will be extracted.  The format is: \newline
        \texttt{smtp\_code = 1; recipient = 2, sender = 4;} \newline i.e.\
        semi-colon or comma separated assignment statements, with the
        variable name on the left and the matching field from the \regex{}
        on the right hand side.  The list of acceptable variable names is:

        \texttt{connection\_cols: client\_hostname, client\_ip, server\_ip,
        \newline \hspace*{2em} server\_hostname} and \texttt{helo.\newline}
        \texttt{result\_cols: sender, recipient, smtp\_code, message\_id,
        \newline \hspace*{2em} pid\_regex, data} and \texttt{child.}

    \item [result\_data, connection\_data] Sometimes rules need to supply a
        piece of data which isn't present in the log line: e.g.\ setting
        \texttt{smtp\_code} when mail is accepted.  The format and allowed
        variables are the same as for \texttt{result\_cols} and
        \texttt{connection\_cols}, except that arbitrary
        data\footnote{Commas and semi-colons cannot be escaped and thus
        cannot be used.  This is intended for use with small amounts of
        data rather than large amounts, so dealing with escape sequences
        seemed unnecessary.} is permitted on the right hand side of the
        assignment.

    \item [action] The action the algorithm will take; a full list can be
        found in Section~\refwithpage{actions-in-detail}.

    \item [queueid] Specifies the matching field from the \regex{} which
        gives the queueid, or zero if the log line doesn't contain a
        queueid.

    \item [hits] is an efficiency measure.  This counter is maintained for
        every rule and incremented each time the rule successfully matches.
        At the start of each run the program sorts the rules in descending
        order of hits, and at the end of the run updates every rule's hits.
        Assuming that the distribution of log lines is reasonably
        consistent between log files, rules matching more commonly
        occurring log lines will be tried before rules matching less
        commonly occurring log lines, lowering the program's execution
        time.  Rule ordering for efficiency is discussed in
        section~\refwithpage{rule ordering for efficiency}.

    \item [hits\_total] The total number of hits for this rule over all
        runs of the parser.

    \item [priority] This is the user-configurable companion to hits: rules
        will be tried in order of priority, overriding hits.  This allows
        more specific rules to take precedence over more general rules
        (described in section~\refwithpage{overlapping rules}).

\end{description}


\subsection{Connections table}

\label{connections table}

Every accepted mail and every connection where there was a rejection will
have a single entry in the connections table containing the following
fields:

\begin{description}

    \item [id] This field uniquely identifies the row.

    \item [server\_ip] The \IP{} address (IPv4 or IPv6) of the server: the
        local server when receiving mail, the remote server when sending
        mail.

    \item [server\_hostname] The hostname of the server, it will be
        \texttt{unknown} if the \IP{} address could not be resolved to a
        hostname via \DNS{}\@.

    \item [client\_ip] The client \IP{} address (IPv4 or IPv6): the remote
        server when receiving mail, the local server when sending mail.

    \item [client\_hostname] The hostname of the client, it will be
        \texttt{unknown} if the \IP{} address could not be resolved to a
        hostname via \DNS{}\@.

    \item [helo] The hostname used in the HELO command.  The HELO
        occasionally changes during a connection, presumably because spam
        or virus senders think it's a good idea.  By default Postfix only
        logs the HELO when it rejects an \SMTP{} command, but it is quite
        easy to rectify this: 

\label{logging helo}

        \begin{enumerate}

            \item Create \texttt{/etc/postfix/log\_helo.pcre}
                containing:\newline
                \hspace*{2em}\texttt{/./~~~~WARN~Logging~HELO}

            \item Modify \texttt{smtpd\_data\_restrictions} in
                \texttt{/etc/postfix/main.cf} to contain\newline
                \hspace*{2em}\texttt{check\_helo\_access~/etc/postfix/log\_helo.pcre}

        \end{enumerate}

        Although \texttt{smtpd\_helo\_restrictions} seems like the natural
        place to log the HELO hostname, there won't be a queueid associated
        with the mail for the first recipient, so that log line cannot be
        associated with the correct mail.  There is guaranteed to be
        queueid when the DATA command has been reached, and thus it will be
        logged by any restrictions taking effect in
        \texttt{smtpd\_data\_restrictions}.  There is no difficulty in
        specifying a HELO-based restriction in
        \texttt{smtpd\_data\_restrictions}, Postfix will perform the check
        correctly.

    \item [queueid] The queueid of the mail if the connection represents an
        accepted mail, or \texttt{NOQUEUE} otherwise.

    \item [start] The timestamp of the first log line, in seconds since the
        epoch.

    \item [end] The timestamp of the last log line, in seconds since the
        epoch.

\end{description}

\subsection{Results table}

\label{results table}

Every log line corresponding to Postfix performing an action other than
INFO or IGNORED will have an entry in the results table, e.g.\ rejecting an
\SMTP{} command, delivering a mail, or bouncing a mail.  Any log line where
the Postfix action is INFO is not interesting in and of itself, but
provides additional information which will be saved with other results;
IGNORED log lines don't even provide useful information.  Each row is
associated with a single connection, though there may be many results per
connection.

\begin{description}

    \item [connection\_id] A reference to the row in the connections table
        this result's table row belongs to.

    \item [rule\_id] A reference to the entry in the rules table which
        matched the log line and created this result.

    \item [warning] Postfix can be configured to log a warning instead of
        enforcing a restriction that would reject an \SMTP{} command --- a
        facility that is quite useful for testing new restrictions.  This
        field will be 1 if the log line parsed was a warning rather than a
        real rejection, or 0 for a real rejection or any other result.

    \item [smtp\_code] The \SMTP{} code associated with the log line.  In
        general an \SMTP{} code is only present for a rejection or final
        delivery; results missing an \SMTP{} code will duplicate the
        \SMTP{} code of other results in the connection.  Some final
        delivery log lines don't contain an \SMTP{} code: in those cases
        the code is faked based on the success or failure represented by
        the log line.

    \item [sender] The sender's email address.  This can change during one
        single connection, when the connection is reused to send multiple
        mails.

    \item [recipient] The recipient address.

    \item [message\_id] The message-id of the accepted mail, or
        \texttt{NULL} if no mail was accepted.

    \item [data] A field available for anything not covered by other
        fields, e.g.\ the rejection message from an \RBL{}\@.

    \item [timestamp] The time the line was logged, in seconds since the
        epoch.

\end{description}

\subsection{Conclusion}

The table containing the rules used by the parser, and both tables
containing the data extracted from the Postfix logs were described, with
the purpose of each field discussed in detail.  A clear, comprehensible
schema is essential when using the extracted data; it's more important when
using the data than when storing it, because storing the data is a
write-once operation, whereas utilising the data requires frequent
searching, sorting and manipulation of the data to produce customised
reports and/or statistics.

\section{Parsing rules}

\label{rules}

\subsection{Introduction}

This section discusses the rules used in parsing Postfix log files,
starting with rule characteristics, continuing with detecting and dealing
with overlapping rules, and the problems they can cause.  An example rule
and a line it would match are provided, plus a description of how the
fields in the rule are used when matching and performing the action (the
details of the table containing the rules have already been described in
section~\refwithpage{rule attributes},).  This section continues with a
discussion of rule efficiency concerns, referring to the graphs in
section~\refwithpage{graphs}, and finishes with an explanation of how to
add or edit rules, including the algorithm used to generate a new \regex{}
from unparsed lines.


\subsection{Rule characteristics}

\label{rule characteristics}

The complexity and variation in Postfix's logs requires similar flexibility
in the parser, which is split into two parts: the parsing algorithm and the
parsing rules which are applied to the lines.  Decoupling the parsing rules
from the associated actions allows new rules to be written and tested
without requiring modifications to the algorithm source code (significantly
lowering the barrier to entry for new or casual users who need to parse new
log lines), and greatly simplifies both algorithm and rules.  Decoupling
also creates a clear separation of functionality: rules handle low level
details of identifying log lines and extracting data from a line, whereas
the algorithm handles the higher level details of following the path a mail
takes through Postfix, assembling the required data before storing it,
dealing with complications arising, etc.

Rule have certain characteristics which may help in understanding the
parser:

\begin{itemize}

    \item Rules are annotated with the name of a Postfix program, and will
        only be used when parsing log lines produced by that
        program.\footnote{There are also generic rules which are used when
        parsing log lines produced by any Postfix program, but only if
        there are also rules specific to that program, and those rules must
        already have been tried and failed on the current line.}  Any given
        rule will only be used to parse a subset of the log lines, and any
        given log line will only be parsed by a subset of the rules.

    \item The first matching rule wins: no further rules are tried against
        that line, but there is a facility for specifying the order of
        rules so that more specific rules can be tried first.

    \item Rules are completely self-contained and can be understood in
        isolation, without reference to any other rules.

    \item There are no sub-rules, so rules have linear computational
        complexity.

\end{itemize}

\label{comparison against context-free grammars}

In context-free grammar terms the parser rules could be described as:

$\text{\textless{}log-line\textgreater{}} \mapsto \text{rule-1} |
\text{rule-2} | \text{rule-3} | \dots | \text{rule-n}$


\subsection{Overlapping rules}

\label{overlapping rules}

The parser does not try to detect overlapping rules;\footnote{It may be
possible to parse each rule's \regex{}, and determine if any overlap.  The
author has not attempted to do this: such a project by itself would
probably qualify for a PhD, and may involve solving the Halting
Problem~\cite{Wikipedia-halting-problem} and circumventing the
Church-Turing Thesis~\cite{Wikipedia-church-turing-thesis}.} that
responsibility is left to the author of the rules.  Unintentionally
overlapping rules lead to inconsistent parsing and data extraction because
the order in which rules are tried against each line is unspecified, and
the first matching rule wins.  Overlapping rules are frequently a
requirement, allowing a more specific rule to match some lines and a more
general rule to match the majority, e.g.\ separating \SMTP{} delivery to a
specific sites from \SMTP{} delivery to the rest of the world.  The
algorithm provides a facility for ordering overlapping rules: the priority
field in each rule (defaults to zero).  Rules are sorted by priority,
highest first, and then rules with the same priority are sorted by the
number of successful matches when parsing the previous log file.  Negative
priorities may be useful for catchall rules.

Detecting overlapping rules is difficult, but the following may be helpful:

\begin{itemize}

    \item Sort by \regex{} and visually inspect the list, e.g.\ with \SQL{}
        similar to: \textbf{select regex from rules order by regex;}

    \item Compare the results of parsing using sorted, shuffled and
        reversed rules.\footnote{See section~\refwithpage{rule efficiency}
        for more details of sorting the rules.}  Parse a number of log
        files using normal sorting, dump a textual representation of the
        connections and results tables, and delete everything from those
        tables.  Repeat with shuffled and reversed sorting.  If there are
        no overlapping rules the tables from each run will be identical;
        differences indicate overlapping rules.  Which rules overlap can be
        determined by examining the differences in the tables: each result
        contains the id of the rule which created it, so the rule
        referenced in the normal table overlaps with the rule referenced in
        the reversed table.  Unfortunately this method cannot prove the
        absence of overlapping rules; it can detect overlapping rules, but
        only if there are log lines in the input files which match more
        than one rule.

\end{itemize}

\subsection{Example rule}

\label{example rule}

This example rule matches the message logged by Postfix when it rejects
mail from a sender address because the appropriate \DNS{} entries are
missing, i.e.\ mail could not be delivered to the sender's address (for
full details see~\cite{reject-unknown-sender-domain}).

This rule would match the following log line:

\begin{verbatim}
NOQUEUE: reject: RCPT from example.com[10.1.1.1]: 
  550 <foo@example.com>: Sender address rejected:
  Domain not found; from=<foo@example.com>
  to=<info@example.net> proto=SMTP
  helo=<smtp.example.com>
\end{verbatim}

% Don't reformat this!
\begin{tabular}[]{ll}

\textbf{Field}      & \textbf{Value}                                    \\
name                & Unknown sender domain                             \\
description         & We do not accept mail from unknown domains        \\
restriction\_name   & reject\_unknown\_sender\_domain                   \\
postfix\_action     & REJECTED                                          \\
program             & \daemon{smtpd}                                    \\
regex               & \verb!^__RESTRICTION_START__ <(__SENDER__)>: !    \\
                    & \verb!Sender address rejected: Domain not found;! \\
                    & \verb!from=<\5> to=<(__RECIPIENT__)> !            \\
                    & \verb!proto=E?SMTP helo=<(__HELO__)>$!            \\
result\_cols        & recipient = 6; sender = 5                         \\
connection\_cols    & helo = 7                                          \\
result\_data        &                                                   \\
connection\_data    &                                                   \\
action              & REJECTION                                         \\
queueid             & 1                                                 \\
hits                & 0                                                 \\
hits\_total         & 0                                                 \\
priority            & 0                                                 \\

\end{tabular}

\vspace{1em}

The various fields are used as follows;

\begin{description}

    \item [name, description, restriction\_name and postfix\_action:] are
        not \newline used by the algorithm, they serve to document the rule
        for the user's benefit.

    \item [program and regex:] If the program in the rule equals the
        program which logged the line the \regex{} will be tried against
        the line; if the match is successful the action will be executed,
        if not the next rule will be tried.  If the program-specific rules
        don't match the log line, the generic rules will be tried in the
        same way.

    \item [action:] will be executed if the \regex{} matches successfully
        (see section~\refwithpage{actions-in-detail} for full details).

    \item [result\_cols, connection\_cols, result\_data and
        connection\_data:] are \newline used by the action to extract and
        save data matched by the \regex{}.

    \item [queueid:] The index of the field in the \regex{} which captured
        the queueid, or zero if the line does not contain a queueid.  This
        allows the correct mail can be found by queueid and actions
        performed on it.

    \item [hits, hits\_total and priority:] hits and priority are used in
        ordering the rules (see section~\refwithpage{rule ordering for
        efficiency}); hits is set to the number of successful matches at
        the end of the parsing run, and hits\_total has hits added to it,
        but is otherwise unused by the algorithm.

\end{description}

Additional fields are captured by \_\_RESTRICTION\_START\_\_, hence the
fields in result\_cols and connection\_cols start at 5 in the example.


\subsection{Rule efficiency}

\label{rule efficiency}

Parsing efficiency is an obvious concern when the parser routinely needs to
deal with 75 MB log files containing 300,000 log lines (for a small mail
server --- large scale mail servers would have much larger log files on a
daily basis).  When generating the data for the graphs included in
appendix~\refwithpage{graphs}, 93 log files (totaling 10.08 GB, 60.72
million lines) were each parsed 10 times, the first run discarded, and the
remaining 9 runs averaged.  The first run is discarded for two reasons:

\begin{enumerate}

    \item The execution time will be higher because the log file must be
        read from disk, whereas for subsequent runs the log file will be
        cached in memory by the operating system.

    \item The execution time will also be higher because the rule ordering
        will be sub-optimal compared to subsequent runs.

\end{enumerate}

Saving results to the database was disabled for the test runs, as that
dominates the run time of the program, and the tests are aimed at measuring
the speed of the parser rather than the speed of the database.

\subsubsection{Algorithmic complexity}

An important property of a parser is how execution time scales with input
size: does it scale linearly, polynomially, or exponentially?
Graph~\refwithpage{execution time vs file size vs number of lines graph}
shows the execution time in seconds, file size in MB and tens of thousands
of lines per log file per log file.  All three lines run roughly in
parallel, giving a visual impression that the algorithm scales linearly
with input size.  This impression is borne out by
graph~\refwithpage{execution time vs file size vs number lines factor},
which plots the ratio of file size vs execution time and number of lines vs
execution time.  As the reader can see, the ratios are quite tightly
banded, showing that the algorithm scales linearly: the much larger log
files between points 60 and 70 on the X axis in
graph~\refwithpage{execution time vs file size vs number of lines graph}
don't cause any abnormality in the corresponding points in
graph~\refwithpage{execution time vs file size vs number lines factor}.


\subsubsection{Rule ordering for efficiency}

\label{rule ordering for efficiency}

Rule ordering was mentioned in section~\refwithpage{rule attributes} and
will be covered in greater detail in this section.  At the time of writing
there are 139 different rules, with the top 10\% matching the vast majority
of the log lines, and the remaining log lines split across the other 90\%
of the rules (as shown in graph~\refwithpage{rule hits graph}).  Assuming
that the distribution of log lines is reasonably steady over time, program
efficiency should benefit from trying more frequently matching rules before
those which match less frequently.  To test this hypothesis three full test
runs were performed with different rule orderings:

\begin{description}

    \item [normal]  The most optimal order, according to the hypothesis:
        rules which match most often will be tried first.

    \item [shuffle] Random ordering --- the rules will be shuffled once
        before use and will retain that ordering for the entirety of the
        log file.  Note that the ordering will change every time the parser
        is executed, so 10 different orderings will be generated for each
        log file in the test run.  This is intended to represent an
        unsorted rule set.

    \item [reverse] Hypothetically the least optimal order: the most
        frequently matching rules will be tried last.

\end{description}

Graphs~\refwithpage{percentage increase of shuffled over normal}
and~\refwithpage{percentage increase of reversed over normal} show the
percentage increase of execution times (mean and standard deviation are
shown in table~\refwithpage{Regex caching/discarding with different groups
of log files}).  Overall this provides a modest but worthwhile
performance increase.

\subsubsection{Caching each regex}

Perl compiles the original \regex{} into an internal representation,
optimising the \regex{} to improve the speed of matching, but this
compilation and optimisation takes CPU time.  Perl automatically caches
static \regexs{}, but dynamic \regexs{} need to be explicitly compiled and
cached.  Graph~\refwithpage{normal regex vs discard regex} shows execution
times with and without caching the \regex{}.  Caching the compiled
\regexs{} is obviously far more efficient; graph~\refwithpage{normal regex
vs discarded regex factor} shows the percentage execution time increase
when not caching each \regex{}.

Caching the compiled \regexs{} is quite simple, and is the single most
effective optimisation implemented in the parser.

\subsection{Creating new rules}

The logs produced by Postfix differ from installation to installation, as
administrators have the freedom to choose the subset of available
restrictions which suits their needs, including using different \RBL{}
services, policy servers, or custom rejection messages.  To facilitate
this, the parser's design separates parsing rules from parsing actions:
adding new actions is difficult, but adding new rules to parse new
rejection messages is trivial (and occurs much more frequently).  The
implementation provides three programs to ease the process of adding new
rules: \texttt{add-new-rule}, \texttt{edit-rule} and \texttt{logs2regexs}
XXX NEED BETTER NAMES\@.

\texttt{add-new-rule} is the simplest of the three, it just starts the
user's preferred editor with a template rule which the user modifies as
required, and then inserts the new rule into the database.  The various
fields have already been described in sections~\refwithpage{rule
attributes},~\refwithpage{rule characteristics} and~\refwithpage{example
rule}, and will not be repeated here.

\texttt{edit-rule} extracts a rule from the database, saves it in a human
readable form, and runs the user's preferred editor for the user to change
the rule as required.  Once the user is satisfied with the changes, the
database will be updated appropriately.

\texttt{logs2regexs} 

\subsection{Conclusion}

This section dealt with the rules used in parsing Postfix log files:

\begin{itemize}

    \item The characteristics of the rules were described.

    \item Detecting overlapping rules and dealing with the problems they
        can cause was covered, including a discussion of why overlapping
        rules can be helpful as well as harmful.

    \item An example log line and the rule matching it illustrated a
        description of how the fields in the rule are used both in the
        matching phase and the subsequent action that is executed.

    \item The database table containing the rules is dealt with in
        section~\refwithpage{rule attributes}, and is not duplicated in
        this section.

    \item The topic of rule efficiency was discussed next, covering the
        effects of caching compiled \regexs{} and optimal ordering of
        rules, with reference to the graphs in
        appendix~\refwithpage{graphs}.

    \item This section finished with an explanation of how to add or edit
        rules, including the algorithm used to generate a new \regex{} from
        unparsed lines.

\end{itemize}

\section{Parsing algorithm}

\label{parsing-algorithm}

Where the rules are quite simple and each rule is completely independent of
its fellows, the algorithm is significantly more complicated and highly
internally interdependent.  The algorithm deals with all the complications
of parsing, the eccentricities and oddities of Postfix logs, and presents
the resulting data in a normalised, easy to use representation.  The
algorithm's task is to follow the journey each mail takes through Postfix,
piecing the data extracted by rules into a coherent whole, saving it in a
useful and consistent form, and performing housekeeping duties.

XXX WHERE SHOULD THE NEXT PARAGRAPH BE PLACED\@?

\label{why separate rules and algorithm}

Separating the rules and algorithm makes it possible to parse new log lines
without modifying the core parsing algorithm.  Although this may seem like
a trivial point, is it substantially more difficult to understand a
program's entire parsing algorithm, identify the correct location to
change, and make the appropriate changes without adversely affecting
parsing, particularly as there may be edge cases which are not immediately
obvious.\footnote{See
section~\refwithpage{yet-more-aborted-delivery-attempts} for a complication
which occurs only four times in 93 log files tested.}  Requiring changes to
the parsing algorithm also complicates upgrades, as the changes must be
preserved during the upgrade, and may clash with changes made by the
developer.  \parsername{} allows the user to add new rules to the database
without changing the parsing algorithm, unless the new log lines to be
parsed require functionality not already provided by the algorithm.  If the
new log lines do require new functionality, new actions can be added to the
parser without modifying existing actions or other parts of the algorithm;
only in the rare case that the new actions require support from other
sections of the code will more extensive changes be required.

\subsection{Introduction}

This section covers the following topics:

\begin{itemize}

    \item A high level overview of the algorithm.

    \item The first set of complications encountered: initially obvious
        difficulties which had to be overcome.

    \item A flow chart showing common paths a mail can take through the
        algorithm.

    \item An explanation of the paths shown in the flow chart.

    \item The actions the parser makes available to rules are covered in
        detail.

    \item Additional complications which have arisen during the development
        of this parser are documented, including their solutions and where
        those solutions are implemented in the algorithm

\end{itemize}

\subsection{A high level overview}

A high level view of the algorithm could be expressed as:

\begin{enumerate}

    \item Mail enters the system via \SMTP{} or local submission; and new
        data structure is created for it.

    \item If the mail is rejected, log all data and finish.

    \item Follow the progress of the accepted mail until it's either
        delivered, bounced or deleted, then log all data, and finish.

\end{enumerate}

Unfortunately that ignores the many complications encountered.


\subsection{Complications encountered}

\label{complications}


\subsubsection{Queueid vs pid}

The mail lacks a queueid until it has been accepted, so log lines must
first be correlated by the \daemon{smtpd} \pid{}, then transition to being
correlated by the queueid.  This is relatively minor, but does require:

\begin{itemize}

    \item Two versions of several functions: \texttt{by\_pid} and
        \texttt{by\_queueid}.

    \item Two state tables to hold the data structure for each connection.

    \item Most importantly: every section of code must know whether it
        needs to lookup the data structures by \pid{} or queueid.

\end{itemize}

\subsubsection{Connection reuse}

\label{connection reuse}

Multiple independent mails may be delivered during one connection: this
requires cloning the current data as soon as a mail is accepted, so that
subsequent mails won't trample over each other's data.  This must be done
every time a mail is accepted, as it's impossible to tell in advance which
connections will accept multiple mails.  It is quite easy to overlook this
complication because only a small minority of connections accept more than
one mail. Happily once the mail has been accepted log entries won't be
correlated by \pid{} for that mail any more (its queueid will be used
instead), so there isn't any ambiguity about which mail a given log line
belongs to.\footnote{Unfortunately this statement is not completely
accurate: see section~\refwithpage{timeouts-during-data-phase} for details.
However in general there isn't any ambiguity about which data structure
should be used for a given log line.}  The original connection will be
discarded unsaved when the client disconnects if it doesn't have any data
worth saving, i.e.\ no rejections.  One unsolved difficulty is
distinguishing between different groups of rejections, e.g.\ when dealing
with the following sequence:

\begin{enumerate}

    \item The client attempts to deliver a mail, but it is rejected.

    \item The client issues the RSET command to reset the session.

    \item The client attempts to deliver another mail, likewise rejected.

\end{enumerate}

There should probably be two different entires in the database resulting
from the above sequence, but currently there will only be one.



\subsubsection{Re-injected mails}

The most difficult complication initially encountered is that locally
addressed mails are not always delivered directly to a mailbox: sometimes
they are addressed to and accepted for a local address but need to be
delivered to one or more remote addresses due to aliases.  When this occurs
a child mail will be injected into the Postfix queue, but without the
explicit logging \daemon{smtpd} or \daemon{postdrop} injected mails have.
Thus the source is not immediately discernible from the log line in which
the mail first appears; from a strictly chronological reading of the logs
it \textit{usually\/} appears as if the child mail has appeared from thin
air.  Subsequently the parent mail will log the creation of the child mail:

\texttt{3FF7C4317: to=<username@example.com>, relay=local, \newline
delay=0, status=sent (forwarded as 56F5B43FD)}

Unfortunately while all log lines from an individual process appear in
chronological order, the order in which log lines from different processes
are interleaved is subject to the vagaries of process scheduling.  In
addition the first log line belonging to the child mail (the log line cited
above properly belongs to the parent mail) is logged by \daemon{qmgr}, so
the order also depends on how busy \daemon{qmgr} is.\footnote{Postfix is
quite paranoid about mail delivery, an excellent characteristic for an
\MTA{} to possess, so it won't log that the child has been created until it
is absolutely certain that the mail has been written to disk.}

Because of this the parser cannot complain when it encounters a log line
from \daemon{qmgr} for a previously unseen mail; it must flag the mail as
coming from an unknown origin, and subsequently clear the flag if and when
the origin of the mail becomes clear.  Obviously the parser could omit
checking of where mails originate from, but the author believes that it is
better to require an explicit source, as bugs in the parser are more likely
to be exposed.

Process scheduling can have a still more confusing effect: quite often the
child mail will be created, delivered and entirely finished with
\textbf{before} the parent logs the creation line!  Thus mails flagged as
coming from an unknown origin cannot be entered into the database when
their final log line is parsed; instead they must be marked as ready for
entry and subsequently entered by the parent mail once it has been
identified.  Quite apart from identifying mail injected in an unknown
fashion, or bugs in the parser, unknown origin mails need to be marked as
such because they lack some data present in the parent mail; they must copy
that data from their parent before being entered in the database.  The
effect of this complication upon the algorithm is discussed more fully in
section~\refwithpage{tracking re-injected mail}.

XXX FIGURE OUT IF I NEED TO COPY DATA ANY MORE, AND DOCUMENT IT IF SO\@.


\newpage
\subsection{Flow chart}

\label{flow-chart}

Figure~\ref{flow chart image} shows the paths the data representing a
mail/connection can take through the parser algorithm.  The flow chart
covers the most common paths only; there are additional, uncommon paths
which are excluded for the sake of clarity, but are described in
section~\refwithpage{additional complications}.

% This is very likely to move around as the flow chart is as big as
% possible, so if any preceding text changes the flow chart may end up on
% the next page, which may not be a bad thing, maybe.  What woould be a bad
% thing is the \clearpage seems to cause a blank page sometimes, if the
% image is just slightly too big.
\showgraph{build/logparser-flow-chart.ps}{Parser flow chart}{flow chart
image}
\clearpage

\subsection{Full algorithm}

\label{full-algorithm}

The intermingling of log entries from different mails immediately rules out
the possibility of handling each mail in isolation; the parser must be
capable of handling multiple mails in parallel, each potentially at a
different stage in its journey, without any interference between mails ---
except in the minority of cases where intra-mail interference is required.
The best way to implement this is to maintain state information for every
unfinished mail and manipulate the appropriate mail correctly for each log
line encountered.  The parser thus requires both a method of mapping log
lines to the correct mail and a method of specifying the action the log
line represents.  The former is achieved by using the \daemon{smtpd}
process id to identify the correct mail during the initial phase, then
switching to the queueid once the mail has been accepted.  The latter uses
the action field of the rule which matched the log line, executing the code
in the function named by the action.

Section~\refwithpage{actions-in-detail} explains the actions in substantive
detail; this section omits such detail because it would clutter and confuse
the algorithm description.  The flow chart in
section~\refwithpage{flow-chart} should also be consulted while reading
this section.  For the sake of clarity this description covers only the
most common paths through the algorithm, as including every path would
hinder understanding; the more uncommon paths are caused by the
complications described in section~\refwithpage{additional complications},
and are covered therein.

\subsubsection{Mail enters the system}

\label{mail-enters-the-system}

Everything starts off with a mail entering the system, whether by local
submission via \daemon{postdrop} or sendmail, by \SMTP{}, by re-injection
due to forwarding, or internally generated by Postfix.  Local submission is
the simplest case: a queueid is assigned immediately and the sender address
is logged (action: pickup; flowchart:~2).

\SMTP{} is more complicated: 

\begin{enumerate}

    \item First there is a connection from the remote client
        (action: connect; flowchart:~1).

    \item This is followed by rejection of sender address, recipient
        addresses, client \IP{} address or hostname, etc. (action:
        rejection; flowchart:~4); acceptance of one or more mails (action:
        clone; flowchart:~5); or some interleaving of both.

    \item The client disconnects (action: disconnect; flowchart:~6).  If
        Postfix has rejected any \SMTP{} commands the data will be saved to
        the database; if not there won't be any data to save (any mails
        accepted will already have been cloned so their data is in another
        data structure).

    \item If one or more mails were accepted there will be more log entries
        for those mails later, see section~\refwithpage{mail-delivery}.

\end{enumerate}

Re-injection due to forwarding sadly lacks explicit log lines of its
own;\footnote{Previously discussed in section~\refwithpage{complications},
complication 3.} re-injection is somewhat awkward to explain because it
overlaps both the mail acceptance and mail delivery sections, so discussion
is deferred to section~\refwithpage{tracking re-injected mail}.

Internally generated mails lack any explicit origin in Postfix 2.2.x and
must be detected using heuristics (see
section~\refwithpage{identifying-bounce-notifications} for details).
Bounce notifications are the primary example of internally generated mails,
though there may be other types.

\subsubsection{Mail delivery}

\label{mail-delivery}

The obvious counterpart to mail entering the system is mail leaving the
system, whether by deletion, bouncing, local delivery, or remote delivery.
All four are handled in exactly the same way:

\begin{enumerate}

    \item Postfix will log the sender and recipient addresses separately
        (action: save\_by\_queueid; flowchart:~10).

    \item Sometimes mail is re-injected and the child mail needs to be
        tracked by the parent mail (action: track; flowchart:~11) ---
        section~\refwithpage{tracking re-injected mail} discusses this in
        detail.

    \item Eventually the mail will be delivered, bounced, or deleted by the
        administrator (action: commit; flowchart:~13).  This is the last
        log line for this particular mail (though it may be indirectly
        referred to if it was re-injected).  If it is neither parent nor
        child of re-injection the data is cleaned up and entered in the
        database (flowchart:~15), then deleted from the state tables
        (flowchart:~16).  Re-injected mails are described in
        section~\refwithpage{tracking re-injected mail}.

\end{enumerate}

It should be reiterated that the actions above happen whether the mail is
delivered to a mailbox, piped to a command, delivered to a remote server,
bounced (due to a mail loop, delivery failure, or five day timeout), or
deleted by the administrator.  The exception is what happens after delivery
to the parent or children of mail re-injected due to forwarding, as
explained in section~\refwithpage{tracking re-injected mail}.

\subsubsection{Tracking re-injected mail}

\label{tracking re-injected mail}

The crux of the problem is that re-injected mails appear in the logs
without explicit logging indicating their source.  There are two implicit
indications:

\begin{enumerate}

    \item The indicator which more commonly introduces re-injection is when
        \daemon{qmgr} selects a mail with a previously unseen
        queueid for delivery (action: mail\_picked\_for\_delivery;
        flowchart:~3), in which case a new data structure will be created.
        The mail will be flagged as having unknown origins; this flag
        should be subsequently cleared once the origin has been
        established.  This may also be an indicator that the mail is a
        bounce notification, see
        section~\refwithpage{identifying-bounce-notifications} for details.

    \item Local delivery re-injects the mail and logs a relayed delivery
        rather than delivering directly to a mailbox or program as it
        usually would (action: track; flowchart:~11).\footnote{Relayed
        delivery is performed by the \SMTP{} client; local delivery means
        local to the server, i.e.\ an address the server is final
        destination for.} In this case the mail may already have been
        created (described above) and the unknown origin flag will be
        cleared; if not a new data structure will be created.  In both
        cases the new mail is marked as a child of the parent.  The log
        line in question is:

        \texttt{3FF7C4317: to=<username@example.com>, relay=local, \newline 
        delay=0, status=sent (forwarded as 56F5B43FD)}

        This second indicator always occurs for re-injected mail but
        typically occurs after the first indicator explained above.  This
        indicator is required to tie the parent and child mails together
        and so is central to the process of tracking re-injected mails.

\end{enumerate}

The algorithm for tracking and saving re-injected mail to the database can
finally be described:

\begin{itemize}

    \item If the mail is of unknown origin it is assumed to be a child mail
        whose parent hasn't yet been identified (action: commit;
        flowchart:~17).  Mark the mail as ready for entry in the database
        (flowchart:~18), and wait for the parent to deal with it
        (flowchart:~19).  The mail should not have subsequent log entries;
        only its parent should refer to it.

    \item If the mail is a child mail then it has already been tracked
        (action: commit; flowchart:~20): the data is cleaned up, missing
        data is copied from its parent if necessary, and the child is
        entered in the database (flowchart:~21), then deleted from the
        state tables (flowchart:~22).  The child mail will be removed from
        the parent mail's list of children (flowchart:~23); if this is the
        last child and the parent has also been entered in the database the
        parent will be deleted from the state tables.

    \item The last alternative is that the mail is a parent mail (action:
        commit; flowchart:~24).  Regardless of the state of its children
        its data is cleaned up and entered in the database (flowchart:~25).
        The parent may have children which are waiting to be entered in the
        database (flowchart:~26); each of those children's data is cleaned
        up and entered in the database, then deleted from the state tables.
        The parent may also have outstanding children which are not yet
        delivered, in which case (flowchart:~27) the parent must wait for
        those children to be finished with.  As soon as the last child is
        deleted from the state tables the parent will also be finished with
        (flowchart:~28), and deleted from the state tables.

\end{itemize}

A parent mail can have multiple children, which may be delivered before or
after the parent mail.  

\subsection{Actions in detail}

\label{actions-in-detail}

Each action is passed the same arguments: 

\begin{description}

    \item [line] The log line, separated into fields:

        \begin{description}

            \item [timestamp] The time the line was logged at.

            \item [host] The hostname of the server which logged the line.

            \item [program] The name of the program which logged the line.

            \item [pid] The \pid{} of the program which logged the line.

            \item [text] The remainder of the line.

        \end{description}

    \item [rule] The matching rule.

    \item [matches] The fields in the line captured by the rule's regex.

\end{description}

The actions:

\begin{description}

    \item [IGNORE] This rule just returns successfully; it is used when a
        line needs to be parsed for completeness but doesn't either provide
        any useful data or require anything to be done.

    \item [CONNECT] Handle a remote client connecting: create a new state
        table entry (indexed by \daemon{smtpd} \pid{}) and save both the
        client hostname and \IP{} address.

    \item [DISCONNECT] Deal with the remote client disconnecting: enter the
        connection in the database, perform any required cleanup, and
        delete the connection from the state tables.

    \item [SAVE\_BY\_QUEUEID] Find the correct mail based on the queueid in
        the log line, and save the data extracted by the \regex{} to it.

    \item [COMMIT] Enter the data from the mail into the database. Entry
        will be postponed if the mail is a child waiting to be tracked.
        Once entered, the mail will be deleted from the state tables.

    \item [TRACK] Track a mail when it is re-injected for forwarding to
        another mail server; this happens when a local address is aliased
        to a remote address.  TRACK will be called when dealing with the
        parent mail, and will create the child mail if necessary. TRACK
        checks if the child has already been tracked, either by this parent
        or by another parent, and issues appropriate warnings in either
        case.

    \item [REJECTION] Deal with Postfix rejecting an \SMTP{} command from
        the remote client: log the rejection with a mail if there is a
        queueid in the log line, or with the connection if not.

    \item [EXPIRY] If Postfix has not managed to deliver a mail after
        trying for five days it will give up and return the mail to the
        sender.  When this happens the mail will not have a combination of
        Postfix programs which passes the valid combinations check (see
        section~\refwithpage{out of order log lines}).  To ensure that the
        mail can be committed the EXPIRY action sets a flag marking the
        mail as expired; the flag later causes the valid combinations check
        to be skipped, so the mail will be committed.

    \item [MAIL\_PICKED\_FOR\_DELIVERY] This action represents Postfix
        picking a mail from the queue to deliver. This action is used for
        both \daemon{qmgr} and \daemon{cleanup} as it needs to deal with
        out of order log lines; see section~\refwithpage{discarding cleanup
        lines} for details.

    \item [PICKUP] The PICKUP action corresponds to the \daemon{pickup}
        service dealing with a locally submitted mail.  Out of order log
        entries may have caused the state table entry to already exist (see
        section~\refwithpage{pickup logging after cleanup}); otherwise it
        is created.  The data from the log line is then saved to the state
        table entry.

    \item [CLONE] Multiple mails may be accepted on a single connection, so
        each time a mail is accepted the connection's state table entry
        must be cloned; if the original data structure was used the second
        and subsequent mails would overwrite one another's data.

    \item [MAIL\_TOO\_LARGE] When the client tries to send a message larger
        than the local server accepts, the mail will be discarded, and the
        client informed.  See TIMEOUT for further discussion; the two are
        handled in exactly the same way.

    \item [TIMEOUT] The connection timed out so the mail currently being
        transferred must be discarded. The mail may have been accepted, in
        which case there's a data structure to dispose of, or it may not in
        which case there is not.  See
        section~\refwithpage{timeouts-during-data-phase} for the gory
        details.

    \item [POSTFIX\_RELOAD] When Postfix stops or reloads its configuration
        it kills all \daemon{smtpd} processes,\footnote{Possibly other
        programs are killed also, but the parser is only affected by and
        interested in \daemon{smtpd} processes exiting.} requiring any
        active connections to be cleaned up, entered in the database, and
        deleted from the state tables.

    \item [SMTPD\_KILLED] Sometimes an event occurs which requires Postfix
        to forcefully kill a \daemon{smtpd}; that \daemon{smtpd} processes'
        active connection must be cleaned up, entered in the database, and
        deleted from the state tables.

    \item [SMTPD\_DIED] Sometimes a \daemon{smtpd} dies or exits
        unsuccessfully; the active connection for that \daemon{smtpd} must
        be cleaned up, entered in the database, and deleted from the state
        tables.

    \item [SMTPD\_WATCHDOG] \daemon{smtpd} processes have a watchdog timer
        to deal with unusual situations --- after five hours the timer will
        expire and the \daemon{smtpd} will exit.  This occurs very
        infrequently, as there are many other timeouts which should occur
        in the intervening hours: \DNS{} timeouts, timeouts reading data
        from the client, etc.  The active connection for that
        \daemon{smtpd} must be cleaned up, entered in the database, and
        deleted from the state tables.

    \item [BOUNCE] Postfix 2.3 (and hopefully subsequent versions) logs the
        creation of bounce messages.  This action creates a new mail if
        necessary, marking it as a bounce notification and removing the
        unknown origin flag if it exists.

\end{description}

\subsection{Additional complications}

\label{additional complications}

\subsubsection{Identifying bounce notifications}

\label{identifying-bounce-notifications}

Postfix 2.2.x (and presumably previous versions) lacks explicit logging
when bounce notifications are generated; suddenly there will be log entries
for a mail which lacks an obvious source.  There are similarities to the
problem of re-injected mails discussed in section~\refwithpage{tracking
re-injected mail}, but unlike the solution described therein bounce
notifications do not eventually have a log line which identifies their
source.  Heuristics must be used to identify bounce notifications, and
those heuristics are:

\begin{enumerate}

    \item The sender address is $<>$.

    \item Neither \daemon{smtpd} nor \daemon{pickup} have logged any
        messages associated with the mail, indicating it was generated
        internally by Postfix, not accepted via \SMTP{} or submitted
        locally by \daemon{postdrop} or sendmail.

    \item The message-id has a specific format: \newline
        \texttt{YYYYMMDDhhmmss.queueid@server.hostname} \newline
        e.g.\ \texttt{20070321125732.D168138A1@smtp.example.com}

    \item The queueid in the message-id must be the same as the queueid of
        the mail: this is what distinguishes bounce notifications generated
        locally from bounce notifications which are being re-injected as a
        result of aliasing.  In the latter case the message-id will be
        unchanged from the original bounce notification, and so even if it
        happens to be in the correct format (e.g.\ if it was generated by
        Postfix on another server) the queueid in the message-id will not
        match the queueid of the mail.

\end{enumerate}

Once a mail has been identified as a bounce notification the unknown origin
flag is cleared and the mail can be cleaned up and entered in the database.

There is a small chance that a mail will be incorrectly identified as a
bounce notification, as the heuristics used may be too broad.  For this to
occur the following conditions would have to be met:

\begin{enumerate}

    \item The mail must have been generated internally by Postfix.

    \item The sender address must be $<>$.

    \item The message-id must have the correct format and match the queueid
        of the mail.  While a mail sent from elsewhere could easily have
        the correct message-id format, the chance that the queueid in the
        message-id would match the queueid of the mail is extremely small.

\end{enumerate}

The most likely cause of mis-identification is if a mail generated
internally by Postfix is identified as a bounce notification when it is a
different type of message; arguably this is a benefit rather than a
drawback, as other mails generated internally by Postfix will be handled
correctly.

Postfix 2.3 (and hopefully subsequent versions) log the creation of a
bounce message.

This check is performed during the COMMIT action.

\subsubsection{Aborted delivery attempts}

\label{aborted-delivery-attempts}

Some mail clients appear\footnote{Due to privacy concerns no attempt has
been made to identify either the users or the software which exhibits this
behaviour.} to send the following sequence of commands during
the \SMTP{} session:

\begin{verbatim}
    EHLO client.hostname
    MAIL FROM: <sender@address>
    RCPT TO: <recipient@address>
    RSET
    MAIL FROM: <sender@address>
    RCPT TO: <recipient@address>
    DATA
\end{verbatim}

The client aborts the first delivery attempt after the first recipient is
accepted, then makes a second delivery attempt which it continues with
until the delivery is complete.

Once again Postfix does not log a message making the client's behaviour
clear, so once again heuristics are required to identify when this
behaviour occurs.  In this case a list of all mails accepted during a
connection is saved in the connection state, and the accepted mails are
examined when the disconnection action is executed.  Each mail is checked
for the following characteristics:

\begin{itemize}

    \item Is the mail missing its \daemon{cleanup} log message?  Every mail
        which passes through Postfix will have a \daemon{cleanup} line;
        lack of a \daemon{cleanup} line is a sure sign the mail didn't make
        it too far.

    \item Were there exactly two \daemon{smtpd} log lines for the mail?
        There should be a connection line and a mail accepted line, nothing
        else.

\end{itemize}

If both checks are successful then the mail is assumed to be one of the
offending bogus mails and is discarded.  There will be no further entries
logged for such mails, so without identifying and discarding them they
accumulate in the state table and will cause clashes if the queueid is
reused.  The mail cannot be entered in the database as the only data
available is the client hostname and \IP{} address, but the database schema
requires many more fields be populated (see
sections~\refwithpage{connections table} and~\refwithpage{results table}).

This check is performed in the DISCONNECT action; it requires support in
the CLONE action where a list of cloned connections is maintained.

\subsubsection{Further aborted delivery attempts}

Some mail clients disconnect abruptly if a second or subsequent recipient
is rejected; they may also disconnect after other errors, but such
disconnections are either unimportant or are handled elsewhere in the
algorithm (section~\refwithpage{timeouts-during-data-phase}).  Sadly
Postfix doesn't log a message saying the mail has been discarded, as should
be expected by now.  The checks to identify this happening are:

\begin{itemize}

    \item Is the mail missing its \daemon{cleanup} log message?  Every mail
        which passes through Postfix will have a \daemon{cleanup} line;
        lack of a \daemon{cleanup} line is a sure sign the mail didn't make
        it too far.

    \item Were there three or more \daemon{smtpd} log lines for the mail?
        There should be a connection line and a mail accepted line,
        followed by one or more rejection lines.

    \item Is the last \daemon{smtpd} log line a rejection line?

\end{itemize}

These checks are made during the DISCONNECT action: if all checks are
successful then the mail is assumed to have been discarded when the client
disconnected.  There will be no further entries logged for such mails, so
without identifying and entering them in the database immediately they
accumulate in the state table and will cause clashes if the queueid is
reused.

\subsubsection{Timeouts during DATA phase}

\label{timeouts-during-data-phase}

The DATA phase of the \SMTP{} conversation is where the headers and body of
the mail are transferred.  Sometimes there is a timeout or the connection
is lost\footnote{For brevity's sake timeout will be used throughout this
section, but everything applies equally to lost connections.} during the
DATA phase; when this occurs Postfix will discard the mail and the parser
needs to discard the data associated with that mail.  It seems more
intuitive to save the data to the database, but if a timeout occurs there
won't yet have been any data gathered for the mail, so there is none
available to save; the timeout is recorded with the connection data
instead.

To deal properly with timeouts the parsing algorithm needs to do the
following in the TIMEOUT action:

\begin{enumerate}

    \item Record the timeout and associated data in the connection's
        results.

    \item If no mails have been accepted yet nothing needs to be done; the
        timeout action ends.  The timeout action is dependant on the clone
        action keeping a list of all mails accepted on each connection.

    \item \ESMTP{} pipelining allows the client to send MAIL FROM, RCPT TO
        and DATA commands in one packet, instead of sending each command
        individually and waiting for the reply before sending subsequent
        commands.  If pipelining is used and Postfix rejects the sender
        address or any of the recipient addresses there is no way for the
        client to tell which command was rejected.  Some clients make no
        attempt to recover and disconnect uncleanly; others presumably
        stop pipelining and restart the mail sending operation.  The client
        could attempt to parse the error message, but that would be
        extremely error prone, whereas not pipelining makes the cause of
        the error message obvious.

        A timeout may thus apply either to an accepted mail or a rejected
        mail.  To distinguish between the two cases the algorithm compares
        the timestamp of the last accepted mail against the timestamp of
        the last line logged by \daemon{smtpd} for that connection.  If the
        \daemon{smtpd} timestamp is later there was a rejection between the
        accepted mail and the timeout, therefore the timeout applies to a
        rejected mail; the timeout has already been recorded so the timeout
        action finishes.  If the mail acceptance timestamp is greater then
        the timeout applies to the just-accepted mail, which will be
        discarded.

\end{enumerate}

This complication is further complicated by the presence of out of order
\daemon{cleanup} lines: see section~\refwithpage{discarding cleanup lines}
for details.

\subsubsection{Discarding cleanup lines}

\label{discarding cleanup lines}

The author has only observed this complication occurring after a timeout,
though there may be other circumstances which trigger it.  Sometimes the
\daemon{cleanup} line is logged after the timeout line; parsing this line
causes the creation of a new state table entry for the queueid in the log
line.  This is incorrect because the line actually belongs to the mail
which has just been discarded, and the next log line for that queueid will
be seen when the queueid is reused, causing a queueid clash and the
appropriate warning.

In the case where the \daemon{cleanup} line is still pending during the
TIMEOUT action the algorithm updates a global cache of queueids, adding the
queueid and the timestamp from the timeout line.  When the next
\daemon{cleanup} line is parsed for that queueid the cache will be checked,
and the line will be deemed part of the discarded mail and discarded if it
meets the following requirements:

\begin{itemize}

    \item The queueid must not have been reused yet, i.e.\ there isn't an
        entry in the state tables for the queueid.

    \item The timestamp of the \daemon{cleanup} line must be within ten
        minutes of the mail acceptance timestamp.  Timeouts happen after
        five minutes, but some data may have been transferred slowly, and
        empirical evidence shows that ten minutes is not unreasonable;
        hopefully it is a good compromise between false positives and false
        negatives.

\end{itemize}

The next \daemon{cleanup} line must meet the criteria above for it to be
discarded because not every connection where a timeout occurs will have a
\daemon{cleanup} line logged for it; if the algorithm blindly discarded the
next \daemon{cleanup} line after a TIMEOUT it would in some cases be
mistaken.  Whether or not the next \daemon{cleanup} line is discarded the
queueid will be removed from the cache of timeout queueids when the next
\daemon{pickup} line containing that queueid is parsed.

During the TIMEOUT action if the mail lacks a \daemon{cleanup} line, the
queueid and other data about the mail, particularly the timestamp of the
last log line, are added to a global cache of timeout queueids.  This cache
is subsequently consulted during the MAIL\_PICKED\_FOR\_DELIVERY action,
and if the queueid is found in the cache and the \daemon{cleanup} line
passes the checks listed above the line will be discarded.

\subsubsection{Pickup logging after cleanup}

\label{pickup logging after cleanup}

Occasionally the \daemon{pickup} line logged when mail is submitted locally
via sendmail appears later in the log file than the \daemon{cleanup} line
for that mail.  This seems to occur during periods of particularly heavy
load, so is most likely due to process scheduling vagaries.  Normally if
the queueid given in the \daemon{pickup} line exists a warning is generated
by the \daemon{pickup} action, but if the following conditions are met it
is assumed that the lines are out of order:

\begin{itemize}

    \item The only program which has logged anything thus far for the mail
        is \daemon{cleanup}.

    \item There is less than a five second difference between the
        timestamps of the \daemon{cleanup} and \daemon{pickup} lines.

\end{itemize}

As always with heuristics there may be circumstances in which these
heuristics match incorrectly,  but none have been identified so far.

This complication is dealt with during the PICKUP action.

\subsubsection{Smtpd stops logging}

\label{smtpd stops logging}

Occasionally a \daemon{smtpd} will just stop logging, without an
immediately obvious reason.  After poring over logs for some time there are
several reasons for this infrequent occurrence:

\begin{enumerate}

    \item Postfix is stopped or its configuration is reloaded.  When this
        happens all \daemon{smtpd} processes exit, and all entries in the
        connections state table must be cleaned up, entered in the database
        and deleted.  The queueid state table is untouched.

    \item Sometimes an \daemon{smtpd} is killed by a signal, so the active
        connection must be cleaned up, entered in the database and deleted
        from the connections state table.

    \item Occasionally a \daemon{smtpd} will die or exit uncleanly, so the
        active connection must be cleaned up, entered in the database and
        deleted from the connections state table.

\end{enumerate}

The above descriptions appear to cover all situations identified thus far
where a \daemon{smtpd} suddenly stops logging.  In addition to removing an
active connection the last accepted mail may need to be discarded, as
detailed in section~\refwithpage{timeouts-during-data-phase}.

These occurrences are handled by the three actions POSTFIX\_RELOAD,
SMTPD\_DIED and SMTPD\_WATCHDOG\@.

\subsubsection{Out of order log lines}

\label{out of order log lines}

Occasionally a log file will have out of order log lines which cannot be
dealt with by the techniques described in sections~\refwithpage{tracking
re-injected mail},~\refwithpage{discarding cleanup
lines}~or~\refwithpage{pickup logging after cleanup}.  In the 93 log files
used for testing this occurs only five times in 60,721,709 log lines, but
for completeness of the algorithm it should be dealt with.  The five
occurrences in the test log files have the same characteristics: the
\daemon{local} log line showing delivery to a local mailbox occurs after
the \daemon{qmgr} log line showing removal of the mail from the queue
because delivery is completed.  This causes problems: the mail is not
complete, so entry into the database fails; a new mail is created when the
\daemon{local} line is parsed and remains in the state tables; four
warnings are issued per pair of out of order log lines.

The solution to this problem is to examine the list of programs which have
logged messages for each mail, comparing the list against a table of
known-good combinations of programs (this check is performed during the
COMMIT action).  If the mail's combination is found in the valid list the
mail can be entered in the database; if the combination is not found entry
must be postponed and the mail flagged for later entry.  The
SAVE\_BY\_QUEUEID action checks for the flag and retries entry if it's
found; if the additional log lines have caused the mail to reach a valid
combination entry will proceed, otherwise it must be postponed once more.

The list of valid combinations is explained below.  Every mail will
additionally have log entries from \daemon{cleanup} and \daemon{qmgr}; any
mail may also have log entries from \daemon{bounce}, \daemon{postsuper}, or
both.

\begin{description}

    \item [\daemon{local}:] Local delivery of a bounce notification, or
        local delivery of a forwarded or tracked mail.

    \item [\daemon{local}, \daemon{pickup}:] Mail submitted locally on the
        server, delivered locally on the server.

    \item [\daemon{local}, \daemon{pickup}, \daemon{smtp}:] Locally
        submitted mail, \newline both local and remote delivery.

    \item [\daemon{local}, \daemon{smtp}, \daemon{smtpd}:] Mail accepted
        from a remote client, both local and remote delivery.

    \item [\daemon{local}, \daemon{smtpd}:] Mail accepted from a remote
        client, local delivery only.

    \item [\daemon{pickup}, \daemon{smtp}:] Locally submitted mail, remote
        delivery only.

    \item [\daemon{smtp}:] Remote delivery of forwarded or tracked mail, or
        a bounce notification.

    \item [\daemon{smtp}, \daemon{smtpd}:] Mail accepted from a remote
        client, then remotely delivered (typically relaying mail for
        clients on the local network).

\end{description}

This check applies to accepted mails only, not to rejected mails.

\subsubsection{Yet more aborted delivery attempts}

\label{yet-more-aborted-delivery-attempts}

The aborted delivery attempts described in
section~\refwithpage{aborted-delivery-attempts} occur frequently, but the
aborted delivery attempts described in this section only occur four times
in the 93 log files used for testing.  The symptoms are the same as in
section~\refwithpage{aborted-delivery-attempts}, except that there
\textit{is\/} a \daemon{cleanup} log line; there does not appear to be
anything in the log file to explain why there are no further log messages.
The only way to detect these mails is to periodically scan all mails in the
state tables, deleting any mails displaying the following characteristics:

\begin{itemize}

    \item The timestamp of the last log line for the mail must be 12 hours
        or more earlier than the last log line parsed.

    \item There must be exactly two \daemon{smtpd} and one \daemon{cleanup}
        log entries for the mail, with no additional log entries.

\end{itemize}

12 hours is a somewhat arbitrary time period, but it is far longer than
Postfix would delay delivery of a mail in the queue.\footnote{This may be a
problem if Postfix is not running for an extended period of time.}  The
state tables are scanned for mails matching the characteristics above each
time the end of a log file is reached, and matching mails are deleted.

\subsection{Conclusion}

This section has presented the parsing algorithm, starting with a high
level overview of the basic algorithm, and the first group of difficulties
thwarted.  The revised algorithm was detailed, preceded by a flow chart
depicting the paths a mail may take through the algorithm, and followed by
the actions which are available to rules were described in full.  The
section concludes with the remaining complications discovered and overcome
in the process of refining and completing the algorithm.  Detecting,
diagnosing and defeating complications forms the largest single portion of
this document, mirroring the development of the parser,  The complications
are described in the order they were overcome, with subsequent problems
affecting fewer mails (often by an order or magnitude), though the time
required to solve problems increased with each successive problem.

\section{Coverage}

\label{parsing coverage}

\subsection{Introduction}

The discussion of the parser's coverage of Postfix log files is separated
into two parts: log lines covered and mails covered.  The first is
important because the parser should handle all (relevant) log lines it's
given; the second is equally important because the parser must properly
deal with every mail if it is to be useful.  Improving the former is
less intrusive, as it just requires new rules to be written; improving the
latter is much more intrusive as it requires changes to the parser
algorithm, and it can also be much harder to notice a deficiency.

\subsection{Log lines covered}

\label{log-lines-covered}

Parsing a log line is a three stage process:

\begin{enumerate}

    \item Check if there are any rules for the program which produced the
        log line; if not then skip the line.  

    \item Try each rule until a matching rule is found.

    \item Execute the action specified by the rule.

\end{enumerate}

Full coverage of log lines requires the following:

\begin{enumerate}

    \item Each program of interest must have at least one rule or its log
        lines will be silently skipped; in the extreme case of zero rules
        the parser would happily skip every log line.  There may be any
        number of log lines from other programs intermingled in the log
        file, and there are some Postfix programs which don't produce any
        log lines of interest.

    \item There must be a rule to match each different line produced by
        each program; if a line is not successfully matched the parser will
        issue a warning.  Rules should be as specific and tightly bound as
        possible to ensure accurate parsing:\footnote{A rule which matches
        zero or more of any character will successfully parse every log
        line, but not in a meaningful way.} most log lines contain fixed
        strings and have a rigid pattern, so this is not a problem.

    \item The appropriate action to take --- discussed in
        section~\refwithpage{mails-covered}.

\end{enumerate}

Full coverage of log lines is easy to achieve yet hard to maintain.  It is
easy to achieve full coverage for a limited set of log files (at the time
of writing the parser has 139 rules, fully parsing 93 contiguous log files
from Postfix 2.2 and 2.3), and new rules are easy to add.  Maintaining full
coverage is hard because other servers have different restrictions with
custom messages, \RBL{} messages change over time, major releases of
Postfix change warning messages (usually adding more information), etc.,\
so over time the log lines drift and change.  Graph~\refwithpage{rule hits
graph} shows the number of hits for each rule over all 93 log files; it's
obvious that a small number of rules match the vast majority of the lines,
and more than half the rules match fewer than 100 times.

\subsection{Mails covered}

\label{mails-covered}

Coverage of mails is much more difficult to determine accurately than
coverage of lines.  The parser can dump its state tables in a human
readable form; examining these tables with reference to the log files is
the best way to detect mails which were not handled properly (many of the
complications discussed in section~\refwithpage{additional complications}
were detected in this way).  The parser issues warnings when it detects any
errors, some of which may alert the user to a problem, e.g.\ when a queueid
is reused before the previous mail is fully dealt with, when a queueid or
\pid{} is not found,\footnote{There will often be warnings about a missing
queueid or \pid{} in the first few hundred or thousand log lines because
the earlier log lines for those connections or mails are in the previous
log file; loading the saved state from the previous log file will solve
this problem.} or when there are problems tracking a child mail (see
section~\refwithpage{tracking re-injected mail}).  There should be few or
no warnings when parsing, and when finished parsing the state table should
only contain entries for mails which had yet to be delivered when the log
files ended, or started before the log files began.

At the time of writing the parser is being tested with 93 log files.  There
are 5 warnings produced, but because the parser errs on the side of
producing more warnings rather than fewer those 5 warnings represent 3
instances of 1 problem: 3 connections started before the first log file, so
their initial log entries are missing, leading to warnings when their log
lines are parsed.

The state tables contain entries for mails not yet delivered when the
parser finishes execution.  Ideally all they should contain are mails which
are awaiting delivery after the period covered by the logs, though they may
also contain mails whose initial entries are not contained in the logs.
Any other entries are evidence of a failure in parsing or an aberration in
the logs.  After parsing the 93 test log files the state tables contain 18
entries, breaking down into:

\begin{itemize}

    \item 1 connection which started only seconds before the logs ended and
        had not yet completed.

    \item 1 mail which had been accepted only seconds before the logs ended
        and had not yet been delivered.

    \item 9 mails whose initial log entries were not present in the logs.

    \item 7 mails which had yet to be delivered due to repeated failures.

\end{itemize}

The 13,850,793 connections and mails accepted, rejected or delivered by
Postfix during this period are handled correctly by the parser.

\subsection{Conclusion}

Parser coverage is divided into two topics in this section: log lines
covered, and mails covered.  The former is initially more important, as the
parser must successfully parse every line if it is to be complete, but
subsequently the latter takes precedence because reproducing the path a
mail takes through Postfix is the aim of the parser.  Increasing the
percentage of log lines parsed is relatively simple and not intrusive:
adding new rules or modifying existing rules is simplified by the
separation of rules and algorithm.  Improving the logical coverage is
harder, as the actions taken by Postfix must be reconstructed, and the new
sequence of actions integrated into the existing model without breaking
the existing sequences.  Detecting a deficiency in the parsing algorithm is
also significantly harder than detecting unparsed log lines, as the parser
will warn about any unparsed line, whereas discovering a flaw in the
parser requires understanding of the warnings produced and the mails
remaining in the state table.  Rectifying a flaw in the parser requires an
understanding of both the parser and Postfix's logs, and investigative work
to determine the cause of the deficiency, followed by further examination
of the logs in developing a solution.

\section{Limitations and possible improvements}

\label{limitations-improvements}

\subsection{Introduction}

Every piece of software suffers from some limitations and there is almost
always room for improvement.

\subsection{Limitations}

\begin{enumerate}

    \item Each new Postfix release requires new rules to be written to cope
        with the new log lines.  Similarly using a new \RBL{}, new policy
        server or new administrator defined rejection messages require new
        rules.

    \item It appears that the hostname used in the HELO command is not
        logged if the mail is accepted.\footnote{Tested with Postfix 2.2.10
        and 2.3.11; this may possibly have changed in Postfix 2.4.}
        Rectifying this has already been described in
        section~\refwithpage{logging helo}.

    \item The algorithm does not distinguish between mails where one or
        more mails are rejected and a subsequent mail is accepted; it will
        appear in the database as one mail with lots of rejections followed
        by acceptance (this has already been mentioned in
        section~\refwithpage{connection reuse}).  I don't believe it's
        possible to make this distinction given the data Postfix logs,
        though it might be possible to write a policy server to provide
        additional logging.

    \item The program will not detect parsing the same log file twice,
        resulting in the database containing duplicate entries.

    \item The parser does not distinguish between logs produced by
        different sources when parsing; all results will be saved to the
        same database.  This may be viewed as an advantage, as logs from
        different sources can be combined in the same database, or it may
        be viewed as a limitation as there is no facility to distinguish
        between logs from different sources in the same database.  If the
        results of parsing logs from different sources must remain
        separate, the parser can easily be instructed to use a different
        database to store the results in.

\end{enumerate}

\subsection{Possible improvements}

\begin{itemize}

    \item Write the policy server referred to in limitation 3 above.

\end{itemize}

\subsection{Conclusion}

This section has covered the limitations of the parser and possible
improvements which may be implemented in the future.

\section{Conclusion}

\label{conclusion}

XXX WRITE THIS


\appendix


\section{Other Postfix log parsers reviewed}

\label{other-parsers}

\subsection{Introduction}

It is important to compare and contrast newly developed programs,
algorithms and parsers against those already available, to accurately judge
what, if any, improvements are delivered by the newcomers.  There are not
that many previously developed Postfix log parsers, indeed it was quite
difficult to find ten parsers to review for this project, and the
functionality offered ranges from quite basic to much more mature,
depending on the needs of the creator.  The programs are not reviewed from
an independent viewpoint; the objective is to compare and contrast each
program with \parsername{}.  One important difference between \parsername{}
and the parsers reviewed is that only \parsername{} makes it possible to
parse new log lines without modifying the core parsing algorithm; see
section~\refwithpage{why separate rules and algorithm} for a discussion of
why the separation of rules from the algorithm is beneficial.


\subsection{Parsers reviewed}

XXX COMPARE AND CONTRAST RATHER THAN REVIEW\@.

\begin{description}

    \item [Pflogsumm] \textit{pflogsumm is designed to provide an over-view
        of Postfix activity, with just enough detail to give the
        administrator a ``heads up'' for potential trouble spots.\/}

        Pflogsumm produces a report designed for troubleshooting, and does
        not support saving any data; parsing concentrates on extracting
        only the data required to produce the report.  Both the parsing and
        reporting are difficult to extend, as it is a specialised tool.  It
        does not attempt to correlate log lines by queueid or \pid{}, nor
        does it need to deal with the complications encountered during this
        project.  Pflogsumm produces a useful report, and successfully
        dealt with all the Postfix logs tested with.

        \url{http://jimsun.linxnet.com/postfix_contrib.html} \newline (Last
        checked 2007/08/13.)

    \item [Sawmill Universal Log File Analysis and Reporting] \textit{
        Sawmill is a \newline Postfix log analyzer (it also support 686
        other log formats).  It can process log files in Postfix format,
        and generate dynamic statistics from them, analyzing and reporting
        events.  Sawmill can parse Postfix logs, import them into a SQL
        database (or its own built-in database), aggregate them, and
        generate dynamically filtered reports, all through a web interface.
        Sawmill can perform Postfix analysis on any platform, including
        Window, Linux, FreeBSD, OpenBSD, Mac OS, Solaris, other UNIX, and
        more.\/}
        
        Sawmill is a general purpose commercial product which parses 687
        log file formats (correct as of 2007/04/15) and produces reports.
        Its data extraction facilities are quite limited, though it does
        extract three different sets of data for Postfix (one is beta as of
        2007/04/15), but they do not appear to be interlinked, nor does it
        save sufficient data for the purposes of this project.  No attempt
        is made to correlate log lines or deal with the difficulties
        documented in sections~\refwithpage{complications}
        and~\refwithpage{additional complications}.\footnote{If any attempt
        is made there is no reference to it in the documentation available
        on the website.} The source code is available in an obfuscated form
        only (presumably for a fee), and the product is quite expensive, as
        it requires a license per report which is to be generated.  The web
        interface allows creation of dynamic reports based on any field,
        but due to the associated cost the author has not experimented with
        it.

        \url{http://www.thesawmill.co.uk/formats/postfix.html} \newline
        Fields extracted: from, to, server, UID, relay, status, number of
        recipients, origin hostname, origin \IP{} and virus.  The fields
        \textit{server}, \textit{uid\/} and \textit{virus\/} are not
        explained in their documentation: \textit{server\/} is probably the
        server the mail is delivered to, and \textit{uid\/} might be the
        uid of the user submitting mail locally.  Postfix does not perform
        any form of virus checking (though it has many options for
        cooperating with an external virus scanner), so the
        \textit{virus\/} field is a mystery.

        \url{http://www.thesawmill.co.uk/formats/postfix_ii.html} \newline
        Fields extracted: from, to, \RBL{}, client hostname and client
        \IP{}\@.

        \url{http://www.thesawmill.co.uk/formats/beta_postfix.html}
        \newline Fields extracted: from, to, client hostname, client \IP{},
        relay hostname, relay \IP{}, status, response code, \RBL{} and
        message id.

        (Last checked 2007/09/26.)

    \item [Splunk] \textit{Splunk is an IT Search engine. It is software
        that indexes any format of IT data from any source in real time,
        including logs, configurations, scripts, code, messages, traps,
        alerts, activity reports, stack traces and metrics from all of your
        applications, servers and devices. Splunk lets you search,
        navigate, alert and report on all your IT data in real time using
        an AJAX web interface. You can also share knowledge and Splunk
        solutions with other members of the Splunk community via
        SplunkBase.\/}

        Splunk aims to index all an organisation's logs, providing a
        centralised view capable of searching and correlating diverse log
        sources.  The web interface allows complicated searches, providing
        statistics and graphs in real time.  Saved searches can be
        configured to run periodically and the results emailed to a
        recipient or sent to a shell script, which presumably can publish
        the results as required (though possibly without the graphs and
        detailed statistics).  In the specific case of parsing Postfix
        logs, Splunk extracts some standard fields: to and from addresses,
        HELO hostname, date, host the logs were collected from, and
        protocol.  It parses the standard syslog fields at the beginning of
        the line, and extracts any \texttt{key=<value>} pairs; these are
        the fields listed above.  Searches can be based on the extracted
        fields, and all text in the line is also available for searching.
        Many types of reports are available, though all are variations of a
        bar or pie chart, with the exception of bubble and heatmap graphs.
        It is very easy to drill down through the graphs to extract a
        portion of the data (e.g.\ select the hour with the largest number
        of events, then select a particular host, and finally a specific
        address).  It is not possible to search on partial words, though
        the web interface does provide and refine a list of possible search
        terms as you type.  The searches are quite fast, though as the free
        version has a limit on the amount of data indexed per day the
        volume of data to be searched was merely 45mb; the cheapest
        licensed version costs \$6000, and it still limits the volume of
        data indexed per day.

        The interface is optimised for interactive rather than automated
        queries and it does not appear to be possible to write independent
        tools to utilise the Splunk database.  Splunk is a generic tool, so
        it lacks any Postfix specific support over and above extracting the
        \texttt{key=<value>} fields; most importantly it makes no attempt
        to correlate log lines by queueid or \pid{}, nor to handle any of
        the myriad complications discussed in this document.  Correlation
        of log lines is left up to the user, and Splunk certainly makes the
        job easier than using grep, awk and similar Unix tools unless the
        user has a very high proficiency with those tools.

        \url{http://www.splunk.com/} \newline (Last checked 2007/09/25.)

    \item [Isoqlog] \textit{Isoqlog is an MTA log analysis program written
        in C. It designed to scan qmail, postfix, sendmail and exim logfile
        and produce usage statistics in \HTML{} format for viewing through
        a browser. It produces Top domains output according to Sender,
        Receiver, Total mails and bytes; it keeps your main domain mail
        statistics with regard to Days Top Domain, Top Users values for per
        day, per month and years.\/}

        Isoqlog produces a report listing the number of mails sent by each
        unique sender address, and separately the total number of bytes
        transferred; both reports are produced for daily, monthly and
        annual time spans, but only for the domains listed in its
        configuration file.  It appears to ignore all log lines except for
        those for the current day, though it does maintain a record of data
        previously extracted, which the newly extracted data is merged into
        (no information is provided on the format of the data store).  It
        doesn't utilise rejection log lines in any way, so is unsuitable
        for the purposes of this project.  It doesn't handle any of the
        complications discussed in this document, doesn't gather the
        breadth of data required for this project, and ignores the majority
        of log lines produced by Postfix.

        \url{http://www.enderunix.org/isoqlog/} \newline (Last checked
        2007/08/13.)

    \item [AWStats] \textit{AWStats is a free powerful and featureful tool
        that generates advanced web, streaming, ftp or mail server
        statistics, graphically. This log analyzer works as a CGI or from
        command line and shows you all possible information your log
        contains, in few graphical web pages. It uses a partial information
        file to be able to process large log files, often and quickly. It
        can analyze log files from all major server tools like Apache log
        files (NCSA combined/XLF/ELF log format or common/CLF log format),
        WebStar, IIS (W3C log format) and a lot of other web, proxy, wap,
        streaming servers, mail servers and some ftp servers.\/}

        AWStats will produce simple graphs for many different services, but
        supporting many different services without special purpose code
        restricts it to supporting the \LCD{}.  The data it will extract
        from an \MTA{} log file is limited to: \newline \hspace*{2em}
        time2, email, email\_r, host, host\_r, method, url, code and
        bytesd.  \newline  There does not seem to be an explanation of
        those fields in the documentation.  AWStats has no special purpose
        code to deal with the intricacies of Postfix logs, in fact it
        operates by coercing Postfix logs into Apache\footnote{The Apache
        web server is the most popular HTTP server in use for the past 10
        years; more information is available at
        \url{http://httpd.apache.org/}} format log files, for analysis by
        AWStats' HTTP log file parser.  The converting parser only deals
        with a small portion of the log lines generated by Postfix, and
        though it does correlate log lines by queueid, it does not deal
        with any of the other complications described in this document.

        \url{http://awstats.sourceforge.net/} \newline
        \url{http://awstats.sourceforge.net/awstats.mail.html} \newline
        \url{http://awstats.sourceforge.net/docs/awstats_faq.html#MAIL}
        \newline (Last checked 2007/10/13.)

    \item [Log analyser --- throughput monitor] This utility tracks the
        number of events which occurred over a particular time and warns if
        the frequency of events passes a certain threshold.  It's designed
        to provide real time alerts when dictionary attacks, mail loops or
        similar problems occur. It doesn't attempt to extract or save data,
        correlate log lines, or any of the more advanced tasks described in
        this document, because it is not designed to do so.

        \url{http://home.uninet.ee/~ragnar/throughput_monitor/} \newline
        (Last checked 2007/08/13.)

    \item [Anteater] \textit{The Anteater project is a Mail Traffic
        Analyser. Anteater supports currently the logformat produced by
        Sendmail and by Postfix. The tool is written in 100\% C++ and is
        very easy to customize. Input, output, and the analysis are modular
        class objects with a clear interface. There are eight useful
        analyse modules, writing the result in plain ASCII or \HTML{}, to
        stdout or to files.\/}

        Anteater doesn't have any English documentation so it's difficult,
        nigh impossible, to accurately comment on what analysis is
        performs.  It did not run successfully when tested, and its parsing
        would certainly be out of date as Postfix has evolved considerably
        since this tool was last updated (November 2003).  As it neither
        ran successfully nor has documentation the author can read a
        detailed review cannot be provided.

        \url{http://anteater.drzoom.ch/} \newline (Last checked
        2007/08/13.)

    \item [Yet Another Advanced Logfile Analyser] \textit{yaala is a very
        flexible analyser for all kinds of logfiles. It uses parsers to
        extract information from a logfile, an SQL-like query language to
        relate the information to each other and an output-module to format
        the information appropriately.\/}

        YAALA uses a plugin based system to analyse log files and produce
        \HTML{} output reports.  The core code is merely 153 lines long, as
        all the parsing and report generation is handled by modules.  YAALA
        supports storing previously gathered data; it dumps its internal
        data structures using Perl's Storable module~\cite{perl-storable},
        so with some knowledge of the data structure created by the module
        responsible for parsing Postfix log files it should be possible to
        extract data with an external program.  Using YAALA as a base would
        be only slightly less work than starting from scratch, as both
        input and output modules would need to be written specially; it may
        even be more work to implement the parser within the constraints of
        YAALA\@.  YAALA bundles a Postfix parser, which extracts the
        following fields from specific log lines:

        \hspace*{2em}Aggregations: count (not explained), bytes (sum of
        bytes transferred).\newline \hspace*{2em}Keyfields [sic]: date,
        hour, sender, recipient, defer\_count, delay, incoming\_host,
        outgoing\_host.

        YAALA provides an \SQL{} like mini-language used when generating
        the reports; sample reports can be seen at~\cite{yaala-samples}.
        Example query: \newline \hspace*{2em} \texttt{requests BY file
        WHERE host =\~{} Google}. \newline The mini-language is quite
        limited and cannot be used to extract data for external use, merely
        to create reports.

        YAALA as distributed does not attempt the in-depth parsing
        explained in this document, and implementing such parsing within
        the confines of YAALA would be at least as much work as
        implementing it separately.

        In summary YAALA provides a Postfix parser which only handles the
        most common Postfix log lines, provides reasonably flexible report
        generation from the limited data extracted, but no facilities to
        extract data for use in other tools.

        \url{http://yaala.org/} \newline (Last checked 2007/10/09.)

    \item [Logparser/Lire] \textit{As any good system administrator knows,
        there's a lot more to keep track of in an active network than just
        webservers. Lire is hands down the most versatile log analysis
        software available today. Lire not only keeps you informed about
        your HTTP, FTP, and mail traffic, it also reports on your
        firewalls, your print servers, and your DNS activity. The ever
        growing list of Lire-supported services clearly outstrips any other
        software, in large part thanks to the numerous volunteers who have
        pioneered many new services and features. Lire is a total solution
        for your log analysis needs.\/}

        Lire is a general purpose log parser supporting many different
        types of log file.  Its Postfix parser extracts the following data
        from Postfix logs: \textit{The email servers' reports will show you
        the number of deliveries and the volume of email delivered by day,
        the domains from which you receive or send the most emails, the
        relays most used, etc.\/}; notably rejections are not mentioned or
        handled.  It supports multiple output formats (text, \HTML{},
        \PDF{} and Excel 95).  Logs can be parsed, the extracted data
        saved, and reports generated from it later.  Accessing this data
        from another application is undocumented; given the source code it
        should be possible, with sufficient time and effort, to access the
        data.

        Like AWStats and Logrep, Lire attempts to correlate log lines by
        queueid, but not by \pid{}, so the complete list of recipients for
        a mail should be available; however its parser extracts only part
        of the available data and makes no attempt to deal with the other
        complications described in sections~\refwithpage{complications}
        and~\refwithpage{additional complications}.  It stores data, but
        not in an easily accessible format, and generates reports in
        multiple formats, but those reports do not appear to be
        customisable.

        \url{http://logreport.org/lire.html} \newline (Last checked
        2007/10/09.)

    \item [Logrep] \textit{Logrep is a secure multi-platform framework for
        the collection, extraction, and presentation of information from
        various log files. It features HTML reports, multi dimensional
        analysis, overview pages, SSH communication, and graphs, and
        supports over 30 popular systems including Snort, Squid, Postfix,
        Apache, Sendmail, syslog, ipchains, iptables, NT event logs,
        Firewall-1, wtmp, xferlog, Oracle listener and Pix.\/}

        Logrep extracts the following fields from Postfix logs:

        \begin{itemize}

            \item For mail sent and received: from address, size, and time
                and date.

            \item For mail sent: to addresses, \SMTP{} code, and delay.

            \item For mail received: the hostname of the sender.

        \end{itemize}

        Fields are correlated based on the queueid (referred to as
        sessionname [sic] within Logrep), but not by \pid{}.  The parsing
        is error prone: empty fields are saved when the log line doesn't
        match the \regex{}, though it appears that they will not overwrite
        existing data.  It also counts the number of log lines parsed and
        skipped.  Most notably rejections are completely ignored, making it
        unsuitable for the purposes of this project.  It doesn't attempt to
        address any of the complications in
        sections~\refwithpage{complications} and~\refwithpage{additional
        complications} except for correlating by queueid.

        Logrep does not come with any documentation, though some scant
        documentation is available on its website.  It requires a web
        browser to interact with it, ensuring that automated log processing
        will be difficult.  Sadly all attempts to use Logrep failed, as it
        was unable to access the log files selected; this appears to be a
        bug rather than operator error.\footnote{If it is caused by
        operator error, the interface needs improvement as the (minimal)
        instructions were followed as closely as possible.}  As parsing
        failed it wasn't possible to review the reports Logrep can generate
        (available in \HTML{} only), nor to examine the (undocumented)
        format in which it can save extracted data for subsequent reuse.

        Logrep extracts far less data from Postfix logs than \parsername{},
        completely ignores rejections, is effectively undocumented, doesn't
        deal with the more complicated aspects of Postfix logs, and at the
        time of writing doesn't work properly.

        \url{http://www.itefix.no/phpws/index.php} \newline (Last checked
        2007/11/18.)

    \item [Log Mail Analyser] Please see the previous in-depth discussion
        of Log Mail Analyser in section~\refwithpage{prior art}.

\end{description}

\subsection{Conclusion}

While there are other programs available which perform basic Postfix log
parsing (some to a greater level of detail than others), few attempt to
correlate log lines by queueid (none correlate by \pid{}) to produce an
overall record of the journey each mail traverses through Postfix.  None of
the reviewed parsers collect the breadth of information gathered by
\parsername{}, nor make it as easy to extend the parser to handle new log
lines.  Most other parsers immediately generate a report and discard the
data extracted from the logs; those which don't discard the data retain it
in a format inaccessible to other tools.  All of the parsers reviewed can
produce a report of greater or lesser detail and complexity, a facility not
offered by \parsername{}; reporting is deferred to a subsequent
application.

The overriding difference between \parsername{} and the other parsers
reviewed herein is that none of them aim to achieve the high level of
understanding of Postfix logs achieved by \parsername{}.


\bibliographystyle{logparser-bibliography-style}
\bibliography{logparser-bibliography}
\label{bibliography}

\section{Graphs}

\label{graphs}

\renewcommand{\figurename}{Graph}

\subsection{Introduction}

Graphs are an excellent means of displaying data, transforming a
meaningless stream of numbers into an easily comprehensible form, where
anomalies and patterns are immediately obvious.  These graphs are used to
illustrate the topics discussed in section~\refwithpage{rule efficiency}.
The graphs in the first section cover parser scalability, demonstrating
that performance scales linearly with input size.  The impact of rule
ordering is shown in section~\refwithpage{rule ordering graphs}, and the
anomalous dips and peaks apparent in some graphs are explained.  The third
group of graphs vividly shows the huge impact that caching compiled
\regexs{} has on parser performance.  This section concludes with a
breakdown of the rule hits accumulated during a single test run of the
parser.

\subsection{Parser scalability}

\showgraph{build/plot-normal-filesize-numlines.ps}{Execution time vs file
size vs number of lines}{execution time vs file size vs number of lines
graph}

The Y axis in graph~\refwithpage{execution time vs file size vs number of
lines graph} represents the following:

\begin{enumerate}

    \item The time required, in seconds, to parse the log file.

    \item The size of the log file, in megabytes.

    \item The number of lines in the log file, divided by 10000.

\end{enumerate}

Graph~\refwithpage{execution time vs file size vs number lines factor}
shows the ratio of execution time vs file size and number of lines (higher
is better, it means more bytes or lines processed per second).  The ratios
are quite tightly banded with the exception of log files 22 and 62--68,
where they are noticeably higher; graph~\refwithpage{execution time vs file
size vs number lines factor} and table~\refwithpage{execution time vs file
size vs number lines factor table} show that the parser's execution time
scales linearly with input size.

\showgraph{build/plot-normal-filesize-numlines-factor.ps}{Ratio of file
size and number of lines to execution time}{execution time vs file size vs
number lines factor}

\showtable{build/stats-normal-filesize-line-count-include}{Ratio of file
size \& number of lines to execution time: statistics}{execution time vs
file size vs number lines factor table}

\clearpage

\subsection{Rule ordering}

\label{rule ordering graphs}

\showgraph{build/plot-normal-shuffle-factor.ps}{Percentage increase of
shuffled over normal}{percentage increase of shuffled over normal}

\showgraph{build/plot-normal-reverse-factor.ps}{Percentage increase of
reversed over normal}{percentage increase of reversed over normal}

\subsubsection{Why are there dips in the graphs?}
\label{Why are there dips in the graphs?}

The dips at log files 22 and 62--68 correspond to peaks in log file size in
graph~\refwithpage{execution time vs file size vs number of lines graph},
and peaks in graph~\refwithpage{execution time vs file size vs number lines
factor} (where a peak means that more lines are processed per second, i.e.\
performance is better).  The explanation for this took some time to arrive
at, but it turns out to be reasonably simple.  The large log files in
question were caused by a mail forwarding loop, where the distribution of
log lines is quite different to normal, resulting in different performance
characteristics.

The mail loop was set up by a user modifying his mail forwarding to:
\newline \hspace*{2em}\texttt{$\backslash$username, username@domain}
\newline This instructs Postfix to deliver the mail to the local user, and
also forward it to the remote address; this is generally not a problem
except that the remote address in this case is the user's address, creating
an infinite loop.  To prevent this happening Postfix examines the
Delivered-To header in the mail, and if the mail has already been delivered
to the current address it is bounced back to the sender with the error
message \texttt{mail forwarding loop for username@domain}.  Ordinarily this
works well, but unfortunately in this case the user noticed they had not
received any mail in a while and opted to send a test mail to themselves,
causing a loop not caught by Postfix as described below.

\begin{enumerate}

    \item Postfix accepts a mail from username@domain, for username@domain.

    \item Postfix delivers the mail to the local mailbox and
        username@domain, as instructed by the user's forwarding
        instructions. The forwarded mail has a
        \texttt{Delivered-To:~username@domain} header added, and the
        envelope sender address is username@domain.  Log lines are added by
        \daemon{local}, \daemon{qmgr} (twice), \daemon{cleanup} and finally
        \daemon{pickup}.

    \item Postfix accepts the mail for username@domain, but while
        delivering it notices that the \texttt{Delivered-To} header already
        contains the address it's currently delivering to, and therefore
        sends a bounce notification with sender address \textit{$<>$\/} to
        the original sender: username@domain.  Log lines are added by
        \daemon{local}, \daemon{qmgr} (twice) and \daemon{cleanup}.

    \item Postfix accepts the bounce notification and delivers it to both
        the local mailbox and to username@domain, as instructed by the
        user's forwarding instructions.  A
        \texttt{Delivered-To:~username@domain} header is added to the
        forwarded bounce notification, which now has an envelope sender
        address of username@domain.  Log lines are added by \daemon{local},
        \daemon{qmgr} (twice), \daemon{cleanup}, and finally
        \daemon{pickup}.

    \item Postfix accepts the forwarded bounce notification but while
        delivering the mail it notices that the \texttt{Delivered-To}
        header already contains the address currently being delivered to,
        and sends a bounce notification to the sender: username@domain.
        Log lines are added by \daemon{local}, \daemon{qmgr} (twice) and
        \daemon{cleanup}.

    \item GOTO 2.

\end{enumerate}

The sequence described above occurs extremely rapidly because Postfix does
not have to deliver the mail to an external system, so mails are delivered,
bounced and generated as fast as the disks can keep up, resulting in a huge
volume of logs.

The vast majority of log lines when a mail loop occurs are from Postfix
components which have a small number of rules associated with them, whereas
in general \daemon{smtpd} adds the majority of log lines, and also has the
highest number of rules.  \daemon{smtpd} log lines are distributed across
rules much more evenly than the log lines of \daemon{qmgr}, \daemon{local},
\daemon{cleanup} or \daemon{pickup}, so the average number of rules
required to parse a \daemon{smtpd} log line is much higher that the average
number required to parse other log lines.  

These two characteristics combine to reduce the average number or rules
required to parse a log line when there is a mail loop, as shown by the
peaks in graph~\refwithpage{execution time vs file size vs number lines
factor}.  When the rule ordering is reversed the majority of log lines
generated by a mail loop will be parsed with very few rules, whereas
without a mail loop the majority of log lines require a large number of
rules; this leads to a noticeable drop in the average time required to
parse a log line, as shown in graph~\refwithpage{percentage increase of
reversed over normal}.  The number of rules which need to be consulted when
the ordering is shuffled varies between the optimum and nadir, and the
performance varies proportionally.

The difference between logs with a mail loop and logs without can be seen
in table~\refwithpage{Rule ordering with and without a mail loop} showing
the increases for the different rule orderings and combinations of log
files:

\showtable{build/stats-normal-shuffle-reverse-include}{Rule ordering with
and without a mail loop}{Rule ordering with and without a mail loop}



\subsection{Caching regexs}

The following graphs show the impact that not caching compiled \regexs{}
has on parser performance: on typical log files the execution time when not
caching compiled \regexs{} is 500--600\% of the execution time when
caching; reversing the perspective shows that cached execution time is
merely 17--20\% of non-cached execution time.  Caching compiled \regexs{}
is probably the single most effective optimisation possible in the parser's
implementation.

\showgraph{build/plot-cached-discarded.ps}{Regex: cached vs
discarded}{normal regex vs discard regex}

\showgraph{build/plot-cached-discarded-factor.ps}{Regex caching: percentage
execution time increase}{normal regex vs discarded regex factor}

Two large dips can be seen in graph~\refwithpage{normal regex vs discarded
regex factor} at log files 22 and 62--68, corresponding to the spikes in
log file size in graphs~\refwithpage{execution time vs file size vs number
of lines graph} and~\refwithpage{normal regex vs discard regex}.  The
reason for the anomalous log files has already been explained in
section~\refwithpage{Why are there dips in the graphs?}.

The distribution of log lines across rules when there is a mail loop is
much different and the average number of rules consulted per log line is
much lower; this results in far fewer \regex{} compilations per line than
when there isn't a mail loop, and a correspondingly decreased execution
time.  The increases in execution time when not caching \regexs{} for
different combinations of log files are summarised in the table below:

\showtable{build/stats-cached-discarded-include-for-graph}
{Regex caching/discarding with different groups of log files}
{Regex caching/discarding with different groups of log files}


\subsection{Rule hits}
\label{rule hits}

The number of hits per rule is quite unevenly spread, resembling a Power
Law distribution~\cite{powerlaw}.

\showgraph{build/plot-hits.ps}{Hits per rule}{rule hits graph}

As graph~\refwithpage{rule hits graph} is quite difficult to read it has
been separated into three sections: low, middle and high.

\showgraph{build/plot-hits-low.ps}{Hits per rule (low)}{hits per rule low}

It is apparent from the low graph (graph~\refwithpage{hits per rule low})
that some rules have few or no hits; those with zero hits are rules which
were written to parse log files used during development of the parser but
not utilised in the test runs performed for this document.

\showgraph{build/plot-hits-middle.ps}{Hits per rule (middle)}{hits per rule
middle}

\showgraph{build/plot-hits-high.ps}{Hits per rule (high)}{hits per rule
high}

\clearpage

\subsection{Conclusion}

The graphs presented in this section illustrate the topics discussed in
section~\refwithpage{rule efficiency}.  The first collection of graphs are
about parser scalability, showing the linear relationship between execution
time and input size.  Section~\refwithpage{rule ordering graphs} relates to
the effect of rule ordering on execution time, and the unexpected
consequences of specific inputs.  The necessity of caching compiled
\regexs{} is attested to by the third group of graphs, where the difference
between caching and discarding is staggering.  This section concluded with
a breakdown of the rule hits accumulated during a single test run of the
parser.


% This fools the syntax highlighting so we get a new fold here, but it
% doesn't create a new section.
\ifacronymfirstuse{SMTP}{
\section{Glossary}
}{}
% Redefine the command used to produce the glossary title, because the
% default command produces an unnumbered section whereas I want a numbered
% section.
\renewcommand{\glossarytitle}{\section{Glossary}\label{Glossary}}
\printglossary{}
% Redefine the command a second time to produce acronyms instead of a
% glossary.
\renewcommand{\glossarytitle}{\section{Acronyms}\label{Acronyms}}
\printacronym{}

% The contents of the glossary.
\glossary{name={SQLite3},description={
    \textit{SQLite is a small C library that implements a self-contained,
    embeddable, zero-configuration SQL database engine.\/} SQLite3 is an
    \SQL{} implementation focusing on correctness, simplicity and speed.
    Unlike other \SQL{} implementations it does not require a separate
    server process, greatly simplifying deployment of an application
    utilising it.  More details can be found at~\cite{sqlite-features} or
    \url{http://www.sqlite.org/}.
}}

\glossary{name={Phishing},description={
    Phishing~\cite{Wikipedia-phishing} is an attempt to acquire information
    by masquerading as an entity trusted by the user, e.g.\ a bank.
}}

\glossary{name={Backscatter},description={
    When a spammer or worm sends mail with forged sender addresses,
    innocent sites are flooded with undeliverable mail notifications; this
    is called backscatter mail.
}}

\glossary{name={Joe job},description={
    A joe job is when spam mail is sent with a faked sender address with
    the intention of sullying the good name of the owner of the address.
    Joe jobs are a cause of backscatter, though by no means the only cause.
}}

\glossary{name={Epoch},description={
    Most operating systems store the current time and timestamps of files
    etc.\ as seconds elapsed since the epoch, the beginning of time as far
    as the operating system is concerned.  On Unix and Unix-derived systems
    the epoch is 1970/01/01 00:00:00; on other operating systems it may be
    different.
}}

\glossary{name={NULL},description={
    NULL is a special term used in \SQL{} databases indicating the absence
    of data for the field.
}}

% Postfix components
\glossary{name={bounce},description={
    The bounce daemon is responsible for sending bounce notifications in
    Postfix versions later than 2.2.  The definitive documentation is
    \url{http://www.postfix.org/bounce.8.html}.
}}

\glossary{name={cleanup},description={
    Cleanup processes all incoming mail, sitting between qmgr and the
    programs which accept mail (smtpd and pickup).  It removes duplicate
    recipient addresses, inserts missing headers, and optionally rewrites
    addresses if configured to do so.  The definitive
    documentation is \url{http://www.postfix.org/cleanup.8.html}.
}}

\glossary{name={local},description={
    Local is the Postfix component responsible for local delivery of mail
    (i.e.\ delivered on the server Postfix is running on), whether it be to
    a user's mailbox or a program such as a mailing list manager or
    procmail (\url{http://www.procmail.org/}).  It also handles aliases and
    processing of a user's \texttt{.forward} file.  The definitive
    documentation is \url{http://www.postfix.org/local.8.html}.
}}

\glossary{name={pickup},description={
    Pickup is the service which deals with mail submitted locally via
    postdrop and sendmail; it passes all submitted mail to cleanup.  The
    definitive documentation is \url{http://www.postfix.org/pickup.8.html}.
}}

\glossary{name={postdrop},description={
    Postdrop is used when submitting mail locally on the server: it creates
    a new mail in the queue and copies its input into the mail.  Subsequent
    delivery of the mail is the responsibility of other Postfix components.
    The definitive documentation is
    \url{http://www.postfix.org/postdrop.1.html}.
}}

\glossary{name={postsuper},description={
    Maintenance task such as deleting mails from the queue, putting mail on
    hold (no further delivery attempts will be made until it is released
    from hold, also by postsuper), and consistency checking of the mail
    queue.  The definitive documentation is available at
    \url{http://www.postfix.org/postsuper.1.html}.
}}

\glossary{name={qmgr},description={
    Qmgr is the Postfix daemon which manages the mail queue, determining
    which mails will be delivered next.  Qmgr orders the mails based on the
    recipient for local mails and the destination server for remote
    addresses, ensuring that it balances the aims of achieving maximum
    concurrency while avoiding overwhelming destinations or wasting time
    and resources on non-responsive destinations.  The definitive
    documentation is \url{http://www.postfix.org/qmgr.8.html}.
}}

\glossary{name={sendmail},description={
    Postfix provides a command that is compatible with the Sendmail
    (\url{http://www.sendmail.org/}) mail submission program that all Unix
    commands which send mail depend on; Postfix sendmail executes postdrop
    to place a new mail in the queue.  The definitive documentation is
    \url{http://www.postfix.org/sendmail.1.html}.
}}

\glossary{name={smtp},description={
    Delivery of mail over \SMTP{} is performed by the smtp component.  The
    definitive documentation is \url{http://www.postfix.org/smtp.8.html}.
}}

\glossary{name={smtpd},description={
    Smtpd is the Postfix program which accepts mail via \SMTP{}, and
    implements all the anti-spam restrictions Postfix provides.  The
    definitive documentation is \url{http://www.postfix.org/smtpd.8.html}.
}}

\glossary{name={Bayesian spam filtering},description={
    Bayesian spam filtering is a method of classifying mail based on the
    frequency that the words in the mail have previously appeared in a spam
    corpus and a ham (non-spam) corpus.  A full description is beyond the
    scope of this document, see~\cite{bayesian-filtering, a-plan-for-spam}
    for a detailed explanation.
}}

\glossary{name={Bayesian poisoning},description={
    Bayesian poisoning is the addition of innocuous or unrelated words to a
    spam mail in the hope of defeating Bayesian spam filtering.  E.g.\ the
    word Viagra would be firmly in the spam corpus for most people, but by
    adding the words \textit{schedule}, \textit{meeting}, \textit{moving
    forward\/} and \textit{best business practices\/} to a mail received by
    a manager, the Bayesian spam filter might tip the balance from bad to
    good, if the non-spam words outweigh the spam words.
}}

\glossary{name={$<>$},sort={<>},description={
    $<>$ is the sender address used for mail which should not be replied
    to, e.g.\ bounce notifications.  In \SMTP{} all addresses are enclosed
    in $<>$, so \textit{username\at{}domain\/} becomes
    \textit{$<$username\at{}domain$>$\/}; thus $<>$ is actually an empty
    address, but is always written as $<>$ for clarity.  All mail servers
    must accept mail sent from $<>$, or they are in violation of
    \RFC{}~2821~\cite{RFC2821}.
}}

\glossary{name={queueid},description={
    Each mail in Postfix's queue is assigned a queueid to uniquely identify
    it.  Queueids are assigned from a limited pool, so although they are
    guaranteed to be unique for the lifetime of the mail, given sufficient
    time they will be reused.
}}

\glossary{name={IPv4},description={
    Internet Protocol~\cite{Wikipedia-ipv4} version 4 is the fourth version
    of the Internet Protocol used to interconnect computers on the
    Internet.  It is the first widely deployed version of IP, and has been
    in use for over 25 years.
}}

\glossary{name={IPv6},description={
    Internet Protocol~\cite{Wikipedia-ipv6} version 6 is the latest version
    of the Internet Protocol used to interconnect computers on the
    Internet.  It is the successor to IPv4, bringing with it a greatly
    expanded address space, allowing many more computers to use the
    Internet simultaneously.  IPv4 and IPv6 will coexist for many years to
    come as existing networks transition from the former to the latter.
}}

\end{document}
