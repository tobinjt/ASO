% $Id$
\documentclass[a4paper,12pt,draft]{article}

% Useful stuff for math mode.
\usepackage{amstext}
% Wrapping of URLs; this doesn't work in the bibliography, but the breakurl
% package does.
%\usepackage{url}
% The name of the program, so I only have to change it in one place.
\newcommand{\parsername}{\PLP{}}
\newcommand{\parsernames}{\PLP{}'s}
% Include images
\usepackage[final]{graphicx}
% Change how nested enumerate environments are labelled.
\renewcommand{\labelenumii}{\roman{enumii}:}
\renewcommand{\refname}{Bibliography}
% Add the bibliography into the table of contents.
\usepackage[section,numbib]{tocbibind}
% Provides commands to distinguish between pdf and dvi output.
\usepackage{ifpdf}

\usepackage[acronym=true,style=altlist,number=none,toc=true]{glossary}
\makeglossary{}
\makeacronym{}

% When creating a \PDF{} make the table of contents into links to the pages
% (without horrible red borders) and include bookmarks.  The title and
% author don't work - I think either gnuplot or graphviz clobbers it.
\ifpdf{}
    \usepackage[pdftex]{hyperref}
\else{}
    \usepackage[dvips]{hyperref}
\fi{}
\hypersetup{
    pdftitle    = {Parsing Postfix log files},
    pdfauthor   = {John Tobin},
    final       = true,
    pdfborder   = {0 0 0},
%    breaklinks  = true,
}
% This is necessary for URLs in the bibliography; I dunno why the url
% package doesn't work; don't include it when generating \PDF{} output.
\ifpdf{}
\else{}
    \usepackage{breakurl}
\fi{}
\usepackage{lastpage}

% \showgraph{filename}{caption}{label}
\newcommand{\showgraph}[3]{
    \begin{figure}[hbt!]
        \caption{#2}\label{#3}
        \includegraphics{#1}
    \end{figure}
}

%\showtable{filename}{caption}{label}
\newcommand{\showtable}[3]{
    \begin{table}[ht]
        \caption{#2}\label{#3}
        \input{#1}
    \end{table}
}

% Replacement for \ref{}, adds the page number too.
\newcommand{\refwithpage}[1]{%
    \empty{}\ref{#1} [page~\pageref{#1}]%
}
% section references, automatically add \textsection
\newcommand{\sectionref}[1]{%
    \textsection{}\refwithpage{#1}%
}

% A command to format a Postfix daemon's name
\newcommand{\daemon}[1]{%
    \texttt{postfix/#1}%
}

% This is ridiculous, but I can't put @ in glossary entries, so . . .
\newcommand{\at}[0]{%
    @%
}

\newcommand{\tab}[0]{%
    \hspace*{2em}%
}

\newcommand{\numberOFlogFILES}[0]{%
    93%
}

\newcommand{\numberOFrules}[0]{%
    157%
}

% This may keep hyperref happy about the Abstract entry in the table of
% contents.
\newcounter{dummy}

\begin{document}

% Pull in the acronyms early, so they can be used throughout the text.
\input{logparser-acronyms.tex}

\title{Parsing Postfix log files}
\author{John Tobin \\ School of Computer Science and Statistics \\
Trinity College \\ Dublin 2 \\ Ireland \\ tobinjt@cs.tcd.ie}
\maketitle

\refstepcounter{dummy}
\addcontentsline{toc}{section}{Abstract}
\begin{abstract}

    Parsing Postfix logs is much more difficult than it first appears, but
    it is possible to achieve a high degree of accuracy in understanding
    the logs, and thus reconstructing the actions taken by Postfix to
    generate the logs.  This paper describes the creation of a parser,
    documenting the parsing algorithm, explaining the difficulties
    encountered and the solutions developed, with reference to an
    implementation which stores data gleaned from the logs in an SQL
    database.  The gathered data can then be used to optimise current
    anti-spam measures, provide a baseline to test new anti-spam measures
    against, or to produce statistics showing how effective those measures
    are.

\end{abstract}

\newpage
\tableofcontents
\listoffigures
\listoftables

\newpage
\section{Introduction}

\label{introduction}

Most mail server administrators will have performed some basic processing
of Postfix logs at one time or another, whether it was to debug a problem,
explain to a user why their mail is being rejected, or check whether their
new anti-spam measures are working.  The more adventurous will have
generated statistics to show how many hits each of their anti-spam measures
has gotten in the last week, and possibly even generated some graphs to
clearly illustrate the point to management or users.\footnote{This was the
author's first real foray into processing Postfix logs.}  Very few will
have performed in-depth parsing and analysis of their logs, where the
parsing must correlate the log lines per-connection or per-queueid rather
than processing lines independently.  One of the barriers to this type of
processing is the unstructured nature of Postfix logs, where each log line
was added on an ad hoc basis as a requirement was discovered or new
functionality was added.\footnote{A history of all changes made to Postfix
is distributed with the source code, available from
\url{http://www.postfix.org/}} Further complication arises because the set
of rejection messages is not fixed: new messages can be added by the
administrator with custom checks; every \RBL{} returns a different
explanatory message; policy servers may log different messages depending on
the characteristics of the connection; there are many ways in which the log
lines may differ between servers, even within the same organisation:
servers may be configured differently, or running different version of
Postfix.  This paper documents the difficult process of parsing Postfix
logs, presenting a program which parses logs and places the resulting data
into a database for later use.  The gathered data can then be used to
optimise current anti-spam measures, provide a baseline to test new
anti-spam measures against, or to produce statistics showing how effective
those measures are.  There are numerous other uses for such data: improving
server performance by identifying troublesome destinations and
reconfiguring appropriately or dedicating transports to those destinations;
identifying regular high volume uses (e.g.\ customer newsletters) and
restricting those uses to off-peak times; detecting virus outbreaks which
propagate via email; as a base for billing customers on a shared server.
Preserving the raw data enables users to develop a multitude of uses far
beyond those conceived of by the author.

\vspace{1em}\noindent\textbf{Layout of the paper:}

Section~\ref{background} provides background information useful in
understanding the paper, parser and algorithm.  It introduces the idea of
using a database schema as an \API{}, providing an interface to the data
gathered that is language-neutral.  The unusual separation of rules,
actions and algorithm is discussed, giving the reasons that approach was
taken when designing the parser.

This algorithm requires a database for storing both the rules used when
parsing and the results gleaned from parsing.  The database schema used is
described in \sectionref{database schema}, explaining in detail the tables
used for storing the data gleaned from the logs and the table that stores
the rules.

Section~\ref{rules} discusses the parsing rules in detail, explaining the
purpose and usage of each field in a rule, referring to an example rule and
sample data it matches successfully against.  The pros and cons of
overlapping rules are discussed, including techniques for detecting
unintentional overlaps.  Rule efficiency concerns are discussed, in
particular the optimisations used by the algorithm, with reference to the
graphs in appendix~\refwithpage{graphs}.  The final section is a
description of using the tools provided with the parser to generate a
\regex{} from related log lines for new rules.

Section~\ref{parsing-algorithm} contains the core of the paper, describing
a naive parsing algorithm and the complications encountered which shaped
the full algorithm.  A flow chart and a discussion of the emergent
behaviour exhibited by the algorithm accompanies a comprehensive
explanation of the different stages of the algorithm.  The actions taken
during execution of the algorithm are described, followed by the process of
adding a new action.  The section concludes with an in-depth description of
the further complications and their solutions which complete the algorithm.

Section~\ref{parsing coverage} analyses the coverage the parser achieves
over a set of \numberOFlogFILES{} log files taken from a mail server
handling mail for over 1000 users.  Coverage is described both in terms of
the fraction of lines parsed and the fraction of mails and connections
successfully reconstructed by the parsing algorithm, including dealing with
false negatives and a discussion of the difficulties in identifying false
positives.  A random sampling of log lines are parsed, and the correctness
of the results manually verified, as part of determining the coverage of
the parser.

Section~\ref{limitations-improvements} lists the limitations of the
algorithm, then suggests some ways of dealing with them, with the goal of
improving parsing and reproduction of the journey a mail takes through
Postfix.

Section~\ref{conclusion} contains the paper's conclusion.

Appendix~\ref{other-parsers} discusses other parsers and why they were
deemed unsuitable for the task, including why they could not be improved or
expanded upon.

The bibliography contains references to the resources used in developing
the algorithm, writing the program, and preparing this paper.  Also listed
are some additional resources expected to be helpful in understanding
\SMTP{}, Postfix, anti-spam techniques, or the paper.

Appendix~\ref{graphs} contains graphs illustrating the topics previously
discussed in \sectionref{rule efficiency}, rule efficiency, and explains
the irregularities observed in these graphs.

Appendix~\ref{Glossary} provides a glossary of terms used in the paper.

Appendix~\ref{Acronyms} provides a list of acronyms used in the paper.

\section{Background}

\label{background}

\subsection{Introduction}

This section provides background information helpful in understanding the
remainder of the paper.  It begins with a discussion of the motivation
underlying the project, followed by some technical information: the use of
a database as an \API{}\@; a brief introduction to
\ifacronymfirstuse{SMTP}{the}{} \SMTP{}\@; and a longer introduction to
Postfix, concentrating on the topics most relevant to this paper, namely
Postfix anti-spam restrictions and policy servers.  The assumptions made in
designing and implementing the parser are explained, as are the conventions
used in this paper.  Other projects which attempt to parse Postfix logs are
summarised (full details are available in
appendix~\refwithpage{other-parsers}), finishing with a review of
previously published research in this area.

\subsection{Motivation}

This paper and the program it describes are part of a larger project to
optimise a server's Postfix restrictions, generate statistics and graphs,
and provide a platform on which new restrictions can be trialled and
evaluated to see if they are beneficial in the fight against spam.  The
program parses Postfix logs and populates a database with the data gleaned
from those logs, providing a consistent and simple view of the logs which
future tools can utilise.  The gathered data can then be used to optimise
current anti-spam measures, provide a baseline to test new anti-spam
measures against, or to produce statistics showing how effective those
measures are.\footnote{\sectionref{introduction} lists some more example
uses.}

A short example of the optimisation possible using data from the database
is which Postfix restrictions reject the highest number of mails:

\begin{verbatim}
SELECT name, description, restriction_name, hits_total
    FROM rules
    WHERE postfix_action = 'REJECTED'
    ORDER BY hits_total DESC;
\end{verbatim}

If the database supports sub-selects percentages can be
obtained:

\begin{verbatim}
SELECT name, description, restriction_name, hits_total,
        (hits_total * 100.0 /
            (SELECT SUM(hits_total)
                FROM rules
                WHERE postfix_action = 'REJECTED'
            )
        ) || '%' AS percentage
    FROM rules
    WHERE postfix_action = 'REJECTED'
    ORDER BY hits_total DESC;
\end{verbatim}

\SQL{} note: $||$ is the concatenation operator in SQLite3; if the database
containing the extracted data does not support this syntax, then simply
remove `` $||$ '$\%$'\hspace{1ex}'' from the query --- the results will be
the same, just slightly less visually pleasing.

Another example is determining which restrictions are not effective: this
example shows which restrictions had fewer than 100 hits on the last log
file parsed.

\begin{verbatim}
SELECT name, description, restriction_name, hits,
        (hits * 100.0 /
            (SELECT SUM(hits)
                FROM rules
                WHERE postfix_action = 'REJECTED'
            )
        ) || '%' AS percentage
    FROM rules
    WHERE postfix_action = 'REJECTED'
        AND hits < 100
    ORDER BY hits ASC;
\end{verbatim}

These database queries yield summary statistics about the efficiency of
spam avoidance techniques that is far less feasible to assess directly from
log files without prior pre-processing into a database along the lines
proposed, implemented and tested herein.

\subsection{Database as Application Programming Interface}

The database populated by this program provides a simple interface to
Postfix logs.  Although the interface is a database schema, it is in effect
quite similar to any other \API{} provided by shared code: it insulates
both user and provider of the \API{} from changes in the implementation of
the \API{}\@.  The algorithm implemented by the parser can be improved;
support can be added for earlier or later releases of Postfix; bugs can be
fixed or limitations removed from the parser; these changes will not cause
the user to be negatively impacted.  Statistics and/or graphs can be
generated from the database; new restrictions can be tested and the results
inspected; trends in the fight against spam can emerge from historical data
saved in the database; the parser remains the same as the usage adapts.
Using a database simplifies writing programs which need to interact with
the data in several ways:

\begin{enumerate}

    \item The majority of programming languages have existing code
        available allowing access to databases, so the majority of
        languages will be able to access the results obtained by running
        the parser.  If this parser was written to be utilised as shared
        code there would be little or no difficulty in using it when
        writing in the programing language it is implemented in, but every
        other programing language wishing to utilise it would require an
        interface layer to be written.  This requirement would drastically
        reduce the viability of any project wishing to build upon the work
        of this project.

    \item Databases provide complex querying and sorting functionality to
        the user without requiring large amounts of programming.  All
        databases provide a program, of varying complexity and
        sophistication, which can be used for ad hoc queries with minimal
        investment of time.

    \item Databases are easily extensible, e.g.:

        \begin{itemize}

            \item Other tables can be added to the database, e.g.\ to cache
                historical data.

            \item New columns can be added to the tables used by the
                program, with sufficient DEFAULT clauses or a clever
                TRIGGER or two.\footnote{Please refer to an \SQL{} guide
                for explanations of these terms,
                e.g.~\cite{sql-for-web-nerds}}

            \item A VIEW gives a custom arrangement of data with very
                little effort.

            \item If the database supports it, access can be granted on a
                fine-grained basis so that the finance department can
                produce invoices, the helpdesk can run limited queries as
                part of dealing with support calls, and the administrators
                have full access to the data.

            \item Triggers can be written to perform actions when certain
                events occur.  In pseudo-\SQL{}\@:

\begin{verbatim}
CREATE TRIGGER ON INSERT INTO results
    WHERE sender = 'boss@example.com'
        AND postfix_action = 'REJECTED'
    SEND PANIC EMAIL TO 'postmaster@example.com';
\end{verbatim}

        \end{itemize}


    \item \SQL{} is reasonably standard and many people will already be
        familiar with it; for those unfamiliar with it there are lots of
        readily available resources from which to learn.  Although every
        vendor implements a different dialect of \SQL{}, the basics are the
        same everywhere (analogous to the overall similarities and minor
        differences between British English, American English and
        Australian English).  A good introduction to \SQL{} can be found
        at~\cite{sql-for-web-nerds}, others
        are~\cite{w3schools-sql-tutorial, sqlcourse.com}.  Depending on the
        database in use there may be tools available which reduce or remove
        the requirement to know \SQL{}; for SQLite (the default database
        used by the implementation) there are several
        available~\cite{sqlite-guis}.

\end{enumerate}

Storing the results in a database will also increase the efficiency of
using those results, as the logs need only be parsed once; indeed the
results may be used by someone with no access to the original logs.



\subsection{Simple Mail Transfer Protocol}
\label{SMTP background}

The \SMTPlong{}, originally defined in \RFC{}~821~\cite{RFC821} and updated
in \RFC{}~2821~\cite{RFC2821}, is used for transferring mail between the
sending and receiving \MTA{}\@.  It is a simple, human readable, plain text
protocol, making it quite easy to test and debug problems with it.  Despite
the simplicity many virus and/or spam sending programs fail to implement it
properly, so requiring strict adherence to the protocol specification is
beneficial in protecting against spam and viruses.\footnote{This violates
the principle of \textit{Be liberal in what you accept, and conservative in
what you send\/} from \RFC{}~760~\cite{rfc760}, but unfortunately that
principle was written in a friendlier time.  Given the deluge of spam that
mail servers are subjected to daily, a more appropriate maxim could be:
\textit{Require strict adherence to \RFC{}~2821; implement the strongest
restrictions you can; relax the restrictions and adherence only when
legitimate mail is impeded.\/}  It's not as friendly, nor as catchy, but it
more accurately reflects the current situation.} A typical \SMTP{}
conversation resembles the following (the lines starting with a three digit
number are sent by the server, all other lines are sent by the client):

\begin{verbatim}
220 smtp.example.com ESMTP
HELO client.example.com
250 smtp.example.com
MAIL FROM: <alice@example.com>
250 2.1.0 Ok
RCPT TO: <bob@example.com>
250 2.1.5 Ok
DATA
354 End data with <CR><LF>.<CR><LF>
Message headers and body sent here.
.
250 2.0.0 Ok: queued as D7AFA38BA
QUIT
221 2.0.0 Bye
\end{verbatim}

An example deviation from the protocol:

\begin{verbatim}
220 smtp.example.com ESMTP
HELO client.example.com
250 smtp.example.com
MAIL FROM: Alice N. Other alice@example.com
501 5.1.7 Bad sender address syntax
RCPT TO: Bob in Sales/Marketing bob@example.com
503 5.5.1 Error: need MAIL command
DATA
503 5.5.1 Error: need RCPT command
Message headers and body sent here.
.
502 5.5.2 Error: command not recognized
QUIT
221 2.0.0 Bye
\end{verbatim}

This client is so poorly written that not only does it present the sender
and recipient addresses improperly, it ignores the error messages returned
by the server and carries on regardless.

A detailed description of \SMTP{} is beyond the scope of this document:
introductory guides can be found at~\cite{smtp-intro-01, smtp-intro-02}.

\subsection{Postfix}

\label{postfix background}

Postfix is a \MTA{} with the following design aims (in order of
importance): security, flexibility of configuration, scalability, and high
performance.  It features extensive, extensible, optional anti-spam
restrictions, allowing an administrator to deploy those restrictions which
they judge suitable for their site's needs, rather than a fixed set chosen
by Postfix's author.  These restrictions can be selectively applied,
combined and bypassed on a per-client, per-recipient or per-sender basis,
allowing varying levels of severity and/or permissiveness.  Postfix
leverages simple lookup tables to support arbitrarily complicated
user-defined sequences of restrictions and exceptions, with policy
servers\footnote{Policy servers will be explained in \sectionref{policy
servers}.} as the ultimate in flexibility.  Administrators can also supply
their own rejection messages to make it clear to senders why exactly their
mail was rejected.  Unfortunately this flexibility has a cost: complexity
in the logs generated.  While it is easy to use standard Unix text
processing utilities to determine the fate of an individual email,
following the journey an email takes through Postfix can be quite
difficult.  The logs tend to follow a 90\%-10\% pattern: 90\% of the time
the journey is simple, but the other 10\% of the time requires 90\% of the
code.\footnote{These numbers don't have a solid scientific basis, they're
based on gut feeling from writing and debugging the software.}

Postfix's design follows the Unix philosophy of \textit{Write programs that
do one thing and do it well\/}~\cite{unix-philosophy}, and is separated
into various component programs to perform the tasks required of an
\MTA{}\@: receive mail, send mail, local delivery of mail, etc. --- full
details can be found at~\cite{postfix-overview}.  Each log line contains
the name of the Postfix program which produced it, and this information is
used when determining which rules should be used to parse each log line
(see \sectionref{rule characteristics} for details.)

\subsubsection{Mixing and matching Postfix restrictions}

Postfix restrictions are documented fully in~\cite{smtpd_access_readme,
smtpd_per_user_control, policy-servers}, the following is a brief overview.

Postfix uses several restriction lists (each containing zero or more
restrictions) to decide the fate of an \SMTP{} command, one list for each
stage of the \SMTP{} conversation: client connection, HELO command, MAIL
FROM command, RCPT TO command (possibly multiple times), DATA command, and
end of data.  By default restriction lists will not be evaluated until the
first RCPT TO command is received, because some clients don't deal with
earlier rejections properly; a side benefit of this delay is that Postfix
has more information available when logging the rejection.\footnote{The
\texttt{smtpd\_delay\_reject} parameter in Postfix's configuration controls
this behaviour.}  Postfix uses simple lookup tables as the deciding factor
for some restrictions, e.g.\newline
\tab{}\texttt{check\_client\_access~cidr:/etc/postfix/client\_access}
\newline The line above can be broken down as follows:

\begin{description}

    \item [check\_client\_access] The name of the restriction to evaluate.

    \item [cidr] The type of the lookup table.

    \item [/etc/postfix/client\_access] The file containing the lookup
        table.

\end{description}

check\_client\_access checks whether the \IP{} address of the connected
client matches the left hand side of each line in the file and returns the
right hand side of the first matching line.  Other restrictions determine
their result by consulting external sources, e.g.\
\texttt{reject\_rbl\_client rbl.example.com} checks whether the \IP{}
address of the client is present in the listed \RBL{}, rejecting the
command if the client is listed.

Each restriction is evaluated to produce a result of \textit{reject},
\textit{permit}, \textit{dunno\/} or the name of another restriction to be
evaluated.\footnote{Other results are possible, for full details
see~\cite{smtpd_access_readme, smtpd_per_user_control, policy-servers}.}
The meaning of \textit{permit\/} and \textit{reject\/} is fairly obvious;
\textit{dunno\/} means to stop evaluating the current restriction and
continue with the next restriction in the list, allowing more specific
cases to be used as exceptions to more general cases.  When the result is
the name of another restriction Postfix will evaluate the new restriction,
allowing restrictions to be chosen based on the client \IP{} address, HELO
hostname, sender address, recipient address, etc.  The administrator can
define new restrictions as a list of existing restrictions, allowing
arbitrarily long and complex sequences of lookups and restrictions.

This description is necessarily brief, for further details
see~\cite{smtpd_access_readme, smtpd_per_user_control, policy-servers}.


\subsubsection{Policy servers}

\label{policy servers}

A policy server~\cite{policy-servers} is an external program that accepts
state information from Postfix for each \SMTP{} command not rejected by an
earlier restriction; the policy server can utilise that state information
to implement whatever logic is required.  E.g.\ some users can be
restricted to sending mail on the third Tuesday after pay day only --- this
example may actually be useful in a payroll system to prevent problems from
spam or (worse) phishing mails with faked sender addresses.  More commonly
encountered scenarios are:

\begin{itemize}

    \item Checking \SPF{} records~\cite{openspf, Wikipedia-spf}.
        \SPF{}\label{spf introduction}
        records specify which mail servers are allowed to send mail
        claiming to be from a particular domain.  The intention is to
        reduce spam from faked sender addresses,
        backscatter~\cite{postfix-backscatter} and joe
        jobs~\cite{Wikipedia-joe-job}; however there has been a lot of
        resistance to the proposal because it breaks or vastly complicates
        some features of \SMTP{}\@.

    \item Greylisting~\cite{greylisting} is a technique that temporarily
        rejects mail when the triple of (sender, recipient, remote \IP{}
        address) is unknown; on second and subsequent delivery attempts
        from that triple the mail will be accepted.  The assumption is that
        maintaining a list of failed addresses and retrying after a
        temporary failure is uneconomical for a spammer, but that a
        legitimate mail server must retry.  Sadly spammers are using
        increasingly complex and well written programs to distribute spam,
        frequently using an \ISP{} provided \SMTP{} server from a
        compromised machine on the \ISP{}'s network.  Greylisting will
        slowly become less useful, but it does block a large percentage of
        spam mail at the moment; the most effective restrictions over the
        \numberOFlogFILES{} log files used in testing the parser are shown
        in table~\refwithpage{Summary of rejections}.  Greylisting is
        obviously worth using, at least at the moment, particularly when
        you factor in Greylisting's position as the final restriction which
        a mail must overcome: Greylisting only takes effect for mails which
        have passed every other restriction.

        \begin{table}[ht]
            \caption{Summary of rejections}\label{Summary of rejections}
            \input{build/restriction-table-include.tex}
        \end{table}

    \item Using a scoring system such as
        Policyd-weight~\cite{policyd-weight} where tests accumulate points
        against the sending system --- if the eventual score is too high
        the mail is rejected.

    \item Rate limiting or throttling on a per-sender, per-client or
        per-recipient basis as performed by Policyd~\cite{policyd}.

\end{itemize}

Example attributes taken from~\cite{policy-servers}:

\begin{tabular}[]{ll}

    request                 & smtpd\_access\_policy     \\
    protocol\_state         & RCPT                      \\
    protocol\_name          & SMTP                      \\
    helo\_name              & some.domain.tld           \\
    queue\_id               & 8045F2AB23                \\
    sender                  & foo@bar.tld               \\
    recipient               & bar@foo.tld               \\
    recipient\_count        & 0                         \\
    client\_address         & 1.2.3.4                   \\
    client\_name            & another.domain.tld        \\
    reverse\_client\_name   & another.domain.tld        \\
    instance                & 123.456.7                 \\

\end{tabular}



\subsection{Assumptions}

The algorithm described and the program implementing it make a small number
of (hopefully safe and reasonable) assumptions:

\begin{itemize}

    \item The logs are whole and complete: nothing has been removed, either
        deliberately or accidentally (e.g.\ log rotation gone awry, file
        system filling up, logging system unable to cope with the volume of
        logs).  On a well run system it is extremely unlikely that any of
        these problems will arise, though it is of course possible,
        particularly when undergoing a deluge of spam.

    \item Postfix logs sufficient information to make it possible to
        accurately reconstruct the actions it has taken.

    \item The Postfix queue has not been tampered with, causing unexplained
        appearance or disappearance of mail.

\end{itemize}

In some ways this task is similar to reverse engineering or replicating a
black box program based solely on its inputs and outputs.  Although the
source code is available,\footnote{Reading and understanding the source
code would require a significant investment of time; the source code for
Postfix 2.4.6 is 16MB.} there are advantages to treating Postfix as a black
box while developing the parser:

\begin{itemize}

    \item The parser is developed using real world logs rather than the
        idealised logs someone would naturally envisage reading the source
        code.

    \item The source code cannot accurately communicate the variety of
        orderings in which log lines are written to the log file, as
        process scheduling interferes with it.

    \item The parser acts as a second source of information, with the
        information gathered from empirical evidence.  An interesting
        project would be to compare the empirical knowledge inherent in the
        parsing algorithm with the documentation and source code of
        Postfix.

\end{itemize}


\subsection{Parser design}

\label{parser design}

It should be clear from the earlier Postfix background (\sectionref{postfix
background}) that logs produced by Postfix are not fixed; they vary widely
from host to host, depending on the set of restrictions chosen by the
administrator.  With this in mind, one of the parser's design aims was to
make adding new rules as easy as possible, to enable administrators to
properly parse their log lines.  The main design decision which enables
this is the separation of rules from the parsing algorithm, and secondly
the division of the algorithm into discrete actions.

\label{why separate rules and algorithm}

Decoupling the parsing rules from the associated actions allows new rules
to be written and tested without requiring modifications to the algorithm
source code (significantly lowering the barrier to entry for new or casual
users who need to parse new log lines), and greatly simplifies both
algorithm and rules.  Decoupling also creates a clear separation of
functionality: rules handle low level details of identifying log lines and
extracting data from a line, whereas the algorithm handles the higher level
details of following the path a mail takes through Postfix, assembling the
required data before storing it, dealing with complications arising, etc.

Separating the rules from the algorithm makes it possible to parse new log
lines without modifying the core parsing algorithm.  Although this may seem
like a trivial point, is it substantially more difficult to understand a
program's entire parsing algorithm, identify the correct location to
change, and make the appropriate changes without adversely affecting
parsing, particularly as there may be edge cases which are not immediately
obvious.\footnote{See \sectionref{yet-more-aborted-delivery-attempts} for a
complication which occurs only four times in \numberOFlogFILES{} log files
tested.} Requiring changes to the parsing algorithm also complicates
upgrades, as the changes must be preserved during the upgrade, and may
clash with changes made by the developer.  \parsername{} allows the user to
add new rules to the database without changing the parsing algorithm,
unless the new log lines to be parsed require functionality not already
provided by the algorithm.  If the new log lines do require new
functionality, new actions can be added to the parser without modifying
existing actions or other parts of the algorithm; only in the rare case
that the new actions require support from other sections of the code will
more extensive changes be required.

There is some similarity between the parser's design and William Wood's
\ATN{}~\cite{atns, nlpip}, a tool used in Computational Linguistics for
creating grammars to parse or generate sentences.  The resemblance between
\ATN{} and the parser is accidental, but it is interesting how two
apparently different approaches share an underlying separation of concerns;
this appears to be a natural division of responsibility and functionality.

% Do Not Reformat!

\begin{tabular}[]{lll}
    \textit{\ATN{}\/}   & \textit{Parser\/} & \textit{Similarity\/}     \\
    Networks            & Algorithm         & Determines the sequence 
                                              of transitions            \\
                        &                   & or actions which 
                                              constitutes a valid       \\
                        &                   & input.                    \\
    Transitions         & Actions           & Save data and impose
                                              conditions the            \\
                        &                   & input must meet to be
                                              considered valid.         \\
    Abbreviations       & Rules             & Responsible for 
                                              classifying input.        \\
\end{tabular}

\subsection{Conventions used in the paper}

The words \textit{connection\/} and \textit{mail\/} are often used
interchangeably in this paper; in general the word used was chosen based on
the context it appears in.

\subsection{Other Postfix log parsers}

Ten other parsers have been reviewed in
appendix~\refwithpage{other-parsers} as part of the background research for
this project.  None of the reviewed parsers perform the type of advanced
parsing and log correlation described here; all are intended to perform a
specific parsing and reporting task, rather than be a generic parser,
extracting data and leaving generation of reports from the data to other
programs.  Some parsers save the data extracted to a data store but the
majority discard all data once they have finished running, making
historical analysis impossible.  The other parsers reviewed all produce a
report of greater or lesser complexity and detail, whereas the program
described here doesn't attempt to produce a report at all; that
responsibility is deferred to a separate program, to be developed later.
The parsing algorithm and program described here are designed to enable
much more detailed log analysis by providing a stable platform for
subsequent programs to develop upon.



\subsection{Previous research in this area}

\label{prior art}

There only appears to be one prior paper published about parsing Postfix
log files: \textit{Log Mail Analyzer: Architecture and Practical
Utilizations\/}~\cite{log-mail-analyser}.  The aim of \LMA{} is quite
different from this parser: it attempts to present correlated log data in a
form suitable for a systems administrator to search using the myriad of
standard Unix text processing utilities already available, producing both a
\CSV{} file and either a MySQL or Berkeley DB database.  The decision to
support both \CSV{} and Berkeley DB appears to have been a limiting factor:
\CSV{} is a very simple format where a record is stored in a single line,
with fields separated by a comma or other punctuation symbol.  Problems
with \CSV{} files include the difficulty in escaping separators, providing
multiple values for a field (e.g.\ multiple recipients), adding new fields,
and lack of a bundled schema describing the fields.

Berkeley DB is not an \SQL{} database, supporting only \textbf{(key,
value)} pairs; in the main table the key is an integer referred to by
secondary tables and the value used is a \CSV{} line containing all the
data for that row.  The secondary by-sender, by-recipient, by-date and
by-\IP{} tables use the sender/recipient/date/\IP{} as the key, and a
\CSV{} list of integers referring to the main table is the value.  In
effect this re-implements \SQL{} foreign keys, but without the
functionality offered by even the most basic of \SQL{} databases (joins,
ordering, searches, etc.).  It also requires custom code to search on some
combination of the above, though the authors of \LMA{} did provide some
queries: IP-STORY, FROM-STORY, DAILY-EMAIL and DAILY-REJECT\@.  Berkeley DB
appears to be the least useful of the three output formats

The data stored is limited to time and date, hostname and \IP{} address of
client, \IP{} of server, sender and recipient addresses, \SMTP{} code, and
size (for accepted mails only).  Handling of multiple recipients, \SMTP{}
codes or remote servers\footnote{A single mail may be sent to multiple
remote servers if it was addressed to recipients in different domains, or
Postfix needs to try multiple servers for one or more recipients.} is not
explained in the paper; experimental observation shows that multiple
records are added when there are multiple recipients (sadly the records are
not associated in any way), and presumably the same approach is taken when
there are multiple destination servers.

The schema used with the MySQL database is undocumented, but at least it's
possible to discover the schema with an \SQL{} database, unlike with
Berkeley DB\@; all \SQL{} databases embed the database schema into the
database and provide commands for displaying it.  Berkeley DB does not
embed a schema, as there is neither requirement nor benefit; it only
provides \textbf{(key, value)} pairs, so any additional structuring of the
data is imposed by the application, thus it behoves the application to
document this structure.

Parsing in \LMA{} requires major changes to the code to parse new log lines
or extract additional data.  It does not appear to deal with any of the
complications discussed in \sectionref{complications} (initial
complications) and \sectionref{additional complications} (additional
complications).  It does not differentiate between different types of
rejections, so it is not suitable for the purposes of this project; the
data about which restriction caused the rejection is discarded, whereas an
important goal of this project is to retain that data to aid optimisation
and evaluation of Postfix restrictions.  \LMA{} fails to parse Postfix log
files generated on Solaris hosts because the fields added automatically
differ from those added on Linux hosts; logs from Solaris hosts (and
possibly other operating systems) thus require preprocessing before parsing
by \LMA{}.

\LMA{} does provide some simple reports: IP-STORY, FROM-STORY, DAILY-EMAIL
and DAILY-REJECT\@.  IP-STORY shows the \CSV{} lines with the specified
client \IP{} address; FROM-STORY shows the \CSV{} lines with the specified
sender address; DAILY-EMAIL shows the \CSV{} lines for the specified day.
DAILY-REJECT initially failed with an error message from the Perl
interpreter; after making some corrections to the code it worked, and
produced the \CSV{} lines for the specified day where the \SMTP{} code
signifies a rejection.  All of these reports are trivially simple to
produce from the \CSV{} file using the standard Unix tool \texttt{awk},
e.g.\ the most complicated, DAILY-REJECT, is merely:

\begin{verbatim}
awk -F\| '$1 ~ /2007-01-26/ && $7 ~ /^[450]|deferred/
    { print $0; print " "; }' lma_output.txt
\end{verbatim}

Notes about the command above: there is an unnecessary line containing only
a single space output after each matching line, for compatibility with the
output of DAILY-REJECT\@.  Some records have an \SMTP{} code of 0, though
this is not a valid \SMTP{} code, so presumably this is due to \LMA{} not
parsing a line properly; \LMA{} considers this code to be a rejection, so
the command above must too.  In a tiny proportion of records the \SMTP{}
code is \textit{deferred\/} (again, not a valid code, and most likely due
to mis-parsing), and is treated as a rejection by \LMA{}, so the equivalent
\texttt{awk} command must do so too.

There are a number of differences in the output from DAILY-REJECT and the
\texttt{awk} command:

\begin{itemize}

    \item \LMA{} does not escape or replace the separators used in the
        \CSV{} records when they are present in sender or recipient
        addresses, leading to some mangled records; these records are not
        present in the output from \texttt{awk}.

    \item Where there are multiple records in the \CSV{} file, DAILY-REJECT
        produces only one output line, whereas the \texttt{awk} command
        produces all matching lines.

\end{itemize}

In summary, \LMA{} appears to be a proof of concept, written to demonstrate
the point of their paper (that having this information in an accessible
fashion is useful to systems administrators), rather than a program
designed to be extensible and useful in a production environment.

% Literature review notes:
%
% Hard-coded parsing, requiring code changes to add more.  Attempts to
% correlate log lines, saves data to database for data mining purposes.
% Hard to extend/expand/understand.  Appears to only save: date and hour,
% \DNS{} name and \IP{} address host, mail server \IP{} address, sender,
% receiver and e-mail status (sent, rejected).  Undocumented schema.
% Design decision to use \CSV{} as an intermediate format between the log
% file and the database seems to have been restrictive.  Appears to require
% a queueid but majority of log entries (e.g. rejections) lack a queueid.
% Supports whitelisting \IP{} addresses when parsing logs, but whitelisting
% when generating reports/data mining would be preferable.  Supporting
% Berkeley DB is probably limiting the software - an example is the
% difficulty in searching a pipe-delimited string, so they have
% re-implemented foreign keys with tables keyed by ip address etc. pointing
% at the main table - this also won't scale well.  There doesn't appear to
% be any attempt to deal with the complications I've encountered: their
% parsing isn't detailed enough to encounter them.  It doesn't run
% properly; doesn't create any output; throws up errors.

\subsection{Conclusion}

This section has provided background information on several topics relevant
to the remainder of the paper.  It started with the motivation behind the
project, continuing with explanations of:

\begin{itemize}

    \item Using a database as an \API{}.

    \item \SMTP{}.

    \item Postfix restrictions and policy servers.

\end{itemize}

The assumptions which must be satisfied for the parser to work correctly
were discussed; the conventions used in the paper were listed; a brief
comparison of this project against other Postfix parsers was provided,
followed by a review of the previously published literature.

\section{Database schema}
\label{database schema}

The database is an integral part of the parser: it stores the rules and the
data gleaned by applying those rules to Postfix log files.  Understanding
the database schema is important in understanding the actions of the
parser, and essential to developing further applications which utilise the
data gathered.

\subsection{Introduction}

The database schema can be conceptually divided in two: the rules which are
used to parse log files, and the data saved from the parsing of log files.
Rules have the fields required to parse the log lines, extract data to be
saved, and the action to be executed; they also have several fields which
aid the user in understanding what each rule parses.  The rules are
described in detail in \sectionref{rules} but the fields are covered here.

The data saved from parsing the logs is also divided into two tables as
described below: connections and results.  The connections table contains a
row for every mail accepted and every connection where there was a
rejection; the individual fields will be described in the forthcoming
\sectionref{connections table}.  The results table has one or more rows per
row in the connections table, depending on the specifics of the
mail/connection; the fields will be covered in detail in
\sectionref{results table}.

An important but easily overlooked benefit of storing the rules in the
database is the connection between rules and results: if more information
is required when examining a result, the rules which produced the database
entries are available for inspection (and each result references the rule
which created it).  There is no ambiguity about which rule resulted in a
particular result, eliminating one potential source of confusion.

\subsection{Rules table}

\label{rule attributes}

Rules are discussed in detain in \sectionref{rules}, but the rules table is
covered here.  Rules are created by the user, not the parser, and will not
be modified by the parser (except for the hits and hits\_total fields).
Rules parse the individual lines, telling the parser which fields to
extract and what action to take for that line.

Each rule defines the following:

\begin{description}

    \item [name] A short name for the rule.

    \item [description] Something must have occurred to cause Postfix to
        log each line (e.g.\ a remote client connecting causes a connection
        line to be logged).  This field describes the action causing the
        log lines this rule matches.

    \item [restriction\_name] The restriction which caused the mail to be
        rejected.  Only applicable to rules which have a result of
        \texttt{rejected}, other rules will have an empty string.

    \item [postfix\_action] This is the action Postfix must have taken to
        generate this line, with two exceptions:

        \begin{description}

            \item [INFO] Represents an unspecified intermediate action that
                the parser is not interested in per se, but which does log
                useful information, supplementing other log lines.

            \item [IGNORED] An action which is not only uninteresting in
                itself, but which also provides no useful data.

        \end{description}

        Uninteresting lines are parsed so that any lines the parser isn't
        capable of handling become immediately obvious errors.

    \item [program] The program (\daemon{smtpd}, \daemon{qmgr}, etc.) whose
        log lines the rule applies to.  This avoids needlessly trying rules
        which won't match the line, or worse, might match unintentionally.
        Rules whose program is \texttt{*} will be tried against any lines
        which aren't parsed by program specific rules.

    \item [regex] The \regex{} to match the log line against.  The \regex{}
        will first have several keywords expanded: this simplifies reading
        and writing rules; avoids needless repetition of complex \regex{}
        components; allows the components to be corrected and/or improved
        in one location; and makes each \regex{} largely self-documenting.

        The following keywords are expanded (full explanations can be found
        in the source code):

        \_\_SENDER\_\_, \_\_RECIPIENT\_\_, \_\_MESSAGE\_ID\_\_,
        \_\_HELO\_\_, \newline \_\_EMAIL\_\_, \_\_HOSTNAME\_\_, \_\_IP\_\_,
        \_\_IPv4\_\_, \_\_IPv6\_\_, \newline \_\_SMTP\_CODE\_\_,
        \_\_RESTRICTION\_START\_\_, \_\_QUEUEID\_\_, \newline
        \_\_COMMAND\_\_, \_\_SHORT\_CMD\_\_, \_\_DELAYS\_\_, \_\_DELAY\_\_,
        \_\_DSN\_\_ and \_\_CONN\_USE\_\_.

        Additional fields are captured by \_\_RESTRICTION\_START\_\_, so
        rules using it will start the fields in result\_cols,
        connection\_cols, etc.\ at 5.

        For efficiency the keywords are expanded and every rule's \regex{}
        is compiled before attempting to parse the log file --- otherwise
        each \regex{} would be recompiled each time it was used, resulting
        in a large, data dependent slowdown.  Rule efficiency concerns are
        discussed in \sectionref{rule efficiency}.

    \item [result\_cols, connection\_cols] Specifies how the fields in the
        log line will be extracted.  The format is: \newline
        \texttt{smtp\_code = 1; recipient = 2, sender = 4;} \newline i.e.\
        semi-colon or comma separated assignment statements, with the
        variable name on the left and the matching field from the \regex{}
        on the right hand side.  The list of acceptable variable names is:

        \texttt{connection\_cols: client\_hostname, client\_ip, server\_ip,
        \newline \tab{} server\_hostname} and \texttt{helo.\newline}
        \texttt{result\_cols: sender, recipient, smtp\_code, message\_id,
        \newline \tab{} pid\_regex, data} and \texttt{child.}

    \item [result\_data, connection\_data] Sometimes rules need to supply a
        piece of data which isn't present in the log line: e.g.\ setting
        \texttt{smtp\_code} when mail is accepted.  The format and allowed
        variables are the same as for \texttt{result\_cols} and
        \texttt{connection\_cols}, except that arbitrary
        data\footnote{Commas and semi-colons cannot be escaped and thus
        cannot be used.  This is intended for use with small amounts of
        data rather than large amounts, so dealing with escape sequences
        seemed unnecessary.} is permitted on the right hand side of the
        assignment.

    \item [action] The action the algorithm will take; a full list can be
        found in \sectionref{actions-in-detail}.

    \item [queueid] Specifies the matching field from the \regex{} which
        gives the queueid, or zero if the log line doesn't contain a
        queueid.

    \item [hits] is an efficiency measure.  This counter is maintained for
        every rule and incremented each time the rule successfully matches.
        At the start of each run the program sorts the rules in descending
        order of hits, and at the end of the run updates every rule's hits.
        Assuming that the distribution of log lines is reasonably
        consistent between log files, rules matching more commonly
        occurring log lines will be tried before rules matching less
        commonly occurring log lines, lowering the program's execution
        time.  Rule ordering for efficiency is discussed in
        \sectionref{rule ordering for efficiency}.

    \item [hits\_total] The total number of hits for this rule over all
        runs of the parser.

    \item [priority] This is the user-configurable companion to hits: rules
        will be tried in order of priority, overriding hits.  This allows
        more specific rules to take precedence over more general rules
        (described in \sectionref{overlapping rules}).

\end{description}


\subsection{Connections table}

\label{connections table}

Every accepted mail and every connection where there was a rejection will
have a single entry in the connections table containing the following
fields:

\begin{description}

    \item [id] This field uniquely identifies the row.

    \item [server\_ip] The \IP{} address (IPv4 or IPv6) of the server: the
        local server when receiving mail, the remote server when sending
        mail.

    \item [server\_hostname] The hostname of the server, it will be
        \texttt{unknown} if the \IP{} address could not be resolved to a
        hostname via \DNS{}\@.

    \item [client\_ip] The client \IP{} address (IPv4 or IPv6): the remote
        server when receiving mail, the local server when sending mail.

    \item [client\_hostname] The hostname of the client, it will be
        \texttt{unknown} if the \IP{} address could not be resolved to a
        hostname via \DNS{}\@.

    \item [helo] The hostname used in the HELO command.  The HELO
        occasionally changes during a connection, presumably because spam
        or virus senders think it's a good idea.  By default Postfix only
        logs the HELO hostname when it rejects an \SMTP{} command, but it
        is quite easy to rectify this:

\label{logging helo}

        \begin{enumerate}

            \item Create \texttt{/etc/postfix/log\_helo.pcre}
                containing:\newline \tab{}\texttt{/./~~~~WARN~Logging~HELO}

            \item Modify \texttt{smtpd\_data\_restrictions} in
                \texttt{/etc/postfix/main.cf} to contain\newline
                \tab{}\texttt{check\_helo\_access~/etc/postfix/log\_helo.pcre}

        \end{enumerate}

        Although \texttt{smtpd\_helo\_restrictions} seems like the natural
        place to log the HELO hostname, there won't be a queueid associated
        with the mail for the first recipient, so that log line cannot be
        associated with the correct mail.  There is guaranteed to be a
        queueid when the DATA command has been reached, and thus it will be
        logged by any restrictions taking effect in
        \texttt{smtpd\_data\_restrictions}.  There is no difficulty in
        specifying a HELO-based restriction in
        \texttt{smtpd\_data\_restrictions}, Postfix will perform the check
        correctly.

        Logging the HELO hostname in this fashion also prevents the
        complication described in \sectionref{Mail deleted before delivery
        is attempted} from occurring when, but only in the case where there
        is a single recipient; in that case the recipient address will be
        logged also.

    \item [queueid] The queueid of the mail if the connection represents an
        accepted mail, or \texttt{NOQUEUE} otherwise.

    \item [start] The timestamp of the first log line, in seconds since the
        epoch.

    \item [end] The timestamp of the last log line, in seconds since the
        epoch.

\end{description}

\subsection{Results table}

\label{results table}

Every log line corresponding to Postfix performing an action other than
INFO or IGNORED will have an entry in the results table, e.g.\ rejecting an
\SMTP{} command, delivering a mail, or bouncing a mail.  Any log line where
the Postfix action is INFO is not interesting in and of itself, but
provides additional information which will be saved with other results;
IGNORED log lines don't even provide useful information.  Each row is
associated with a single connection, though there may be many results per
connection.

\begin{description}

    \item [connection\_id] A reference to the row in the connections table
        this result's table row belongs to.

    \item [rule\_id] A reference to the entry in the rules table which
        matched the log line and created this result.

    \item [warning] Postfix can be configured to log a warning instead of
        enforcing a restriction that would reject an \SMTP{} command --- a
        facility that is quite useful for testing new restrictions.  This
        field will be 1 if the log line parsed was a warning rather than a
        real rejection, or 0 for a real rejection or any other result.

    \item [smtp\_code] The \SMTP{} code associated with the log line.  In
        general an \SMTP{} code is only present for a rejection or final
        delivery; results missing an \SMTP{} code will duplicate the
        \SMTP{} code of other results in the connection.  Some final
        delivery log lines don't contain an \SMTP{} code: in those cases
        the code is faked based on the success or failure represented by
        the log line.

    \item [sender] The sender's email address.  This can change during one
        single connection, when the connection is reused to send multiple
        mails.

    \item [recipient] The recipient address.

    \item [size] The size of the mail; it will only be present for
        delivered mails.

    \item [message\_id] The message-id of the accepted mail, or
        \texttt{NULL} if no mail was accepted.

    \item [data] A field available for anything not covered by other
        fields, e.g.\ the rejection message from an \RBL{}\@.

    \item [timestamp] The time the line was logged, in seconds since the
        epoch.

\end{description}

\subsection{Conclusion}

The table containing the rules used by the parser, and both tables
containing the data extracted from the Postfix logs were described, with
the purpose of each field discussed in detail.  A clear, comprehensible
schema is essential when using the extracted data; it's more important when
using the data than when storing it, because storing the data is a
write-once operation, whereas utilising the data requires frequent
searching, sorting and manipulation of the data to produce customised
reports and/or statistics.

\section{Parsing rules}

\label{rules}

\subsection{Introduction}

This section discusses the rules used in parsing Postfix log files,
starting with rule characteristics, continuing with detecting and dealing
with overlapping rules, and the problems they can cause.  An example rule
and a line it would match are provided, plus a description of how the
fields in the rule are used when matching and performing the action (the
details of the table containing the rules have already been described in
\sectionref{rule attributes},).  This section continues with a discussion
of rule efficiency concerns, referring to the graphs in
\sectionref{graphs}, and finishes with an explanation of the algorithm used
to generate a new \regex{} from unparsed lines.

Please refer to \sectionref{parser design} for a discussion of why the
rules and actions have been separated in the parser's design.

\subsection{Rule characteristics}

\label{rule characteristics}


Rule have certain characteristics which may help in understanding the
parser:

\begin{itemize}

    \item Rules are annotated with the name of a Postfix program, and will
        only be used when parsing log lines produced by that
        program.\footnote{There are also generic rules which are used when
        parsing log lines produced by any Postfix program, but only if
        there are also rules specific to that program, and those rules must
        already have been tried and failed on the current line.}  Any given
        rule will only be used to parse a subset of the log lines, and any
        given log line will only be parsed by a subset of the rules.

    \item The first matching rule wins: no further rules are tried against
        that line, but there is a facility for specifying the order of
        rules so that more specific rules can be tried first.

    \item Rules are completely self-contained and can be understood in
        isolation, without reference to any other rules.

    \item There are no sub-rules, so rules have linear computational
        complexity.

\end{itemize}

\label{comparison against context-free grammars}

In context-free grammar terms the parser rules could be described as:

$\text{\textless{}log-line\textgreater{}} \mapsto \text{rule-1} |
\text{rule-2} | \text{rule-3} | \dots | \text{rule-n}$


\subsection{Overlapping rules}

\label{overlapping rules}

The parser does not try to detect overlapping rules;\footnote{It may be
possible to parse each rule's \regex{}, and determine if any overlap.  The
author has not attempted to do this: such a project by itself would
probably qualify for a PhD, and may involve solving the Halting
Problem~\cite{Wikipedia-halting-problem} and circumventing the
Church-Turing Thesis~\cite{Wikipedia-church-turing-thesis}.} that
responsibility is left to the author of the rules.  Unintentionally
overlapping rules lead to inconsistent parsing and data extraction because
the order in which rules are tried against each line is unspecified, and
the first matching rule wins.  Overlapping rules are frequently a
requirement, allowing a more specific rule to match some lines and a more
general rule to match the majority, e.g.\ separating \SMTP{} delivery to a
specific sites from \SMTP{} delivery to the rest of the world.  The
algorithm provides a facility for ordering overlapping rules: the priority
field in each rule (defaults to zero).  Rules are sorted by priority,
highest first, and then rules with the same priority are sorted by the
number of successful matches when parsing the previous log file.  Negative
priorities may be useful for catchall rules.

Detecting overlapping rules is difficult, but the following may be helpful:

\begin{itemize}

    \item Sort by \regex{} and visually inspect the list, e.g.\ with \SQL{}
        similar to: \textbf{select regex from rules order by regex;}

    \item Compare the results of parsing using sorted, shuffled and
        reversed rules.\footnote{See \sectionref{rule efficiency} for more
        details of sorting the rules.}  Parse a number of log files using
        normal sorting, dump a textual representation of the connections
        and results tables, and delete everything from those tables.
        Repeat with shuffled and reversed sorting.  If there are no
        overlapping rules the tables from each run will be identical;
        differences indicate overlapping rules.  Which rules overlap can be
        determined by examining the differences in the tables: each result
        contains the id of the rule which created it, so the rule
        referenced in the normal table overlaps with the rule referenced in
        the reversed table.  Unfortunately this method cannot prove the
        absence of overlapping rules; it can detect overlapping rules, but
        only if there are log lines in the input files which match more
        than one rule.

\end{itemize}

\subsection{Example rule}

\label{example rule}

This example rule matches the message logged by Postfix when it rejects
mail from a sender address because the appropriate \DNS{} entries are
missing, i.e.\ mail could not be delivered to the sender's address (for
full details see~\cite{reject-unknown-sender-domain}).

This rule would match the following log line:

\begin{verbatim}
NOQUEUE: reject: RCPT from example.com[10.1.1.1]:
  550 <foo@example.com>: Sender address rejected:
  Domain not found; from=<foo@example.com>
  to=<info@example.net> proto=SMTP
  helo=<smtp.example.com>
\end{verbatim}

% Don't reformat this!
\begin{tabular}[]{ll}

\textbf{Field}      & \textbf{Value}                                    \\
name                & Unknown sender domain                             \\
description         & We do not accept mail from unknown domains        \\
restriction\_name   & reject\_unknown\_sender\_domain                   \\
postfix\_action     & REJECTED                                          \\
program             & \daemon{smtpd}                                    \\
regex               & \verb!^__RESTRICTION_START__ <(__SENDER__)>: !    \\
                    & \verb!Sender address rejected: Domain not found;! \\
                    & \verb!from=<\5> to=<(__RECIPIENT__)> !            \\
                    & \verb!proto=E?SMTP helo=<(__HELO__)>$!            \\
result\_cols        & recipient = 6; sender = 5                         \\
connection\_cols    & helo = 7                                          \\
result\_data        &                                                   \\
connection\_data    &                                                   \\
action              & REJECTION                                         \\
queueid             & 1                                                 \\
hits                & 0                                                 \\
hits\_total         & 0                                                 \\
priority            & 0                                                 \\

\end{tabular}

\vspace{1em}

The various fields are used as follows;

\begin{description}

    \item [name, description, restriction\_name and postfix\_action:] are
        not \newline used by the algorithm, they serve to document the rule
        for the user's benefit.

    \item [program and regex:] If the program in the rule equals the
        program which logged the line the \regex{} will be tried against
        the line; if the match is successful the action will be executed,
        if not the next rule will be tried.  If the program-specific rules
        don't match the log line, the generic rules will be tried in the
        same way.

    \item [action:] will be executed if the \regex{} matches successfully
        (see \sectionref{actions-in-detail} for full details).

    \item [result\_cols, connection\_cols, result\_data and
        connection\_data:] are \newline used by the action to extract and
        save data matched by the \regex{}.

    \item [queueid:] The index of the field in the \regex{} which captured
        the queueid, or zero if the line does not contain a queueid.  This
        allows the correct mail can be found by queueid and actions
        performed on it.

    \item [hits, hits\_total and priority:] hits and priority are used in
        ordering the rules (see \sectionref{rule ordering for efficiency});
        hits is set to the number of successful matches at the end of the
        parsing run, and hits\_total has hits added to it, but is otherwise
        unused by the algorithm.

\end{description}

Additional fields are captured by \_\_RESTRICTION\_START\_\_, hence the
fields in result\_cols and connection\_cols start at 5 in the example.


\subsection{Rule efficiency}

\label{rule efficiency}

Parsing efficiency is an obvious concern when the parser routinely needs to
deal with 75 MB log files containing 300,000 log lines (for a small mail
server --- large scale mail servers would have much larger log files on a
daily basis).  When generating the data for the graphs included in
appendix~\refwithpage{graphs}, \numberOFlogFILES{} log files (totaling
10.08 GB, 60.72 million lines) were each parsed 10 times, the first run
discarded, and the remaining 9 runs averaged.  The first run is discarded
for two reasons:

\begin{enumerate}

    \item The execution time will be higher because the log file must be
        read from disk, whereas for subsequent runs the log file will be
        cached in memory by the operating system.

    \item The execution time will also be higher because the rule ordering
        will be sub-optimal compared to subsequent runs.

\end{enumerate}

Saving results to the database was disabled for the test runs, as that
dominates the run time of the program, and the tests are aimed at measuring
the speed of the parser rather than the speed of the database.

\subsubsection{Algorithmic complexity}

An important property of a parser is how execution time scales with input
size: does it scale linearly, polynomially, or exponentially?
Graph~\refwithpage{execution time vs file size vs number of lines graph}
shows the execution time in seconds, file size in MB and tens of thousands
of lines per log file per log file.  All three lines run roughly in
parallel, giving a visual impression that the algorithm scales linearly
with input size.  This impression is borne out by
graph~\refwithpage{execution time vs file size vs number lines factor},
which plots the ratio of file size vs execution time and number of lines vs
execution time.  As the reader can see, the ratios are quite tightly
banded, showing that the algorithm scales linearly: the much larger log
files between points 60 and 70 on the X axis in
graph~\refwithpage{execution time vs file size vs number of lines graph}
don't cause any abnormality in the corresponding points in
graph~\refwithpage{execution time vs file size vs number lines factor}.


\subsubsection{Rule ordering for efficiency}

\label{rule ordering for efficiency}

Rule ordering was mentioned in \sectionref{rule attributes} and will be
covered in greater detail in this section.  At the time of writing there
are \numberOFrules{} different rules, with the top 10\% matching the vast
majority of the log lines, and the remaining log lines split across the
other 90\% of the rules (as shown in graph~\refwithpage{rule hits graph}).
Assuming that the distribution of log lines is reasonably steady over time,
program efficiency should benefit from trying more frequently matching
rules before those which match less frequently.  To test this hypothesis
three full test runs were performed with different rule orderings:

\begin{description}

    \item [normal]  The most optimal order, according to the hypothesis:
        rules which match most often will be tried first.

    \item [shuffle] Random ordering --- the rules will be shuffled once
        before use and will retain that ordering for the entirety of the
        log file.  Note that the ordering will change every time the parser
        is executed, so 10 different orderings will be generated for each
        log file in the test run.  This is intended to represent an
        unsorted rule set.

    \item [reverse] Hypothetically the least optimal order: the most
        frequently matching rules will be tried last.

\end{description}

Graphs~\refwithpage{percentage increase of shuffled over normal}
and~\refwithpage{percentage increase of reversed over normal} show the
percentage increase of execution times (mean and standard deviation are
shown in table~\refwithpage{Regex caching/discarding with different groups
of log files}).  Overall this provides a modest but worthwhile
performance increase.

\subsubsection{Caching each regex}

Perl compiles the original \regex{} into an internal representation,
optimising the \regex{} to improve the speed of matching, but this
compilation and optimisation takes CPU time; far more CPU time, in fact,
than the actual matching takes.  Perl automatically caches static
\regexes{}, but dynamic \regexes{} need to be explicitly compiled and
cached.  Graph~\refwithpage{normal regex vs discard regex} shows execution
times with and without caching the \regex{}.  Caching the compiled
\regexes{} is obviously far more efficient; graph~\refwithpage{normal regex
vs discarded regex factor} shows the percentage execution time increase
when not caching each \regex{}.

Caching the compiled \regexes{} is quite simple, and is the single most
effective optimisation implemented in the parser.

\subsection{Creating new rules}

\label{creating new rules}

The logs produced by Postfix differ from installation to installation,
because administrators have the freedom to choose the subset of available
restrictions which suits their needs, including using different \RBL{}
services, policy servers, or custom rejection messages.  To facilitate easy
parsing of new log lines, the parser's design separates parsing rules from
parsing actions: adding new actions is difficult, but adding new rules to
parse new rejection messages is trivial (and also occurs much more
frequently).  The implementation provides a program to ease the process of
creating new rules from unparsed lines, based on the algorithm developed by
Risto Vaarandi~\cite{risto-vaarandi} for his \SLCT{}~\cite{slct-paper}.
The differences between the two algorithms will be outlined as part of the
general explanation below.

The core of the algorithm is quite simple: log lines are generally created
by substituting variable words into a fixed pattern, and analysis of the
frequency with which each word occurs can be used to determine whether the
word is fixed or variable.  This classification can be used to group
similar log lines and generate a \regex{} to match each group of lines.

There are 4 steps in the algorithm:

\begin{description}

    \item [Pre-process the file]  The new algorithm leverages the knowledge
        gained while writing rules and performs a large number of
        substitutions on the input lines, replacing commonly occurring
        variable terms (e.g.\ email addresses, \IP{} addresses, the
        standard start of rejection messages, etc.) with \regex{} keywords
        which the parser will expand when it loads the rule (see the
        \regex{} entry in \sectionref{rule attributes}).  The purpose of
        this phase is to utilise the existing knowledge to create more
        accurate \regexes{}.  The new log lines are written to a temporary
        file, which all subsequent stages use instead of the original input
        file.

        In the original algorithm the purpose of the preprocessing stage
        was to reduce the memory consumption of the program.  In the first
        pass it generated a hash from a small range of values for each word
        of each line, incrementing a counter for each hash.  The counters
        will later be used to filter out words: if the word's hash does not
        have a high frequency, the word itself cannot have a high
        frequency, and there is no need to maintain a counter for it,
        reducing the number of counters and thus the programme's memory
        consumption.

    \item [Calculate word frequencies]  The position of words within a line
        is important: a common word does not indicate similarity between
        lines unless it occupies the same position within both
        lines.\footnote{If a variable term within a line contains spaces,
        it will appear to the algorithm as two words rather than one.  This
        will alter the position of subsequent words, so a word occurring in
        different positions in two log lines may indicate similarity, but
        the algorithm does not attempt to deal with this possibility.}  The
        algorithm maintains a counter for each \textit{(word, word's
        position within the line)\/} tuple, incrementing it each time that
        word occurs in that position.

        The original algorithm only maintains counters for words whose hash
        result has a high frequency from the previous phase; this reduces
        the number of counters maintained by the algorithm, reducing the
        memory requirements of the algorithm.  The modified algorithm omits
        this check because the majority of unique or infrequently occurring
        words will have been substituted with keywords during the first
        phase, vastly reducing the number of tuples to maintain counters
        for.

    \item [Classify words based on their frequency]  The frequency of each
        \textit{(word, word's position within the line)\/} tuple is
        checked: if its frequency is greater than the threshold supplied by
        the user (1\% of all lines is generally a good starting point) it
        is classified as a fixed word, otherwise it is classified as a
        variable term.  Variable terms are replaced by \texttt{.+}, which
        means to match zero or more of any character.  

    \item [Build regexes]  The words are reassembled to produce a \regex{}
        matching the line, and a counter is maintained for each \regex{}.
        Contiguous sequences of \texttt{.+} in the newly reassembled
        \regexes{} are collapsed to a single \texttt{.+}; any resulting
        duplicate \regexes{} are combined, and their counters summed.  If
        the frequency of a \regex{} is lower than the threshold supplied by
        the user the \regex{} is discarded.\footnote{This is a second,
        independent threshold, but once again 1\% of input lines is a good
        starting point; obviously the threshold depends on the number and
        type of input lines.}  The new \regexes{} are printed for the user
        to add to the database, either as new rules or merged into the
        \regexes{} of existing rules; the counter for each \regex{} is also
        printed, giving the user an indication of how many of the input
        lines that \regex{} should match.  Discarding \regexes{} will
        result in some of the input lines not being matched; this utility
        should be run again once the unmatched lines have been isolated by
        running the parser, including the new \regexes{}, with the same
        input which produced the original set of unparsed lines.

\end{description}

A second utility is also provided which reads a list of new \regexes{} and
the input given to the first utility.  It tries to match each input line
against each \regex{}, counting the number of lines which match each
\regex{}, warning the user if an input line is matched by more than one
\regex{}, and additionally warning if an input line is not matched by any
\regex{}.  It displays a summary of how many input lines each \regex{}
matched, comparing it to the expected number of matches; this provides the
user with an easy method of checking if the \regexes{} produced by the
first utility are correctly matching the input lines they are based upon.
A future version of this utility will also group input lines by \regex{},
so the user can tweak the \regexes{} if required.

\subsection{Conclusion}

This section dealt with the rules used in parsing Postfix log files:

\begin{itemize}

    \item The characteristics of the rules were described.

    \item Detecting overlapping rules and dealing with the problems they
        can cause was covered, including a discussion of why overlapping
        rules can be helpful as well as harmful.

    \item An example log line and the rule matching it illustrated a
        description of how the fields in the rule are used both in the
        matching phase and the subsequent action that is executed.

    \item The database table containing the rules is dealt with in
        \sectionref{rule attributes}, and is not duplicated in this
        section.

    \item The topic of rule efficiency was discussed next, covering the
        effects of caching compiled \regexes{} and optimal ordering of
        rules, with reference to the graphs in
        appendix~\refwithpage{graphs}.

    \item This section finished with an explanation of the algorithm used
        to generate a new \regex{} from unparsed lines.

\end{itemize}

\section{Parsing algorithm}

\label{parsing-algorithm}

Where the rules are quite simple and each rule is completely independent of
its fellows, the algorithm is significantly more complicated and highly
internally interdependent.  The algorithm deals with all the complications
of parsing, the eccentricities and oddities of Postfix logs, and presents
the resulting data in a normalised, easy to use representation.  The
algorithm's task is to follow the journey each mail takes through Postfix,
piecing the data extracted by rules into a coherent whole, saving it in a
useful and consistent form, and performing housekeeping duties.

Please refer to \sectionref{parser design} for a discussion of why the
rules and actions have been separated in the parser's design.

\subsection{Introduction}

This section covers the following topics:

\begin{itemize}

    \item A high level overview of the algorithm.

    \item The first set of complications encountered: initially obvious
        difficulties which had to be overcome.

    \item A flow chart showing common paths a mail can take through the
        algorithm, and discussing the emergent behaviour observed in the
        parser.

    \item An explanation of the paths shown in the flow chart.

    \item The actions the parser makes available to rules are covered in
        detail.

    \item Additional complications which have arisen during the development
        of this parser are documented, including their solutions and where
        those solutions are implemented in the algorithm

\end{itemize}

\subsection{A high level overview}

A high level view of the algorithm could be expressed as:

\begin{enumerate}

    \item Mail enters the system via \SMTP{} or local submission; and new
        data structure is created for it.

    \item If the mail is rejected, log all data and finish.

    \item Follow the progress of the accepted mail until it's either
        delivered, bounced or deleted, then log all data, and finish.

\end{enumerate}

Unfortunately that ignores the many complications encountered.


\subsection{Complications encountered}

\label{complications}


\subsubsection{Queueid vs pid}

The mail lacks a queueid until it has been accepted, so log lines must
first be correlated by the \daemon{smtpd} \pid{}, then transition to being
correlated by the queueid.  This is relatively minor, but does require:

\begin{itemize}

    \item Two versions of several functions: \texttt{by\_pid} and
        \texttt{by\_queueid}.

    \item Two state tables to hold the data structure for each connection.

    \item Most importantly: every section of code must know whether it
        needs to lookup the data structures by \pid{} or queueid.

\end{itemize}

\subsubsection{Connection reuse}

\label{connection reuse}

Multiple independent mails may be delivered during one connection: this
requires cloning the current data as soon as a mail is accepted, so that
subsequent mails won't trample over each other's data.  This must be done
every time a mail is accepted, as it's impossible to tell in advance which
connections will accept multiple mails.  It is quite easy to overlook this
complication because only a small minority of connections accept more than
one mail. Happily once the mail has been accepted log entries won't be
correlated by \pid{} for that mail any more (its queueid will be used
instead), so there isn't any ambiguity about which mail a given log line
belongs to.\footnote{Unfortunately this statement is not completely
accurate: see \sectionref{timeouts-during-data-phase} for details.  However
in general there isn't any ambiguity about which data structure should be
used for a given log line.}  The original connection will be discarded
unsaved when the client disconnects if it doesn't have any data worth
saving, i.e.\ no rejections.  One unsolved difficulty is distinguishing
between different groups of rejections, e.g.\ when dealing with the
following sequence:

\begin{enumerate}

    \item The client attempts to deliver a mail, but it is rejected.

    \item The client issues the RSET command to reset the session.

    \item The client attempts to deliver another mail, likewise rejected.

\end{enumerate}

There should probably be two different entires in the database resulting
from the above sequence, but currently there will only be one.



\subsubsection{Re-injected mails}

The most difficult complication initially encountered is that locally
addressed mails are not always delivered directly to a mailbox: sometimes
they are addressed to and accepted for a local address but need to be
delivered to one or more remote addresses due to aliases.  When this occurs
a child mail will be injected into the Postfix queue, but without the
explicit logging \daemon{smtpd} or \daemon{postdrop} injected mails have.
Thus the source is not immediately discernible from the log line in which
the mail first appears; from a strictly chronological reading of the logs
it \textit{usually\/} appears as if the child mail has appeared from thin
air.  Subsequently the parent mail will log the creation of the child mail:

\texttt{3FF7C4317: to=<username@example.com>, relay=local, \newline
delay=0, status=sent (forwarded as 56F5B43FD)}

Unfortunately while all log lines from an individual process appear in
chronological order, the order in which log lines from different processes
are interleaved is subject to the vagaries of process scheduling.  In
addition the first log line belonging to the child mail (the log line cited
above properly belongs to the parent mail) is logged by \daemon{qmgr}, so
the order also depends on how busy \daemon{qmgr} is.\footnote{Postfix is
quite paranoid about mail delivery, an excellent characteristic for an
\MTA{} to possess, so it won't log that the child has been created until it
is absolutely certain that the mail has been written to disk.}

Because of this the parser cannot complain when it encounters a log line
from \daemon{qmgr} for a previously unseen mail; it must flag the mail as
coming from an unknown origin, and subsequently clear the flag if and when
the origin of the mail becomes clear.  Obviously the parser could omit
checking of where mails originate from, but the author believes that it is
better to require an explicit source, as bugs in the parser are more likely
to be exposed.

Process scheduling can have a still more confusing effect: quite often the
child mail will be created, delivered and entirely finished with
\textbf{before} the parent logs the creation line!  Thus mails flagged as
coming from an unknown origin cannot be entered into the database when
their final log line is parsed; instead they must be marked as ready for
entry and subsequently entered by the parent mail once it has been
identified.

\subsection{Flow chart}

\label{flow-chart}

Figures~\refwithpage{flow chart image part 1} and~\refwithpage{flow chart
image part 2} shows the paths the data representing a mail/connection can
take through the parser algorithm.  The flow chart covers the most common
paths only; there are additional, uncommon paths which are excluded for the
sake of clarity, but are described in \sectionref{additional
complications}.

The rules and actions exhibit emergent behaviour~\cite{Wikipedia-Emergence}
which is far more complicated than the behaviour described by the
algorithm.  The top-level parser could be written in pseudo-code like so
(with indentation level indicating flow control):

\begin{verbatim}
for each line in the input files:
    for each rule defined by the user:
        if this rule matches the input line:
            perform the action specified by the rule
            skip the remaining rules
            process the next input line
    warn the user that the input line was not parsed
\end{verbatim}

Yet despite the brevity of the algorithm above the (simplified) flow chart
below contains:

\begin{itemize}

    \item Twenty one states (not including explicit branches).

    \item Three entry points (1, 2, 3).

    \item Five exit points (8, 14, 17, 20, 25).

    \item Five loops (4--4, 4--5--4, 5--5, 9--9, 9--10--9).

    \item Four explicit branches (7, 13, 15, 18); these are decisions taken
        by an action, determining what will happen next to a mail.

    \item Four implicit branches, where the transition is determined by the
        next log line which is processed for that mail, e.g.\ state 1 can
        transition to either state 4 or state 5.  (1--\{4,5\},
        4--\{4,5,6\}, 5--\{4,5,6\}, 9--\{9,10,11,12\})

\end{itemize}

Several more states and corresponding branches are omitted for the sake of
clarity (these extra states are caused by solving the difficulties
described in \sectionref{additional complications}).

The solution to the \textit{out of order log log lines\/} complication
encountered during development (\sectionref{out of order log lines})
requires that a list of the acceptable combinations of Postfix programs a
mail can pass through; however this does not correspond to the
\textit{actions\/} a mail must pass through.  Before that complication was
overcome, i.e.\ for the majority of the development of this parser, there
was nothing in the parser explicitly encoding the states or paths a mail
could take; the vast majority of the parser has no need for this flow of
data to be specified.  This eases the process of adding new actions
(described in \sectionref{adding new actions}), as there is no requirement
to explicitly insert the action into an algorithmic version of the flow
chart below.

% This is very likely to move around as the flow chart is as big as
% possible, so if any preceding text changes the flow chart may end up on
% the next page, which may not be a bad thing, maybe.  What would be a bad
% thing is the \clearpage seems to cause a blank page sometimes, if the
% image is just slightly too big.

\showgraph{build/logparser-flow-chart-part-1}{Parser flow chart part
1}{flow chart image part 1}

\showgraph{build/logparser-flow-chart-part-2}{Parser flow chart part
2}{flow chart image part 2}

\clearpage

\subsection{Full algorithm}

\label{full-algorithm}

The intermingling of log entries from different mails immediately rules out
the possibility of handling each mail in isolation; the parser must be
capable of handling multiple mails in parallel, each potentially at a
different stage in its journey, without any interference between mails ---
except in the minority of cases where intra-mail interference is required.
The best way to implement this is to maintain state information for every
unfinished mail and manipulate the appropriate mail correctly for each log
line encountered.  The parser thus requires both a method of mapping log
lines to the correct mail and a method of specifying the action the log
line represents.  The former is achieved by using the \daemon{smtpd}
process id to identify the correct mail during the initial phase, then
switching to the queueid once the mail has been accepted.  The latter uses
the action field of the rule which matched the log line, executing the code
in the function named by the action.

\sectionref{actions-in-detail} explains the actions in substantive detail;
this section omits such detail because it would clutter and confuse the
algorithm description.  The flow chart in \sectionref{flow-chart} should
also be consulted while reading this section.  For the sake of clarity this
description covers only the most common paths through the algorithm, as
including every path would hinder understanding; the more uncommon paths
are caused by the complications described in \sectionref{additional
complications}, and are covered therein.

\subsubsection{Mail enters the system}

\label{mail-enters-the-system}

Everything starts off with a mail entering the system, whether by local
submission via \daemon{postdrop} or sendmail, by \SMTP{}, by re-injection
due to forwarding, or internally generated by Postfix.  Local submission is
the simplest case: a queueid is assigned immediately and the sender address
is logged (action: pickup; flowchart:~2).

\SMTP{} is more complicated:

\begin{enumerate}

    \item First there is a connection from the remote client (action:
        connect; flowchart:~1).

    \item This is followed by rejection of sender address, recipient
        addresses, client \IP{} address or hostname, etc. (action:
        rejection; flowchart:~4); acceptance of one or more mails (action:
        clone; flowchart:~5); or some interleaving of both.

    \item The client disconnects (action: disconnect; flowchart:~6).  If
        Postfix has rejected any \SMTP{} commands the data will be saved to
        the database; if not there won't be any data to save (any mails
        accepted will already have been cloned so their data is in another
        data structure).

    \item If one or more mails were accepted there will be more log entries
        for those mails later, see \sectionref{mail-delivery}.

\end{enumerate}

Re-injection due to forwarding sadly lacks explicit log lines of its
own;\footnote{Previously discussed in \sectionref{complications},
complication 3.} re-injection is somewhat awkward to explain because it
overlaps both the mail acceptance and mail delivery sections, so discussion
is deferred to \sectionref{tracking re-injected mail}.

Internally generated mails lack any explicit origin in Postfix 2.2.x and
must be detected using heuristics (see
\sectionref{identifying-bounce-notifications} for details).  Bounce
notifications are the primary example of internally generated mails, though
there may be other types.

\subsubsection{Mail delivery}

\label{mail-delivery}

The obvious counterpart to mail entering the system is mail leaving the
system, whether by deletion, bouncing, local delivery, or remote delivery.
All four are handled in exactly the same way:

\begin{enumerate}

    \item Postfix will log the sender and recipient addresses separately
        (action: save\_by\_queueid; flowchart:~9).

    \item Sometimes mail is re-injected and the child mail needs to be
        tracked by the parent mail (action: track; flowchart:~10) ---
        \sectionref{tracking re-injected mail} discusses this in
        detail.

    \item Eventually the mail will be delivered, bounced, or deleted by the
        administrator (action: commit; flowchart:~12).  This is the last
        log line for this particular mail (though it may be indirectly
        referred to if it was re-injected).  If it is neither parent nor
        child of re-injection the data is cleaned up and entered in the
        database (flowchart:~14), then deleted from the state tables.
        Re-injected mails are described in \sectionref{tracking re-injected
        mail}.

\end{enumerate}

It should be reiterated that the actions above happen whether the mail is
delivered to a mailbox, piped to a command, delivered to a remote server,
bounced (due to a mail loop, delivery failure, or five day timeout), or
deleted by the administrator.  The exception is what happens after delivery
to the parent or children of mail re-injected due to forwarding, as
explained in \sectionref{tracking re-injected mail}.

\subsubsection{Tracking re-injected mail}

\label{tracking re-injected mail}

The crux of the problem is that re-injected mails appear in the logs
without explicit logging indicating their source.  There are two implicit
indications:

\begin{enumerate}

    \item The indicator which more commonly introduces re-injection is when
        \daemon{qmgr} selects a mail with a previously unseen queueid for
        delivery (action: mail\_picked\_for\_delivery; flowchart:~3), in
        which case a new data structure will be created.  The mail will be
        flagged as having unknown origins; this flag should be subsequently
        cleared once the origin has been established.  This may also be an
        indicator that the mail is a bounce notification, see
        \sectionref{identifying-bounce-notifications} for details.

    \item Local delivery re-injects the mail and logs a relayed delivery
        rather than delivering directly to a mailbox or program as it
        usually would (action: track; flowchart:~10).\footnote{Relayed
        delivery is performed by the \SMTP{} client; local delivery means
        local to the server, i.e.\ an address the server is final
        destination for.} In this case the mail may already have been
        created (described above) and the unknown origin flag will be
        cleared; if not a new data structure will be created.  In both
        cases the new mail is marked as a child of the parent.  The log
        line in question is:

        \texttt{3FF7C4317: to=<username@example.com>, relay=local, \newline
        delay=0, status=sent (forwarded as 56F5B43FD)}

        This second indicator always occurs for re-injected mail but
        typically occurs after the first indicator explained above.  This
        indicator is required to tie the parent and child mails together
        and so is central to the process of tracking re-injected mails.

\end{enumerate}

The algorithm for tracking and saving re-injected mail to the database can
finally be described:

\begin{itemize}

    \item If the mail is of unknown origin it is assumed to be a child mail
        whose parent hasn't yet been identified (action: commit;
        flowchart:~15).  Mark the mail as ready for entry in the database
        (flowchart:~16), and wait for the parent to deal with it
        (flowchart:~17).  The mail should not have subsequent log entries;
        only its parent should refer to it.

    \item If the mail is a child mail then it has already been tracked
        (action: commit; flowchart:~18): the data is cleaned up, the child
        is entered in the database (flowchart:~19), then deleted from the
        state tables.  The child mail will be removed from the parent
        mail's list of children (flowchart:~20); if this is the last child
        and the parent has also been entered in the database the parent
        will be deleted from the state tables.

    \item The last alternative is that the mail is a parent mail (action:
        commit; flowchart:~21).  Regardless of the state of its children
        its data is cleaned up and entered in the database (flowchart:~22).
        The parent may have children which are waiting to be entered in the
        database (flowchart:~23); each of those children's data is cleaned
        up and entered in the database, then deleted from the state tables.
        The parent may also have outstanding children which are not yet
        delivered, in which case (flowchart:~24) the parent must wait for
        those children to be finished with.  As soon as the last child is
        deleted from the state tables the parent will also be finished with
        (flowchart:~25), and deleted from the state tables.

\end{itemize}

A parent mail can have multiple children, which may be delivered before or
after the parent mail.

\subsection{Actions in detail}

\label{actions-in-detail}

Each action is passed the same arguments:

\begin{description}

    \item [line] The log line, separated into fields:

        \begin{description}

            \item [timestamp] The time the line was logged at.

            \item [host] The hostname of the server which logged the line.

            \item [program] The name of the program which logged the line.

            \item [pid] The \pid{} of the program which logged the line.

            \item [text] The remainder of the line.

        \end{description}

    \item [rule] The matching rule.

    \item [matches] The fields in the line captured by the rule's \regex{}.

\end{description}

The actions:

\begin{description}

    \item [BOUNCE] Postfix 2.3 (and hopefully subsequent versions) logs the
        creation of bounce messages.  This action creates a new mail if
        necessary; if the mail already exists the unknown origin flag will
        be removed.  The action also marks the mail as a bounce
        notification.  To deal with complication \sectionref{Further out of
        order log lines} this action checks a cache of recent bounce mails
        to avoid creating bogus bounce mails when lines are logged out of
        order.

    \item [CLONE] Multiple mails may be accepted on a single connection, so
        each time a mail is accepted the connection's state table entry
        must be cloned; if the original data structure was used the second
        and subsequent mails would overwrite one another's data.

    \item [COMMIT] Enter the data from the mail into the database. Entry
        will be postponed if the mail is a child waiting to be tracked.
        Once entered, the mail will be deleted from the state tables.

    \item [CONNECT] Handle a remote client connecting: create a new state
        table entry (indexed by \daemon{smtpd} \pid{}) and save both the
        client hostname and \IP{} address.

    \item [DISCONNECT] Deal with the remote client disconnecting: enter the
        connection in the database, perform any required cleanup, and
        delete the connection from the state tables.

    \item [EXPIRY] If Postfix has not managed to deliver a mail after
        trying for five days it will give up and return the mail to the
        sender.  When this happens the mail will not have a combination of
        Postfix programs which passes the valid combinations check (see
        \sectionref{out of order log lines}).  To ensure that the mail can
        be committed the EXPIRY action sets a flag marking the mail as
        expired; the flag later causes the valid combinations check to be
        skipped, so the mail will be committed.

    \item [IGNORE] This rule just returns successfully; it is used when a
        line needs to be parsed for completeness but doesn't either provide
        any useful data or require anything to be done.

    \item [MAIL\_PICKED\_FOR\_DELIVERY] This action represents Postfix
        picking a mail from the queue to deliver. This action is used for
        both \daemon{qmgr} and \daemon{cleanup} as it needs to deal with
        out of order log lines; see \sectionref{discarding cleanup lines}
        for details.

    \item [MAIL\_TOO\_LARGE] When the client tries to send a message larger
        than the local server accepts, the mail will be discarded, and the
        client informed.  See TIMEOUT for further discussion; the two are
        handled in exactly the same way.

    \item [PICKUP] The PICKUP action corresponds to the \daemon{pickup}
        service dealing with a locally submitted mail.  Out of order log
        entries may have caused the state table entry to already exist (see
        \sectionref{pickup logging after cleanup}); otherwise it is
        created.  The data from the log line is then saved to the state
        table entry.

    \item [POSTFIX\_RELOAD] When Postfix stops or reloads its configuration
        it kills all \daemon{smtpd} processes,\footnote{Possibly other
        programs are killed also, but the parser is only affected by and
        interested in \daemon{smtpd} processes exiting.} requiring any
        active connections to be cleaned up, entered in the database, and
        deleted from the state tables.

    \item [REJECTION] Deal with Postfix rejecting an \SMTP{} command from
        the remote client: log the rejection with a mail if there is a
        queueid in the log line, or with the connection if not.

    \item [SAVE\_BY\_QUEUEID] Find the correct mail based on the queueid in
        the log line, and save the data extracted by the \regex{} to it.

    \item [SMTPD\_DIED] Sometimes a \daemon{smtpd} dies or exits
        unsuccessfully; the active connection for that \daemon{smtpd} must
        be cleaned up, entered in the database, and deleted from the state
        tables.

    \item [SMTPD\_KILLED] Sometimes an \daemon{smtpd} is killed by a
        signal~\cite{Wikipedia-unix-signals}; the active connection for
        that \daemon{smtpd} must be cleaned up, entered in the database,
        and deleted from the state tables.

    \item [SMTPD\_WATCHDOG] \daemon{smtpd} processes have a watchdog timer
        to deal with unusual situations --- after five hours the timer will
        expire and the \daemon{smtpd} will exit.  This occurs very
        infrequently, as there are many other timeouts which should occur
        in the intervening hours: \DNS{} timeouts, timeouts reading data
        from the client, etc.  The active connection for that
        \daemon{smtpd} must be cleaned up, entered in the database, and
        deleted from the state tables.

    \item [TIMEOUT] The connection timed out so the mail currently being
        transferred must be discarded. The mail may have been accepted, in
        which case there's a data structure to dispose of, or it may not in
        which case there is not.  See
        \sectionref{timeouts-during-data-phase} for the gory details.

    \item [TRACK] Track a mail when it is re-injected for forwarding to
        another mail server; this happens when a local address is aliased
        to a remote address.  TRACK will be called when dealing with the
        parent mail, and will create the child mail if necessary. TRACK
        checks if the child has already been tracked, either by this parent
        or by another parent, and issues appropriate warnings in either
        case.

    \item [DELETE] Deals with mail deleted by the administrator using the
        administrative command \daemon{postsuper}.  This action adds a
        dummy recipient address if required, then invokes the COMMIT action
        to handle adding the mail to the database.  The complication this
        action deals with is described fully in \sectionref{Mail deleted
        before delivery is attempted}.  

\end{description}

\subsection{Adding new actions}

\label{adding new actions}

Adding new actions is almost as simple as adding new rules, though actions
are not stored in the database.  The implementor writes a subroutine which
accepts the standard arguments given to actions, and registers it as an
action by calling the parser subroutine add\_actions() with the name of the
new action subroutine as a parameter.  No other work is required on the
implementor's part; all of their attention and effort can be focused on the
correctness of their action.  The only negative aspect is that the process
involves editing the parser source code, which makes upgrading to a later
version of the parser more difficult, though by no means impossible.  If
the author of the new action wishes they can take advantage of the parser's
object oriented implementation~\cite{Wikipedia-object-orientation} by
subclassing it and implementing their changes in the derived class,
allowing future upgrading of the parser with greatly reduced chance of
conflicts.\footnote{The real difference between the two approaches is where
the new code is placed.  The simpler option is to change the parser code
directly, but those changes will then have to be made to subsequent
versions of the parser, and as the scope of the changes increases so does
the chance of conflict.  The more time consuming option is to write a
subclass containing the new actions and change the program which invokes
the parser so that it uses the subclass rather than the parser; the changes
required to the program invoking the parser are minor and much less likely
to lead to conflicts when upgrading to future versions of the parser.  An
alternative is to submit new actions to the author of the parser for
inclusion in future versions, resulting in two benefits: the new actions do
not need to be maintained separately, and other users of the parser will
avail of the new functionality.} The action may need to extend the list of
valid combinations described in \sectionref{out of order log lines} if the
addition creates a different set of acceptable programs, but this is
unlikely to occur, as it would require parsing log lines from Postfix
components the parser currently ignores.\footnote{The mail server used for
development does not utilise either the \daemon{lmtp} or \daemon{virtual}
delivery agents, so this parser does not have rules to handle log lines
from those components.  Adding new rules to parse those component's log
lines is a simple process, though if their behaviour differs significantly
from the \daemon{smtp} or \daemon{local} delivery agents new actions may be
required.  The mail server in question is a production mail server handling
mail for a university department; the benefit is that the logs used exhibit
the idiosyncrasies and foibles a mail server in the wild must deal with,
but the downside is that significantly altering the configuration just to
log messages from a different Postfix component is not an option.}


\subsection{Additional complications}

\label{additional complications}

The complications described in this section are listed in the order in
which they were encountered during development of the parser.  Each of
these complications caused the parser to operate incorrectly, generating
either warning messages or leaving mails in the state table.  The frequency
of occurrence is much higher at the start of the list, with the first
complication occurring several orders of magnitude more frequently than the
last.  When deciding which problem to address next the most common was
always chosen, as resolving the most common problem would yield the biggest
improvement in the parser, and also prune the greatest number of mails from
the state tables and error messages.


\subsubsection{Identifying bounce notifications}

\label{identifying-bounce-notifications}

Postfix 2.2.x (and presumably previous versions) lacks explicit logging
when bounce notifications are generated; suddenly there will be log entries
for a mail which lacks an obvious source.  There are similarities to the
problem of re-injected mails discussed in \sectionref{tracking re-injected
mail}, but unlike the solution described therein bounce notifications do
not eventually have a log line which identifies their source.  Heuristics
must be used to identify bounce notifications, and those heuristics are:

\begin{enumerate}

    \item The sender address is $<>$.

    \item Neither \daemon{smtpd} nor \daemon{pickup} have logged any
        messages associated with the mail, indicating it was generated
        internally by Postfix, not accepted via \SMTP{} or submitted
        locally by \daemon{postdrop} or sendmail.

    \item The message-id has a specific format: \newline
        \texttt{YYYYMMDDhhmmss.queueid@server.hostname} \newline
        e.g.\ \texttt{20070321125732.D168138A1@smtp.example.com}

    \item The queueid in the message-id must be the same as the queueid of
        the mail: this is what distinguishes bounce notifications generated
        locally from bounce notifications which are being re-injected as a
        result of aliasing.  In the latter case the message-id will be
        unchanged from the original bounce notification, and so even if it
        happens to be in the correct format (e.g.\ if it was generated by
        Postfix on another server) the queueid in the message-id will not
        match the queueid of the mail.

\end{enumerate}

Once a mail has been identified as a bounce notification the unknown origin
flag is cleared and the mail can be cleaned up and entered in the database.

There is a small chance that a mail will be incorrectly identified as a
bounce notification, as the heuristics used may be too broad.  For this to
occur the following conditions would have to be met:

\begin{enumerate}

    \item The mail must have been generated internally by Postfix.

    \item The sender address must be $<>$.

    \item The message-id must have the correct format and match the queueid
        of the mail.  While a mail sent from elsewhere could easily have
        the correct message-id format, the chance that the queueid in the
        message-id would match the queueid of the mail is extremely small.

\end{enumerate}

The most likely cause of mis-identification is if a mail generated
internally by Postfix is identified as a bounce notification when it is a
different type of message; arguably this is a benefit rather than a
drawback, as other mails generated internally by Postfix will be handled
correctly.

Postfix 2.3 (and hopefully subsequent versions) log the creation of a
bounce message.

This check is performed during the COMMIT action.

\subsubsection{Aborted delivery attempts}

\label{aborted-delivery-attempts}

Some mail clients appear\footnote{Due to privacy concerns no attempt has
been made to identify either the users or the software which exhibits this
behaviour.} to send the following sequence of commands during
the \SMTP{} session:

\begin{verbatim}
    EHLO client.hostname
    MAIL FROM: <sender@address>
    RCPT TO: <recipient@address>
    RSET
    MAIL FROM: <sender@address>
    RCPT TO: <recipient@address>
    DATA
\end{verbatim}

The client aborts the first delivery attempt after the first recipient is
accepted, then makes a second delivery attempt which it continues with
until the delivery is complete.

Once again Postfix does not log a message making the client's behaviour
clear, so once again heuristics are required to identify when this
behaviour occurs.  In this case a list of all mails accepted during a
connection is saved in the connection state, and the accepted mails are
examined when the disconnection action is executed.  Each mail is checked
for the following: is the mail missing its \daemon{cleanup} log message?
Every mail which passes through Postfix will have a \daemon{cleanup} line;
lack of a \daemon{cleanup} line is a sure sign the mail didn't make it too
far.

If this check is successful then the mail is assumed to be one of the
offending bogus mails and is discarded.  There will be no further entries
logged for such mails, so without identifying and discarding them they
accumulate in the state table and will cause clashes if the queueid is
reused.  The mail cannot be entered in the database as the only data
available is the client hostname and \IP{} address, but the database schema
requires many more fields be populated (see \sectionref{connections table}
and \sectionref{results table}).

This check is performed in the DISCONNECT action; it requires support in
the CLONE action where a list of cloned connections is maintained.

\subsubsection{Further aborted delivery attempts}

Some mail clients disconnect abruptly if a second or subsequent recipient
is rejected; they may also disconnect after other errors, but such
disconnections are either unimportant or are handled elsewhere in the
algorithm (\sectionref{timeouts-during-data-phase}).  Sadly Postfix doesn't
log a message saying the mail has been discarded, as should be expected by
now.  The checks to identify this happening are:

\begin{itemize}

    \item Is the mail missing its \daemon{cleanup} log message?  Every mail
        which passes through Postfix will have a \daemon{cleanup} line;
        lack of a \daemon{cleanup} line is a sure sign the mail didn't make
        it too far.

    \item Were there three or more \daemon{smtpd} log lines for the mail?
        There should be a connection line and a mail accepted line,
        followed by one or more rejection lines.

    \item Is the last \daemon{smtpd} log line a rejection line?

\end{itemize}

These checks are made during the DISCONNECT action: if all checks are
successful then the mail is assumed to have been discarded when the client
disconnected.  There will be no further entries logged for such mails, so
without identifying and entering them in the database immediately they
accumulate in the state table and will cause clashes if the queueid is
reused.

\subsubsection{Timeouts during DATA phase}

\label{timeouts-during-data-phase}

The DATA phase of the \SMTP{} conversation is where the headers and body of
the mail are transferred.  Sometimes there is a timeout or the connection
is lost\footnote{For brevity's sake timeout will be used throughout this
section, but everything applies equally to lost connections.} during the
DATA phase; when this occurs Postfix will discard the mail and the parser
needs to discard the data associated with that mail.  It seems more
intuitive to save the data to the database, but if a timeout occurs there
won't yet have been any data gathered for the mail, so there is none
available to save; the timeout is recorded with the connection data
instead.

To deal properly with timeouts the parsing algorithm needs to do the
following in the TIMEOUT action:

\begin{enumerate}

    \item Record the timeout and associated data in the connection's
        results.

    \item If no mails have been accepted yet nothing needs to be done; the
        timeout action ends.  The timeout action is dependant on the clone
        action keeping a list of all mails accepted on each connection.

    \item \ESMTP{} pipelining allows the client to send MAIL FROM, RCPT TO
        and DATA commands in one packet, instead of sending each command
        individually and waiting for the reply before sending subsequent
        commands.  If pipelining is used and Postfix rejects the sender
        address or any of the recipient addresses there is no way for the
        client to tell which command was rejected.  Some clients make no
        attempt to recover and disconnect uncleanly; others presumably
        stop pipelining and restart the mail sending operation.  The client
        could attempt to parse the error message, but that would be
        extremely error prone, whereas not pipelining makes the cause of
        the error message obvious.

        A timeout may thus apply either to an accepted mail or a rejected
        mail.  To distinguish between the two cases the algorithm compares
        the timestamp of the last accepted mail against the timestamp of
        the last line logged by \daemon{smtpd} for that connection.  If the
        \daemon{smtpd} timestamp is later there was a rejection between the
        accepted mail and the timeout, therefore the timeout applies to a
        rejected mail; the timeout has already been recorded so the timeout
        action finishes.  If the mail acceptance timestamp is greater then
        the timeout applies to the just-accepted mail, which will be
        discarded.

\end{enumerate}

This complication is further complicated by the presence of out of order
\daemon{cleanup} lines: see \sectionref{discarding cleanup lines} for
details.

\subsubsection{Discarding cleanup lines}

\label{discarding cleanup lines}

The author has only observed this complication occurring after a timeout,
though there may be other circumstances which trigger it.  Sometimes the
\daemon{cleanup} line is logged after the timeout line; parsing this line
causes the creation of a new state table entry for the queueid in the log
line.  This is incorrect because the line actually belongs to the mail
which has just been discarded, and the next log line for that queueid will
be seen when the queueid is reused, causing a queueid clash and the
appropriate warning.

In the case where the \daemon{cleanup} line is still pending during the
TIMEOUT action the algorithm updates a global cache of queueids, adding the
queueid and the timestamp from the timeout line.  When the next
\daemon{cleanup} line is parsed for that queueid the cache will be checked,
and the line will be deemed part of the discarded mail and discarded if it
meets the following requirements:

\begin{itemize}

    \item The queueid must not have been reused yet, i.e.\ there isn't an
        entry in the state tables for the queueid.

    \item The timestamp of the \daemon{cleanup} line must be within ten
        minutes of the mail acceptance timestamp.  Timeouts happen after
        five minutes, but some data may have been transferred slowly, and
        empirical evidence shows that ten minutes is not unreasonable;
        hopefully it is a good compromise between false positives and false
        negatives.

\end{itemize}

The next \daemon{cleanup} line must meet the criteria above for it to be
discarded because not every connection where a timeout occurs will have a
\daemon{cleanup} line logged for it; if the algorithm blindly discarded the
next \daemon{cleanup} line after a TIMEOUT it would in some cases be
mistaken.  Whether or not the next \daemon{cleanup} line is discarded the
queueid will be removed from the cache of timeout queueids when the next
\daemon{pickup} line containing that queueid is parsed.

During the TIMEOUT action if the mail lacks a \daemon{cleanup} line, the
queueid and other data about the mail, particularly the timestamp of the
last log line, are added to a global cache of timeout queueids.  This cache
is subsequently consulted during the MAIL\_PICKED\_FOR\_DELIVERY action,
and if the queueid is found in the cache and the \daemon{cleanup} line
passes the checks listed above the line will be discarded.

\subsubsection{Pickup logging after cleanup}

\label{pickup logging after cleanup}

Occasionally the \daemon{pickup} line logged when mail is submitted locally
via sendmail appears later in the log file than the \daemon{cleanup} line
for that mail.  This seems to occur during periods of particularly heavy
load, so is most likely due to process scheduling vagaries.  Normally if
the queueid given in the \daemon{pickup} line exists a warning is generated
by the \daemon{pickup} action, but if the following conditions are met it
is assumed that the lines are out of order:

\begin{itemize}

    \item The only program which has logged anything thus far for the mail
        is \daemon{cleanup}.

    \item There is less than a five second difference between the
        timestamps of the \daemon{cleanup} and \daemon{pickup} lines.

\end{itemize}

As always with heuristics there may be circumstances in which these
heuristics match incorrectly,  but none have been identified so far.

This complication is dealt with during the PICKUP action.

\subsubsection{Smtpd stops logging}

\label{smtpd stops logging}

Occasionally a \daemon{smtpd} will just stop logging, without an
immediately obvious reason.  After poring over logs for some time there are
several reasons for this infrequent occurrence:

\begin{enumerate}

    \item Postfix is stopped or its configuration is reloaded.  When this
        happens all \daemon{smtpd} processes exit, and all entries in the
        connections state table must be cleaned up, entered in the database
        and deleted.  The queueid state table is untouched.

    \item Sometimes an \daemon{smtpd} is killed by a signal, so the active
        connection must be cleaned up, entered in the database and deleted
        from the connections state table.

    \item Occasionally a \daemon{smtpd} will die or exit uncleanly, so the
        active connection must be cleaned up, entered in the database and
        deleted from the connections state table.

\end{enumerate}

The above descriptions appear to cover all situations identified thus far
where a \daemon{smtpd} suddenly stops logging.  In addition to removing an
active connection the last accepted mail may need to be discarded, as
detailed in \sectionref{timeouts-during-data-phase}.

These occurrences are handled by the three actions POSTFIX\_RELOAD,
SMTPD\_DIED and SMTPD\_WATCHDOG\@.

\subsubsection{Out of order log lines}

\label{out of order log lines}

Occasionally a log file will have out of order log lines which cannot be
dealt with by the techniques described in \sectionref{tracking re-injected
mail}, \sectionref{discarding cleanup lines} or \sectionref{pickup logging
after cleanup}.  In the \numberOFlogFILES{} log files used for testing this
occurs only five times in 60,721,709 log lines, but for completeness of the
algorithm it should be dealt with.  The five occurrences in the test log
files have the same characteristics: the \daemon{local} log line showing
delivery to a local mailbox occurs after the \daemon{qmgr} log line showing
removal of the mail from the queue because delivery is completed.  This
causes problems: the mail is not complete, so entry into the database
fails; a new mail is created when the \daemon{local} line is parsed and
remains in the state tables; four warnings are issued per pair of out of
order log lines.

The solution to this problem is to examine the list of programs which have
logged messages for each mail, comparing the list against a table of
known-good combinations of programs (this check is performed during the
COMMIT action).  If the mail's combination is found in the valid list the
mail can be entered in the database; if the combination is not found entry
must be postponed and the mail flagged for later entry.  The
SAVE\_BY\_QUEUEID action checks for the flag and retries entry if it's
found; if the additional log lines have caused the mail to reach a valid
combination entry will proceed, otherwise it must be postponed once more.

The list of valid combinations is explained below.  Every mail will
additionally have log entries from \daemon{cleanup} and \daemon{qmgr}; any
mail may also have log entries from \daemon{bounce}, \daemon{postsuper}, or
both.

\begin{description}

    \item [\daemon{local}:] Local delivery of a bounce notification, or
        local delivery of a forwarded or tracked mail.

    \item [\daemon{local}, \daemon{pickup}:] Mail submitted locally on the
        server, delivered locally on the server.

    \item [\daemon{local}, \daemon{pickup}, \daemon{smtp}:] Locally
        submitted mail, \newline both local and remote delivery.

    \item [\daemon{local}, \daemon{smtp}, \daemon{smtpd}:] Mail accepted
        from a remote client, both local and remote delivery.

    \item [\daemon{local}, \daemon{smtpd}:] Mail accepted from a remote
        client, local delivery only.

    \item [\daemon{pickup}, \daemon{smtp}:] Locally submitted mail, remote
        delivery only.

    \item [\daemon{smtp}:] Remote delivery of forwarded or tracked mail, or
        a bounce notification.

    \item [\daemon{smtp}, \daemon{smtpd}:] Mail accepted from a remote
        client, then remotely delivered (typically relaying mail for
        clients on the local network).

    \item [\daemon{smtpd}, \daemon{postsuper}:] Mail accepted from a remote
        client, then deleted by the administrator before any delivery
        attempt (administrator deleting unwanted mail, typically due to a
        mail loop or joe-job).  Notice that \daemon{postsuper} is required,
        not optional, for this combination.

\end{description}

This check applies to accepted mails only, not to rejected mails.

\subsubsection{Yet more aborted delivery attempts}

\label{yet-more-aborted-delivery-attempts}

The aborted delivery attempts described in
\sectionref{aborted-delivery-attempts} occur frequently, but the aborted
delivery attempts described in this section only occur four times in the
\numberOFlogFILES{} log files used for testing.  The symptoms are the same
as in \sectionref{aborted-delivery-attempts}, except that there
\textit{is\/} a \daemon{cleanup} log line; there does not appear to be
anything in the log file to explain why there are no further log messages.
The only way to detect these mails is to periodically scan all mails in the
state tables, deleting any mails displaying the following characteristics:

\begin{itemize}

    \item The timestamp of the last log line for the mail must be 12 hours
        or more earlier than the last log line parsed.

    \item There must be exactly two \daemon{smtpd} and one \daemon{cleanup}
        log entries for the mail, with no additional log entries.

\end{itemize}

12 hours is a somewhat arbitrary time period, but it is far longer than
Postfix would delay delivery of a mail in the queue.\footnote{This may be a
problem if Postfix is not running for an extended period of time.}  The
state tables are scanned for mails matching the characteristics above each
time the end of a log file is reached, and matching mails are deleted.

\subsubsection{Mail deleted before delivery is attempted}

\label{Mail deleted before delivery is attempted}

Postfix logs the recipient address when delivery of a mail is attempted, so
if delivery has yet to be attempted the parser cannot determine the
recipient address or addresses.  This is a problem when mail is arriving
faster than Postfix can attempt delivery, and the administrator deletes
some of the mail (because it's the result of a mail loop, mail bomb, or
joe-job) before Postfix has had a chance to try to deliver it.  In this
case the recipient address will not have been logged, so a dummy recipient
address needs to be added, as every mail is required to have at least one
recipient.  This complication is generally observed in few log files, but
typically when it does occur there will be many instances of it, closely
grouped.

\subsubsection{Further out of order log lines}

\label{Further out of order log lines}

This is yet another complication which only occurs during periods of
extremely high load, when process scheduling vagaries and even hard disk
access times cause strange behaviour.  In this complication bounce
notification mails are created, delivered and deleted from the queue,
\textit{before\/} the log line from \daemon{bounce} is logged.  To deal
with this the COMMIT action maintains a cache of recently committed bounce
notification mails, which the BOUNCE action subsequently checks if the
bounce mail isn't already in the state tables. If a mail exists in the
cache under the correct queueid, and its start time is less than ten
seconds before the timestamp of the bounce log line, it is assumed that the
bounce notification mail has already been processed and the BOUNCE action
does not create one.  If the queueid exists it is removed from the cache,
as it has either just been used or it is too old to use in future.  Whether
the BOUNCE action creates a mail or finds one existing in the state tables,
it flags the mail as having been seen by the BOUNCE action; if this flag is
present the COMMIT action will not add the mail to the cache of recent
bounce notification mails.\footnote{This is not required to correctly deal
with the complication, but is an optimisation to reduce memory usage of the
parser; on the occasions the author has observed this action occurring
there have been a huge number of bounce notification mails generated; if
every bounce notification mail was cached it would dramatically increase
the memory requirements of the parser.}  The cache of bounce notification
mails will be pruned whenever the parser's state is saved, though if the
size of the cache ever becomes a problem it could be pruned periodically to
keep it in check.

\subsection{Conclusion}

This section has presented the parsing algorithm, starting with a high
level overview of the basic algorithm, and the first group of difficulties
thwarted.  The revised algorithm was detailed, preceded by a flow chart
depicting the paths a mail may take through the algorithm, and followed by
the actions which are available to rules were described in full.  The
parser exhibits emergent behaviour far more complicated than the behaviour
encoded in the algorithm, and this behaviour is covered in the flow chart
subsection.  The section concludes with the remaining complications
discovered and overcome in the process of refining and completing the
algorithm.  Detecting, diagnosing and defeating complications forms the
largest single portion of this document, mirroring the development of the
parser,  The complications are described in the order they were overcome,
with subsequent problems affecting fewer mails (often by an order or
magnitude), though the time required to solve problems increased with each
successive problem.

\section{Coverage}

\label{parsing coverage}

\subsection{Introduction}

The discussion of the parser's coverage of Postfix log files is separated
into two parts: log lines covered and mails covered.  The first is
important because the parser should handle all (relevant) log lines it's
given; the second is equally important because the parser must properly
deal with every mail if it is to be useful.  Improving the former is
less intrusive, as it just requires new rules to be written; improving the
latter is much more intrusive as it requires changes to the parser
algorithm, and it can also be much harder to notice a deficiency.

\subsection{Log lines covered}

\label{log-lines-covered}

Parsing a log line is a three stage process:

\begin{enumerate}

    \item Check if there are any rules for the program which produced the
        log line; if not then skip the line.

    \item Try each rule until a matching rule is found.

    \item Execute the action specified by the rule.

\end{enumerate}

Full coverage of log lines requires the following:

\begin{enumerate}

    \item Each program of interest must have at least one rule or its log
        lines will be silently skipped; in the extreme case of zero rules
        the parser would happily skip every log line.  There may be any
        number of log lines from other programs intermingled in the log
        file, and there are some Postfix programs which don't produce any
        log lines of interest.

    \item There must be a rule to match each different line produced by
        each program; if a line is not successfully matched the parser will
        issue a warning.  Rules should be as specific and tightly bound as
        possible to ensure accurate parsing:\footnote{A rule which matches
        zero or more of any character will successfully parse every log
        line, but not in a meaningful way.} most log lines contain fixed
        strings and have a rigid pattern, so this is not a problem.

    \item The appropriate action to take --- discussed in
        \sectionref{mails-covered}.

\end{enumerate}

Full coverage of log lines is easy to achieve yet hard to maintain.  It is
easy to achieve full coverage for a limited set of log files (at the time
of writing the parser has \numberOFrules{} rules, fully parsing
\numberOFlogFILES{} contiguous log files from Postfix 2.2 and 2.3), and new
rules are easy to add.  Maintaining full coverage is hard because other
servers have different restrictions with custom messages, \RBL{} messages
change over time, major releases of Postfix change warning messages
(usually adding more information), etc.,\ so over time the log lines drift
and change.  Graph~\refwithpage{rule hits graph} shows the number of hits
for each rule over all \numberOFlogFILES{} log files; it's obvious that a
small number of rules match the vast majority of the lines, and more than
half the rules match fewer than 100 times.

Warnings are issued for any lines which are not parsed; no warnings are
issued for unparsed lines while testing with the \numberOFlogFILES{} test
log files, so it can be safely concluded that there are zero false
negatives.  False positives are harder to quantify: short of examining each
of the 60,721,709 log lines and determining which \regex{} parsed it, there
is no way to be sure that every line was parsed by the correct \regex{},
making it impossible to quantify the false positive rate; however a random
sample of 6039 log lines was parsed and the results checked manually to
ensure that the correct \regex{} parsed each line.  The sample was
generated by running the following command:

\verb!    perl -e 'print if (rand 1 < 0.0001)' -n LOG_FILES!

\noindent{}to randomly extract roughly one line in every 10,000.  Although
on initial appearances exercising only 36 rules (from a total of
\numberOFrules{}) when parsing 6039 log lines seems quite low, after
examining graph~\refwithpage{rule hits graph} it becomes apparent that such
a low hit rate is to be expected; the reader should also bear in mind that
even when parsing all \numberOFlogFILES{} log files not all the rules are
exercised (some of the rules are for parsing log lines which only appear in
other log files).

\subsection{Mails covered}

\label{mails-covered}

Coverage of mails is much more difficult to determine accurately than
coverage of lines.  The parser can dump its state tables in a human
readable form; examining these tables with reference to the log files is
the best way to detect mails which were not handled properly (many of the
complications discussed in \sectionref{additional complications} were
detected in this way).  The parser issues warnings when it detects any
errors, some of which may alert the user to a problem, e.g.\ when a queueid
is reused before the previous mail is fully dealt with, when a queueid or
\pid{} is not found,\footnote{There will often be warnings about a missing
queueid or \pid{} in the first few hundred or thousand log lines because
the earlier log lines for those connections or mails are in the previous
log file; loading the saved state from the previous log file will solve
this problem.} or when there are problems tracking a child mail (see
\sectionref{tracking re-injected mail}).  There should be few or no
warnings when parsing, and when finished parsing the state table should
only contain entries for mails which had yet to be delivered when the log
files ended, or started before the log files began.

At the time of writing the parser is being tested with \numberOFlogFILES{}
log files.  There are 5 warnings produced, but because the parser errs on
the side of producing more warnings rather than fewer those 5 warnings
represent 3 instances of 1 problem: 3 connections started before the first
log file, so their initial log entries are missing, leading to warnings
when their log lines are parsed.

The state tables contain entries for mails not yet delivered when the
parser finishes execution.  Ideally all they should contain are mails which
are awaiting delivery after the period covered by the logs, though they may
also contain mails whose initial entries are not contained in the logs.
Any other entries are evidence of a failure in parsing or an aberration in
the logs.  After parsing the \numberOFlogFILES{} test log files the state
tables contain 18 entries, breaking down into:

\begin{itemize}

    \item 1 connection which started only seconds before the logs ended and
        had not yet completed.

    \item 1 mail which had been accepted only seconds before the logs ended
        and had not yet been delivered.

    \item 9 mails whose initial log entries were not present in the logs.

    \item 7 mails which had yet to be delivered due to repeated failures.

\end{itemize}

There are no mails in the state tables which should not be present, thus it
can be concluded that there are zero false negatives.  Once again,
determining the false positive rate is much harder, as manually checking
the results of parsing 13,850,793 connections and mails accepted, rejected,
bounced or delivered is infeasible.  There is considerable circumstantial
evidence that the false positive rate is quite low:

\begin{itemize}

    \item The parser is quite verbose when complaining about known problems
        (e.g.\ if a mail is tracked twice as described in
        \sectionref{tracking re-injected mail}), and no such warnings are
        produced during the test runs.

    \item Queueids and \pids{} naturally group together log lines belonging
        to one mail or connection, respectively; it is extremely unlikely
        that a log line would be associated with the wrong connection.

    \item When dealing with the complications described in
        \sectionref{complications} and \sectionref{additional
        complications} the solutions are as specific and restrictive as
        possible, with the goal of minimising the number of false
        positives.  In addition the solution to the \textit{Out of order
        log files\/} problem described in \sectionref{out of order log
        lines} imposes conditions which each reassembled mail must comply
        with to be acceptable.

    \item Every effort has been made while developing to make the parser as
        precise, demanding and particular as possible.

\end{itemize}

Whereas verifying by inspection that the parser correctly deals with all
60,721,709 lines in the test logs is infeasible, verifying a subset of
those logs is a tractable, if extremely time consuming, task.  A sample of
logs was obtained by randomly selecting a log file:

\verb!    perl -Mstrict -Mwarnings -MList::Util=shuffle \!\newline
\verb!            -e 'print [shuffle(@ARGV)]->[0];'!

The first 6000 lines of this log file (roughly 0.01\% of the total) was
extracted:

\verb!    sed -n -e '1,6000p' logfile > test-log-segment!

It is important that the log lines used are contiguous so that all log
entries are present for as many of the connections/mails as possible.  This
log segment was parsed with all debugging options enabled, resulting in
167,448 lines of output (27.908 lines of output per line of input).  All
167,448 lines were examined in conjunction with the input log and a dump of
the resulting database, verifying that for each of the input lines the
parser used the correct rule, executed the correct action, which in turn
produced the correct result, and inserted the correct data in the database.
The log segment produced 4 warnings, 10 mails remaining in the state
tables, and 1625 connections entered in the database.

Given the circumstantial and experimental evidence detailed above, the
author is confident that the false positive rate when reconstructing a mail
is exceedingly low, if not approaching zero.

\subsection{Conclusion}

Parser coverage is divided into two topics in this section: log lines
covered, and mails covered.  The former is initially more important, as the
parser must successfully parse every line if it is to be complete, but
subsequently the latter takes precedence because reproducing the path a
mail takes through Postfix is the aim of the parser.  Increasing the
percentage of log lines parsed is relatively simple and not intrusive:
adding new rules or modifying existing rules is simplified by the
separation of rules and algorithm.  Improving the logical coverage is
harder, as the actions taken by Postfix must be reconstructed, and the new
sequence of actions integrated into the existing model without breaking
the existing sequences.  Detecting a deficiency in the parsing algorithm is
also significantly harder than detecting unparsed log lines, as the parser
will warn about any unparsed line, whereas discovering a flaw in the
parser requires understanding of the warnings produced and the mails
remaining in the state table.  Rectifying a flaw in the parser requires an
understanding of both the parser and Postfix's logs, and investigative work
to determine the cause of the deficiency, followed by further examination
of the logs in developing a solution.

\section{Limitations and possible improvements}

\label{limitations-improvements}

\subsection{Introduction}

Every piece of software suffers from some limitations and there is almost
always room for improvement.

\subsection{Limitations}

\begin{enumerate}

    \item Each new Postfix release requires new rules to be written to cope
        with the new log lines.  Similarly using a new \RBL{}, new policy
        server or new administrator defined rejection messages require new
        rules.

    \item It appears that the hostname used in the HELO command is not
        logged if the mail is accepted.\footnote{Tested with Postfix 2.2.10
        and 2.3.11; this may possibly have changed in Postfix 2.4.}
        Rectifying this has already been described in \sectionref{logging
        helo}.

    \item The algorithm does not distinguish between mails where one or
        more mails are rejected and a subsequent mail is accepted; it will
        appear in the database as one mail with lots of rejections followed
        by acceptance (this has already been mentioned in
        \sectionref{connection reuse}).  I don't believe it's possible to
        make this distinction given the data Postfix logs, though it might
        be possible to write a policy server to provide additional logging.

    \item The program will not detect parsing the same log file twice,
        resulting in the database containing duplicate entries.

    \item The parser does not distinguish between logs produced by
        different sources when parsing; all results will be saved to the
        same database.  This may be viewed as an advantage, as logs from
        different sources can be combined in the same database, or it may
        be viewed as a limitation as there is no facility to distinguish
        between logs from different sources in the same database.  If the
        results of parsing logs from different sources must remain
        separate, the parser can easily be instructed to use a different
        database to store the results in.

\end{enumerate}

\subsection{Possible improvements}

\begin{itemize}

    \item Write the policy server referred to in limitation 3 above.

\end{itemize}

\subsection{Conclusion}

This section has covered the limitations of the parser and possible
improvements which may be implemented in the future.

\newpage
\section{Conclusion}

\label{conclusion}

Parsing Postfix logs appears at first sight to be an almost trivial task,
especially given previous experience in parsing logs, but it turns out to
be a much more taxing project than initially expected.  The variety and
breadth of log lines produced by Postfix is quite surprising, as a quick
survey of sample log files gives the impression that the number of distinct
log lines is quite small; this impression is due to the uneven distribution
exhibited by logs produced in normal operation (see graph~\refwithpage{rule
hits graph} for a vivid illustration of this).  


Given the diverse nature of the log lines and the ease with which
administrators can cause new lines to be logged (\sectionref{postfix
background}), enabling users to easily extend the parser to deal with new
log lines is a design imperative (\sectionref{parser design}).  Despite the
resulting initial increase in complexity the job is quite tractable, though
it does raise interesting efficiency and optimisation questions (answered
in \sectionref{rule efficiency}).  Providing a tool (\sectionref{creating
new rules}) to ease the generation of \regexes{} from unparsed log lines
should greatly help users add rules to parse formerly unparsed lines.


The implementation not only substantially eases the parsing of new log
lines, it makes adding new actions (\sectionref{parsing-algorithm}) an
almost trivial task.  The simplicity of adding a new action
(\sectionref{adding new actions}) frees the implementor from worrying about
how their action might disrupt parsing of other lines or the behaviour of
other actions, allowing their concentration to be focused on correctly
implementing their action.


The division of the parser into rules and actions is unusual because it
divides the two so completely; although parsers are often divided (lex and
yacc~\cite{lex-and-yacc} based parsers being an obvious example), the parts
are usually quite internally interdependent, and will be combined into a
complete parser by the compilation process; in contrast \parsername{} keeps
the rules and actions separate until the parser runs.  XXX EXTEND THIS\@;
SOMETHING ABOUT THE CONTROL FLOW BEING DIFFERENT, I.E. RULE DETERMINES
ACTION, RATHER THAN ACTION EXPECTING INPUT\@.


The emergent behaviour~\cite{Wikipedia-Emergence} exhibited by the rules
and actions is also interesting, and is discussed beside the flow chart
depicting said emergent behaviour (\sectionref{flow-chart}).  This emergent
behaviour greatly eases 


The real difficulties arise once the parser is successfully dealing with
90\% of the log lines, as the irregularities and complications previously
explained only begin to become apparent once the vast majority of log lines
have been parsed successfully.  Dealing with numerous, infrequently
occurring log lines is a simple task, albeit tiresome, whereas dealing with
mails where information appears to be missing is much more grueling.
Trawling through the logs, looking for something out of the ordinary,
possibly hundreds or even thousands of lines away from the last mention of
the queueid in question, is extremely time consuming and error prone.  In
many cases the task is not to spot the line which is unusual, but to spot
that a line normally present is missing.  In all cases the evidence must be
used to construct a hypothesis to explain the irregularities, and that
hypothesis must then be tested in the parser; if successful the parser must
be modified to deal with the irregularities, without adversely affecting
the existing functionality.  The complications described in
\sectionref{additional complications} were solved in the order they are
described in, and that order closely resembles the frequency with which
they occur; the most frequently occurring complications dominate the
warning messages produced, and so naturally they are the first
complications to be dealt with.

\appendix


\section{Other Postfix log parsers reviewed}

\label{other-parsers}

\subsection{Introduction}

It is important to compare and contrast newly developed programs,
algorithms and parsers against those already available, to accurately judge
what, if any, improvements are delivered by the newcomers.  There are not
that many previously developed Postfix log parsers, indeed it was quite
difficult to find ten parsers to review for this project, and the
functionality offered ranges from quite basic to much more mature,
depending on the needs of the creator.  The programs are not reviewed from
an independent viewpoint; the objective is to compare and contrast each
program with \parsername{}.  One important difference between \parsername{}
and the parsers reviewed is that only \parsername{} makes it possible to
parse new log lines without modifying the core parsing algorithm; see
\sectionref{why separate rules and algorithm} for a discussion of why the
separation of rules from the algorithm is beneficial.


\subsection{Parsers reviewed}

\begin{description}

    \item [Pflogsumm] \textit{pflogsumm is designed to provide an over-view
        of Postfix activity, with just enough detail to give the
        administrator a ``heads up'' for potential trouble spots.\/}

        Pflogsumm produces a report designed for troubleshooting, rather
        than for in-depth analysis.  It does not support saving any data,
        nor does it extract more data than is required for producing the
        report.  Both the parsing and reporting are difficult to extend as
        it is a specialised tool, unlike the easily extensible design of
        \parsername{}.  It does not attempt to correlate log lines by
        queueid or \pid{}, nor does it need to deal with the complications
        encountered during this project.  Pflogsumm produces a useful
        report, and successfully dealt with all the Postfix logs tested
        with, but is not a suitable base for this project.

        \url{http://jimsun.linxnet.com/postfix_contrib.html} \newline (Last
        checked 2007/08/13.)

    \item [Sawmill Universal Log File Analysis and Reporting] \textit{
        Sawmill is a \newline Postfix log analyzer (it also support 686
        other log formats).  It can process log files in Postfix format,
        and generate dynamic statistics from them, analyzing and reporting
        events.  Sawmill can parse Postfix logs, import them into a SQL
        database (or its own built-in database), aggregate them, and
        generate dynamically filtered reports, all through a web interface.
        Sawmill can perform Postfix analysis on any platform, including
        Window, Linux, FreeBSD, OpenBSD, Mac OS, Solaris, other UNIX, and
        more.\/}

        Sawmill is a general purpose commercial product which parses 687
        log file formats (correct as of 2007/12/04) and produces reports.
        Its data extraction facilities are quite limited, though it does
        extract three different sets of data for Postfix (one is beta as of
        2007/12/04), but they do not appear to be interlinked, nor does it
        save sufficient data for the purposes of this project.  No attempt
        is made to correlate log lines or deal with the difficulties
        documented in \sectionref{complications} \sectionref{additional
        complications}.\footnote{If any attempt is made there is no
        reference to it in the documentation available on the website.} The
        source code is available in an obfuscated form only (presumably for
        a fee), and the product is quite expensive, as it requires a
        license per report which is to be generated; in contrast
        \parsername{} is free and the code is freely available.  The web
        interface allows creation of dynamic reports based on any field,
        but due to the associated cost the author has not experimented with
        it.  Presumably as the company charges per report the user wishes
        to generate the data store (if there is one) is deliberately
        inaccessible and undocumented to prevent the user bypassing the
        program and generating their own reports.

        \url{http://www.thesawmill.co.uk/formats/postfix.html} \newline
        Fields extracted: from, to, server, UID, relay, status, number of
        recipients, origin hostname, origin \IP{} and virus.  The fields
        \textit{server}, \textit{uid\/} and \textit{virus\/} are not
        explained in their documentation: \textit{server\/} is probably the
        server the mail is delivered to, and \textit{uid\/} might be the
        uid of the user submitting mail locally.  Postfix does not perform
        any form of virus checking (though it has many options for
        cooperating with an external virus scanner), so the
        \textit{virus\/} field is a mystery.

        \url{http://www.thesawmill.co.uk/formats/postfix_ii.html} \newline
        Fields extracted: from, to, \RBL{}, client hostname and client
        \IP{}\@.

        \url{http://www.thesawmill.co.uk/formats/beta_postfix.html}
        \newline Fields extracted: from, to, client hostname, client \IP{},
        relay hostname, relay \IP{}, status, response code, \RBL{} and
        message id.

        Even if the three data sets were linked together Sawmill would
        extract less data than \parsername{}, and it does not appear to
        extract data about rejections except when the rejection is caused
        by an \RBL{} check.

        (Last checked 2007/12/04.)

    \item [Splunk] \textit{Splunk is an IT Search engine. It is software
        that indexes any format of IT data from any source in real time,
        including logs, configurations, scripts, code, messages, traps,
        alerts, activity reports, stack traces and metrics from all of your
        applications, servers and devices. Splunk lets you search,
        navigate, alert and report on all your IT data in real time using
        an AJAX web interface. You can also share knowledge and Splunk
        solutions with other members of the Splunk community via
        SplunkBase.\/}

        Splunk aims to index all an organisation's logs, providing a
        centralised view capable of searching and correlating diverse log
        sources.  The web interface supports complicated searches,
        providing statistics and graphs in real time, a facility not
        provided by \parsername{} (report generation has been deferred to a
        subsequent program).  Saved searches\footnote{The author was unable
        to save searches, though that may have been due to limitations in
        the free version.} can be run periodically and the results emailed
        to a recipient or sent to a shell script, which presumably can
        publish the results as required (though possibly without the graphs
        and detailed statistics); \parsername{} provides the database and
        leaves it to the user to utilise it in any way, whenever they want,
        with no restrictions on usage of the data.  The interface is
        optimised for interactive rather than automated queries and it does
        not appear to be possible to write independent tools to utilise the
        Splunk database.  Many types of reports are available, though most
        are variations of a bar or pie chart, with the exception of bubble
        and heatmap graphs.  It is very easy to drill down through the
        graphs to extract a portion of the data (e.g.\ select the hour with
        the largest number of events, then select a particular host, and
        finally a specific address), though it is not possible to search on
        partial words.  The searches are quite fast, though as the free
        version has a limit on the amount of data indexed per day the
        volume of data to be searched was merely 45MB\@; the cheapest
        licensed version costs \$6000, and still limits the volume of data
        indexed per day.

        In the specific case of parsing Postfix logs, Splunk extracts some
        standard fields: to and from addresses, HELO hostname, date, host
        the logs were collected from, and protocol.  It parses the standard
        syslog fields at the beginning of the line, and extracts any
        \texttt{key=<value>} pairs occurring after the standard syslog
        prologue; these pairs are the fields listed above.  Searches can be
        based on the extracted fields, and all text in the line is also
        available for searching.  \parsername{} extracts noticeably more
        data, though it does not make the full text of the line available,
        and the full power of \SQL{} is available when searching, allowing
        the user to search on arbitrarily complicated conditions.

        Splunk is a generic tool, so it lacks any Postfix specific support
        over and above extracting the \texttt{key=<value>} fields; most
        importantly it makes no attempt to correlate log lines by queueid
        or \pid{}, nor to handle any of the myriad complications discussed
        in this document.

        \url{http://www.splunk.com/} \newline (Last checked 2007/09/25.)

    \item [Isoqlog] \textit{Isoqlog is an MTA log analysis program written
        in C. It designed to scan qmail, postfix, sendmail and exim logfile
        and produce usage statistics in \HTML{} format for viewing through
        a browser. It produces Top domains output according to Sender,
        Receiver, Total mails and bytes; it keeps your main domain mail
        statistics with regard to Days Top Domain, Top Users values for per
        day, per month and years.\/}

        Isoqlog produces a report listing the number of mails sent by each
        unique sender address, and separately the total number of bytes
        transferred; both reports are produced for daily, monthly and
        annual time spans, but only for the domains listed in its
        configuration file (making it impossible to produce reports for
        every sender domain).  It appears to ignore all log lines except
        for those for the current day, though it does maintain a record of
        data previously extracted, which the newly extracted data is merged
        into (no information is provided on the format of the data store).
        The data extracted appears to be limited to the number of mails
        sent by each sender, unlike the copious amounts of data extracted
        by \parsername{}.  It doesn't utilise rejection log lines in any
        way, so is unsuitable for the purposes of this project.  Its
        parsing is completely inextensible, indeed is almost
        incomprehensible, relying on \texttt{scanf(3)}, fixed offsets and
        low level string manipulation; it is the opposite end of the
        spectrum to \parsernames{} parsing.  It doesn't handle any of the
        complications discussed in this document, doesn't gather the
        breadth of data required for this project, and ignores the majority
        of log lines produced by Postfix.

        \url{http://www.enderunix.org/isoqlog/} \newline (Last checked
        2007/08/13.)

    \item [AWStats] \textit{AWStats is a free powerful and featureful tool
        that generates advanced web, streaming, ftp or mail server
        statistics, graphically. This log analyzer works as a CGI or from
        command line and shows you all possible information your log
        contains, in few graphical web pages. It uses a partial information
        file to be able to process large log files, often and quickly. It
        can analyze log files from all major server tools like Apache log
        files (NCSA combined/XLF/ELF log format or common/CLF log format),
        WebStar, IIS (W3C log format) and a lot of other web, proxy, wap,
        streaming servers, mail servers and some ftp servers.\/}

        AWStats will produce simple graphs for many different services, but
        supporting many different services without special purpose code
        restricts it to supporting the \LCD{}.  The data it will extract
        from an \MTA{} log file is limited in comparison to \parsername{}:
        \newline \tab{} time2, email, email\_r, host, host\_r, method, url,
        code and bytesd.\newline  There does not seem to be an explanation
        of any of those fields in the documentation (\parsername{} provides
        copious documentation).  AWStats has no special purpose code to
        deal with the intricacies of Postfix logs, in fact it operates by
        coercing Postfix logs into Apache\footnote{The Apache web server is
        the most popular HTTP server in use for the past 10 years; more
        information is available at \url{http://httpd.apache.org/}.} format
        log files, for analysis by AWStats' HTTP log file parser.  The
        converting parser only deals with a small portion of the log lines
        generated by Postfix, silently skipping those it cannot deal with,
        and does not distinguish between different types of rejection;
        there is no easy way to extend it to handle new log lines.
        Although though it does correlate log lines by queueid, it does not
        deal with any of the other complications described in this
        document.

        \url{http://awstats.sourceforge.net/} \newline
        \url{http://awstats.sourceforge.net/awstats.mail.html} \newline
        \url{http://awstats.sourceforge.net/docs/awstats_faq.html#MAIL}
        \newline (Last checked 2007/10/13.)

    \item [Log analyser --- throughput monitor] This utility tracks the
        number of events which occurred over a particular time and warns if
        the frequency of events passes a certain threshold.  It's designed
        to provide real time alerts when dictionary attacks, mail loops or
        similar problems occur. It doesn't attempt to extract or save data,
        correlate log lines, or any of the more advanced tasks described in
        this document, because it is not designed to do so.  The user must
        construct the \regexes{} to match significant input lines before it
        will parse any logs.  This program was not reviewed in further
        detail because its aims are so far removed from the aims of this
        project.

        \url{http://home.uninet.ee/~ragnar/throughput_monitor/} \newline
        (Last checked 2007/08/13.)

    \item [Anteater] \textit{The Anteater project is a Mail Traffic
        Analyser. Anteater supports currently the logformat produced by
        Sendmail and by Postfix. The tool is written in 100\% C++ and is
        very easy to customize. Input, output, and the analysis are modular
        class objects with a clear interface. There are eight useful
        analyse modules, writing the result in plain ASCII or \HTML{}, to
        stdout or to files.\/}

        Anteater doesn't have any English documentation so it's difficult,
        nigh impossible, for this author to accurately comment on what
        analysis it performs.  It did not run successfully when tested, and
        its parsing would certainly be out of date as Postfix has evolved
        considerably since this tool was last updated (November 2003).  As
        it neither ran successfully nor has documentation the author can
        read a detailed review cannot be provided.

        \url{http://anteater.drzoom.ch/} \newline (Last checked
        2007/08/13.)

    \item [Yet Another Advanced Logfile Analyser] \textit{yaala is a very
        flexible analyser for all kinds of logfiles. It uses parsers to
        extract information from a logfile, an SQL-like query language to
        relate the information to each other and an output-module to format
        the information appropriately.\/}

        YAALA uses a plugin based system to analyse log files and produce
        \HTML{} output reports, with all the parsing and report generation
        handled by modules.  Using YAALA as a base would be only slightly
        less work than starting from scratch, as both input and output
        modules would need to be written specially; it may even be more
        work to implement the parser within the constraints of YAALA\@.
        YAALA supports storing previously gathered data using Perl's
        Storable module~\cite{perl-storable}, so with enough knowledge of
        the data structure YAALA stores it should be possible to extract
        data with another Perl program also using the Storable module;
        \parsername{} uses a well documented database which is accessible
        from the majority of programming languages.

        YAALA provides a Postfix parser which extracts the following fields
        from specific log lines:

        \tab{}Aggregations: count (not explained), bytes (sum of bytes
        transferred).\newline \tab{}Keyfields [sic]: date, hour, sender,
        recipient, defer\_count, delay, incoming\_host, outgoing\_host.

        YAALA extracts most of the fields \parsername{} does, but it does
        not maintain separate counters for each restriction like
        \parsername{}, so its data cannot be used for optimisation, testing
        or understanding of restrictions.  YAALA's Postfix parser does not
        deal with the complications explained in this document, though it
        does correlate log lines by queueid.

        YAALA provides a mini-language based on \SQL{} that is used when
        generating reports; sample reports can be seen
        at~\cite{yaala-samples}.  Example query: \newline \tab{}
        \texttt{requests BY file WHERE host =\~{} Google} \newline The
        mini-language is quite limited and cannot be used to extract data
        for external use, merely to create reports.

        In summary YAALA provides a Postfix parser which only handles the
        most common Postfix log lines, provides reasonably flexible report
        generation from the limited data extracted, but has no facilities
        to extract data for use in other tools.

        \url{http://yaala.org/} \newline (Last checked 2007/10/09.)

    \item [Logparser/Lire] \textit{As any good system administrator knows,
        there's a lot more to keep track of in an active network than just
        webservers. Lire is hands down the most versatile log analysis
        software available today. Lire not only keeps you informed about
        your HTTP, FTP, and mail traffic, it also reports on your
        firewalls, your print servers, and your DNS activity. The ever
        growing list of Lire-supported services clearly outstrips any other
        software, in large part thanks to the numerous volunteers who have
        pioneered many new services and features. Lire is a total solution
        for your log analysis needs.\/}

        Lire is a general purpose log parser supporting many different
        types of log file.  Its Postfix parser extracts the following data
        from Postfix logs: \textit{The email servers' reports will show you
        the number of deliveries and the volume of email delivered by day,
        the domains from which you receive or send the most emails, the
        relays most used, etc.\/}; notably rejections are not mentioned or
        dealt with, and unlike \parsername{} there is no facility to extend
        the parser.  It supports multiple output formats for generated
        reports (text, \HTML{}, \PDF{} and Excel 95) but the reports do not
        appear to be customisable; \parsername{} does not produce any
        reports.  Lire supports saving extracted data for later report
        generation, but accessing this data from another application is
        undocumented; given the source code it should be possible, with
        sufficient time and effort, to access the data from an external
        program.

        Like AWStats and Logrep, Lire attempts to correlate log lines by
        queueid, but not by \pid{}, so the complete list of recipients for
        a mail should be available; however its parser extracts only part
        of the available data and makes no attempt to deal with the other
        complications described in \sectionref{complications}
        \sectionref{additional complications}.  \parsername{} uses an
        \SQL{} database to make accessing the extracted data as easy as
        possible.

        \url{http://logreport.org/lire.html} \newline (Last checked
        2007/10/09.)

    \item [Logrep] \textit{Logrep is a secure multi-platform framework for
        the collection, extraction, and presentation of information from
        various log files. It features HTML reports, multi dimensional
        analysis, overview pages, SSH communication, and graphs, and
        supports over 30 popular systems including Snort, Squid, Postfix,
        Apache, Sendmail, syslog, ipchains, iptables, NT event logs,
        Firewall-1, wtmp, xferlog, Oracle listener and Pix.\/}

        Logrep extracts roughly half the fields \parsername{} does:

        \begin{itemize}

            \item For mail sent and received: from address, size, and time
                and date.

            \item For mail sent: to addresses, \SMTP{} code, and delay.

            \item For mail received: the hostname of the sender.

        \end{itemize}

        It also counts the number of log lines parsed and skipped.  Log
        lines are correlated based on the queueid (referred to as
        sessionname [sic] within Logrep), but not by \pid{}.  The parsing
        is error prone: empty fields are saved when the log line doesn't
        match the \regex{}, though it appears that they will not overwrite
        existing data.  Most notably rejections are completely ignored,
        making it unsuitable for the purposes of this project.  It doesn't
        attempt to address any of the complications in
        \sectionref{complications} \sectionref{additional complications}
        except for correlating by queueid.

        Logrep does not come with any documentation, though some scant
        documentation is available on its website (\parsername{} provides
        copious documentation).  It requires a web browser to interact with
        it, ensuring that automated log processing will be difficult,
        whereas automated processing is a key part of \parsernames{}
        design.  Sadly all the author's attempts to use Logrep failed, as
        it was unable to access the log files selected; this appears to be
        a bug rather than operator error.\footnote{If it is caused by
        operator error, the interface needs improvement as the (minimal)
        instructions were followed as closely as possible, and multiple
        attempts were made.}  As parsing failed it wasn't possible to
        review the reports Logrep can generate (available in \HTML{} only),
        nor to examine the (undocumented) format in which it can save
        extracted data for subsequent reuse.

        Logrep extracts far less data from Postfix logs than \parsername{},
        completely ignores rejections, is effectively undocumented, doesn't
        deal with the more complicated aspects of Postfix logs, and at the
        time of writing doesn't work properly.

        \url{http://www.itefix.no/phpws/index.php} \newline (Last checked
        2007/11/18.)

    \item [Log Mail Analyser] Please see the previous in-depth discussion
        of Log Mail Analyser in \sectionref{prior art}.

\end{description}

\subsection{Conclusion}

While there are other programs available which perform basic Postfix log
parsing (some to a greater level of detail than others), few attempt to
correlate log lines by queueid (none correlate by \pid{}) to produce an
overall record of the journey each mail traverses through Postfix.  None of
the reviewed parsers collect the breadth of information gathered by
\parsername{}, nor make it as easy to extend the parser to handle new log
lines.  Most other parsers immediately generate a report and discard the
data extracted from the logs; those which don't discard the data retain it
in a format inaccessible to other tools.  All of the parsers reviewed can
produce a report of greater or lesser detail and complexity, a facility not
offered by \parsername{}; reporting is deferred to a subsequent
application.

The overriding difference between \parsername{} and the other parsers
reviewed herein is that none of them aim to achieve the high level of
understanding of Postfix logs achieved by \parsername{}.


\bibliographystyle{logparser-bibliography-style}
\bibliography{logparser-bibliography}
\label{bibliography}

\section{Graphs}

\label{graphs}

\renewcommand{\figurename}{Graph}

\subsection{Introduction}

Graphs are an excellent means of displaying data, transforming a
meaningless stream of numbers into an easily comprehensible form, where
anomalies and patterns are immediately obvious.  These graphs are used to
illustrate the topics discussed in \sectionref{rule efficiency}.  The
graphs in the first section cover parser scalability, demonstrating that
performance scales linearly with input size.  The impact of rule ordering
is shown in \sectionref{rule ordering graphs}, and the anomalous dips and
peaks apparent in some graphs are explained.  The third group of graphs
vividly shows the huge impact that caching compiled \regexes{} has on
parser performance.  This section concludes with a breakdown of the rule
hits accumulated during a single test run of the parser.

\subsection{Parser scalability}

\showgraph{build/plot-normal-filesize-numlines}{Execution time vs file
size vs number of lines}{execution time vs file size vs number of lines
graph}

The Y axis in graph~\refwithpage{execution time vs file size vs number of
lines graph} represents the following:

\begin{enumerate}

    \item The time required, in seconds, to parse the log file.

    \item The size of the log file, in megabytes.

    \item The number of lines in the log file, divided by 10000.

\end{enumerate}

Graph~\refwithpage{execution time vs file size vs number lines factor}
shows the ratio of execution time vs file size and number of lines (higher
is better, it means more bytes or lines processed per second).  The ratios
are quite tightly banded with the exception of log files 22 and 62--68,
where they are noticeably higher; graph~\refwithpage{execution time vs file
size vs number lines factor} and table~\refwithpage{execution time vs file
size vs number lines factor table} show that the parser's execution time
scales linearly with input size.

\showgraph{build/plot-normal-filesize-numlines-factor}{Ratio of file
size and number of lines to execution time}{execution time vs file size vs
number lines factor}

\showtable{build/stats-normal-filesize-line-count-include}{Ratio of file
size \& number of lines to execution time: statistics}{execution time vs
file size vs number lines factor table}

\clearpage

\subsection{Rule ordering}

\label{rule ordering graphs}

\showgraph{build/plot-normal-shuffle-factor}{Percentage increase of
shuffled over normal}{percentage increase of shuffled over normal}

\showgraph{build/plot-normal-reverse-factor}{Percentage increase of
reversed over normal}{percentage increase of reversed over normal}

\subsubsection{Why are there dips in the graphs?}
\label{Why are there dips in the graphs?}

The dips at log files 22 and 62--68 correspond to peaks in log file size in
graph~\refwithpage{execution time vs file size vs number of lines graph},
and peaks in graph~\refwithpage{execution time vs file size vs number lines
factor} (where a peak means that more lines are processed per second, i.e.\
performance is better).  The explanation for this took some time to arrive
at, but it turns out to be reasonably simple.  The large log files in
question were caused by a mail forwarding loop, where the distribution of
log lines is quite different to normal, resulting in different performance
characteristics.

The mail loop was set up by a user modifying his mail forwarding to:
\newline \tab{}\texttt{$\backslash$username, username@domain} \newline This
instructs Postfix to deliver the mail to the local user, and also forward
it to the remote address; this is generally not a problem except that the
remote address in this case is the user's address, creating an infinite
loop.  To prevent this happening Postfix examines the Delivered-To header
in the mail, and if the mail has already been delivered to the current
address it is bounced back to the sender with the error message
\texttt{mail forwarding loop for username@domain}.  Ordinarily this works
well, but unfortunately in this case the user noticed they had not received
any mail in a while and opted to send a test mail to themselves, causing a
loop not caught by Postfix as described below.

\begin{enumerate}

    \item Postfix accepts a mail from username@domain, for username@domain.

    \item Postfix delivers the mail to the local mailbox and
        username@domain, as instructed by the user's forwarding
        instructions. The forwarded mail has a
        \texttt{Delivered-To:~username@domain} header added, and the
        envelope sender address is username@domain.  Log lines are added by
        \daemon{local}, \daemon{qmgr} (twice), \daemon{cleanup} and finally
        \daemon{pickup}.

    \item Postfix accepts the mail for username@domain, but while
        delivering it notices that the \texttt{Delivered-To} header already
        contains the address it's currently delivering to, and therefore
        sends a bounce notification with sender address \textit{$<>$\/} to
        the original sender: username@domain.  Log lines are added by
        \daemon{local}, \daemon{qmgr} (twice) and \daemon{cleanup}.

    \item Postfix accepts the bounce notification and delivers it to both
        the local mailbox and to username@domain, as instructed by the
        user's forwarding instructions.  A
        \texttt{Delivered-To:~username@domain} header is added to the
        forwarded bounce notification, which now has an envelope sender
        address of username@domain.  Log lines are added by \daemon{local},
        \daemon{qmgr} (twice), \daemon{cleanup}, and finally
        \daemon{pickup}.

    \item Postfix accepts the forwarded bounce notification but while
        delivering the mail it notices that the \texttt{Delivered-To}
        header already contains the address currently being delivered to,
        and sends a bounce notification to the sender: username@domain.
        Log lines are added by \daemon{local}, \daemon{qmgr} (twice) and
        \daemon{cleanup}.

    \item Repeat from step two; this will continue indefinitely unless an
        administrator intervenes and deletes the appropriate mails from the
        queue.

\end{enumerate}

The sequence described above occurs extremely rapidly because Postfix does
not have to deliver the mail to an external system, so mails are delivered,
bounced and generated as fast as the disks can keep up, resulting in a huge
volume of logs.

The vast majority of log lines when a mail loop occurs are from Postfix
components which have a small number of rules associated with them, whereas
in general \daemon{smtpd} adds the majority of log lines, and also has the
highest number of rules.  \daemon{smtpd} log lines are distributed across
rules much more evenly than the log lines of \daemon{qmgr}, \daemon{local},
\daemon{cleanup} or \daemon{pickup}, so the average number of rules
required to parse a \daemon{smtpd} log line is much higher that the average
number required to parse other log lines.

These two characteristics combine to reduce the average number or rules
required to parse a log line when there is a mail loop, as shown by the
peaks in graph~\refwithpage{execution time vs file size vs number lines
factor}.  When the rule ordering is reversed the majority of log lines
generated by a mail loop will be parsed with very few rules, whereas
without a mail loop the majority of log lines require a large number of
rules; this leads to a noticeable drop in the average time required to
parse a log line, as shown in graph~\refwithpage{percentage increase of
reversed over normal}.  The number of rules which need to be consulted when
the ordering is shuffled varies between the optimum and nadir, and the
performance varies proportionally.

The difference between logs with a mail loop and logs without can be seen
in table~\refwithpage{Rule ordering with and without a mail loop} showing
the increases for the different rule orderings and combinations of log
files:

\showtable{build/stats-normal-shuffle-reverse-include}{Rule ordering with
and without a mail loop}{Rule ordering with and without a mail loop}



\subsection{Caching regexes}

The following graphs show the impact that not caching compiled \regexes{}
has on parser performance: on typical log files the execution time when not
caching compiled \regexes{} is 500--600\% of the execution time when
caching; reversing the perspective shows that cached execution time is
merely 17--20\% of non-cached execution time.  Caching compiled \regexes{}
is probably the single most effective optimisation possible in the parser's
implementation.

\showgraph{build/plot-cached-discarded}{Regex: cached vs
discarded}{normal regex vs discard regex}

\showgraph{build/plot-cached-discarded-factor}{Regex caching: percentage
execution time increase}{normal regex vs discarded regex factor}

Two large dips can be seen in graph~\refwithpage{normal regex vs discarded
regex factor} at log files 22 and 62--68, corresponding to the spikes in
log file size in graphs~\refwithpage{execution time vs file size vs number
of lines graph} and~\refwithpage{normal regex vs discard regex}.  The
reason for the anomalous log files has already been explained in
\sectionref{Why are there dips in the graphs?}.

The distribution of log lines across rules when there is a mail loop is
much different and the average number of rules consulted per log line is
much lower; this results in far fewer \regex{} compilations per line than
when there isn't a mail loop, and a correspondingly decreased execution
time.  The increases in execution time when not caching \regexes{} for
different combinations of log files are summarised in the table below:

\showtable{build/stats-cached-discarded-include-for-graph}
{Regex caching/discarding with different groups of log files}
{Regex caching/discarding with different groups of log files}


\subsection{Rule hits}
\label{rule hits}

The number of hits per rule is quite unevenly spread, resembling a Power
Law distribution~\cite{powerlaw}.

\showgraph{build/plot-hits}{Hits per rule}{rule hits graph}

As graph~\refwithpage{rule hits graph} is quite difficult to read it has
been separated into three sections: low, middle and high.

\showgraph{build/plot-hits-low}{Hits per rule (low)}{hits per rule low}

It is apparent from the low graph (graph~\refwithpage{hits per rule low})
that some rules have few or no hits; those with zero hits are rules which
were written to parse log files used during development of the parser but
not utilised in the test runs performed for this document.

\showgraph{build/plot-hits-middle}{Hits per rule (middle)}{hits per rule
middle}

\showgraph{build/plot-hits-high}{Hits per rule (high)}{hits per rule
high}

\clearpage

\subsection{Conclusion}

The graphs presented in this section illustrate the topics discussed in
\sectionref{rule efficiency}.  The first collection of graphs are about
parser scalability, showing the linear relationship between execution time
and input size.  \sectionref{rule ordering graphs} relates to the effect of
rule ordering on execution time, and the unexpected consequences of
specific inputs.  The necessity of caching compiled \regexes{} is attested
to by the third group of graphs, where the difference between caching and
discarding is staggering.  This section concluded with a breakdown of the
rule hits accumulated during a single test run of the parser.


% This fools the syntax highlighting so we get a new fold here, but it
% doesn't create a new section.
\ifacronymfirstuse{SMTP}{
\section{Glossary}
}{}
% Redefine the command used to produce the glossary title, because the
% default command produces an unnumbered section whereas I want a numbered
% section.
\renewcommand{\glossarytitle}{\section{Glossary}\label{Glossary}}
\printglossary{}
% Redefine the command a second time to produce acronyms instead of a
% glossary.
\renewcommand{\glossarytitle}{\section{Acronyms}\label{Acronyms}}
\printacronym{}

% The contents of the glossary.
\glossary{name={SQLite3},description={
    \textit{SQLite is a small C library that implements a self-contained,
    embeddable, zero-configuration SQL database engine.\/} SQLite3 is an
    \SQL{} implementation focusing on correctness, simplicity and speed.
    Unlike other \SQL{} implementations it does not require a separate
    server process, greatly simplifying deployment of an application
    utilising it.  More details can be found at~\cite{sqlite-features} or
    \url{http://www.sqlite.org/}.
}}

\glossary{name={Phishing},description={
    Phishing~\cite{Wikipedia-phishing} is an attempt to acquire information
    by masquerading as an entity trusted by the user, e.g.\ a bank.
}}

\glossary{name={Backscatter},description={
    When a spammer or worm sends mail with forged sender addresses,
    innocent sites are flooded with undeliverable mail notifications; this
    is called backscatter mail.
}}

\glossary{name={Joe job},description={
    A joe job is when spam mail is sent with a faked sender address with
    the intention of sullying the good name of the owner of the address.
    Joe jobs are a cause of backscatter, though by no means the only cause.
}}

\glossary{name={Epoch},description={
    Most operating systems store the current time and timestamps of files
    etc.\ as seconds elapsed since the epoch, the beginning of time as far
    as the operating system is concerned.  On Unix and Unix-derived systems
    the epoch is 1970/01/01 00:00:00; on other operating systems it may be
    different.
}}

\glossary{name={NULL},description={
    NULL is a special term used in \SQL{} databases indicating the absence
    of data for the field.
}}

% Postfix components
\glossary{name={bounce},description={
    The bounce daemon is responsible for sending bounce notifications in
    Postfix versions later than 2.2.  The definitive documentation is
    \url{http://www.postfix.org/bounce.8.html}.
}}

\glossary{name={cleanup},description={
    Cleanup processes all incoming mail, sitting between qmgr and the
    programs which accept mail (smtpd and pickup).  It removes duplicate
    recipient addresses, inserts missing headers, and optionally rewrites
    addresses if configured to do so.  The definitive
    documentation is \url{http://www.postfix.org/cleanup.8.html}.
}}

\glossary{name={lmtp},description={
    Delivery of mail over \LMTP{} is performed by the lmtp component.  The
    definitive documentation is \url{http://www.postfix.org/lmtp.8.html}.
}}

\glossary{name={local},description={
    Local is the Postfix component responsible for local delivery of mail
    (i.e.\ delivered on the server Postfix is running on), whether it be to
    a user's mailbox or a program such as a mailing list manager or
    procmail (\url{http://www.procmail.org/}).  It also handles aliases and
    processing of a user's \texttt{.forward} file.  The definitive
    documentation is \url{http://www.postfix.org/local.8.html}.
}}

\glossary{name={pickup},description={
    Pickup is the service which deals with mail submitted locally via
    postdrop and sendmail; it passes all submitted mail to cleanup.  The
    definitive documentation is \url{http://www.postfix.org/pickup.8.html}.
}}

\glossary{name={postdrop},description={
    Postdrop is used when submitting mail locally on the server: it creates
    a new mail in the queue and copies its input into the mail.  Subsequent
    delivery of the mail is the responsibility of other Postfix components.
    The definitive documentation is
    \url{http://www.postfix.org/postdrop.1.html}.
}}

\glossary{name={postsuper},description={
    Maintenance task such as deleting mails from the queue, putting mail on
    hold (no further delivery attempts will be made until it is released
    from hold, also by postsuper), and consistency checking of the mail
    queue.  The definitive documentation is available at
    \url{http://www.postfix.org/postsuper.1.html}.
}}

\glossary{name={qmgr},description={
    Qmgr is the Postfix daemon which manages the mail queue, determining
    which mails will be delivered next.  Qmgr orders the mails based on the
    recipient for local mails and the destination server for remote
    addresses, ensuring that it balances the aims of achieving maximum
    concurrency while avoiding overwhelming destinations or wasting time
    and resources on non-responsive destinations.  The definitive
    documentation is \url{http://www.postfix.org/qmgr.8.html}.
}}

\glossary{name={sendmail},description={
    Postfix provides a command that is compatible with the Sendmail
    (\url{http://www.sendmail.org/}) mail submission program that all Unix
    commands which send mail depend on; Postfix sendmail executes postdrop
    to place a new mail in the queue.  The definitive documentation is
    \url{http://www.postfix.org/sendmail.1.html}.
}}

\glossary{name={smtp},description={
    Delivery of mail over \SMTP{} is performed by the smtp component.  The
    definitive documentation is \url{http://www.postfix.org/smtp.8.html}.
}}

\glossary{name={smtpd},description={
    Smtpd is the Postfix program which accepts mail via \SMTP{}, and
    implements all the anti-spam restrictions Postfix provides.  The
    definitive documentation is \url{http://www.postfix.org/smtpd.8.html}.
}}

\glossary{name={virtual},description={
    Virtual is the Postfix component responsible for delivery of mails to
    virtual domains.  With \daemon{local} delivery the destination is
    determined only by the portion of the email address on the left side of
    the \at{}, whereas with \daemon{virtual} delivery the destination is
    determined by the entire email address, e.g.\ if the server considers
    itself responsible for both \textbf{example.org} and
    \textbf{example.net} domains: \daemon{local} considers
    \textbf{john\at{}example.org} and \textbf{john\at{}example.net} to have
    the same mailbox, whereas \daemon{virtual} considers them to have
    different mailboxes.  Virtual delivery is used when a server hosts
    multiple domains where a username may be present in more than one
    domain but represent different users in each.  The definitive
    documentation is
    \url{http://www.postfix.org/virtual.8.html}.
}}

\glossary{name={Bayesian spam filtering},description={
    Bayesian spam filtering is a method of classifying mail based on the
    frequency that the words in the mail have previously appeared in a spam
    corpus and a ham (non-spam) corpus.  A full description is beyond the
    scope of this document, see~\cite{bayesian-filtering, a-plan-for-spam}
    for a detailed explanation.
}}

\glossary{name={Bayesian poisoning},description={
    Bayesian poisoning is the addition of innocuous or unrelated words to a
    spam mail in the hope of defeating Bayesian spam filtering.  E.g.\ the
    word Viagra would be firmly in the spam corpus for most people, but by
    adding the words \textit{schedule}, \textit{meeting}, \textit{moving
    forward\/} and \textit{best business practices\/} to a mail received by
    a manager, the Bayesian spam filter might tip the balance from bad to
    good, if the non-spam words outweigh the spam words.
}}

\glossary{name={$<>$},sort={<>},description={
    $<>$ is the sender address used for mail which should not be replied
    to, e.g.\ bounce notifications.  In \SMTP{} all addresses are enclosed
    in $<>$, so \textit{username\at{}domain\/} becomes
    \textit{$<$username\at{}domain$>$\/}; thus $<>$ is actually an empty
    address, but is always written as $<>$ for clarity.  All mail servers
    must accept mail sent from $<>$, or they are in violation of
    \RFC{}~2821~\cite{RFC2821}.
}}

\glossary{name={queueid},description={
    Each mail in Postfix's queue is assigned a queueid to uniquely identify
    it.  Queueids are assigned from a limited pool, so although they are
    guaranteed to be unique for the lifetime of the mail, given sufficient
    time they will be reused.
}}

\glossary{name={IPv4},description={
    Internet Protocol~\cite{Wikipedia-ipv4} version 4 is the fourth version
    of the Internet Protocol used to interconnect computers on the
    Internet.  It is the first widely deployed version of IP, and has been
    in use for over 25 years.
}}

\glossary{name={IPv6},description={
    Internet Protocol~\cite{Wikipedia-ipv6} version 6 is the latest version
    of the Internet Protocol used to interconnect computers on the
    Internet.  It is the successor to IPv4, bringing with it a greatly
    expanded address space, allowing many more computers to use the
    Internet simultaneously.  IPv4 and IPv6 will coexist for many years to
    come as existing networks transition from the former to the latter.
}}

\glossary{name={hash},description={
    A hashing function transforms a string of characters to a number.
    There are many possible uses for the resulting number: a common usage
    is to maintain a data structure indexed by strings in an efficient
    manner.  A full description is beyond the scope of this paper, further
    information can be found at~\cite{hash-functions}.
}}

\glossary{name={Solaris},description={
    Solaris is a Unix-derived Operating System produced by Sun Microsystems
    (\url{http://www.sun.com/software/solaris/}).
}}

\glossary{name={awk},description={AWK is a general purpose programming
    language that is designed for processing text-based data, and is
    available as a standard utility on all Unix systems.
}}

\glossary{name={syslog},description={Syslog is the standard logging
    mechanism on Unix systems: the program sends log messages to syslog,
    then syslog filters and stores the messages according to the
    configuration the administrator has chosen.
}}

\end{document}
