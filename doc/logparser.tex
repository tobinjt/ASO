% $Id$
\documentclass[a4paper,12pt,draft]{article}

% Better typesetting of URLs.
\usepackage{url}
% Include images
\usepackage[final]{graphicx}
% Change how nested enumerate environments are labelled.
\renewcommand{\labelenumii}{\roman{enumii}:}

\begin{document}

\title{Parsing Postfix log files}
\author{John Tobin \\ School of Computer Science and Statistics \\ 
Trinity College \\ Dublin 2 \\ Ireland \\ tobinjt@cs.tcd.ie}
\date{}
\maketitle

\begin{abstract}

    Parsing Postfix logs is much more difficult than it first appears but
    it is possible to achieve a high degree of accuracy in understanding
    the logs and reconstructing the actions taken by Postfix to generate
    the logs.  This paper describes the process required, documenting the
    parsing algorithm and rules, explaining the difficulties encountered,
    with reference to an implementation which stores data gathered from the
    logs in an SQL database.  The gathered data can then be used to
    optimise current anti-spam measures, provide a baseline to test new
    anti-spam measures against, or to produce statistics showing how
    effective those measures are.

\end{abstract}

XXX WHAT IS THE CONCLUSION???  FIRST THING IS TO COMPARE TO OTHER PARSERS
REVIEWED

XXX FIGURE OUT WHEN TO USE MAIL AND WHEN TO USE CONNECTION\@; POSSIBLY STATE
THAT THEY'RE MORE OR LESS EQUIVALENT

\newpage
\tableofcontents

\newpage
\section{Introduction}

\label{introduction}

Most mail server administrators will have performed some basic processing
of Postfix logs at one time or another, whether it was to debug a problem,
explain to a user why their mail is being rejected, or check whether their
new anti-spam measures are working.  The more adventurous will have
generated statistics to show how many hits each of their anti-spam measures
has gotten in the last week, and possibly even generated some graphs to
clearly illustrate the point to management or complaining
users.\footnote{This was the author's first real foray into processing
Postfix logs.}  Very few will have performed in-depth parsing and analysis
of their logs, where the parsing must correlate the log lines
per-connection or per-queueid rather than processing lines independently.
One of the barriers to this type of processing is the unstructured nature
of Postfix logs, where each logging line was added on an ad hoc basis as a
requirement was discovered or new functionality was added.  Further
complication because the list of rejection messages is not fixed: new
messages can be added by the administrator; every Real-time Black List
returns a different explanatory message; policy servers may have different
messages depending on the characteristics of the connection; there are many
ways in which the log lines may differ.  This paper documents the difficult
process of parsing Postfix logs, presenting a program which parses logs and
places the resulting data into a database.  The gathered data can then be
used to optimise current anti-spam measures, provide a baseline to test new
anti-spam measures against, or to produce statistics showing how effective
those measures are.  There are numerous other uses for such data: improving
server performance by identifying troublesome destinations and
reconfiguring appropriately or dedicating transports to those destinations;
identifying high volume uses (e.g.\ customer newsletters) and restricting
those uses to off-peak times; as a base for billing customers on a shared
server; etc.

Section~\ref{background} provides background information about the
following: the motivation for this project; the Postfix Mail Transport
Agent; a short overview of the parser; the assumptions inherent in the
design of the algorithm and program; using a database schema as an API\@;
and a comparison against other Postfix log parsers.

Section~\ref{rules} discusses the parsing rules in detail, explaining their
structure, giving an example rule and sample data it matches successfully
against.

Section~\ref{parsing-algorithm} contains the meat of the paper, describing
a naive parsing algorithm and the complications encountered which shaped
the full algorithm, followed by a comprehensive explanation of the
different stages of the algorithm and the actions taken during the
execution of the algorithm.

Section~\ref{limitations-improvements} lists the limitations of the program
and/or the algorithm, then suggests some ways of dealing with the
limitations, with the goal of improving parsing and reproducing the journey
a mail takes through the internals of Postfix.

Section~\ref{conclusion} describes the results of the project to date and
contains the paper's conclusion.

Section~\ref{references} catalogues resources used in developing the
algorithm, writing the program and preparing this paper.  
Any resources referred to in the paper are also given, plus some additional
resources expected to be helpful in understanding Postfix, anti-spam
techniques, or the paper.

\section{Background}

\label{background}

\subsection{Motivation}

This paper and the program it describes are part of a larger project to
optimise a server's Postfix restrictions, generate statistics and graphs,
and provide a platform on which new restrictions can be trialled and
evaluated to see if they are worth using in the fight against spam.  The
program parses Postfix logs and populates a database with the data gleaned
from those logs, providing a consistent and simple view of the logs which
future tools can utilise.  The gathered data can then be used to optimise
current anti-spam measures, provide a baseline to test new anti-spam
measures against, or to produce statistics showing how effective those
measures are.\footnote{Section~\ref{introduction} lists some more example
uses.}

A short example of the optimisation possible using data from the database
is optimal ordering of Postfix restrictions:

\begin{verbatim}
SELECT name, description, restriction_name, rule_order
    FROM rules
    WHERE postfix_action = 'REJECTED'
    ORDER BY rule_order DESC;
\end{verbatim}

If the database supports sub-selects percentages can be
obtained:

\begin{verbatim}
SELECT name, description, restriction_name, rule_order,
        (rule_order * 100.0 /
            (SELECT SUM(rule_order)
                FROM rules
                WHERE postfix_action = 'REJECTED'
            )
        ) || '%' AS percentage
    FROM rules
    WHERE postfix_action = 'REJECTED'
    ORDER BY rule_order DESC;
\end{verbatim}

SQL note: $||$ is the concatenation operator in SQLite3; if the database
containing the extracted data does not support this syntax, then simply
remove `` $||$ '$\%$'\hspace{1ex}'' from the query --- the results will be
the same, just slightly less visually pleasing.

Another example is determining which restrictions are not effective: this
example shows which rules had less than 100 hits on the last log file
parsed.

\begin{verbatim}
SELECT name, description, restriction_name, rule_order,
        (rule_order * 100.0 /
            (SELECT SUM(rule_order)
                FROM rules
                WHERE postfix_action = 'REJECTED'
            )
        ) || '%' AS percentage
    FROM rules
    WHERE postfix_action = 'REJECTED'
        AND rule_order < 100
    ORDER BY rule_order ASC;
\end{verbatim}

\subsection{Database as Application Programming Interface}

The database populated by this program provides a simple interface to
Postfix logs.  Although the interface is a database schema, it is in effect
quite similar to the API provided by a library: it insulates both user and
provider of the API from changes in the implementation of the other.  The
algorithm implemented by the program can be improved; support can be added
for earlier or later releases of Postfix; bugs can be fixed or limitations
removed from the program.  Statistics and/or graphs can be generated from
the database; new restrictions can be tested and the results inspected;
trends in the fight against spam can emerge from historical data saved in
the database.  Using a database simplifies writing programs which need to
interact with the data in several ways:

\begin{enumerate}

    \item Libraries exist for the majority of programming languages
        allowing access to databases, whereas if this parser was written
        as a library a new interface layer would need to be written for
        every programming language using API\@.

    \item Databases provide complex querying and sorting functionality to
        the user without requiring large amounts of programming.  All
        databases provide one or more programs, of variable complexity and
        sophistication, which can be used for ad hoc queries with minimal
        investment of time.

    \item Databases are easily extensible, e.g.:
        
        \begin{itemize}

            \item Other tables can be added to the database.

            \item New columns can be added to the tables used by the
                program, with sufficient DEFAULT clauses or a clever
                TRIGGER or two.

            \item A VIEW gives a custom arrangement of data with very
                little effort.

            \item If the database supports it, access can be granted on a
                fine-grained basis so that the finance department can
                produce invoices, the helpdesk can run limited queries as
                part of dealing with support calls, and the administrators
                have full access to the data.

            \item Triggers can be written to perform actions when certain
                events occur.  In pseudo-SQL\@:

\begin{verbatim}
CREATE TRIGGER ON INSERT INTO results
    WHERE sender = 'boss@example.com'
        AND postfix_action = 'REJECTED'
    SEND PANIC EMAIL TO 'postmaster@example.com';
\end{verbatim}

        \end{itemize}

\end{enumerate}



\subsection{Postfix background}

Postfix is a highly configurable, high performance, secure and scalable
Mail Transport Agent.  It features extensive optional anti-spam
restrictions, allowing an administrator to deploy those restrictions which
they judge suitable for their site's needs, rather than a fixed set chosen
by Postfix's author.  These restrictions can be selectively applied,
combined and bypassed on a per-client, per-recipient or per-sender basis,
allowing varying levels of defense and/or permissiveness.  Postfix
leverages simple lookup tables to support arbitrarily complicated
user-defined sequences of restrictions and exceptions, with policy
servers\footnote{Policy servers will be explained shortly.} as the ultimate
in flexibility.  Administrators can also supply their own rejection
messages to make it clear to senders why exactly their mail was rejected.
Unfortunately this flexibility has a cost: complexity in the logs
generated.  While it is easy to use \texttt{grep(1)} to determine the fate
of an individual email, following the journey an email takes through
Postfix can be quite difficult.  The logs tend to follow a 90\%-10\%
pattern: 90\% of the time the journey is simple, but the other 10\% of the
time requires 90\% of the code.\footnote{These numbers don't have a solid
scientific basis, they're based on gut feeling from writing and debugging
the software.}

\subsubsection{Policy servers}

A policy server \cite{policy-servers} is an external program that accepts
state information from Postfix for each SMTP command not rejected by an
earlier restriction; the policy server can utilise that state information
to implement whatever logic is required.  E.g.\ some users can be
restricted to sending mail on the third Tuesday after pay day only --- this
example may actually be useful in a payroll system to prevent spam or
(worse) phishing mails with faked sender addresses.  More commonly
encountered scenarios are:

\begin{itemize}

    \item Checking Sender Policy Framework records \cite{openspf},
        \cite{wikipedia-spf}.  SPF records specify which mail servers are
        allowed to send mail claiming to be from a particular domain.  The
        intention is to reduce spam from faked sender addresses,
        backscatter \cite{postfix-backscatter} and joe-jobs
        \cite{wikipedia-joe-job}; however there
        has been a lot of resistance to the proposal because it breaks or
        vastly complicates some features of SMTP.

    \item Greylisting \cite{greylisting}, a technique that temporarily
        rejects mail where the triple of (sender, recipient, remote IP
        address) is unknown; the assumption is that it's uneconomical for a
        spammer to retry, but that a legitimate mail server must retry.
        Sadly spammers are using increasingly complex and well written
        programs to distribute spam, frequently using an ISP provided SMTP
        server from a compromised machine on its network.  Greylisting is
        slowly becoming less useful, but it does block a large percentage
        of spam mail at the moment.

    \item Using a scoring system such as Policyd-weight
        \cite{policyd-weight} where tests accumulate points against the
        sending system --- if the eventual score is too high the mail is
        rejected.
        
    \item Rate limiting or throttling on a per-sender, per-client or
        per-recipient basis as performed by Policyd \cite{policyd}.

\end{itemize}

Example attributes taken from \cite{policy-servers}:

\begin{tabular}[]{ll}

    request                 & smtpd\_access\_policy     \\
    protocol\_state         & RCPT                      \\
    protocol\_name          & SMTP                      \\
    helo\_name              & some.domain.tld           \\
    queue\_id               & 8045F2AB23                \\
    sender                  & foo@bar.tld               \\
    recipient               & bar@foo.tld               \\
    recipient\_count        & 0                         \\
    client\_address         & 1.2.3.4                   \\
    client\_name            & another.domain.tld        \\
    reverse\_client\_name   & another.domain.tld        \\
    instance                & 123.456.7                 \\

\end{tabular}



\subsection{Parser background}

Before getting into detail about the parser a brief overview is in order.
The parser is split into two parts: the parsing algorithm and the rules
which are applied to the lines.  Rules can be thought of as the Lex part of
a Lex and YACC style parser; rules identify the line and return some data,
which the algorithm (the YACC part) receives and performs the requested
action.  Rules are solely concerned with identifying a line and extracting
data from it, whereas the algorithm's task is to follow the journey each
mail takes through Postfix, piecing the data together into a coherent
whole, saving it in a useful and consistent form, and performing
housekeeping duties.  Readers familiar with context-free grammars may find
Section \ref{comparison against context-free grammars} helpful.

\subsection{Assumptions}

The algorithm described and the program implementing it make a small number
of (hopefully safe and reasonable) assumptions:

\begin{itemize}

    \item The logs are whole and complete; nothing has been removed, either
        deliberately or accidentally (e.g.\ log rotation gone awry, file
        system filling up, logging system unable to cope with the volume of
        logs).

    \item Postfix logs sufficient information to make it possible to
        accurately reconstruct the actions it has taken.

    \item The Postfix queue has not been tampered with, causing unexplained
        appearance or disappearance of mail.

\end{itemize}

In some ways this task is similar to reverse engineering or replicating a
black box program based solely on its inputs and outputs.  Although the
source code is available, reading and understanding it would require a
significant investment of time:

\begin{tabular}[]{llll}

    Postfix 2.2.11  & Postfix 2.3.8   & Postfix 2.4.0 &                   \\
    71548           & 82224           & 83965         & lines of code     \\
    60962           & 67146           & 68675         & lines of comments \\
    16117           & 17647           & 18069         & lines are blank   \\
    148627          & 167017          & 170709        & lines in total    \\

\end{tabular}


\subsection{Other parsers}

10 other parsers have been reviewed in Appendix~\ref{other-parsers} as part
of the background research for this project.  None of the reviewed parsers
perform the type of advanced parsing and log correlation described here;
all are intended to perform a specific parsing and reporting task, rather
than be a generic parser and leave generating reports from the data to
other programs.  Some save the data extracted to a database but the
majority discard all data once finished running, making historical analysis
impossible.  The other parsers listed all produce a report of greater or
lesser complexity and detail, whereas the program described here doesn't
attempt to produce a report at all; that responsibility is deferred to a
separate program, to be developed later.  The parsing algorithm and program
described here are designed to enable much more detailed log analysis by
providing a stable platform for subsequent programs to develop upon.


\section{Adaptable parsing: user defined rules}

\label{rules}

The complexity and variation in Postfix's logs requires similar flexibility
in the parser; decoupling the parsing rules from the associated actions
allows new rules to be written and tested without requiring modifications
to the algorithm source code (significantly lowering the barrier to entry
for new or casual users), and greatly simplifies both algorithm and rules.
It also creates a clear separation of functionality: rules handle low level
details of identifying and extracting data from a line, whereas the
algorithm handles the higher level details of following the path a mail
takes through Postfix, assembling the required data, etc.

Rule have certain characteristics which may help in understanding the
parser:

\begin{itemize}

    \item The first matching rule wins: no further rules are tried against
        that line, but there is a facility for specifying the order of
        rules so that more specific rules can be tried first.

    \item Rules are completely self-contained and can be understood in
        isolation, without reference to any other rules.

    \item There are no sub-rules, so rules have linear computational
        complexity.

\end{itemize}


\subsection{Comparison against context-free grammars}

\label{comparison against context-free grammars}

XXX DESCRIBE IT IN LEFT: RIGHT PRODUCTION TERMS, IF POSSIBLE.

\subsection{Rule attributes}

Each rule defines the following:

\begin{description}

    \item [name] A short name for the rule.

    \item [description] Something must have occurred to cause Postfix to
        log each line (e.g. a remote client connecting causes a connection
        line to be logged).  The description field is generally used to
        describe the action causing the log lines this rule matches.

    \item [restriction\_name] The restriction which caused the mail to be
        rejected.  Only applicable to rules which have a result of
        \texttt{rejected}, other rules will have an empty string.

    \item [program] The program (postfix/smtp, postfix/smtpd, postfix/qmgr,
        etc.) whose log lines the rule applies to.  This avoids needlessly
        trying rules which won't match the line, or worse, might match
        unintentionally.

    \item [regex] The regular expression\footnote{See
        http://en.wikipedia.org/wiki/Regular\_expression for more
        information about regular expressions in general.} to match the log
        line against.  The regex will first have several keywords expanded:
        this simplifies reading and writing rules, avoids needless
        repetition of complex regex components, allows the components
        to be corrected and/or improved in one location, and makes each
        regex largely self-documenting.
        
        The following keywords are expanded (full explanations can be found
        in the source code):

        \_\_SENDER\_\_, \_\_RECIPIENT\_\_, \_\_MESSAGE\_ID\_\_,
        \_\_HELO\_\_, \newline \_\_EMAIL\_\_, \_\_HOSTNAME\_\_, \_\_IP\_\_,
        \_\_IPv4\_\_, \_\_IPv6\_\_, \newline \_\_SMTP\_CODE\_\_,
        \_\_RESTRICTION\_START\_\_, \_\_QUEUEID\_\_, \newline
        \_\_COMMAND\_\_, \_\_SHORT\_CMD\_\_, \_\_DELAYS\_\_, \_\_DELAY\_\_,
        \_\_DSN\_\_ and \_\_CONN\_USE\_\_.


        For efficiency the keywords are expanded and every rule's regex is
        compiled before attempting to parse the log file --- otherwise each
        regex would be recompiled each time it was used, resulting in a
        large, data dependent slowdown.  Graph \ref{normal regex vs discard
        regex} shows execution times gathered over 93 log files, with and
        without caching the regex; also shown on the graph are the file
        sizes and number of lines in each file.  Graph \ref{normal regex vs
        disarded regex factor} shows the percentage execution time increase
        when not caching each regex.  Logging to the database was disabled
        for the test runs.

    \item [result\_cols, connection\_cols] Specifies how the fields in the
        log line will be extracted.  The format is: \newline
        \texttt{smtp\_code = 1; recipient = 2, sender = 4;} \newline
        i.e.\ semi-colon or comma separated assignment statements, with the
        variable name on the left and the matching field from the regex on
        the right hand side.  The list of acceptable variable names is:

        \texttt{client\_hostname, client\_ip, server\_ip, server\_hostname,
        \newline helo, sender, recipient, smtp\_code, data and child.}

    \item [result\_data, connection\_data] Sometimes rules need to supply a
        piece of data which isn't present in the log line: e.g.\ setting
        \texttt{smtp\_code} when mail is accepted.  The format and allowed
        variables are the same as for \texttt{result\_cols} and
        \texttt{connection\_cols}, except that arbitrary
        data\footnote{Commas and semi-colons cannot be escaped and thus
        cannot be used.  This is intended for small amounts of data rather
        than large, so dealing with escape sequences seemed unnecessary.}
        is permitted on the right hand side of the assignment.

    \item [postfix\_action] This is the action Postfix must have taken to
        generate this line, with two exceptions:

        \begin{itemize}

            \item [info] Represents an unspecified intermediate action that
                the parser is not interested in per se, but which does log
                useful information, supplementing other log lines.

            \item [ignored] An action which is not only uninteresting in
                itself, but which also provides no useful data.

        \end{itemize}

        Uninteresting lines are parsed so that any lines the parser isn't
        capable of handling become immediately obvious errors.

    \item [action] The action the algorithm will take, e.g.
        \begin{description}

            \item [ignore] Ignore this line.  The simplest action, it is
                required because every line from a program of interest to
                the parser must be parsed properly; any unparsed line is
                considered an error.

            \item [commit] The mail is finished with, enter it in the
                database, and remove the mail from memory.

        \end{description}

        A full list can be found in Section~\ref{actions-in-detail}.

    \item [queueid] Specifies the match from the regex which gives the
        queueid, or zero if the log line doesn't contain a queueid.

    \item [rule\_order] This is an efficiency measure.  This counter is
        maintained for every rule and incremented each time the rule
        successfully matches.  At the start of each run the program sorts
        the rules in descending rule\_order, and at the end of the run
        updates every rule's rule\_order.  Assuming that the distribution
        of log lines is reasonably consistent, rules matching more commonly
        occurring log lines will be tried before rules matching less
        commonly occurring log lines, lowering the program's execution
        time.
\ref{normal vs shuffled vs reversed ordering}

        Averaged over 10 test runs:

        \begin{description} 

            \item [Ascending order (best):] run time was 56.684 seconds,
                standard deviation was 0.29 seconds.

            \item [Descending order (worst):] run time was 64.037
                seconds (12.97\% slower), standard deviation was 0.16
                seconds.

            \item [Shuffled order:] run time was 57.19 seconds
                (0.89\% slower), standard deviation was 0.95 seconds.

        \end{description}

        XXX RERUN OVER MORE LOG FILES

        XXX RERUN AGAIN ONCE POSTFIX 2.3 RULES HAVE BEEN ADDED TO SEE WHAT
        THE DIFFERENCE IS.  THERE ARE 110 POSTFIX 2.2 RULES AS OF
        2007/04/18

        Logging to the database was disabled for the test runs.

        It is hoped that this will prove to have greater effectiveness as
        the number of rules grows; currently the effect is negligible,
        particularly in comparison to compiling each regex once.

    \item [priority] This is the user-configurable companion to
        rule\_order: rules with a higher priority will be tried first,
        overriding rule\_order, allowing more specific rules to take
        precedence over more general rules.

\end{description}


\subsection{Example rule}

This example rule matches the message Postfix logs when it rejects mail
from a sender address where the domain has neither an MX record nor an A
record, i.e.\ mail could not be delivered to the sender's address.  For full
details see
http://www.postfix.org/postconf.5.html\#reject\_unknown\_sender\_domain

This rule would match the following log line:

\begin{verbatim}
NOQUEUE: reject: RCPT from example.com[10.1.1.1]: 
  550 <foo@bar.baz>: Sender address rejected: Domain not found;
  from=<foo@bar.baz> to=<info@example.com> proto=SMTP
  helo=<smtp.bar.baz>
\end{verbatim}

XXX CARL SUGGESTS EXPANDING ON THIS SOME MORE, BUT I DON'T KNOW HOW\@.
PERHAPS WHEN THE ALGORITHM IS PROPERLY EXPLAINED IT'LL BE EASIER\@?  MAKE
REFERENCES TO THE ALGORITHM, EXPLAIN MORE CLEARLY HOW THE FIELDS ARE USED,
HOW THE RULE MATCHES, AND SO ON\@.  SAVE\_BY\_PID IS GONE NOW, USE A
DIFFERENT EXAMPLE.

% Don't reformat this!
\begin{tabular}[]{ll}

name                & Unknown sender domain                             \\
description         & We do not accept mail from unknown domains        \\
program             & postfix/smtpd                                     \\
restriction\_name   & reject\_unknown\_sender\_domain                   \\
regex               & \verb!^<(__SENDER__)>: Sender address rejected: ! \\
                    & \verb!  Domain not found; from=<\1> !             \\
                    & \verb!  to=<(__RECIPIENT__)> proto=E?SMTP !       \\
                    & \verb!  helo=<(__HELO__)>$!                       \\
result\_cols        & recipient = 2; sender = 1                         \\
connection\_cols    & helo = 3                                          \\
result\_data        &                                                   \\
connection\_data    &                                                   \\
postfix\_action     & REJECTED                                          \\
action              & SAVE\_BY\_PID                                     \\
queueid             & 0                                                 \\
rule\_order         & 0                                                 \\
priority            & 0                                                 \\

\end{tabular}


\section{Parsing algorithm}

\label{parsing-algorithm}

While the rules have more lines of code than the algorithm, the rules are
quite simple and each rule is completely independent of its fellows.  The
algorithm is significantly more complicated and highly internally
interdependent.


\subsection{A naive approach}

A high level view of the algorithm could be expressed as:

\begin{enumerate}

    \item Mail enters the system via SMTP or local submission.

    \item If the mail is rejected, log all data and finish.

    \item Follow the progress of the accepted mail until it's either
        delivered, bounced or deleted, log all data, and finish.

\end{enumerate}

Unfortunately it's not that easy.


\subsection{Complications encountered}

\label{complications}

\begin{enumerate}

    \item The mail lacks a queueid until it has been accepted, so log lines
        must first be correlated by the smtpd pid, then transition to being
        correlated by the queueid.  This is relatively minor, but does
        require:

        \begin{itemize}

            \item Two versions of several functions, \texttt{by\_pid} and
                \texttt{by\_queueid}.

            \item Two containing data structures to hold the data
                structure for each connection.

            \item Most importantly: every section of code must know whether
                it is needs to lookup the data structures by pid or
                queueid.

        \end{itemize}

    \item Multiple independent mails may be delivered during one
        connection: this requires cloning the current data as soon as a
        mail is accepted, so that subsequent mails won't trample over each
        other's data.  This must be done every time a mail is accepted, as
        it's impossible to tell in advance which connections will accept
        multiple mails.  It is quite easy to overlook this complication
        because only a small minority of connections accept more than one
        mail. Happily once the mail has been accepted log entries won't be
        correlated by pid for that mail any more (its queueid will be used
        instead), so there isn't any ambiguity about which mail a given log
        line belongs to.  The original connection will be discarded unsaved
        when the client disconnects.  One unsolved difficulty is
        distinguishing between different groups of rejections, e.g.\ when
        dealing with the following sequence:

        \begin{enumerate}

            \item The client attempts to deliver a mail, but it is
                rejected.

            \item The client issues the RSET command to reset the session.

            \item The client attempts to deliver another mail, likewise
                rejected.

        \end{enumerate}

        There should probably be two different entires in the database
        resulting from the above sequence, but currently there will only be
        one.

    \item The most difficult complication is that mails are not always
        delivered directly to a mailbox (or program; there is very little
        difference between the two): sometimes they are accepted for a
        local address but need to be delivered to one or more remote
        addresses due to aliases.  When this occurs a child mail will be
        injected into the postfix queue, but without the explicit logging
        smtpd or sendmail injected mails have, so the source is not
        immediately discernible from the log line in which the mail first
        appears; from a strictly linear reading of the logs it
        \textit{usually\/} appears as if the child mail has appeared from
        thin air.  Subsequently the parent mail will log the creation of
        the child mail:

        \texttt{3FF7C4317: to=<username@example.com>, relay=local, \newline 
        delay=0, status=sent (forwarded as 56F5B43FD)}

        Unfortunately while all log lines from an individual process appear
        in chronological order, the order in which log lines from different
        processes are interleaved is subject to the vagaries of process
        scheduling.  In addition the first log line belonging to the child
        mail (the log line cited above properly belongs to the parent mail)
        is logged by qmgr,\footnote{Qmgr is the Postfix daemon which
        manages the mail queue, determining which mails will have delivery
        attempted next.} so the order also depends on how busy qmgr
        is.\footnote{Postfix is quite paranoid about mail delivery, an
        excellent characteristic for an MTA to possess, so it won't log
        that the child has been created until it is absolutely certain that
        the mail has been written to disk.}

        Because of this the parser cannot complain when it encounters a log
        line from qmgr for a previously unseen mail; it must flag the mail
        as coming from an unknown origin and subsequently clear the flag if and
        when the origin of the mail becomes clear.  Obviously the parser
        could omit checking of where mails originate from, but I feel that
        it is better to require an explicit source, as bugs are more likely
        to be exposed.

        Process scheduling can have a still more confusing effect: quite
        often the child mail will be created, delivered and entirely
        finished with \textbf{before} the parent logs the creation line!
        Thus mails flagged as coming from an unknown origin cannot be
        entered into the database when their final log line is parsed;
        instead they must be marked as database ready and subsequently
        entered by the parent mail once it has been identified.  Quite
        apart from identifying mail injected in an unknown fashion, or bugs
        in the parser, unknown origin mails need to be marked as such
        because they lack some data present in the parent mail, and must
        copy that data from their parent before being entered in the
        database.  XXX EITHER REMOVE THE LAST LINE OR EXPAND ON IT SOME
        MORE\@.  FIGURE OUT WHY MORE DATA ISN'T REQUIRED\@.

        The algorithm requirements are discussed in
        section~\ref{tracking-re-injected-mail}.


\end{enumerate}

\newpage
\subsection{Flow chart}

\label{flow-chart}

This flow chart shows the paths the data representing a mail/connection can
take through the parser algorithm.

\includegraphics{logparser-flow-chart.ps}

\subsection{Full algorithm}

\label{full-algorithm}

The intermingling of log entries from different mails immediately rules out
the possibility of handling each mail in isolation; the parser must be
capable of handling multiple mails in parallel, each potentially at a
different stage in its journey, without any interference between mails ---
except in the minority of cases where intra-mail interference is required.
I felt that the best way to implement this was to maintain state
information for every unfinished mail and manipulate the appropriate mail
correctly for each log line encountered.  The parser thus requires both a
method of mapping log lines to the correct mail and a method of specifying
the action the log line represents.  The former is achieved by using the
pid of the smtpd to identify the correct mail during the initial phase,
then switching to the queueid once the mail has been accepted.  The latter
uses the action field of the rule which matched the log line, executing the
code in the function named by the action.  From a high level viewpoint this
design shares significant similarity with how an editor maintains state for
multiple files and performs the actions the user initiates on the correct
file.

Section~\ref{actions-in-detail} explains the actions in substantive detail;
this section will omit such detail because it would clutter and confuse the
algorithm description.  The data flow chart in section~\ref{flow-chart}
should be consulted while reading this section also.

\subsubsection{Mail enters the system}

\label{mail-enters-the-system}

Everything starts off with a mail entering the system, whether by local
submission via postdrop/sendmail, by SMTP, or by re-injection due to
forwarding.  Local submission is the simplest of the three: a queueid is
assigned immediately and the sender address is logged (action: pickup;
flowchart:~2).

SMTP is more complicated: 

\begin{enumerate}
        
    \item First there is a connection from the remote client
        (action: connect; flowchart:~1).

    \item This is followed by rejection of sender, recipients, client
        address, etc. (action: save\_by\_pid; flowchart:~4); acceptance of
        one or more mails (action: clone, then save\_by\_pid;
        flowchart:~5,~4); or some interleaving of both.
        
    \item The client disconnects (action: disconnect; flowchart:~6).  As
        soon as a mail is accepted it is assigned a queueid; thus if there
        is no queueid Postfix didn't accept any mail over that connection,
        it rejected all attempted deliveries due to the configured
        restrictions.  Absence of a queueid is checked for during
        disconnection handling (flowchart:~7) and if none is found the data
        is cleaned up and entered in the database (flowchart:~8;
        subroutines: fixup\_connection and commit\_connection), then
        removed from memory (flowchart:~9).

    \item If one or more mails were accepted there will be more log entries
        later, see section~\ref{mail-delivery}.

\end{enumerate}

Re-injection due to forwarding sadly lacks explicit log lines of its
own;\footnote{Previously discussed in section~\ref{complications},
complication 3.}  Re-injection is somewhat awkward to explain because it
overlaps both the mail acceptance and mail delivery sections.  See
section~\ref{tracking-re-injected-mail} for a full discussion.

\subsubsection{Mail delivery}

\label{mail-delivery}

The obvious counterpart to mail entering the system is mail leaving the
system, whether it is by deletion, bouncing, local or remote delivery.  All
three are handled in exactly the same way:

\begin{enumerate}

    \item The sender and recipient addresses will be logged separately
        (action: save\_by\_queueid; flowchart:~10).

    \item Sometimes mail is re-injected and the child mail needs to be
        tracked by the parent mail (action: track; flowchart:~11) --- see
        section~\ref{tracking-re-injected-mail} for further details.

    \item Eventually the mail will be delivered, bounced, or deleted by the
        administrator (action: commit; flowchart:~12).  This is the last
        log line for this particular mail (though it may be indirectly
        referred to if it was re-injected).  If it is neither parent nor
        child of re-injection the data is cleaned up and entered in the
        database (flowchart:~14; subroutines fixup\_connection and
        commit\_connection), then deleted from memory (flowchart:~15).  For
        re-injected mails see section~\ref{tracking-re-injected-mail}.

\end{enumerate}

I'd like to reiterate that the actions above happen whether the mail is
delivered to a mailbox, piped to a command, delivered to a remote server,
bounced (due to a mail loop, delivery failure, or five day timeout), or
deleted by the administrator.  The exception is what happens after delivery
to the parent or children of mail re-injected due to forwarding, and that
is explained in section~\ref{tracking-re-injected-mail}.

\subsubsection{Tracking re-injected mail}

\label{tracking-re-injected-mail}

It's probably apparent by now that tracking re-injected mails is the single
most complex part of the parser.  Section~\ref{complications}, complication
3 has most of the previous discussion, should you wish to review it.  

The crux of the problem is that re-injected mails appear in the logs
without explicit logging indicating their source.  There are two implicit
indications:

\begin{enumerate}

    \item The indicator which more commonly introduces re-injection is qmgr
        selecting a mail with a previously unseen queueid for delivery
        (action: qmgr\_chooses\_mail; flowchart:~3), in which case a new
        data structure will be created.  The mail will be flagged as having
        unknown origins; this flag should be subsequently cleared once the
        origin has been established.  The data is then saved (flowchart:~3;
        subroutine: save).  

    \item Local delivery re-injects the mail and logs a relayed delivery
        rather than delivering directly to a mailbox or program as it
        usually would (action: track; flowchart:~11).\footnote{Relayed
        delivery is performed by the SMTP client; local delivery means
        local to the server, i.e.\ an address the server accepts mail for.}
        In this case the mail may already have been created (action:
        qmgr\_chooses\_mail; flowchart:~3; described above) and the unknown
        origin flag will be cleared; if not a new data structure will be
        created.  In both cases the new mail is marked as a child of the
        parent.  The log line in question is:

        \texttt{3FF7C4317: to=<username@example.com>, relay=local, \newline 
        delay=0, status=sent (forwarded as 56F5B43FD)}

        This second indicator always occurs for re-injected mail but
        typically it occurs after the first indicator explained above, and
        so rarely introduces re-injection.  This indicator is required to
        tie the parent and child mails together and so is central to the
        process of tracking re-injected mails.

\end{enumerate}

The algorithm for tracking and saving re-injected mail to the database will
finally be described:

\begin{itemize}

    \item If the mail is of unknown origin it is a child mail whose parent
        hasn't yet been identified (action: commit; flowchart:~16).  Mark
        the mail as ready for entry in the database (flowchart:~17), and
        wait for the parent to deal with it ( flowchart:~18).  The mail
        should never have another directly associated log entry; only its
        parent should refer to it.

    \item If the mail is a child mail then it has already been tracked
        (action: commit; flowchart:~19): the data is cleaned up, pulling
        data from its parent if necessary, and entered in the database
        (flowchart:~20; subroutines fixup\_connection and
        commit\_connection), then deleted from memory (flowchart:~21).  The
        child mail will inform the parent mail that it is finished with
        (flowchart:~22); if this is the last child and the parent is also
        finished the parent will delete itself.  The child mail is now
        finished with.

    \item The last alternative is that the mail is a parent mail (action:
        commit; flowchart:~23).  Regardless of the state of its children
        the data is cleaned up and entered in the database ( flowchart:~24;
        subroutines fixup\_connection and commit\_connection).  The parent
        may have children which are waiting to be entered in the database (
        flowchart:~25); for each of those children their data is cleaned up
        and entered in the database, then the child is finished with and
        its data discarded.  The parent may also have outstanding children
        which are not yet delivered, in which case (flowchart:~26) the
        parent must wait for all children to finish.  As soon as the last
        child is finished with the parent is also finished with
        (flowchart:~27).

\end{itemize}

A parent mail can have multiple children, which may be delivered before or
after the parent mail.  

\subsection{Actions in detail}

\label{actions-in-detail}

This list contains all the actions available in the algorithm.  Each action
is passed the same arguments: 

\begin{description}

    \item [\$line] The log line, with the standard syslog(3) fields parsed,
        removed and made available.
        
    \item [\$rule] The matching rule
        
    \item [@matches] The fields in the line captured by \$rule's regex.

\end{description}

\begin{description}

    \item [IGNORE] IGNORE just returns successfully; it is used when a line
        needs to be parsed for completeness but doesn't either provide any
        useful data or require anything to be done.

    \item [CONNECT] Handle a remote client connecting: create a new state
        table entry (indexed by smtpd pid) and save both the client
        hostname and IP address.

    \item [DISCONNECT] Deal with the remote client disconnecting: enter the
        connection in the database, perform any required cleanup, and
        delete the connection from the state tables.

    \item [SAVE\_BY\_PID] Use the pid from \$line to find the correct
        connection and call \$self->save() with the appropriate arguments -
        see save() in SUBROUTINES for more details. If the connection
        doesn't exist a connection marked faked will be created and a
        warning issued.

    \item [SAVE\_BY\_QUEUEID] Use the queueid from \$rule and @matches to
        find the correct connection and call \$self->save() with the
        appropriate arguments - see save() in SUBROUTINES for more details.
        If the connection doesn't exist a connection marked faked will be
        created and a warning issued.

    \item [COMMIT] Enter the data into the database. Entry may be postponed
        if the mail is a child waiting to be tracked.

    \item [TRACK] Track a mail when it is forwarded to another mail server;
        this happens when a local address is aliased to a remote address.
        TRACK will be called when dealing with the parent mail, and will
        create the child mail if necessary. TRACK checks if the child has
        already been tracked, either with this parent or with another
        parent, and issues appropriate warnings in either case. Tracking
        children is discussed extensively in the paper written about this
        parser; details about obtaining the paper are given in the SEE ALSO
        section.

    \item [REJECTION] Deal with postfix rejecting an SMTP command from the
        remote client: log the rejection with the accepted mail if there is
        one, otherwise log it with the connection.

    \item [MAIL\_PICKED\_FOR\_DELIVERY] This action represents Postfix
        picking a mail from the queue to deliver. This action is used for
        both pickup and cleanup due to out of order log lines.

    \item [PICKUP] Pickup is the service which deals with mail submitted
        locally via /usr/sbin/sendmail. This action creates a new state
        table entry and saves data to it, unless out of order logging has
        caused the cleanup line to be logged first. Lines are assumed to be
        out of order if the only program seen thus far is cleanup and there
        is less than five seconds difference between the timestamps of the
        two lines.

    \item [CLONE] Multiple mails may be accepted on a single connection, so
        each time a mail is accepted the connection's state table entry
        must be cloned; if the original data structure was used the second
        and subsequent mails would corrupt the data structure.

    \item [MAIL\_TOO\_LARGE] When the client tries to send a larger message
        than the local server accepts the mail will be discarded.  See
        TIMEOUT for further discussion; the two are handled in exactly the
        same way.

    \item [TIMEOUT] The connection timed out so the mail currently being
        transferred must be discarded. The mail may have been accepted, in
        which case there's a data structure to dispose of, or it may not in
        which case there's none. The gory details can be found in the
        internals documentation.



\end{description}

\subsection{Additional complications}

\subsubsection{Identifying bounce notifications}

\label{identifying-bounce-notifications}

Postfix 2.2.x (and presumably previous versions) lacks explicit logging
when bounce notifications are generated; suddenly there will be log entries
for a mail which lacks an obvious source.  There are similarities to the
problem of child mails discussed in section
\ref{tracking-re-injected-mail}, but unlike the solution described therein
bounce notifications do not eventually have a log line which identifies
their source.  Heuristics must be used to identify bounce notifications,
and those heuristics are:

\begin{enumerate}

    \item The sender address will be $<>$.

    \item Neither smtpd nor pickup will have logged any messages associated
        with the mail, indicating it was generated internally by Postfix,
        not accepted via SMTP or submitted locally by sendmail.

    \item The mail will not have been tagged as a tracked mail (see section
        \ref{tracking-re-injected-mail}).

    \item The message-id has a specific format: \newline
        \texttt{YYYYMMDDhhmmss.queueid@server.hostname} \newline
        e.g. \texttt{20070321125732.D168138A1@smtp.cs.tcd.ie}

        The message-id is generated by Postfix for this bounce
        notification, so the queueid in the message-id must be the same as
        the queueid of the mail: this is what distinguishes bounce
        notifications generated locally from bounce notifications which are
        being forwarded inwards as a result of aliasing.  In the later case
        the message-id will be unchanged, and so even if it happens to be
        in the correct format (e.g. if it was generated by Postfix on
        another server) the queueid in the message-id will not match the
        queueid of the mail.

\end{enumerate}

Once a mail has been identified as a bounce notification the unknown origin
flag is cleared and the mail can be entered in the database.

There is a small chance that a mail will be incorrectly identified as a
bounce notification, as the heuristics used may be too broad.  For this to
occur the following conditions would have to be met:

\begin{enumerate}

    \item The mail must have been generated internally by Postfix.

    \item The sender address must be $<>$.

    \item The message-id must have the correct format and match the queueid
        of the mail.  Whist it is possible that a mail sent from elsewhere
        could easily have the correct message-id format, the chance that
        the queueid in the message-id would match the queueid of the mail
        is extremely small.

\end{enumerate}

The most likely cause of mis-identification is if a mail generated
internally by Postfix is identified as a bounce notification when it is a
different type of message; arguably this is a benefit rather than a
drawback, as future mails generated internally by Postfix will be handled
correctly.  XXX FIND OUT IF DSNS FIT THE PATTERN.

\subsubsection{Aborted delivery attempts}

\label{aborted-delivery-attempts}

Some mail clients appear to send the following sequence of commands during
the SMTP session:

\begin{verbatim}
    EHLO client.hostname
    MAIL FROM: <sender@address>
    RCPT TO: <recipient@address>
    RSET
    MAIL FROM: <sender@address>
    RCPT TO: <recipient@address>
    DATA
\end{verbatim}

The client aborts the first delivery attempt after the first recipient is
accepted, then makes a second delivery attempt which it continues with
until the delivery is complete. XXX VERIFY THIS AND DETERMINE WHAT CLIENTS
EXHIBIT THIS BEHAVIOUR XXX

Once again Postfix does not log a message making the client's behaviour
clear, so once again heuristics are required to identify the when this
behaviour occurs.  In this case a list of all mails accepted during a
connection are saved in the connection state, and the accepted mails
examined when the disconnection line is parsed and the disconnection action
executed.  Each mail is checked for the following characteristics:

\begin{itemize}

    \item Is the mail missing its cleanup log message?  Every mail which
        passes through Postfix will have a cleanup line; lack of a cleanup
        line is a sure sign the mail didn't make it too far.

    \item Were there exactly two smtpd log lines for the mail?  There
        should be a connection line and a mail accepted line.

\end{itemize}

If both checks are successful then the mail is assumed to be one of the
offending bogus mails and is discarded.  There will be no further entries
logged for such mails, so without identifying and discarding them they
accumulate in the state table and will cause clashes if the queueid is
reused.

\subsubsection{Further aborted delivery attempts}

Some mail clients disconnect abruptly if a second or subsequent recipient
is rejected; they may also disconnect after other errors, but such
disconnections are already being handled.  Sadly Postfix doesn't log a
message saying the mail has been discarded, as should be expected by now.
The checks to identify this happening are:

\begin{itemize}

    \item Is the mail missing its cleanup log message?  Every mail which
        passes through Postfix will have a cleanup line; lack of a cleanup
        line is a sure sign the mail didn't make it too far.

    \item Were there three or more smtpd log lines for the mail?  There
        should be a connection line and a mail accepted line, followed by
        one or more rejection lines.

    \item Is the last smtpd log line a rejection line?

\end{itemize}

These checks are made during the DISCONNECT action: if all checks are
successful then the mail is assumed to have been discarded when the client
disconnected.  There will be no further entries logged for such mails, so
without identifying and discarding them they accumulate in the state table
and will cause clashes if the queueid is reused.

\subsubsection{Timeouts during DATA phase}

\label{timeouts-during-data-phase}

The DATA phase of the SMTP conversation is where the headers and body of the
mail are transferred.  Sometimes there is a timeout or the connection is
lost\footnote{For brevity's sake timeout will be used throughout this
section, but everything applies equally to lost connections.} during the
DATA phase; when this occurs Postfix will discard the mail and the parser
needs to discard the data associated with that mail.  It seems more
intuitive to save the data to the database, but if a timeout occurs there
won't yet have been any data gathered for the mail, so there is none
available to save; the timeout is saved with the connection data instead.

To deal properly with timeouts the parsing algorithm needs to do the
following in the timeout action:

\begin{enumerate}

    \item Record the timeout and associated data in the connection's
        results.

    \item If no mails have been accepted yet nothing needs to be done; the
        timeout action ends.  The timeout action is dependant on the clone
        action keeping a list of all mails accepted on each connection.

    \item Because of ESMTP pipelining the client can send MAIL FROM, RCPT
        TO and DATA commands in one packet, instead of sending each command
        individually and waiting for the reply before sending subsequent
        commands.  If pipelining is used and Postfix rejects the sender
        address or any of the recipient addresses there is no way for the
        client to tell which command was rejected, nor does there appear to
        be a way for the SMTP conversation to recover, so clients
        disconnect uncleanly.

        A timeout may thus apply either to an accepted mail, or a rejected
        mail.  To distinguish between the two cases the algorithm compares
        the timestamp of the last accepted mail against the timestamp of
        the last line logged by smtpd for that connection.  If the smtpd
        timestamp is later there was a rejection between the accepted mail
        and the timeout, therefore the timeout applies to a rejected mail;
        the timeout has already been recorded so the timeout action
        finishes.  If the mail acceptance timestamp is greater then the timeout
        applies to the just-accepted mail.

    \item The mail needs to be discarded, but there is yet another
        complication: sometimes the cleanup line is logged after the
        timeout line.  If the cleanup line has already been seen the mail
        can be discarded (both from global state and the connection's list
        of accepted mails) and the timeout action finishes.
        
        In the case where the cleanup line is still pending the algorithm
        updates a global cache of queueids, adding the queueid and the
        timestamp from the timeout line.  When the next cleanup line is
        parsed for that queueid the cache will be checked, and will be
        deemed part of the discarded mail and ignored if it meets the
        following requirements:

        \begin{itemize}

            \item The queueid must not have been reused yet, i.e. there
                isn't an entry in the state tables for the queueid.

            \item The timestamp of the cleanup line must be within six
                minutes of the mail acceptance timestamp.  Timeouts happen
                after five minutes, so six gives a one minute leeway.

        \end{itemize}

        The next cleanup line must meet the criteria above for it to be
        discarded because not every connection where a timeout occurs will
        have a cleanup line logged for it; if the algorithm blindly
        discarded the next cleanup line it would in some cases be mistaken.
        Whether or not the line is discarded the queueid will be removed
        from the cache of timeout queueids.

\end{enumerate}

\subsubsection{Pickup logging after cleanup}

\label{pickup-logging-after-cleanup}

Occasionally the pickup line logged when mail is submitted locally via
sendmail appears later in the logfile than the cleanup line for that mail.
This seems to occur during periods of particularly heavy load, so is most
likely due to process scheduling vagaries.  Normally if the queueid given
in the pickup line exists a warning is generated by the pickup action, but
if the following conditions are met it is assumed that the lines are out of
order:

\begin{itemize}

    \item The only program which logged anything for the mail is cleanup.

    \item There is less than a five second difference in the timestamps of
        the cleanup and pickup lines.

\end{itemize}

As always with heuristics there may be circumstances in which these
heuristics incorrectly match, but none have been identified so far.

\section{Parsing coverage}

XXX THIS SECTION NEEDS TO BE WRITTEN

\label{parsing-coverage}

\subsection{Log lines covered}

\label{log-lines-covered}

\subsection{Mails covered}

\label{mails-covered}

\section{Limitations and possible improvements}

\label{limitations-improvements}

Every piece of software suffers from some limitations and there is almost
always room for improvement.

\subsection{Limitations}

\begin{itemize}

    \item Each new Postfix release requires new rules to be written to cope
        with the new log lines.  Similarly new RBLs, new policy servers and
        new administrator defined rejection messages require new rules.

    \item The program doesn't cope with mails which span log files:
        currently they are reported as unfinished mails at the end of the
        first run and are complained about when encountered during the next
        run because they don't have a source.

    \item It appears that the hostname used in the HELO command is not
        logged if the mail is accepted.\footnote{Tested with Postfix
        2.2.10; this may possibly have changed in Postfix 2.3, or the
        upcoming 2.4.}  It should be reasonably simple to write a policy
        server which causes Postfix to log a warning containing the HELO
        hostname when the DATA command is accepted.\footnote{See
        http://www.postfix.org/SMTPD\_POLICY\_README.html for full
        documentation about policy servers.}


    \item The algorithm does not distinguish between mails where one or
        more mails are rejected and a subsequent mail is accepted; it will
        appear in the database as one mail with lots of rejections followed
        by acceptance.  I don't believe it's possible to make this
        distinction given the data Postfix logs, though it might be
        possible to write a policy server to provide additional
        logging.

    \item The program will not detect parsing the same log file twice,
        resulting in the database containing duplicate entries.

\end{itemize}

\subsection{Possible improvements}
\begin{itemize}

    \item A progress bar would be useful when run interactively, as the
        program takes roughly one minute per 10MB of
        logs.\footnote{Approximately 80\% of the run time is consumed by
        logging to the database.}  Obviously performance is entirely
        dependant on the machine the program is running on.

    \item Write the policy servers referred to in the limitations above.

    \item Save state at the end of each run; load it at the start of the
        next run.  This should allow the program to cope with mails
        spanning log files.  This might also require purging old mails once
        they've been in the state tables for too long, though the presence
        of very old mails indicates a bug in either the implementation or
        the specification.

\end{itemize}

\section{Conclusion}

\label{conclusion}


\appendix


\section{References}

\label{references}

\subsection{Other Postfix log parsers}

\label{other-parsers}

\begin{description}

    \item [pflogsumm.pl] \textit{pflogsumm.pl is designed to provide an
        over-view of postfix activity, with just enough detail to give the
        administrator a ``heads up'' for potential trouble spots.\/}
        \url{http://jimsun.linxnet.com/postfix_contrib.html}

    \item [Sawmill Universal Log File Analysis and Reporting] is a
        general purpose commercial product which parses 687 log file
        formats (correct as of 2007/04/15) and produces reports.  I have not
        experimented with it due to its limited data extraction facilities;
        it does have three different sets of data for Postfix (as of
        2007/04/15, one is beta), but they do not appear to be interlinked,
        nor does it save sufficient data for the purposes of this project.
        The source code is available in an obfuscated form only (presumably
        for a fee), and the product is quite expensive, as it requires a
        license for each report which is to be generated. \newline
        \url{http://www.thesawmill.co.uk/formats/postfix.html} \newline
        \url{http://www.thesawmill.co.uk/formats/postfix_ii.html} \newline
        \url{http://www.thesawmill.co.uk/formats/beta_postfix.html}

    \item [Splunk] XXX INVESTIGATE FURTHER \url{http://www.splunk.com}

    \item [Isoqlog] \textit{Isoqlog is an MTA log analysis program written
        in C. It designed to scan qmail, postfix, sendmail and exim logfile
        and produce usage statistics in HTML format for viewing through a
        browser. It produces Top domains output according to Sender,
        Receiver, Total mails and bytes; it keeps your main domain mail
        statistics with regard to Days Top Domain, Top Users values for per
        day, per month and years.\/}  \newline
        \url{http://www.enderunix.org/isoqlog/}

    \item [AWStats] \textit{AWStats is a free powerful and featureful tool
        that generates advanced web, streaming, ftp or mail server
        statistics, graphically.\/}
        
        AWStats will produce simple graphs for many different services, but
        that restricts it to supporting the Lowest Common Denominator: the
        data it will extract from an MTA log file is:
        \texttt{time2, email, email\_r, host, host\_r, method, url, code,
        and bytesd.} \newline
        \url{http://awstats.sourceforge.net/} \newline
        \url{http://awstats.sourceforge.net/awstats.mail.html} \newline
        \url{http://awstats.sourceforge.net/docs/awstats_faq.html#MAIL}

    \item [Log analyser --- throughput monitor] This utility tracks the
        number of events which occurred over a particular time and warns if
        the frequency of events passes a certain threshold.  It's designed
        to provide real time alerts when dictionary attacks, mail loops or
        similar problems occur.  \newline
        \url{http://home.uninet.ee/~ragnar/throughput_monitor/}

    \item [Anteater] \textit{The Anteater project is a Mail Traffic
        Analyser. Anteater supports currently the logformat produced by
        Sendmail and by Postfix. The tool is written in 100\% C++ and is
        very easy to customize. Input, output, and the analysis are modular
        class objects with a clear interface. There are eight useful
        analyse modules, writing the result in plain ASCII or HTML, to
        stdout or to files.\/}

        Anteater hasn't been updated since November 2003, and doesn't have
        any English documentation.
        
        \url{http://anteater.drzoom.ch/}

    \item [Yet Another Advanced Logfile Analyser] uses a plugin based
        system to analyse log files and produce output reports.  The core
        code is merely 91 lines long, as all the parsing and report
        generation is handled by modules.  Using YAALA as a base would be
        only slightly less work as both input and output modules would need
        to be written; it may even be more work to implement the parser
        within the constraints of YAALA\@.

        \url{http://yaala.org/}

    \item [Logparser/Lire] Lire is another general purpose log parser, but
        is developed under the GNU GPL\@.  Like other general purpose log
        parsers it doesn't extract enough data from Postfix logs to be
        worthwhile.  Development also seems to have stalled, with only a
        minor update since October 2004.  \newline
        \url{http://logreport.org/lire/}

    \item [Logrep] \textit{Logrep is a secure multi-platform framework for
        the collection, extraction, and presentation of information from
        various log files.\/}

        The parsing Logrep performs is extremely basic, counting the number
        of different processes executed, from address, to addresses, number
        of recipients, bytes transferred and delay.  \newline
        \url{http://www.itefix.no/phpws/index.php}

\end{description}

\subsection{Postfix resources}

\subsection{SMTP references}

XXX ADD RESOURCES/REFERENCES HERE AND THEN MAKE REFERENCES TO THEM FROM WITHIN THE PAPER


% Bibliography.
\bibliographystyle{logparser-bibliography-style}
\bibliography{logparser-bibliography}

\section{Graphs}

\subsection{Regex: cached vs compiled every time}
\label{normal regex vs discard regex}
\includegraphics{plot-cached-discarded.ps}


\subsection{Regex: percentage execution time increase}
\label{normal regex vs disarded regex factor}
\includegraphics{plot-cached-discarded-factor.ps}


\subsection{Normal vs shuffled vs reversed ordering}
\label{normal vs shuffled vs reversed ordering}
\includegraphics{plot-normal-shuffle-reverse.ps}

\end{document}
