% $Id$
\documentclass[a4paper,12pt,draft]{article}

% Better typesetting of URLs.
\usepackage{url}
% Include images
\usepackage[final]{graphicx}
\renewcommand{\labelenumii}{\roman{enumii}:}

\begin{document}

\title{Parsing Postfix log files}
\author{John Tobin \\ School of Computer Science and Statistics \\ 
Trinity College \\ Dublin 2 \\ Ireland \\ tobinjt@cs.tcd.ie}
\date{}
\maketitle

\begin{abstract}

    Parsing Postfix logs is much more difficult than it first appears but
    it is possible to achieve a high degree of accuracy in understanding
    the logs and reconstructing the actions taken by Postfix to generate
    the logs.  This paper describes the process required, documenting the
    parsing algorithm and rules, explaining the difficulties encountered,
    with reference to an implementation which stores data gathered from the
    logs in an SQL database.  The gathered data can then be used to
    optimise current anti-spam measures, provide a baseline to test new
    anti-spam measures against, or to produce statistics showing how
    effective those measures are.

\end{abstract}

XXX WHAT IS THE CONCLUSION???

XXX FIGURE OUT WHEN TO USE MAIL AND WHEN TO USE CONNECTION\@; POSSIBLY STATE
THAT THEY'RE MORE OR LESS EQUIVALENT

\newpage
\tableofcontents

\newpage
\section{Introduction}

\label{introduction}

Most mail server administrators will have performed some basic processing
of Postfix logs at one time or another, whether it was to debug a problem,
explain to a user why their mail is being rejected, or to check whether
their new anti-spam measures are working.  The more adventurous will have
generated statistics to show how many hits each of their anti-spam measures
has gotten in the last week, and possibly even generated some graphs to
clearly illustrate the point to management or complaining
users.\footnote{This was the author's first real foray into processing
Postfix logs.}  Very few will have performed in-depth parsing and analysis
of their logs, where the parsing must correlate the log lines
per-connection or per-queueid rather than processing lines independently.
One of the barriers to this type of processing is the unstructured nature
of Postfix logs, where each logging line was added on an ad hoc basis as a
requirement was discovered or new functionality was added.  Further
complication arises as a result of the facility to have rejection messages
defined by the administrator; every Real-time Black List returns a
different explanatory message; policy servers may have different messages
depending on the characteristics of the connection; there are many ways in
which the log lines may differ.  This paper documents the difficult process
of parsing Postfix logs, presenting a program which parses logs and places
the resulting data into a database.  The gathered data can then be used to
optimise current anti-spam measures, provide a baseline to test new
anti-spam measures against, or to produce statistics showing how effective
those measures are.  There are numerous other uses for such data: improving
server performance by identifying troublesome destinations and
reconfiguring appropriately or dedicating transports to those destinations;
identifying high volume uses (e.g.  customer newsletters) and restricting
those uses to off-peak times; as a base for billing customers on a shared
server; etc.

Section~\ref{background} provides background information about: the
motivation for this project; the Postfix Mail Transport Agent; a short
overview of the parser; the assumptions inherent in the design of the
algorithm and program; using a database schema as an API; and a comparison
against other Postfix log parsers.

Section~\ref{rules} discusses the parsing rules in detail, explaining their
structure, giving an example rule and sample data it matches successfully
against.

Section~\ref{parsing-algorithm} contains the meat of the paper, describing
a naive parsing algorithm and the complications encountered which shaped
the full algorithm, followed by a comprehensive explanation of the
different stages of the algorithm and the actions taken during the
execution of the algorithm.

Section~\ref{limitations-improvements} lists the limitations of the program
and/or the algorithm, then suggests some ways of dealing with the
limitations, with the goal of improving parsing and reproducing the journey
a mail takes through the internals of Postfix.

Section~\ref{conclusion} describes the results of the project to date.  XXX
IMPROVE THIS\@.

Section~\ref{references} catalogues resources used in developing the
algorithm, writing the program and preparing this paper.  
Any resources referred to in the paper are also given, plus some additional
resources expected to be helpful in understanding Postfix, anti-spam
techniques, or the paper.

\section{Background}

\label{background}

\subsection{Motivation}

This paper and the program it describes are part of a larger project to
optimise a server's Postfix restrictions, generate statistics and graphs,
and provide a platform on which new restrictions can be trialled and
evaluated to see if they are worth using in the fight against spam.  The
program parses Postfix logs and populates a database with the data gleaned
from those logs, providing a consistent and simple view of the logs which
future tools can utilise.  The gathered data can then be used to optimise
current anti-spam measures, provide a baseline to test new anti-spam
measures against, or to produce statistics showing how effective those
measures are.\footnote{Section~\ref{introduction} lists some more example
uses.}

A short example of the optimisation possible using data from the database
is optimal ordering of Postfix restrictions:

\begin{verbatim}
SELECT name, description, restriction_name, rule_order
    FROM rules
    WHERE postfix_action = 'REJECTED'
    ORDER BY rule_order DESC;
\end{verbatim}

If the database supports sub-selects percentages can be
obtained:\footnote{This works for SQLite3, where || is the concatenation
operator.  If the database containing the data does not support
concatenation, simply remove ``|| '\%\''' from the query.}

XXX STOP THIS FROM BEING SPLIT ACROSS PAGES IF POSSIBLE

\begin{verbatim}
SELECT name, description, restriction_name, rule_order,
        (rule_order * 100.0 /
            (SELECT SUM(rule_order)
                FROM rules
                WHERE postfix_action = 'REJECTED'
            )
        ) || '%' AS percentage
    FROM rules
    WHERE postfix_action = 'REJECTED'
    ORDER BY rule_order DESC;
\end{verbatim}

Another example is determining which restrictions are not effective:

\begin{verbatim}
SELECT name, description, restriction_name, rule_order,
        (rule_order * 100.0 /
            (SELECT SUM(rule_order)
                FROM rules
                WHERE postfix_action = 'REJECTED'
            )
        ) || '%' AS percentage
    FROM rules
    WHERE postfix_action = 'REJECTED'
        AND rule_order < 100
    ORDER BY rule_order ASC;
\end{verbatim}

\subsection{Database as Application Programming Interface}

The database populated by this program provides a simple interface to
Postfix logs.  Although the interface is a database schema, it is in effect
quite similar to the API provided by a library: it insulates both user and
provider of the API from changes in the implementation of the other.  The
algorithm implemented by the program can be improved; support can be added
for earlier or later releases of Postfix; bugs can be fixed or limitations
removed from the program.  Statistics and/or graphs can be generated from
the database; new restrictions can be tested and the results inspected;
trends in the fight against spam can emerge from historical data saved in
the database.  Using a database simplifies writing programs which need to
interact with the data in several ways:

\begin{enumerate}

    \item Libraries exist for the majority of programming languages
        allowing access to databases, whereas if this program was written
        as a library a new interface layer would need to be written for
        every programming language used with the API\@.

    \item Databases provide complex querying and sorting functionality to
        the user without requiring large amounts of programming.  All
        databases provide one or more programs, of variable complexity and
        sophistication, which can be used for ad hoc queries with minimal
        investment of time.

    \item Databases are easily extensible, e.g.:
        
        \begin{itemize}

            \item Other tables can be added to the database.

            \item New columns can be added to the tables used by the
                program, with sufficient DEFAULT clauses or a clever
                TRIGGER or two.

            \item A VIEW gives a custom arrangement of data with very
                little effort.

            \item If the database supports it, access can be granted on a
                fine-grained basis so that the finance department can
                produce invoices, the helpdesk can run limited queries as
                part of dealing with support calls, and the administrators
                have full access to the data.

            \item Triggers can be written to perform actions when certain
                events occur.  In pseudo-SQL\@:

\begin{verbatim}
CREATE TRIGGER ON INSERT INTO results
    WHERE sender = 'boss@example.com'
        AND postfix_action = 'REJECTED'
    SEND PANIC EMAIL TO 'postmaster@example.com';
\end{verbatim}

        \end{itemize}

\end{enumerate}



\subsection{Postfix background}

Postfix is a highly configurable, high performance, secure and scalable
Mail Transport Agent.  It features extensive anti-spam restrictions,
allowing a mail administrator to deploy those restrictions which they judge
suitable for their needs, rather than a fixed set chosen by Postfix's
author.  These restrictions can be selectively applied, combined and
bypassed on a per-client, per-recipient or per-sender basis, allowing
varying levels of defense and or permissiveness.  Postfix leverages simple
lookup tables to support arbitrarily complicated user-defined sequences of
restrictions and exceptions, with the ultimate in flexibility being the
facility to consult an external process which can implement whatever logic
is required: some users can be restricted to sending mail on the third
Tuesday after pay day only, if that is of any use.\footnote{This example
may actually be useful in a payroll system.  More commonly encountered
scenarios are restricting posting to large mailing lists (e.g.\ sending
weekly offers to customers) to out of office hours, rate limiting clients,
or checking SPF records (see http://www.openspf.org/ or
http://en.wikipedia.org/wiki/Sender\_Policy\_Framework for details)}
Administrators can also supply their own rejection messages to make it
clear to senders why exactly their mail was rejected.  Unfortunately this
flexibility has a cost: complexity in the logs generated.  While it is easy
to use \texttt{grep(1)} to determine the fate of an individual email,
following the journey an email takes through Postfix can be quite
difficult.  The logs tend to follow a 90\%-10\% pattern: 90\% of the time
the journey is simple, but the other 10\% of the time requires 90\% of the
code.\footnote{These numbers don't have a solid scientific basis, they're
based on gut feeling from writing and debugging the software.}

\subsection{Parser background}

Before getting into detail about the parser a brief overview is in order.
The parser is split into two parts: the parsing algorithm and the rules
which are applied to the lines.  Rules can be thought of as the Lex part of
a Lex and YACC style parser; rules identify the line and return some data,
which the algorithm (the YACC part) receives and performs the requested
action.  Rules are solely concerned with identifying a line and extracting
data from it, whereas the algorithm's task is to follow the journey each
mail takes through Postfix, piecing the data together into a coherent
whole, saving it in a useful and consistent form, and performing
housekeeping duties.


\subsection{Assumptions}

The algorithm described and the program implementing it make a small number
of (hopefully safe and reasonable) assumptions:

\begin{itemize}

    \item The logs are whole and complete; nothing has been removed, either
        deliberately or accidentally (e.g.\ log rotation gone awry, file
        system filling up, logging system unable to cope with the volume of
        logs).

    \item Postfix logs sufficient information to make it possible to
        accurately reconstruct the actions it has taken.

    \item The Postfix queue has not been tampered with, causing unexplained
        appearance or disappearance of mail.

\end{itemize}

In some ways this task is similar to reverse engineering or replicating a
black box program based solely on its inputs and outputs.  Although the
source code is available, reading and understanding it would require a
significant investment of time:

\begin{tabular}[]{llll}

    Postfix 2.2.11  & Postfix 2.3.8   & Postfix 2.4.0 &                   \\
    71548           & 82224           & 83965         & lines of code     \\
    60962           & 67146           & 68675         & lines of comments \\
    16117           & 17647           & 18069         & lines are blank   \\
    148627          & 167017          & 170709        & lines in total    \\

\end{tabular}


\subsection{Other parsers}

10 other parsers have been reviewed in Appendix~\ref{other-parsers} as part
of the background research for this project.  None of the reviewed parsers
perform the type of advanced parsing and log correlation described here;
all are intended to perform a specific parsing and reporting task, rather
than be a generic parser and leave generating reports from the data to
other programs.  Some save the data extracted to a database but the
majority discard all data once finished running, making historical analysis
impossible.  The other parsers listed all produce a report of greater or
lesser complexity and detail, whereas the program described here doesn't
attempt to produce a report at all; that responsibility is deferred to a
separate program, to be developed later.  The parsing algorithm and program
described here are designed to enable much more detailed log analysis by
providing a stable platform for subsequent programs to develop upon.

XXX DO I NEED TO SAY MORE??


\section{Adaptable parsing: user defined rules}

\label{rules}

The complexity and variation in Postfix's logs requires similar flexibility
in the parser; decoupling the parsing rules from the associated actions
allows new rules to be written and tested without requiring modifications
to the algorithm source code (significantly lowering the barrier to entry
for new or casual users), and greatly simplifies both algorithm and rules.
It also creates a clear separation of functionality: rules handle low level
details of identifying and extracting data from a line, whereas the
algorithm handles the higher level details of following the path a mail
takes through Postfix, assembling the required data, etc.

Rule have certain characteristics which may help in understanding the
parser:

\begin{itemize}

    \item The first matching rule wins: no further rules are tried against
        that line,\footnote{With one exception, details are provided later
        in Section~\ref{actions-in-detail}.} but there is a facility for
        the specifying the order of the rules so that more specific rules
        can be tried first.

    \item Rules are completely self-contained and can be understood in
        isolation, without reference to any other rules.

    \item There are no sub-rules, so rules have linear computational
        complexity.

\end{itemize}

\subsection{Rule attributes}

Each rule defines the following:

\begin{description}

    \item [name] A short name for the rule.

    \item [description] A longer, more detailed description of the rule.

    \item [program] The program (smtp, smtpd, qmgr, etc.) whose log lines
        the rule applies to.  This avoids needlessly trying rules which
        won't match the line, or worse, might match unintentionally.

    \item [restriction\_name] The restriction which caused the mail to be
        rejected.  Only applicable to rules which have a result of
        \texttt{rejected}, other rules will have an empty string.

    \item [regex] The regular expression\footnote{See
        http://en.wikipedia.org/wiki/Regular\_expression for more
        information about regular expressions in general.} to match the log
        line against.  The regex will first have several keywords expanded:
        this simplifies reading and writing rules, avoids needless
        repetition of complex regex components, and allows the components
        to be corrected and/or improved in one location.
        
        The following keywords are expanded:
        
        \texttt{\_\_SENDER\_\_, \_\_RECIPIENT\_\_, \_\_HELO\_\_,
        \_\_HOSTNAME\_\_, \_\_IP\_\_, \newline\_\_SMTP\_CODE\_\_,
        \_\_QUEUEID\_\_ and \_\_COMMAND\_\_.}

        \_\_COMMAND\_\_ is an SMTP command, e.g. MAIL FROM\@; hopefully the
        others are reasonably self explanatory.

        For efficiency the keywords are expanded and every rule's regex is
        compiled before attempting to parse the log file --- otherwise each
        regex would be recompiled each time it was used, resulting in an
        468.95\% slowdown.

        Averaged over 10 test runs:

        \begin{description} 

            \item [Compiled once and saved:] run time was 55.47 seconds,
                standard deviation was 0.247 seconds.

            \item [Compiled every time and discarded:] run time was 260.14
                seconds (4 minutes, 20.14 seconds) (468.95\% slower),
                standard deviation was 0.96 seconds.

        \end{description}

        Logging to the database was disabled for the test runs.

        XXX RERUN OVER MORE LOG FILES

        XXX DO I NEED TO SAY ANYTHING MORE ABOUT THIS??

    \item [result\_cols, connection\_cols] Specifies how the fields in the
        log line will be extracted.  The format is: \newline
        \texttt{client\_hostname = 1; recipient = 2, sender = 4;} \newline
        i.e.\ semi-colon or comma separated assignment statements, with the
        variable name on the left and the matching field from the regex on
        the right hand side.  The list of acceptable variable names is:

        \texttt{client\_hostname, client\_ip, server\_ip, server\_hostname,
        \newline helo, sender, recipient, smtp\_code, data and child.}

    \item [result\_data, connection\_data] Sometimes rules need to supply a
        piece of data which isn't present in the log line: e.g.\ setting
        \texttt{smtp\_code} when mail is accepted.  The format and allowed
        variables are the same as for \texttt{result\_cols} and
        \texttt{connection\_cols}, except that arbitrary
        data\footnote{Commas and semi-colons cannot be escaped and thus
        cannot be used.  This is intended for small amounts of data rather
        than large, so dealing with escape sequences seemed unnecessary.}
        is permitted on the right hand side of the assignment.

    \item [postfix\_action] This is the action Postfix must have taken to
        generate this line, with two exceptions:

        \begin{itemize}

            \item [info] Represents an unspecified intermediate action that
                the parser is not interested in per se, but which does log
                useful information, supplementing other log lines.

            \item [ignored] An action which is not only uninteresting in
                itself, but which also provides no useful data.

        \end{itemize}

        Uninteresting lines are parsed so that any lines the parser isn't
        capable of handling become immediately obvious errors.

    \item [action] The action the algorithm will take, e.g.
        \begin{description}

            \item [ignore] Ignore this line.  The simplest action, it is
                required because every line from a program of interest to
                the parser must be parsed properly; any unparsed line is
                considered an error.

            \item [save\_by\_pid] Use the pid from the log line to lookup a
                connection, then save the data from the log line in the
                connection data structure thus found.

            \item [commit] The mail is finished with, enter it in the
                database, and remove the mail from memory.

        \end{description}

        A full list can be found in Section~\ref{actions-in-detail}.

    \item [queueid] Specifies the match from the regex which gives the
        queueid, or zero if the log line doesn't contain a queueid.

    \item [rule\_order] This is an efficiency measure.  This counter is
        maintained for every rule and incremented each time the rule
        successfully matches.  At the start of each run the program sorts
        the rules in descending rule\_order, and at the end of the run
        updates every rule's rule\_order.  Assuming that the distribution
        of log lines is reasonably consistent, rules matching more commonly
        occurring log lines will be tried before rules matching less
        commonly occurring log lines, lowering the program's execution
        time.

        Averaged over 10 test runs:

        \begin{description} 

            \item [Ascending order (best):] run time was 56.684 seconds,
                standard deviation was 0.29 seconds.

            \item [Descending order (worst):] run time was 64.037
                seconds (12.97\% slower), standard deviation was 0.16
                seconds.

            \item [Shuffled order:] run time was 57.19 seconds
                (0.89\% slower), standard deviation was 0.95 seconds.

        \end{description}

        XXX RERUN OVER MORE LOG FILES

        Logging to the database was disabled for the test runs.

        It is hoped that this will prove to have greater effectiveness as
        the number of rules grows; currently the effect is negligible,
        particularly in comparison to compiling each regex once.

    \item [priority] This is the user-configurable companion to
        rule\_order: rules with a higher priority will be tried first,
        overriding rule\_order, allowing more specific rules to take
        precedence over more general rules.

\end{description}


\subsection{Example rule}

This example rule matches the message Postfix logs when it rejects mail
from a sender address where the domain has neither an MX record nor an A
record, i.e.\ mail could not be delivered to the sender's address.  For full
details see
http://www.postfix.org/postconf.5.html\#reject\_unknown\_sender\_domain

This rule would match the following log line:

\begin{verbatim}
NOQUEUE: reject: RCPT from example.com[10.1.1.1]: 
  550 <foo@bar.baz>: Sender address rejected: Domain not found;
  from=<foo@bar.baz> to=<info@example.com> proto=SMTP
  helo=<smtp.bar.baz>
\end{verbatim}

XXX CARL SUGGESTS EXPANDING ON THIS SOME MORE, BUT I DON'T KNOW HOW\@.
PERHAPS WHEN THE ALGORITHM IS PROPERLY EXPLAINED IT'LL BE EASIER\@?  MAKE
REFERENCES TO THE ALGORITHM, EXPLAIN MORE CLEARLY HOW THE FIELDS ARE USED,
HOW THE RULE MATCHES, ETC\@.

% Don't reformat this!
\begin{tabular}[]{ll}

name                & Unknown sender domain                             \\
description         & We do not accept mail from unknown domains        \\
program             & postfix/smtpd                                     \\
restriction\_name   & reject\_unknown\_sender\_domain                   \\
regex               & \verb!^<(__SENDER__)>: Sender address rejected: ! \\
                    & \verb!  Domain not found; from=<\1> !             \\
                    & \verb!  to=<(__RECIPIENT__)> proto=E?SMTP !       \\
                    & \verb!  helo=<(__HELO__)>$!                       \\
result\_cols        & recipient = 2; sender = 1                         \\
connection\_cols    & helo = 3                                          \\
result\_data        &                                                   \\
connection\_data    &                                                   \\
postfix\_action     & REJECTED                                          \\
action              & SAVE\_BY\_PID                                     \\
queueid             & 0                                                 \\
rule\_order         & 0                                                 \\
priority            & 0                                                 \\

\end{tabular}


\section{Parsing algorithm}

\label{parsing-algorithm}

While the rules have more lines of code than the algorithm, the rules are
quite simple and each rule is completely independent of its fellows.  The
algorithm is significantly more complicated and highly internally
interdependent.


\subsection{A naive approach}

A high level view of the algorithm could be expressed as:

\begin{enumerate}

    \item Mail enters the system via SMTP or local submission.

    \item If the mail is rejected, log all data and finish.

    \item Follow the progress of the accepted mail until it's either
        delivered, bounced or deleted, log all data, and finish.

\end{enumerate}

Unfortunately it's not that easy.


\subsection{Complications encountered}

\label{complications}

\begin{enumerate}

    \item The mail lacks a queueid until it has been accepted, so log lines
        must first be correlated by the smtpd pid, then transition to being
        correlated by the queueid.  This is relatively minor, but does
        require:

        \begin{itemize}

            \item Two versions of several functions, \texttt{by\_pid} and
                \texttt{by\_queueid}.

            \item Two containing data structures to hold the data
                structure for each connection.

            \item Most importantly: every section of code must know whether
                it is needs to lookup the data structures by pid or
                queueid.

        \end{itemize}

    \item Multiple independent mails may be delivered during one
        connection: this requires cloning the current data as soon as a
        mail is accepted, so that subsequent mails won't trample over each
        other's data.  This must be done every time a mail is accepted, as
        it's impossible to tell in advance which connections will accept
        multiple mails.  It is quite easy to overlook this complication
        because only a small minority of connections accept more than one
        mail. Happily once the mail has been accepted log entries won't be
        correlated by pid for that mail any more (its queueid will be used
        instead), so there isn't any ambiguity about which mail a given log
        line belongs to.  The original connection will be discarded unsaved
        when the client disconnects.  One unsolved difficulty is
        distinguishing between different groups of rejections, e.g.\ when
        dealing with the following sequence:

        \begin{enumerate}

            \item The client attempts to deliver a mail, but it is
                rejected.

            \item The client issues the RSET command to reset the session.

            \item The client attempts to deliver another mail, likewise
                rejected.

        \end{enumerate}

        There should probably be two different entires in the database
        resulting from the above sequence, but currently there will only be
        one.

    \item The most difficult complication is that mails are not always
        delivered directly to a mailbox (or program; there is very little
        difference between the two): sometimes they are accepted for a
        local address but need to be delivered to one or more remote
        addresses due to aliases.  When this occurs a child mail will be
        injected into the postfix queue, but without the explicit logging
        smtpd or sendmail injected mails have, so the source is not
        immediately discernible from the log line in which the mail first
        appears; from a strictly linear reading of the logs it
        \textit{usually\/} appears as if the child mail has appeared from
        thin air.  Subsequently the parent mail will log the creation of
        the child mail:

        \texttt{3FF7C4317: to=<username@example.com>, relay=local, \newline 
        delay=0, status=sent (forwarded as 56F5B43FD)}

        Unfortunately while all log lines from an individual process appear
        in chronological order, the order in which log lines from different
        processes are interleaved is subject to the vagaries of process
        scheduling.  In addition the first log line belonging to the child
        mail (the log line cited above properly belongs to the parent mail)
        is logged by qmgr,\footnote{Qmgr is the Postfix daemon which
        manages the mail queue, determining which mails will have delivery
        attempted next.} so the order also depends on how busy qmgr
        is.\footnote{Postfix is quite paranoid about mail delivery, an
        excellent characteristic for an MTA to possess, so it won't log
        that the child has been created until it is absolutely certain that
        the mail has been written to disk.}

        Because of this the parser cannot complain when it encounters a log
        line from qmgr for a previously unseen mail; it must mark the mail
        as potentially fake and subsequently clear the faked flag if and
        when the origin of the mail becomes clear.  Obviously the parser
        could omit checking of where mails originate from, but I feel that
        it is better to require an explicit source, as bugs are more likely
        to be exposed.

        Process scheduling can have a still more confusing effect: quite
        often the child mail will be created, delivered and entirely
        finished with \textbf{before} the parent logs the creation line!
        Thus mails marked as faked cannot be entered into the database when
        their final log line is parsed; instead they must be marked as
        database ready and subsequently entered by the parent mail once it
        has been identified.  Quite apart from identifying mail injected in
        an unknown fashion, or bugs in the parser, faked mails need to be
        marked as such because they lack some data present in the parent
        mail, and must copy that data from their parent before being
        entered in the database.  XXX EITHER REMOVE THE LAST LINE OR EXPAND
        ON IT SOME MORE\@.  FIGURE OUT WHY MORE DATA ISN'T REQUIRED\@.

        The algorithm requirements are discussed in
        section~\ref{tracking-re-injected-mail}.


\end{enumerate}

\newpage
\subsection{Flow chart}

\label{flow-chart}

This flow chart shows the paths the data representing a mail/connection can
take through the parser algorithm.

\includegraphics{logparser-flow-chart.ps}

\subsection{Full algorithm}

\label{full-algorithm}

The intermingling of log entries from different mails immediately rules out
the possibility of handling each mail in isolation; the parser must be
capable of handling multiple mails in parallel, each potentially at a
different stage in its journey, without any interference between mails ---
except in the minority of cases where intra-mail interference is required.
I felt that the best way to implement this was to maintain state
information for every unfinished mail and manipulate the appropriate mail
correctly for each log line encountered.  The parser thus requires both a
method of mapping log lines to the correct mail and a method of specifying
the action the log line represents.  The former is achieved by using the
pid of the smtpd to identify the correct mail during the initial phase,
then switching to the queueid once the mail has been accepted.  The latter
uses the action field of the rule which matched the log line, executing the
code in the function named by the action.  From a high level viewpoint this
design shares significant similarity with how an editor maintains state for
multiple files and performs the actions the user initiates on the correct
file.

Section~\ref{actions-in-detail} explains the actions in substantive detail;
this section will omit such detail because it would clutter and confuse the
algorithm description.  The flow chart in section~\ref{flow-chart} should
be consulted while reading this section also.

\subsubsection{Mail enters the system}

\label{mail-enters-the-system}

Everything starts off with a mail entering the system, whether by local
submission via postdrop/sendmail, by SMTP, or by re-injection due to
forwarding.  Local submission is the simplest of the three: a queueid is
assigned immediately and the sender address is logged (action: pickup;
flowchart:~2).

SMTP is more complicated: 

\begin{enumerate}
        
    \item First there is a connection from the remote client
        (action: connect; flowchart:~1).

    \item This is followed by rejection of sender, recipients, client
        address, etc. (action: save\_by\_pid; flowchart:~4); acceptance of
        one or more mails (action: clone, then save\_by\_pid;
        flowchart:~5,~4); or some interleaving of both.
        
    \item The client disconnects (action: disconnect; flowchart:~6).  As
        soon as a mail is accepted it is assigned a queueid; if there is no
        queueid Postfix didn't accept any mail over that connection, it
        rejected all attempted deliveries due to the configured
        restrictions.  Absence of a queueid is checked for during
        disconnection handling (action: still disconnect; flowchart:~7) and
        if none is found the data is cleaned up and entered in the database
        (action: still disconnect; flowchart:~8; subroutines:
        fixup\_connection and commit\_connection), then removed from memory
        (action: still disconnect; flowchart:~9).

    \item If one or more mails were accepted there will be more log entries
        later, see section~\ref{mail-delivery}.

\end{enumerate}

Re-injection due to forwarding sadly lacks explicit log lines of its
own;\footnote{Previously discussed in section~\ref{complications},
complication 3.}  Re-injection is somewhat awkward to explain because it
overlaps both the mail acceptance and mail delivery sections.  See
section~\ref{tracking-re-injected-mail} for a full discussion.

\subsubsection{Mail delivery}

\label{mail-delivery}

The obvious counterpart to mail entering the system is mail leaving the
system, whether it is by deletion, bouncing, local or remote delivery.  All
three are handled in exactly the same way:

\begin{enumerate}

    \item The sender and recipient addresses will be logged separately
        (action: save\_by\_queueid; flowchart:~10).

    \item Sometimes mail is re-injected and the child mail needs to be
        tracked by the parent mail (action: track; flowchart:~11) --- see
        section~\ref{tracking-re-injected-mail}.

    \item Eventually the mail will be delivered (even if re-injected),
        bounced, or deleted by the administrator (action: commit;
        flowchart:~12).  This is the last log line for this particular mail
        (though it may be indirectly referred to if it was re-injected).
        If it is neither parent nor child of re-injection the data is
        cleaned up and entered in the database (action: still commit;
        flowchart:~14; subroutines fixup\_connection and
        commit\_connection), then deleted from memory (action: still
        commit; flowchart:~15).  For re-injected mails see
        section~\ref{tracking-re-injected-mail}.

\end{enumerate}

I'd like to reiterate that the actions above happen whether the mail is
delivered to a mailbox, piped to a command, delivered to a remote server,
bounced (due to a mail loop, delivery failure, or five day timeout), or
deleted by the administrator.  The exception is when the mail is
re-injected due to forwarding, and that deserves its own section, which is
coming up next.

\subsubsection{Tracking re-injected mail}

\label{tracking-re-injected-mail}

It's probably apparent by now that tracking re-injected mails is the single
most complex part of the parser.  Section~\ref{complications} complication
3 has most of the previous discussion, should you wish to review it.  

XXX EXPAND THIS

there are two implicit indications:

\begin{enumerate}

    \item The more commonly occurring indicator is qmgr selecting a mail
        with a previously unseen queueid for delivery (action:
        qmgr\_chooses\_mail; flowchart:~3), in which case a new data
        structure will be created.  The mail will be marked as faked; this
        flag should be subsequently cleared.  The data is then saved
        (action: still qmgr\_chooses\_mail; flowchart:~3).  

    \item This indicator will be seen for every mail that is re-injected
        but is less common in identifying re-injected mail because it
        typically occurs after the indicator explained above.  Local
        delivery re-injects the mail and logs a successful delivery rather
        than delivering directly to a mailbox or program (action: track;
        flowchart:~11).  In this case the mail may already have been
        created (by qmgr\_chooses\_mail, above); if not a new data
        structure will be created.  In both cases the new mail is marked as
        a child of the parent.

\end{enumerate}


\subsection{Actions in detail}

\label{actions-in-detail}

XXX USE THE SAME DOCUMENTATION HERE AS IN Parser.pm

This list contains all the actions available in the algorithm, in the order
in which they tend to occur in practice.

\begin{description}

    \item [ignore] Do nothing with this line, merely parse it so that
        unparsed lines can be reported.

    \item [connect] Handle a connection from a remote client.  Complains if
        a connection already exists for this client

    \item [restriction\_start] 

    \item [save\_by\_pid] 

    \item [clone] 

    \item [disconnect] 

    \item [save\_by\_queueid] 

    \item [qmgr\_chooses\_mail] 
        however qmgr selects every mail for delivery, so most of the time
        this action merely falls through to saving the information from the
        log line (action: save\_by\_queueid; flowchart:~10).

    \item [track] 

    \item [commit] 

    \item [pickup] 

\end{description}


\section{Limitations and possible improvements}

\label{limitations-improvements}

Every piece of software suffers from some limitations and there is almost
always room for improvement.

\subsection{Limitations}

\begin{itemize}

    \item Each new Postfix release requires new rules to be written to cope
        with the new messages.  Similarly new RBLs, new policy servers and
        new administrator defined rejection messages require new rules.

    \item The program doesn't cope with mails which span log files:
        currently they are reported as unfinished mails at the end of the
        first run and are complained about when encountered during the next
        run because they don't have a source.

    \item It appears that the hostname used in the HELO command is not
        logged if the mail is accepted.\footnote{Tested with Postfix
        2.2.10; this may possibly have changed in Postfix 2.3, or the
        upcoming 2.4.}  It should be reasonably simple to write a policy
        server which causes Postfix to log a warning containing the HELO
        hostname when the DATA command is accepted.\footnote{See
        http://www.postfix.org/SMTPD\_POLICY\_README.html for full
        documentation about policy servers.}


    \item The algorithm does not distinguish between mails where one or
        more mails are rejected and a subsequent mail is accepted; it will
        appear in the database as one mail with lots of rejections followed
        by acceptance.  I don't believe it's possible to make this
        distinction given the data Postfix logs, though it might be
        possible to write a policy server to provide additional
        logging.

    \item The program will not detect parsing the same log file twice,
        resulting in the database containing duplicate entries.

\end{itemize}

\subsection{Possible improvements}
\begin{itemize}

    \item A progress bar would be useful when run interactively, as the
        program takes roughly one minute per 10MB of
        logs.\footnote{Approximately 80\% of the run time is consumed by
        logging to the database.}  Obviously performance is entirely
        dependant on the machine the program is running on.

    \item Write the policy servers referred to in the limitations above.

    \item Save state at the end of each run; load it at the start of the
        next run.  This should allow the program to cope with mails
        spanning log files.  This might also require purging old mails once
        they've been in the state tables for too long, though the presence
        of very old mails indicates a bug in either the implementation or
        the specification.

\end{itemize}

\section{Conclusion}

\appendix


\label{conclusion}

\section{References}

\label{references}

\subsection{Other Postfix log parsers}

\label{other-parsers}

\begin{description}

    \item [pflogsumm.pl] \textit{pflogsumm.pl is designed to provide an
        over-view of postfix activity, with just enough detail to give the
        administrator a "heads up" for potential trouble spots.}
        \url{http://jimsun.linxnet.com/postfix_contrib.html}

    \item [Sawmill Universal Log File Analysis and Reporting] is a
        general purpose commercial product which parses 687 log file
        formats (correct as of 2007/04/15) and produces reports.  I have not
        experimented with it due to its limited data extraction facilities;
        it does have three different sets of data for Postfix (as of
        2007/04/15, one is beta), but they do not appear to be interlinked,
        nor does it save sufficient data for the purposes of this project.
        The source code is available in an obfuscated form only (presumably
        for a fee), and the product is quite expensive, as it requires a
        license for each report which is to be generated. \newline
        \url{http://www.thesawmill.co.uk/formats/postfix.html} \newline
        \url{http://www.thesawmill.co.uk/formats/postfix_ii.html} \newline
        \url{http://www.thesawmill.co.uk/formats/beta_postfix.html}

    \item [Splunk] XXX INVESTIGATE FURTHER \url{http://www.splunk.com}

    \item [Isoqlog] \textit{Isoqlog is an MTA log analysis program written
        in C. It designed to scan qmail, postfix, sendmail and exim logfile
        and produce usage statistics in HTML format for viewing through a
        browser. It produces Top domains output according to Sender,
        Receiver, Total mails and bytes; it keeps your main domain mail
        statistics with regard to Days Top Domain, Top Users values for per
        day, per month and years.}  \newline
        \url{http://www.enderunix.org/isoqlog/}

    \item [AWStats] \textit{AWStats is a free powerful and featureful tool
        that generates advanced web, streaming, ftp or mail server
        statistics, graphically.}
        
        AWStats will produce simple graphs for many different services, but
        that restricts it to supporting the Lowest Common Denominator: the
        data it will extract from an MTA log file is:
        \texttt{time2, email, email\_r, host, host\_r, method, url, code,
        and bytesd.} \newline
        \url{http://awstats.sourceforge.net/} \newline
        \url{http://awstats.sourceforge.net/awstats.mail.html} \newline
        \url{http://awstats.sourceforge.net/docs/awstats_faq.html#MAIL}

    \item [Log analyser - throughput monitor] This utility tracks the
        number of events which occurred over a particular time and warns if
        the frequency of events passes a certain threshold.  It's designed
        to provide real time alerts when dictionary attacks, mail loops or
        similar problems occur.  \newline
        \url{http://home.uninet.ee/~ragnar/throughput_monitor/}

    \item [Anteater] \textit{The Anteater project is a Mail Traffic
        Analyser. Anteater supports currently the logformat produced by
        Sendmail and by Postfix. The tool is written in 100\% C++ and is
        very easy to customize. Input, output, and the analysis are modular
        class objects with a clear interface. There are eight useful
        analyse modules, writing the result in plain ASCII or HTML, to
        stdout or to files.}

        Anteater hasn't been updated since November 2003, and doesn't have
        any English documentation.
        
        \url{http://anteater.drzoom.ch/}

    \item [Yet Another Advanced Logfile Analyser] uses a plugin based
        system to analyse log files and produce output reports.  The core
        code is merely 91 lines long, as all the parsing and report
        generation is handled by modules.  Using YAALA as a base would be
        only slightly less work as both input and output modules would need
        to be written; it may even be more work to implement the parser
        within the constraints of YAALA.

        \url{http://yaala.org/}

    \item [Logparser/Lire] Lire is another general purpose log parser, but
        is developed under the GNU GPL.  Like other general purpose log
        parsers it doesn't extract enough data from Postfix logs to be
        worthwhile.  Development also seems to have stalled, with only a
        minor update since October 2004.  \newline
        \url{http://logreport.org/lire/}

    \item [Logrep] \textit{Logrep is a secure multi-platform framework for
        the collection, extraction, and presentation of information from
        various log files.}

        The parsing Logrep performs is extremely basic, counting the number
        of different processes executed, from address, to addresses, number
        of recipients, bytes transferred and delay.  \newline
        \url{http://www.itefix.no/phpws/index.php}

\end{description}

\subsection{Postfix resources}

\subsection{SMTP references}

XXX ADD RESOURCES/REFERENCES HERE AND THEN MAKE REFERENCES TO THEM FROM WITHIN THE PAPER

\end{document}
