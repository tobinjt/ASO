% $Id$
\documentclass[a4paper,12pt,draft]{article}

% Better typesetting of URLs.
\usepackage{url}
% Include images
\usepackage[final]{graphicx}

\begin{document}

\title{Parsing Postfix log files}
\author{John Tobin \\ School of Computer Science and Statistics \\ 
Trinity College \\ Dublin 2 \\ Ireland \\ tobinjt@cs.tcd.ie}
\date{}
\maketitle

\begin{abstract}
    
    Parsing Postfix logs is much more difficult than it first appears, but
    it is possible.  This paper describes the process required, documenting
    the algorithm and difficulties encountered, with reference to an
    implementation which stores data gathered from the logs in an SQL
    database.

\end{abstract}

XXX WHAT IS THE CONCLUSION??? XXX

\newpage
\tableofcontents

\section{Introduction}

Most mail server administrators will have performed some basic processing
of Postfix logs at one time or another, whether it was to debug a problem,
explain to a user why mail is being rejected, or to check whether their new
anti-spam measures are working.  The more adventurous will have generated
some statistics to show how many hits each of their anti-spam measures have
gotten in the last week, and possibly even generated some graphs to clearly
illustrate the point to management or complaining users.\footnote{This was
the author's first real foray into processing Postfix logs.}  Very few will
have performed in-depth parsing and analysis of their logs, where the
parsing must correlate the log lines per-connection or per-queueid, rather
than processing lines independently.  One of the barriers to this type of
processing is the unstructured nature of Postfix's logs, where each logging
line was added on an ad hoc basis as a requirement was discovered or new
functionality was added.  Further complication arises the ability to have
rejection messages defined by the administrator, and every Real time Black
List returns a different message.  This paper documents the difficult
process of parsing Postfix logs, and presents a program which parses logs
and places the resulting data into a database for further processing by
other applications.


\section{Background}

\subsection{Postfix background}

Postfix is a highly configurable, high performance, secure and scalable
Mail Transport Agent.  It features extensive anti-spam restrictions,
allowing a mail administrator to deploy those restrictions which they judge
suitable for their needs, rather than a fixed set chosen by Postfix's
author.  These restrictions can be applied and combined on a per-host,
per-recipient or per-sender basis, allowing clients/recipients to be
whitelisted with respect to specific restrictions.  Postfix leverages
simple lookup tables to support arbitrarily complicated user-defined
sequences of restrictions and exceptions, with the ultimate in flexibility
being the facility to consult an external process which can implement
whatever logic is required: if you wish you can restrict some users to
sending mail on the third Tuesday after pay day only, if that is of any use
to you.  Administrators can also supply their own rejection messages, to
make it clear to senders why exactly their mail was rejected.
Unfortunately this flexibility has a cost: complexity in the logs
generated.  While it is easy to use \texttt{grep(1)} to determine the fate
of an individual email, following the progress of an email through Postfix
can be quite difficult.  The logs tend to follow a 90\%/10\% pattern: 90\%
of the time following the log entries is simple, but the other 10\% of the
logging requires 90\% of the code.\footnote{These numbers don't have a
solid scientific basis, they're based on gut feeling from writing the
software.}

\subsection{Paper background}

This paper and the program it describes are part of a larger project to
optimise your Postfix restrictions, generate statistics and graphs, and
provide a platform on which new restrictions can be trialled and evaluated
to see if they are worth using in the fight against spam.  The program
parses Postfix logs and populates a database with the data gleamed from
those logs, providing a consistent and simple view of your logs which
future tools can utilise.  The program should be reasonably easily
extensible, so if logging to a database doesn't meet requirements, one
can implement one's own processing.

\subsection{Parser background}

Before getting into detail about the parser, a brief overview is in order.
The parser is split into two parts: the parsing algorithm, and the rules
which are applied to the lines.  

\begin{description}

    \item [Rules] The first matching rule wins: no further rules are tried
        against that line,\footnote{With one exception, detailed later.}
        but there is a facility for the specifying the order of the rules,
        so that more specific rules can be matched first.  Rules are
        completely self-contained: there are no sub-rules, so the
        complexity is linear.  Rules are similar to the tokeniser/Lex part
        of a Lex/YACC parser.

    \item [Algorithm]

\end{description}



\section{Adaptable parsing: user defined rules}

The complexity and variation in Postfix's logs requires similar flexibility
in the parser; decoupling the parsing rules from the associated actions
allows the rules to be loaded from an external source without modifications
to the algorithm source code (significantly lowering the barrier to entry
for new users), and greatly simplifies both algorithm and rules.  The
separation also allows for a clear separation of functionality: rules
handle low level details of identifying a line, whereas the algorithm
handles the higher level details of following the path a mail takes through
postfix, assembling the required information, etc.

Rules are relatively simple, defining the following for each rule:

\begin{description}

    \item [name] A short name for the rule.

    \item [description] A longer, more detailed description of the rule.

    \item [program] The program (smtp, smtpd, qmgr, etc) which the rule
        applies to.  This avoids needlessly trying rules which won't
        match, or worse, might match unintentionally.

    \item [regex] The regex to compare the log line against.  The regex
        will first have several keywords expanded; this simplifies writing
        rules and avoids needless repetition of complex regex components.

    \item [result\_cols and connection\_cols] This is how we extract the
        fields in the log line matched by the regex.  The format is:
        \newline 
        hostname = 1; helo = 2; sender = 4; \newline
        i.e. semi-colon separated assignment statements, with the column
        name on the left and the match from the regex (\$1, \$2 etc) on the
        right hand side (without \$).

    \item [action] The action for the parser to take.

    \item [queueid] The index of the field in regex giving the queueid.

    \item [rule\_order] This is used to sort the rules, so that rules
        matching more commonly occurring log lines will be tried first, and
        is updated every time the program is run.

    \item [priority] This is the user-configurable companion to
        rule\_order: rules with a higher priority will be tried first,
        overriding rule\_order, allowing more specific rules to take
        precedence over more general rules.

    \item [result]  The result the log line represents: rejection,
        acceptance, info, etc.

    \item [result\_data and connection\_data] Sometimes rules need to fake
        a piece of data which isn't present in the log line: the most
        common would be smtp\_code when mail is accepted.  The format is
        the same as result\_cols and connection\_cols.

\end{description}




\section{Parsing algorithm}

While the rules are more than double (treble with 2.3.x rules?) the size of
the parser, the rules are quite simple, and each rule is very much
independent of its fellows.  The parser is significantly more complicated,
and highly interdependent.


\subsection{Naive approach}

A high level view of the algorithm could be expressed as:

\begin{enumerate}

    \item Mail enters the system via SMTP or local submission.

    \item If the mail is rejected, log all data and finish.

    \item Follow the progress of the mail until it's either delivered or
        bounced, log all data, and finish.

\end{enumerate}

Unfortunately it's not that easy.


\subsection{Complications encountered}

\begin{enumerate}

    \item The mail lacks a queueid until it has been accepted, so log lines
        must first be correlated by the smtpd pid, then transition to being
        correlated by the queueid.  This is relatively minor, but does
        require two versions of several functions, \texttt{by\_pid} and
        \texttt{by\_queueid}.

    \item Multiple independent mails may be delivered during one
        connection; this requires cloning the current data, so that
        subsequent mails won't trample over each other.  This must be done
        every time a mail is accepted, as it's impossible to tell in
        advance which connections will accept multiple mails.  It is quite
        easy to overlook this complication because only a small minority of
        connections accept more than one mail. Happily once the mail has
        been accepted log entries won't be correlated by pid for that mail
        any more, so there isn't any ambiguity about which mail a given log
        line belongs to.  The original connection will be discarded when
        the client disconnects.  One possible (as yet unseen) difficulty is
        distinguishing between rejects which belong to the current mail,
        and rejects which belong to previous, unaccepted, mails.

    \item The most difficult complication is that mails are not always
        delivered directly to a mailbox (or program; there is very little
        difference between the two): sometimes they are aliased and
        need to be delivered to an address (or addresses) on another
        server.  When this occurs a child mail will be injected into the
        postfix queue, but without the explicit logging smtpd or sendmail
        injected mails have, so the source is not immediately discernible;
        from a strictly linear reading of the logs it \textit{usually}
        appears as if the child mail has appeared from thin air.
        Subsequently the parent mail will log the creation of the child
        mail:

        \texttt{3FF7C4317: to=<username@example.com>, relay=local, \newline 
        delay=0, status=sent (forwarded as 56F5B43FD)}

        Unfortunately while all log lines from an individual process appear
        in the logs in monotonic order, the order in which log lines from
        different processes are interleaved is subject to the vagaries of
        process scheduling.  In addition the first log line belonging to
        the child mail (the log line cited above properly belongs to the
        parent mail) is logged by qmgr, so depending on how busy qmgr (and
        the server generally) is, the parent's creation log line may appear
        before or after (usually after) the child's log line from
        qmgr.\footnote{Postfix is quite paranoid about mail delivery, an
        excellent characteristic for an MTA to posess, so it won't log that
        the child has been created until it is absolutely certain that the
        mail has been written to disk.  Because the parent then has to be
        informed that the mail has been committed, it's most likely that
        the qmgr will log first.}

        This has the effect that the parser cannot complain when it
        encounters a log line from qmgr for a previously unseen mail; it
        must mark the mail as potentially fake and subsequently clear the
        faked flag if and when the origin of the mail becomes clear.
        Obviously the parser could omit checking of where mails originate
        from, but I feel that it is better to require an explicit source,
        as bugs are more likely to be exposed.

        Process scheduling can have a still more confusing effect: quite
        often the child mail will be created, delivered and removed
        \textbf{before} the parent logs the creation line!  Thus mails
        marked as faked cannot be committed, instead they must be marked as
        commit ready and subsequently committed by the parent mail.  Quite
        apart from identifying mail injected in an unknown fashion, or bugs
        in the parser, faked mails need to be marked as such because they
        lack some data present in the parent mail, and must copy that data
        from their parent.


\end{enumerate}

\subsection{Full algorithm}

Lets start with a flow chart describing the program:

\includegraphics{logparser-flow-chart.ps}
\label{flow-chart}

\section{Limitations and possible improvements}

Every piece of software suffers from some limitations, and there is almost
always room for improvement.

\subsection{Limitations}

\begin{itemize}

    \item Each new Postfix release requires new rules to be written, to
        cope with the new messages.

    \item The program should save state at the end of each run, and reload
        it at the start of the next run, to properly cope with mails which
        span logs.  This might also require purging old mails once they've
        been in the state tables for too long.

    \item It appears that the hostname used in the HELO command is not
        logged if the mail is accepted.\footnote{Tested with Postfix
        2.2.10, this may possibly have changed in Postfix 2.3, or the
        upcoming 2.4.}

\end{itemize}

\subsection{Possible improvements}
\begin{itemize}

    \item A progress bar would be useful when run interactively, as the
        program takes roughly one minute per 10MB of
        logs.\footnote{Approximately 80\% of the run time is consumed by
        logging to the database.}  Obviously performance is entirely
        dependant on the machine the program is running on.

\end{itemize}

\section{Conclusion}

\appendix



\end{document}
