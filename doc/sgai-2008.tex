% $Id$
\documentclass{svmult}
%\documentclass[draft]{svmult}

% Useful stuff for math mode.
\usepackage{amstext}
% Include images
\usepackage[final]{graphicx}
\usepackage{url}

\usepackage{lastpage}

% Extra footnote functionality, including references to earlier footnotes.
\usepackage[bottom]{footmisc}

% Tell Latex to use scalable fonts
\usepackage{type1cm}

% Extra packages recommended by Springer.
\usepackage{mathptmx}
\usepackage{helvet}
\usepackage{courier}
\usepackage{makeidx}
\usepackage{multicol}

% Embed SVN Id etc.
\usepackage{svn}

% Acronyms and glossary entries.
% global=true prevents acronyms being expanded twice
\usepackage[global=true,acronym=true]{glossary}
%\makeglossary{}
%\makeacronym{}

% Nicer citation lists - smaller spaces.
\usepackage{cite}
% Check for unused references.
% \usepackage{refcheck}
\usepackage{varioref}
% Warnings rather than errors when references cross page boundaries.
\vrefwarning{}
% Errors when loops are encountered
\vrefshowerrors{}

% \showgraph{filename}{caption}{label}
\newcommand{\showgraph}[3]{%
    \begin{figure}[btp]%
        \includegraphics{#1}%
        \caption{#2}\label{#3}%
    \end{figure}%
}

%\showtable{filename}{caption}{label}
\newcommand{\showtable}[3]{
    \begin{table}[btp]
        \caption{#2}\label{#3}
        \input{#1}
    \end{table}
}

\newcommand{\tabletopline}[0]{%
    \hline%
    \noalign{\smallskip}%
}

\newcommand{\tablebottomline}[0]{%
    \noalign{\smallskip}%
    \hline%
}

\newcommand{\tablemiddleline}[0]{%
    \noalign{\smallskip}%
    \hline%
    \noalign{\smallskip}%
}

% Replacement for \ref{}, adds the page number too.
\newcommand{\refwithpage}[1]{%
    \empty{}\vref{#1}%
}
% Shorten the text used for references with page numbers.
\renewcommand{\reftextfaraway}[1]{%
    [p.~\pageref{#1}]%
}
% section references, automatically add \textsection
\newcommand{\sectionref}[1]{%
    \textsection{}\vref*{#1}%
}

\newcommand{\refwithlabel}[2]{%
    #1~\vref{#2}%
}
% table references, for consistent formatting.
\newcommand{\graphref}[1]{%
    \refwithlabel{fig.}{#1}%
}
\newcommand{\Graphref}[1]{%
    \refwithlabel{Figure}{#1}%
}
\newcommand{\tableref}[1]{%
    \refwithlabel{table}{#1}%
}
\newcommand{\Tableref}[1]{%
    \refwithlabel{Table}{#1}%
}

% A command to format a Postfix daemon's name
\newcommand{\daemon}[1]{%
    \texttt{postfix/#1}%
}

\newcommand{\tab}[0]{%
    \hspace*{2em}%
}

\newcommand{\numberOFlogFILES}[0]{%
    93%
}

\newcommand{\numberOFrules}[0]{%
    169%
}

\newcommand{\numberOFruleINTERSECTIONS}[0]{%
    14196%
}

\newcommand{\numberOFrulesMINIMUM}[0]{%
    115%
}

\newcommand{\numberOFlogFILESall}[0]{%
    522%
}

% \numberOFrulesMINIMUM as percentage of \numberOFrules
\newcommand{\numberOFrulesMINIMUMpercentage}[0]{%
    68.04\%%
}

% \numberOFrules as percentage increase of \numberOFrulesMINIMUM
\newcommand{\numberOFrulesMAXIMUMpercentage}[0]{%
    46.95\%%
}

\newcommand{\numberOFlogLINES}[0]{%
    60,721,709%
}

\newcommand{\numberOFlogLINEShuman}[0]{%
    60.72 million%
}

\newcommand{\numberOFactions}[0]{%
    18%
}

\begin{document}

\title*{A User-Extensible and Adaptable Parser Architecture}
\author{John Tobin and Carl Vogel}
\institute{John Tobin \email{tobinjt@cs.tcd.ie}
            \and{} Carl Vogel \email{vogel@cs.tcd.ie}
            \at{} School of Computer Science and Statistics,
                    Trinity College, Dublin 2, Ireland.
                    \newline{}
                    Supported by Science Foundation Ireland RFP
                    05/RF/CMS002.
            }

\maketitle

\abstract{%
    Some parsers need to be very precise and strict in what they parse, yet
    must allow users to easily adapt or extend the parser to parse new
    inputs, without requiring that the user have an in-depth knowledge and
    understanding of the parser's internal workings.  This paper presents a
    novel parsing architecture, designed for parsing Postfix log files,
    that aims to make the process of parsing new inputs as simple as
    possible, enabling users to trivially add new rules (to parse variants
    of existing inputs) and relatively easily add new actions (to process a
    previously unknown class of input).  The architecture scales linearly
    as the number of rules and/or size of input increases, making it
    suitable for parsing large corpora or months of accumulated data.
}

%\SVN$Id$
%\SVNId{}  \today{}

\newacronym{PDA}{Pushdown Automata}{}
\newacronym{FA}{Finite Automata}{}
\newacronym{CFL}{Context-Free Languages}{}
\newacronym{SMTP}{Simple Mail Transfer Protocol}{}
\newacronym{ATN}{Augmented Transition Networks}{}
\newacronym{DNSBL}{DNS Black List}{}

\section{Introduction}

\label{Introduction}

The architecture described herein was developed as part of a larger project
to improve anti-spam techniques by analysing the performance of the set of
filters currently in use, optimising the order and membership of the set
based on that analysis, and developing supplemental filters where
deficiencies are identified.  Most anti-spam techniques are
content-based~\cite{a-plan-for-spam, word-stemming, relaxed-online-svms}
which requires accepting the mail in the first place to determine if it is
spam, but rejecting mail during the delivery attempt is preferable: senders
of non-spam mail that is mistakenly rejected will receive an immediate
non-delivery notice; load is reduced on the accepting mail server (allowing
more intensive content-based techniques to be used); users have less mail
to wade through.  Improving the performance of anti-spam techniques that
are applied when mail is being transferred via \SMTP{}\footnotemark{} is
the goal of this project, by providing a platform for reasoning about
anti-spam filters.  The approach chosen to measure performance is to
analyse the log files produced by the \SMTP{} server in use,
Postfix~\cite{postfix}, rather than modifying it to generate statistics:
this approach improves the chances of other sites testing and utilising the
software.  The need arose for a parser capable of dealing with the great
number and variety of log lines produced by Postfix: the parser must be
designed so that adding support for parsing new inputs is a simple task, as
the log lines to be parsed will change over time.  The variety in log lines
occurs for several reasons:\footnotetext{Simple Mail Transfer Protocol
transfers mail across the Internet from the sender to one or more
recipients.  It is a simple, human readable, plain text protocol, making it
quite easy to test and debug problems with it.  The original protocol
definition is RFC~821~\cite{RFC821}, updated in RFC~2821~\cite{RFC2821}.}

\begin{itemize}

    \item Log lines differ between versions of Postfix.
        
    \item The mail administrator can define custom rejection messages.

    \item External resources utilised by Postfix (e.g.\ \DNSBL{} or policy
        servers~\cite{policy-servers}) can also change their messages,
        sometimes without warning.

\end{itemize}

It was hoped to reuse an existing parser rather than starting from scratch:
several parsers were considered and rejected because: they parsed too small
a fraction of the log files; their parsing was too inexact; and/or they did
not extract sufficient data.  The effort required to adapt and improve an
existing parser was judged to be greater than the effort to write one, as
the techniques used by the existing parsers severely limited their
potential: some ignored the majority of log lines, parsing specific log
lines with great accuracy, but without any provision for readily parsing
new or similar log lines; others sloppily parsed the majority of log lines,
but were incapable of distinguishing between log lines of the same type,
e.g.\ rejections, and extracted only a small portion of the data required.
The only prior published work on the subject of parsing Postfix log files
that we are aware of is~\cite{log-mail-analyser}, which aims to present
correlated log data in a form suitable for a systems administrator to
search using the myriad of standard Unix text processing utilities already
available.  A full state of the art review is outside the scope of this
paper but will be included in the thesis resulting from this work; the
relationship to another procedural framework (ATN) is discussed below.

The solution developed is conceptually simple: provide a few generic
functions (\textit{actions\/}), each capable of dealing with an entire
category of log lines (e.g.\ rejecting a mail), accompanied by a multitude
of precise patterns (\textit{rules\/}), each of which matches all log lines
of a specific type and only that type (e.g.\ rejection by a specific
\DNSBL{}).  It is an accepted standard to separate the parsing procedure
from the declarative grammar it operates with; part of the novelty here is
in the way that the grammar is itself partially procedural (each action is
a separate procedure).  This architecture is ideally suited to parsing
inputs where the input is not fully understood or does not conform to a
fixed grammar: the architecture warns about unparsed inputs and other
errors, but continues on parsing as best it can, allowing the developer of
a new parser to decide which deficiencies are most important and require
attention first, rather than being forced to fix the first error that
arises.

\section{Architecture}

\label{Architecture}

The architecture is split into three sections: framework, actions and
rules.  Each will be discussed separately, but first an overview:

\begin{description}

    \item [Framework]  The framework is the structure that actions and
        rules plug into.  It provides the parsing loop, shared data
        storage, loading and validation of rules, storage of results, and
        other support functions.

    \item [Actions]  Each action performs the work required to deal with a
        category of inputs, e.g.\ processing data from rejections.

    \item [Rules]  The rules are responsible for classifying or matching
        inputs, specifying the action to invoke and the regex that matches
        the inputs and extracts data.

\end{description}

For each input the framework tries each rule in turn until it finds a rule
that matches the input, then invokes the action specified by that rule.

Decoupling the parsing rules from their associated actions and the
framework allows new rules to be written and tested without requiring
modifications to the parser source code, significantly lowering the barrier
to entry for casual users who need to parse new inputs, e.g.\ part-time
systems administrators attempting to combat and reduce spam.  Decoupling
the actions from the framework simplifies both framework and actions: the
framework provides services to the actions, but does not need to perform
any tasks specific to the input being parsed; actions benefit from having
services provided by the framework, freeing them to concentrate on the task
of accurately and correctly processing the information provided by rules.

Decoupling also creates a clear separation of functionality: rules handle
low level details of identifying inputs and extracting data; actions handle
the higher level details of assembling the required data, dealing with the
intricacies of the input being parsed, complications arising, etc.;\ the
framework provides services to actions and manages the parsing process.

Some similarity exists between the architecture and William Wood's
\ATN{}~\cite{atns, nlpip}, a tool used in Computational Linguistics for
creating grammars to parse or generate sentences.  The resemblance between
\ATN{} and the architecture (shown in \tableref{Similarities with ATN}) is
accidental, but it is obvious that the two approaches share a similar
division of responsibilities, although having different semantics.

% Do Not Reformat!

\begin{table}[hbtp]
    \caption{Similarities with ATN}\label{Similarities with ATN}
    % The \smallskip{} below stop the text hitting the lines.
    \begin{tabular}[]{lll}
        \tabletopline{}%
        ATN           & Parser Architecture & Similarity                                \\
        \tablemiddleline{}%
        Networks      & Framework           & Determines the sequence of transitions or \\
                      &                     & actions that constitutes a valid input.   \\
        Transitions   & Actions             & Saves data and imposes conditions the     \\
                      &                     & input must meet to be accepted as valid.  \\
        Abbreviations & Rules               & Responsible for classifying input.        \\
        \tablebottomline{}%
    \end{tabular}
\end{table}

\subsection{Framework}

\label{Framework}

The framework takes care of miscellaneous support functions and low level
details of parsing, freeing the programmers writing actions to concentrate
on writing productive code.  It provides the link between actions and
rules, allowing either to be improved independently of the other.  It
provides shared storage to pass data between actions, loads and validates
rules, manages parsing, invokes actions, tracks how often each rule matches
to optimise rule ordering (\sectionref{Rule ordering}), and stores results
in the database.  The framework is largely independent of the parser being
written: the same functionality will be required for most or all parsers,
though some specialised support functions may be provided.  The framework
is the core of the architecture and is deliberately quite simple: the rules
deal with the variation in inputs, and the actions deal with the
intricacies and complications encountered when parsing.

The function that finds the rule matching the input and invokes the
requested action can be expressed in pseudo-code as:

% DO NOT REFORMAT!

\begin{verbatim}
for each input:
    for each rule defined by the user: 
        if this rule matches the input:
            perform the action specified by the rule
            skip the remaining rules
            process the next input
    warn the user that the input was not parsed
\end{verbatim}

\subsection{Actions}

\label{Actions}

Each action contains the code required to deal with a particular category
of input, e.g.\ rejections.  The actions are parser-specific: each parser
author will need to write the required actions from scratch, unless
extending an existing parser.  It is anticipated that parsers utilising
this architecture will have a high ratio of rules to actions, with the aim
of having simpler rules and clearer distinctions between the inputs parsed
by different rules.  In the Postfix log parser developed for this project
there are \numberOFactions{} actions and \numberOFrules{} rules, with an
uneven distribution of rules to actions as shown in \graphref{Distribution
of rules per action}.
\showgraph{build/graph-action-distribution}{Distribution of rules per
action}{Distribution of rules per action} Unsurprisingly the action with
the most associated rules is \texttt{DELIVERY\_REJECTED}, the action that
handles Postfix rejecting a mail delivery attempt; it is followed by
\texttt{SAVE\_DATA}, the action responsible for handling informative log
lines, that supplement the data gathered from other log lines but do not by
themselves represent events.  The third most common action is, perhaps
surprisingly, \texttt{UNINTERESTING}: this action does nothing when
executed, allowing uninteresting log lines to be parsed without causing any
effects (it does not imply that the input is ungrammatical or unparsed).
Generally rules specifying the \texttt{UNINTERESTING} action are rules to
parse log lines that are not associated with a specific mail, e.g.\ notices
about configuration files changing.  The remaining actions only have one or
two associated rules: in some cases this is because there will only ever be
one log line variant for that action, e.g.\ all log lines showing that a
remote client has connected are matched by a single rule and handled by the
\texttt{CONNECT} action; in other cases the action is required to address a
deficiency in the log files, or a complication that arises during parsing.
Taking the \texttt{CONNECT} action as an example, it creates a new data
structure in memory for each client connecting, and saves the data
extracted by the rule; this data will subsequently be entered into the
database.  If a data structure already exists for that connection it is
treated as a symptom of a bug, and the action issues a warning containing
the full contents of the existing data structure, plus the log line that
has just been parsed.

The ability to add special purpose actions to deal with difficulties and
new requirements that are discovered during parser development is one of
the strengths of this architecture.  Instead of writing a single monolithic
function that must be modified to support new behaviour, with all the
attendant risks of adversely affecting the existing parser, when a new
requirement arises an independent action can be written to satisfy it.
Sometime the new action will require the cooperation of other actions,
e.g.\ to set or check a flag.  There is a possibility of introducing
failure when modifying existing actions, but the modifications will be
smaller and occur less frequently than with a monolithic architecture, thus
failures will be less likely and will be easier to test for and diagnose.
The architecture can be implemented in an object oriented style, allowing
sub-classes to extend or override actions in addition to adding new
actions; because each action is an independent procedure, the sub-class
need only modify the action it is overriding, rather than reproducing large
chunks of functionality.

During development of the Postfix log parser it became apparent that in
addition to the obvious variety in log lines, there were many complications
to be overcome.  Some were the result of deficiencies in Postfix's logging
(some of which were improved on or resolved by later versions of Postfix);
others were due to the vagaries of process scheduling, client behaviour,
and administrative actions.  All were successfully accommodated in the
Postfix log parser: adding new actions was enough to overcome several of
the complications; others required modifications to a single existing
action to work around the difficulties; the remainder were resolved by
adapting existing actions to cooperate and exchange extra data, changing
their behaviour as appropriate.

Actions may return a modified input line that will be parsed as if read
from the input stream, allowing for a simplified version of cascaded
parsing~\cite{cascaded-parsing}.  This is a powerful facility allowing
several rules and actions to parse a single input.

\subsection{Rules}

\label{Rules}

Rules categorise inputs, specifying both the regex to match against each
input and the action to invoke when the match is successful.  The Postfix
log parser stores the rules it uses in the same SQL database the results
are stored in, removing any doubt about which set of rules was used to
produce a set of results; other implementations are free to store the rules
in whatever fashion suits their needs.  The framework warns about every
unparsed input, to alert the user that they need to alter or extend their
ruleset; the Postfix log parser successfully parses every log line in the
\numberOFlogFILESall{} log files it is currently being tested with.  The
framework requires each rule to have \texttt{action} and \texttt{regex}
attributes; each implementation is free to add any additional attributes it
requires.  The Postfix log parser adds several attributes to optimise rule
ordering (\sectionref{Rule ordering}), to restrict which log lines each
rule can be matched against, and to describe each rule.  Data is
automatically extracted and saved from the log line based on keywords in
the rule's regex, but there is also a mechanism to specify extra data to be
saved.

Parsing new inputs is generally achieved by creating a new rule that pairs
an existing action with a new regex.  Adding new rules is simple: define
all the required attributes and append the new rule to wherever the
existing rules are stored.  The Postfix log parser supplies a utility based
on Simple Logfile Clustering Tool~\cite{slct-paper} to aid in producing
regexes from previously unparsed log lines.  Other approaches to rule
management can be used, e.g.\ instead of manually adding new rules, machine
learning techniques could be used to automatically generate new rules,
based on the existing rules and input lines.  If this approach was taken
the choice of machine learning technique would be constrained by the size
of typical data sets (see \sectionref{Results}).  Techniques that require
the full data set when training would be impractical: Instance Based
Learning~\cite{instance-based-learning} techniques that automatically
determine which inputs from the training set are valuable and which inputs
can be discarded might reduce the data required to a manageable size.  This
avenue of research and development has not been pursued by the authors, but
could easily be undertaken independently.

The architecture does not try to detect overlapping rules: that
responsibility is left to the author of the rules.  Unintentionally
overlapping rules lead to inconsistent parsing and data extraction because
the first matching rule wins, and the order in which rules are tried
against each input might change between parser invocations.  Overlapping
rules are frequently a requirement, allowing a more specific rule to match
some inputs and a more general rule to match the remainder, e.g.\
separating \SMTP{} delivery to specific sites from \SMTP{} delivery to the
rest of the world.  This can simplify both the general rule and the more
specific rule; additionally rules from different sources can be combined
with a minimum of prior cooperation or modification required.  Overlapping
rules should have a priority attribute to specify their relative ordering;
negative priorities may be useful for catchall rules.

Storing the rules separately to the parser allows external tools to be
written to detect overlapping rules.  Traditional regexes are equivalent in
computational power to \FA{}, and it is possible to convert between the
two, so regex overlap can be detected by finding a non-empty intersection
of two \FA{}\@.  The standard equation for \FA{} intersection (given for
example in~\cite{intersection-of-NFA-using-Z}) is: $FA1 \cap{} FA2 =
\overline{(\overline{FA1} \cup{} \overline{FA2})}$.  Perl 5.10 regexes are
more powerful than traditional regexes, e.g.\ it is possible to match
correctly balanced brackets nested to an arbitrary depth:
\newline{}\tab{}\verb!/^[^<>]*(<(?:(?>[^<>]+)|(?1))*>)[^<>]*$/!\newline{}
Perl 5.10 regexes can maintain an arbitrary state stack and are thus
equivalent in computational power to \PDA{} or \CFL{}, so detecting overlap
may require calculating the intersection of two \PDA{} or \CFL{}\@.  The
intersection of two \CFL{} is not closed, i.e.\ the resulting language
cannot in all cases be parsed by a \CFL{}, so intersection may be
intractable in some cases e.g.:
$a^{*}b^{n}c^{n}~\cap~a^{n}b^{n}c^{*}~\rightarrow~a^{n}b^{n}c^{n}$.

Detecting overlap between $n$ regexes requires $n(n-1)/2$ intersections to
be calculated, resulting in $O(n^2x)$ complexity, where $O(x)$ is the
complexity of calculating intersection.  This is certainly not a task to be
performed on every parsing run: detecting overlap amongst the Postfix log
parser's \numberOFrules{} rules would require calculating
\numberOFruleINTERSECTIONS{} intersections.

It is possible to define pathological regexes that fall into two main
categories: regexes that match every input, and regexes that consume
excessive amounts of CPU time during matching.  Defining a regex that
matches every input is trivial: \verb!/^/! matches the start of every
input.  In general a regex will not consume much CPU time when the match is
successful; usually CPU time is consumed when a regex with a lot of
alteration and variable quantifiers fails to match.  This topic is beyond
the scope of this paper, see~\cite{mastering-regular-expressions} for
in-depth discussion.

The example rule in \tableref{Example rule} matches the following sample
log line logged by Postfix when a remote client connects to deliver mail:
\newline{}\tab{}\verb!connect from client.example.com[192.0.2.3]!

% Do not reformat this!
\begin{table}[htbp]
    \caption{Example rule}
    \empty{}\label{Example rule}
    \begin{tabular}[]{ll}
        \tabletopline{}%
        Attribute                 & Value                                            \\
        \tablemiddleline{}%
        %program                   & \daemon{smtpd}                                   \\
        regex                     & \verb!^connect from (__CLIENT_HOSTNAME__)\[(__CLIENT_IP__)\]$! \\
        action                    & CONNECT                                          \\
        %priority                  & 0                                                \\
        \tablebottomline{}%
    \end{tabular}
\end{table}

\subsection{Architecture Characteristics}

\label{Architecture characteristics}

\begin{description}

    \item [Context-free rules:]  Rules are context-free: they do not take
        into account past or future inputs.  In context-free grammar terms
        the parser rules could be described as:
        $\text{\textless{}input\textgreater{}} \mapsto \text{rule-1} |
        \text{rule-2} | \text{rule-3} | \dots | \text{rule-n}$

    \item [Partially context-sensitive actions and cascaded parsing:]
        Actions can consult the results (or lack of results) of previous
        actions during execution, providing some context sensitivity.
        Actions can not inspect past or future inputs, but actions can
        return a modified input that will be parsed as if read from the
        input stream, allowing for a simplified version of cascaded
        parsing~\cite{cascaded-parsing}.

    \item [Matching rules against inputs is simple:]  The first matching
        rule determines the action that will be invoked: there is no
        backtracking to try alternate rules, no attempt is made to pick a
        \textit{best\/} rule.

    \item [Transduction:]  The parser can be thought of as implementing
        transduction: it takes data in one form (log files) and transforms
        it to another form (a database); other output formats may be more
        suitable for other implementations.

    \item [Closer to Natural Language Processing than using a fixed
        grammar:] Unlike traditional parsers such as those used when
        compiling a programming language, this architecture does not
        require a fixed grammar specification that inputs must adhere to.
        The architecture is capable of dealing with interleaved inputs, out
        of order inputs, and ambiguous inputs where heuristics must be
        applied --- all have arisen and been successfully accommodated in
        the Postfix log parser.

    \item [Line oriented:]  The architecture is line oriented at present:
        there is no facility for rules to consume more input or push unused
        input back onto the input stream.  This was not a deliberate
        decision, rather a consequence of the line oriented nature of the
        Postfix log files; more flexible approaches could be pursued.

\end{description}

\section{Results}

\label{Results}

Parsing efficiency is an obvious concern when the Postfix log parser
routinely needs to parse large log files.  The mail server which generated
the log files used in testing the Postfix log parser accepts approximately
10,000 mails for 700 users per day.  Median log file size is 50MB,
containing 285,000 log lines --- large scale mail servers would have much
larger log files.  When generating the timing data used in this section,
\numberOFlogFILES{} log files (totaling 10.08 GB, \numberOFlogLINEShuman{}
log lines) were each parsed 10 times and the parsing times averaged.
Saving results to the database was disabled for the test runs, as the tests
are aimed at measuring the speed of the Postfix log parser rather than the
speed of the database.  The computer used for test runs is a Dell Optiplex
745 described in \tableref{Computer used to generate statistics}; it was
dedicated to the task of gathering statistics from test runs, and was not
used for any other purpose while test runs were ongoing --- any services
not necessary for running the tests were disabled.  Parsing all
\numberOFlogFILES{} log files in one run took
\input{build/include-timing-run-duration}.

\begin{table}[htbp]
    \caption{Computer used to generate statistics}
    \empty{}\label{Computer used to generate statistics}
    \begin{tabular}[]{ll}
        \tabletopline{}%
        Component  & Component used                                 \\
        \tablemiddleline{}%
        CPU        & 1 dual core 2.40GHz Intel\textregistered{}
                     Core\texttrademark{}2 CPU,                      
                     with 32KB L1 and 4MB L2 cache.                 \\
        RAM        & 2GB 667 MHz DDR RAM\@.                         \\
        Hard disk  & 1 Seagate Barracuda 7200 RPM 250GB SATA disk.  \\
        \tablebottomline{}%
    \end{tabular}
\end{table}

\subsection{Architecture Scalability: Input Size}

An important property of a parser is how parsing time scales relative to
input size: linearly, polynomially, or exponentially?  \Graphref{Parsing
time, file size, and number of log lines} shows the parsing time in
seconds, file size in MB, and number of log lines in tens of thousands, for
each of the \numberOFlogFILES{} log files.  All three lines on the graph
run roughly in parallel, giving the impression that the algorithm scales
linearly with input size.  This impression is borne out by
\graphref{parsing time vs file size vs number lines factor}: the ratios are
tightly banded across the graph, showing that the algorithm scales
linearly.  The ratios increase (i.e.\ improve) for log files 22 and 62--68
despite their large size; that unusually large size is due to a mail
forwarding loop resulting in a greatly increased number of mails delivered
and log lines generated.

\showgraph{build/graph-input-size-vs-parsing-time}{Parsing time, file size,
and number of log lines}{Parsing time, file size, and number of log lines}
\showgraph{build/graph-input-size-vs-parsing-time-ratio}{Ratio of number of
log lines and file size to parsing time}{parsing time vs file size vs
number lines factor}

\subsection{Rule Ordering}

\label{Rule ordering}

At the time of writing the Postfix log parser has \numberOFrules{}
different rules: as shown in \graphref{rule hits graph} 10\% of the rules
match the vast majority of log lines, with the remaining log lines split
across the other 90\% of the rules, similar to a Power Law distribution.
Assuming that the distribution of log lines is reasonably consistent over
time, parser efficiency should benefit from trying more frequently matching
rules before those which match less frequently.  To test this hypothesis
three full test runs were performed with different rule orderings:

\begin{description}[optimal]

    \item [optimal]  Hypothetically the best order: rules which match most
        often will be tried first.

    \item [shuffle] Random ordering, intended to represent an unsorted rule
        set.  Note that the ordering will change every time the parser is
        executed, so 10 different orderings will be generated for each log
        file in the test run.

    \item [reverse] Hypothetically the worst order: the most frequently
        matching rules will be tried last.

\end{description}

\Graphref{Parsing time relative to shuffled} shows the parsing times of
optimal and reversed orderings as a percentage of the parsing time of
shuffled ordering.  Overall this optimisation provides a modest but
worthwhile reduction in parsing time of approximately 10\%.  Optimal rule
ordering is also beneficial in other ways, as described in
\sectionref{Architecture Scalability: Number of Rules}.
\showgraph{build/graph-hits}{Number of log lines matched by each rule}{rule
hits graph} \showgraph{build/graph-optimal-and-reverse-vs-shuffle}{Parsing
time relative to shuffled ordering}{Parsing time relative to shuffled}

\subsection{Architecture Scalability: Number of Rules}

\label{Architecture Scalability: Number of Rules}

How any architecture scales as the number of rules increases is important,
but it is particularly important for this architecture because it is
expected that typical parsers will have a large number of rules.  There are
\numberOFrules{} rules in the full Postfix log parser ruleset, whereas the
minimum number of rules required to parse the \numberOFlogFILES{} log files
is \numberOFrulesMINIMUM{}, \numberOFrulesMINIMUMpercentage{} of the full
ruleset.  A second set of statistics was generated using the minimum
ruleset, and the parsing times compared to those generated using the full
ruleset: the percentage parsing time increase when using the full ruleset
instead of the minimal ruleset for optimal, shuffled and reversed orderings
is shown in \graphref{Percentage parsing time increase of maximum ruleset
over minimum ruleset}.  Clearly that the increased number of rules has a
noticeable performance impact with reverse ordering, and a lesser impact
with shuffled ordering.  The optimal ordering shows a mean increase of
\input{build/include-full-ruleset-vs-minimum-ruleset} in parsing time for a
\numberOFrulesMAXIMUMpercentage{} increase in the number of rules.  These
results show that the architecture scales very well as the number of rules
increases, and that optimally ordering the rules is an important
optimisation enabling this.
\showgraph{build/graph-full-ruleset-vs-minimum-ruleset}{Percentage parsing
time increase of maximum ruleset over minimum ruleset}{Percentage parsing
time increase of maximum ruleset over minimum ruleset}

\subsection{Coverage}

\label{coverage}

The Postfix log parser has two different types of coverage to be measured:
log lines correctly parsed, and mail delivery attempts correctly understood
(the former is a requirement for the latter to be achieved).  Improving the
former is less intrusive, as usually it just requires new rules to be
written; improving the latter is more intrusive as it requires changes to
actions, and it can also be much harder to notice that a deficiency exists.

Correct parsing of log lines must be measured first.  Warnings are issued
for any log lines that are not parsed; no such warnings are issued while
parsing the \numberOFlogFILES{} log files, so it can be safely concluded
that there are zero false negatives.  False positives are harder to
quantify: manually verifying that the correct rule parsed each of the
\numberOFlogLINES{} log lines is infeasible.  A random sample of 6,039 log
lines (0.00994\% of \numberOFlogLINES{}) was parsed and the results
manually verified by inspection to ensure that the correct rule parsed each
log line.  The sample results contained zero false positives, and this
check has been automated to ensure continued accuracy.  The authors are
confident that zero false positives occur when parsing the
\numberOFlogFILES{} log files.

The proportion of mail delivery attempts correctly understood is much more
difficult to determine accurately than the proportion of log lines
correctly parsed.  The implementation can dump its state tables in a human
readable form; examining these tables with reference to the log files and
database is the best way to detect mails that were not handled properly.
The implementation issues warnings when it detects any errors or
discrepancies, which should alert the user to a problem.  There should be
few or no warnings during parsing, and when parsing is finished the state
table should only contain entries for mails starting before or ending after
the log file.  A second sample of 6000 log lines was parsed with all
debugging options enabled, resulting in 167,448 lines of output.  All
167,448 lines were examined in conjunction with the log segment and a dump
of the resulting database, verifying that for each of the log lines the
implementation performed correctly.  The implementation produced 4 warnings
about deficiencies in the log segment, 10 mails correctly remaining in the
state tables, and 1625 correct entries in the database.  No error messages
were produced, therefore there were no false negatives.  Given the evidence
detailed above, the authors are confident that the false positive rate when
understanding a mail delivery attempt is zero.

Experience implementing the Postfix log parser shows that full input
coverage is relatively easy to achieve with this architecture, and that
with sufficient time and effort full understanding of the input is
possible.  Postfix log files would require substantial time and effort to
correctly parse regardless of the architecture used; this architecture
enables an iterative approach to be used (similar to Stepwise
Refinement~\cite{stepwise-refinement}), as is practiced in many other
software engineering disciplines.

\section{Conclusion}

This architecture's greatest strength is the ease with which it can be
adapted to deal with new requirements and inputs.  Parsing a variation of
an existing input is a trivial task: simply modify an existing rule or add
a new rule with an appropriate regex and the task is complete.  Parsing a
new category of input is achieved by writing a new action and appropriate
rules; quite often the new action will not need to interact with existing
actions, but when interaction is required the framework provides the
necessary facilities.  The decoupling of rules from actions allows
different sets of rules to be used with the same actions, e.g.\ a parser
might have actions to process versions one and two of a file format; the
parser will then parse version one, or version two, or both versions,
depending on the ruleset selected.  The architecture makes it possible to
apply commonly used programming techniques such as object orientation,
inheritance, composition, delegation, and roles when designing and
implementing a parser, simplifying the process of working within a team
with shared responsibility for a parser, or when developing and testing
additional functionality.  This architecture is ideally suited to parsing
inputs where the input is not fully understood or does not follow a fixed
grammar: the architecture warns about unparsed inputs and other errors, but
continues on parsing as best it can, allowing the developer of a new parser
to decide which deficiencies are most important and require attention
first, rather than being forced to fix the first error that arises.

The dataset gathered by the Postfix log parser provides the foundation for
the future of this project: applying machine-learning algorithms to the
data to analyse and optimise the set of rules in use, followed by
identifying patterns in the data that could be used to write new filters to
recognise and reject spam rather than accepting it.  The parser provides
the data in a normalised form that is far easier to use as input to new or
existing implementations of machine-learning algorithms than trying to
adapt each algorithm to extract data directly from the log files.  The
current focus is on clustering and decision trees to optimise the order in
which rules are applied; future efforts will involve using data gathered by
the parser to train and test new filters.  This task is very similar to
optimising a black box application based on its inputs and outputs, and
this approach could be applied to optimising the behaviour of any system
given sufficient log messages to analyse.  An alternate approach to black
box optimisation that uses application profiling in conjunction with the
application's error messages to improve the error messages shown to users
is described in~\cite{black-box-error-reporting}; profiling data may be
useful in supplementing systems that fail to provide adequate log messages.

The Postfix log file parser based on this architecture provides a basis for
systems administrators to monitor the effectiveness of their anti-spam
measures and adapt their defences to combat new techniques used by those
sending spam.  This parser is a fully usable application, built to address
a genuine need, rather than a proof of concept whose sole purpose is to
illustrate a new idea; it deals with the oddities and difficulties that
occur in the real world, rather than a clean, idealised scenario developed
to showcase the best features of a new approach.

\bibliographystyle{spmpsci}
\bibliography{logparser-bibliography.bib}

\end{document}
