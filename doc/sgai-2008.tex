% $Id$
\documentclass[draft]{svmult}

% Useful stuff for math mode.
\usepackage{amstext}
% Include images
\usepackage[final]{graphicx}
\usepackage{url}

\usepackage{lastpage}

% Extra footnote functionality, including references to earlier footnotes.
\usepackage[bottom]{footmisc}

% Tell Latex to use scalable fonts
\usepackage{type1cm}

% Extra packages recommended by Springer.
\usepackage{mathptmx}
\usepackage{helvet}
\usepackage{courier}
\usepackage{makeidx}
\usepackage{multicol}

\setlength{\floatsep}{0pt}
\setlength{\intextsep}{0pt}
\setlength{\textfloatsep}{0pt}

% Nicer citation lists - smaller spaces.
\usepackage{cite}
% Check for unused references.
% \usepackage{refcheck}
\usepackage{varioref}
% Warnings rather than errors when references cross page boundaries.
\vrefwarning{}
% Errors when loops are encountered
\vrefshowerrors{}

% \showgraph{filename}{caption}{label}
\newcommand{\showgraph}[3]{%
    \begin{figure}[hbtp]%
        \caption{#2}\label{#3}%
        \includegraphics{#1}%
    \end{figure}%
}

%\showtable{filename}{caption}{label}
\newcommand{\showtable}[3]{
    \begin{table}[hbtp]
        \caption{#2}\label{#3}
        \input{#1}
    \end{table}
}

\newcommand{\tableline}[0]{%
    \noalign{\smallskip}%
    \hline%
    \noalign{\smallskip}%
}

% Replacement for \ref{}, adds the page number too.
\newcommand{\refwithpage}[1]{%
    \empty{}\vref{#1}%
}
% Shorten the text used for references with page numbers.
\renewcommand{\reftextfaraway}[1]{%
    [p.~\pageref{#1}]%
}
% section references, automatically add \textsection
\newcommand{\sectionref}[1]{%
    \textsection{}\vref*{#1}%
}

\newcommand{\refwithlabel}[2]{%
    #1~\vref{#2}%
}
% table references, for consistent formatting.
\newcommand{\graphref}[1]{%
    \refwithlabel{fig.}{#1}%
}
\newcommand{\Graphref}[1]{%
    \refwithlabel{Figure}{#1}%
}
\newcommand{\tableref}[1]{%
    \refwithlabel{tbl.}{#1}%
}
\newcommand{\Tableref}[1]{%
    \refwithlabel{Table}{#1}%
}

% A command to format a Postfix daemon's name
\newcommand{\daemon}[1]{%
    \texttt{postfix/#1}%
}

\newcommand{\tab}[0]{%
    \hspace*{2em}%
}

\newcommand{\numberOFlogFILES}[0]{%
    93%
}

\newcommand{\numberOFrules}[0]{%
    169%
}

\newcommand{\numberOFrulesMINIMUM}[0]{%
    115%
}

\newcommand{\numberOFlogFILESall}[0]{%
    522%
}

% \numberOFrulesMINIMUM as percentage of \numberOFrules
\newcommand{\numberOFrulesMINIMUMpercentage}[0]{%
    68.04\%%
}

% \numberOFrules as percentage increase of \numberOFrulesMINIMUM
\newcommand{\numberOFrulesMAXIMUMpercentage}[0]{%
    46.95\%%
}

\newcommand{\numberOFlogLINES}[0]{%
    60,721,709%
}

\newcommand{\numberOFlogLINEShuman}[0]{%
    60.72 million%
}

\newcommand{\numberOFactions}[0]{%
    18%
}

\begin{document}

\title*{A user-extensible and adaptable parser architecture}
\author{John Tobin and Carl Vogel}
\institute{John Tobin \at{} School of Computer Science and Statistics,
Trinity College, Dublin 2, Ireland. \newline{} \email{tobinjt@cs.tcd.ie}
\and{} Carl Vogel \at{} School of Computer Science and Statistics,
Trinity College, Dublin 2, Ireland. \newline{} \email{vogel@cs.tcd.ie}}
\maketitle

\abstract{%
    Some parsers need to be very precise and strict in what they parse, yet
    must allow users to easily adapt or extend the parser to parse new
    inputs, without requiring that the user have an in-depth knowledge and
    understanding of the parser's internal workings.  This paper presents a
    novel parsing architecture which aims to make the process of parsing
    new inputs as simple as possible, enabling users to trivially add new
    rules (to parse variants of existing inputs) and relatively easily add
    new actions (to process a previously unknown class of input).  The
    architecture scales linearly as the number of rules and/or size of
    input increases, making it suitable for parsing large corpora or months
    of accumulated data.
}

XXX DISCUSS COMPLEXITY\@?

\section{Introduction}

\label{Introduction}

The architecture described herein was developed as part of a larger project
to improve anti-spam techniques by analysing the performance of the set of
filters currently in use, optimising the order and membership of the set
based on that analysis, and developing supplemental filters where there is
a deficiency.  Most anti-spam techniques are
content-based~\cite{a-plan-for-spam, word-stemming, relaxed-online-svms},
which requires accepting the mail in the first place, but rejecting is
preferable in many cases: senders of non-spam mail that is mistakenly
rejected will receive an immediate non-delivery notice; load is reduced on
mail servers (allowing more intensive content-based techniques to be used);
users have less mail to wade through.  Improving the performance of
anti-spam techniques which are applied when mail is being transferred via
SMTP\footnotemark{} is the goal of this project, by providing a platform
for reasoning about anti-spam filters.  The approach chosen to measure
performance was to analyse the log files produced by the mail server,
rather than modifying the mail server to generate performance statistics:
this approach improves the chances of other sites testing and utilising the
software.  The need arose for a parser capable of dealing with the great
number and variety of log lines produced by Postfix~\cite{postfix}, the
SMTP server in use.  The parser must be designed so that adding support for
parsing new log lines is a simple task, as the log lines to be parsed will
change over time.  The variety in log lines occurs for several
reasons:\footnotetext{Simple Mail Transfer Protocol transfers mail from the
sender to one or more recipients across the Internet.  It is a simple,
human readable, plain text protocol, making it quite easy to test and debug
problems with it.  The original protocol definition is
RFC~821~\cite{RFC821}, updated in RFC~2821~\cite{RFC2821}.}

\begin{itemize}

    \item Log lines differ between versions of Postfix.
        
    \item The mail administrator can define custom rejection
        messages.

    \item External resources utilised by Postfix (e.g.\ policy
        servers~\cite{policy-servers} or DNSBLs) can also change their
        messages, sometimes without warning.

\end{itemize}

It was hoped to reuse an existing parser rather than starting from scratch:
several parsers were considered and rejected because their parsing coverage
was too low, their parsing too inexact, and/or they did not extract
sufficient data.  The effort required to adapt and improve an existing
parser was judged to be greater than the effort to write one, as the
techniques used by the existing parsers severely limited their potential:
some ignored the majority of log lines, parsing specific log lines with
great accuracy, but without any provision for readily parsing new or
similar log lines; others sloppily parsed the majority of log lines, but
were incapable of distinguishing between log lines of the same type, e.g.\
rejections, and extracted only a portion of the data required.  The only
prior published work on the subject of parsing Postfix log files
is~\cite{log-mail-analyser}, which aims to present correlated log data in a
form suitable for a systems administrator to search using the myriad of
standard Unix text processing utilities already available.  A full state of
the art review is outside the scope of this paper but will be included in
the thesis resulting from this work.

The solution developed is conceptually simple: provide a few generic
functions (\textit{actions\/}), each capable of dealing with an entire
category of log lines (e.g.\ rejections), accompanied by a multitude of
precise patterns (\textit{rules\/}), each of which matches all log lines of
a specific type, and only that type (e.g.\ rejection by a specific DNSBL).
It is an accepted standard to separate the parsing procedure from the
declarative grammar it operates with.  Part of the novelty here is that the
grammars (rules and actions) are themselves procedures.  This architecture
is ideally suited to parsing inputs where the input is not fully understood
or does not follow a traditional grammar: the architecture warns about
unparsed inputs and other errors, but continues on parsing as best it can,
allowing the developer of a new parser to decide which deficiencies are
most important and require attention first, rather than being forced to fix
the first error which arises.

\section{Architecture}

\label{Architecture}

The architecture is split into three sections: framework, actions and
rules.  Each will be discussed separately, but first an overview:

\begin{description}

    \item [Framework]  The framework is the structure which actions and
        rules fit into.  It provides the parsing loop, loading and
        validation of rules, shared data storage, storage of results, and
        other support functions.

    \item [Actions]  The actions perform the work required to deal with a
        category of log lines, e.g.\ processing data from rejections.

    \item [Rules]  The rules are responsible for classifying or matching
        log lines, specifying the action to invoke and the data to be
        extracted from the log line.

\end{description}

The framework tries each rule in turn until it finds one which matches the
input log line, then invokes the action specified by that rule.

Decoupling the parsing rules from their associated actions and the
framework allows new rules to be written and tested without requiring
modifications to the parser source code, significantly lowering the barrier
to entry for casual users who need to parse new log lines, e.g.\ part-time
systems administrators attempting to reduce spam.  Decoupling the actions
from the framework simplifies both framework and actions: the framework
provides services to the actions, but does not need to perform any tasks
specific to the input being parsed; actions benefit from having services
provided by the framework, freeing them to concentrate on the task of
accurately and correctly processing the information provided by rules.

Decoupling also creates a clear separation of functionality: rules handle
low level details of identifying and extracting data from log lines;
actions handle the higher level details of assembling the required data,
dealing with the intricacies of the input being parsed, complications
arising, etc.;\ the framework provides services to actions and manages the
parsing process.

There is some similarity between the parser's design and William Wood's
Augmented Transition Networks (ATN)~\cite{atns, nlpip}, a tool used in
Computational Linguistics for creating grammars to parse or generate
sentences.  The resemblance between ATN and the parser is accidental, but
it is interesting how two different approaches have a similar division of
responsibilities, while having completely different implementations and
semantics.

% Do Not Reformat!

\begin{table}[ht]
    \caption{Similarities with ATN}\label{Similarities with ATN}
    % The \smallskip{} below stop the text hitting the lines.
    \begin{tabular}[]{lll}
        \hline
        \noalign{\smallskip}
        ATN           & Parser    & Similarity                          \\
        \noalign{\smallskip}
        \hline
        \noalign{\smallskip}
        Networks      & Framework & Determines the sequence of
                                    transitions or actions which        \\
                      &           & constitutes a valid input.          \\
        Transitions   & Actions   & Save data and impose conditions the
                                    input must meet to be               \\
                      &           & considered valid.                   \\
        Abbreviations & Rules     & Responsible for classifying input.  \\
        \noalign{\smallskip}
        \hline
        \noalign{\smallskip}
    \end{tabular}
\end{table}

\subsection{Framework}

\label{Framework}

The framework takes care of miscellaneous support functions and low level
details of parsing, freeing the authors of actions to concentrate on
writing productive code.  It provides the link between actions and rules,
allowing either to be improved independently of the other.  It provides
shared storage to pass data between actions, loads and validates rules,
controls parsing of log files, invokes actions, tracks how often each rule
matches (to optimise rule ordering \sectionref{Rule ordering}), and
stores results in the database.  The framework is largely independent of
the parser being written: the same functionality will be required for most
or all parsers.  The framework is the core of the architecture and is
deliberately quite simple: the rules deal with the varying log lines, and
the actions deal with the intricacies and complications encountered when
parsing.

The function which finds the rule matching the log line and invokes the
requested action can be expressed in pseudo-code as:

% DO NOT REFORMAT!

\begin{verbatim}
for each line in the input files: 
    for each rule defined by the user: 
        if this rule matches the input line:
            perform the action specified by the rule
            skip the remaining rules
            process the next input line
    warn the user that the input line was not parsed
\end{verbatim}

\subsection{Actions}

\label{Actions}

Each action contains the code required to deal with log lines from a
particular category, e.g.\ rejections.  The actions are parser-specific:
each parser author will need to write the actions required from scratch,
unless extending an existing parser.  In the Postfix parser developed for
this project there are \numberOFactions{} actions and \numberOFrules{}
rules; the distribution of rules to actions is very skewed, as shown in
\graphref{Distribution of rules per action}.  The distribution is skewed
for the same reason the architecture is so successful in enabling users to
parse new log lines: most new log lines can be parsed by a combination of a
new rule and an existing action.  It is expected that parsers utilising
this architecture will have a high ratio of rules to actions, hopefully
resulting in simpler rules and clearer distinctions between log lines.
\showgraph{build/graph-action-distribution}{Distribution
of rules per action}{Distribution of rules per action} Unsurprisingly the
action with the highest number of associated rules is \texttt{REJECTION},
the action which handles Postfix rejecting a mail; it is followed by
\texttt{SAVE\_BY\_QUEUEID}, the action responsible for handling informative
inputs: these are log lines that supplement the data gathered from other
log lines (such as rejections), but that do not by themselves represent
interesting events.  The third most common action is, perhaps surprisingly,
\texttt{IGNORE}: the \texttt{IGNORE} action does nothing when executed,
allowing uninteresting log lines to be parsed without causing any effects
(neither \texttt{REJECTION} nor \texttt{IGNORE} imply that the input is
ungrammatical or unparsed).  The parser warns about every unparsed log
line, to alert the user that they need to alter or extend their ruleset;
the Postfix parser successfully parses every input line in the
\numberOFlogFILESall{} log files it is currently being tested with.  The
remaining actions have only one or two associated rules: in some cases this
is because there will only ever be one type of log line for that action,
e.g.\ all log lines showing that a remote client has connected are matched
by a single rule and handled by the \texttt{CONNECT} action; in other cases
the action is required to address a deficiency in the log files, or a
complication which arises during parsing.

\label{example action}

Using the \texttt{CONNECT} action as an example, it creates a new data
structure for each client connecting, and saves the data extracted by the
rule.  If the data structure already exists it is treated as a symptom of a
bug, and the action issues a warning containing the full contents of the
data structure, plus the log line which has just been parsed.

The ability to add special purpose actions to deal with difficulties and
new requirements which are discovered during parser development is one of
the strengths of this architecture: instead of writing one monolithic
function which must be modified to support new behaviour, with all the
attendant risks of adversely affecting the existing parser, when a new
requirement arises an independent action can be written to satisfy it.  In
some cases the new action will require the cooperation of other actions,
e.g.\ to set or check a flag; there is a possibility of introducing failure
by modifying existing actions, but modifications will be smaller and occur
less frequently than with a monolithic architecture, thus failures should
be less likely, and easier to test for and diagnose.  The parser can be
written in an object oriented style, allowing sub-classes to extend or
override actions in addition to adding new actions; because each action is
an independent function, the sub-class need only modify the action it is
interested in, rather than reproducing large chunks of functionality.

\label{complications}

During development of the parser it became apparent that in addition to the
obvious variety in log lines, there were a large number of complications to
be overcome.  Some were the result of deficiencies in Postfix's logging
(some of those deficiencies were improved or resolved by later versions of
Postfix); others were due to the vagaries of process scheduling, client
behaviour, and administrative actions.  All were successfully accommodated
in the parser: adding new actions was enough for several of the
complications; others required modifications to a single existing action to
work around the difficulties; the remainder were resolved by adapting
actions to pass information to subsequent actions (via the framework),
those subsequent actions then check for the extra information and change
their behaviour appropriately.

Actions may also return a modified input line which will be parsed as if
read from the input stream, allowing for a simplified version of cascaded
parsing~\cite{cascaded-parsing}.  This is a powerful facility allowing a
generic rule and action to parse part of the input, followed by more
specific rules and actions parsing the remainder.

\subsection{Rules}

\label{Rules}

Rules categorise inputs, specifying both the data to be extracted from each
log line and the action to invoke.  The Postfix parser stores the rules in
the same SQL database that the results are stored in, removing any doubt as
to which set of rules was used to produce a set of results; other
implementations are free to store the rules in whatever fashion suits their
needs.  Similarly implementations are free to require additional attributes
in their rules; the framework requires \texttt{regex} and \texttt{action}
attributes, to determine whether a rule matches the current input line, and
which action to invoke if it does.  Rules used by the Postfix parser
define the following attributes: XXX DO I NEED TO LIST ALL THE
ATTRIBUTES\@?

\begin{description}

    \item [program] The Postfix program whose log lines the regex should be
        applied to.  This avoids needlessly trying regexes that will not
        match the log line, or worse, might match unintentionally.  Rules
        whose program is ``\texttt{*}'' will be tried against any log lines
        not parsed by program specific rules, allowing generic rules to be
        written.  This is a literal ``\texttt{*}'', not a wildcard or
        regex.

    \item [regex] The Regular Expression to match the log line against.

    \item [result\_cols] Specifies how the fields in the log line will be
        extracted.  The format is:
        \tab{} \texttt{smtp\_code = 1; recipient = 2, sender = 4;} \newline
        i.e.\ semi-colon or comma separated assignment statements, with the
        variable name on the left and the matching capture from the regex
        on the right.

    \item [result\_data] Sometimes rules need to supply a piece of data
        which is not present in the log line: e.g.\ setting
        \texttt{smtp\_code} when mail is accepted.  The format is the same
        as for \texttt{result\_cols}, except that arbitrary data is
        permitted on the right hand side of the assignment.

    \item [action] The action that will be invoked when this rule matches a
        log line.

    \item [hits] This counter is used to optimise the order in which rules
        are tried against the input lines.  Assuming that the distribution
        of log lines is reasonably consistent between log files, rules
        matching more frequently will be tried before rules matching less
        frequently, reducing the parser's execution time.

    \item [priority] Priority overrides hits when sorting rules, allowing
        more specific rules to take precedence over more general rules.  To
        illustrate: most warnings are ignored by a rule whose regex matches
        \verb!^warning:.*$!, but some warnings require a different action
        to be taken, e.g.\ when a client tries to send a mail larger than
        the maximum size the server accepts, the data structure for that
        mail must be discarded because it will not have all the data
        required to save it to the database.

\end{description}

Adding new rules is simple: simply define all the required attributes and
append the new rule to wherever the existing rules are stored.  The
software developed for this project includes a utility based on
SLCT~\cite{slct-paper} to aid in producing regexes to parse previously
unparsed inputs.  Storing the rules separately allows other approaches to
rule management to be used, e.g.\ instead of manually adding new rules when
unparsed log lines are seen, machine learning techniques could be used to
automatically generate new rules, based on the existing rules and input
lines.  If this approach was taken the choice of machine learning technique
would be constrained by the size of typical data sets: the
\numberOFlogFILES{} log files used to generate statistics contain
\numberOFlogLINEShuman{} log lines.  Techniques which require the full data
set when training would be impractical; Instance Based
Learning~\cite{instance-based-learning} techniques which automatically
determine which inputs from the training set are valuable and which inputs
can be discarded would probably reduce the data required to a manageable
size.  This avenue of research and development has not been pursued by the
authors, but could easily be independently undertaken.

\label{overlapping rules}

The parser does not try to detect overlapping rules; that responsibility is
left to the author of the rules.  Unintentionally overlapping rules lead to
inconsistent parsing and data extraction because the first matching rule
wins, and the order in which rules are tried against each line may change
between log files.  Overlapping rules are frequently a requirement,
allowing a more specific rule to match some log lines and a more general
rule to match the majority, e.g.\ separating SMTP delivery to specific
sites from SMTP delivery to the rest of the world.  The algorithm provides
a facility for specifying the order of overlapping rules: the priority
field in each rule (defaults to zero).  Rules are sorted by priority,
highest first, and then rules with the same priority are sorted by the
number of successful matches they had parsing the previous log file.
Negative priorities may be useful for catchall rules.

Storing the rules separately to the parser allows external tools to be
written to detect overlapping rules.  Traditional regexes are equivalent in
computational power to Finite Automata (FA); it is possible to convert
between regexes and FA, so regex overlap could be calculated by finding a
non-empty intersection of two FA\@.  Perl 5.10 regexes are more powerful
than traditional regexes, e.g.\ it is possible to match correctly nested
and balanced brackets to an arbitrary depth:
\newline{}\tab{}\verb!/^[^<>]*(<(?:(?>[^<>]+)|(?1))*>)[^<>]*$/!\newline{}
Perl 5.10 regexes can maintain an arbitrary state stack and are thus
equivalent in computational power to Pushdown Automata (PA), so overlap
detection may require calculating the intersection of two PA.\@  Whether
intersection of FA or PA is required, detecting overlap between $n$ rules
has $\mathcal{O}(n(n-1)/2)$ complexity.  XXX IMPROVE THE MATH DISPLAY\@.

It is possible to define pathological regexes, which fall into two main
categories: regexes which match every input line, and regexes which consume
large amounts of CPU time during matching.  Defining a regex which matches
every input line is trivial: \verb!/^/! will match the start of every input
line.  In general a regex will not consume a large amount of CPU time when
the match is successful; usually CPU time is consumed by a regex with a lot
of alteration and variable quantifiers failing to match.  This topic is
beyond the scope of this paper, see~\cite{mastering-regular-expressions}
for in-depth discussion.

\label{example rule}

The example rule in \tableref{Example rule} matches the message logged by
Postfix when a remote client connects to deliver mail; it would match the
following sample log line:

\begin{verbatim}
    connect from client.example.com[1.2.3.4]
\end{verbatim}

% Do not reformat this!
\begin{table}[htbp]
    \caption{Example rule}
    \empty{}\label{Example rule}
    \begin{tabular}[]{ll}
        \tableline{}%
        Attribute                 & Value                                            \\
        \tableline{}%
        program                   & \daemon{smtpd}                                   \\
        regex                     & \verb!^connect from (__HOSTNAME__)\[(__IP__)\]$! \\
        result\_cols              &                                                  \\
        connection\_cols~\empty{} & client\_hostname = 1; client\_ip = 2             \\
        action                    & CONNECT                                          \\
        priority                  & 0                                                \\
        \tableline{}%
    \end{tabular}
\end{table}

\noindent{}The \texttt{CONNECT} action is described in \sectionref{example
action}.


\subsection{Architecture Characteristics}

\label{Architecture characteristics}

\begin{description}

    \item [Context-free rules:]  Rules are context free: they do not take
        into account inputs which have previously been seen, nor do they
        perform any kind of lookahead to future inputs.  In context-free
        grammar terms the parser rules could be described as:
        $\text{\textless{}input\textgreater{}} \mapsto \text{rule-1} |
        \text{rule-2} | \text{rule-3} | \dots | \text{rule-n}$

    \item [Partially context-sensitive actions and cascaded parsing:]
        Actions can consult the results (or lack of results) of previous
        actions when executing, providing some context sensitivity.
        Actions can not inspect past or future inputs, however actions
        can return a modified input line that will be parsed as if read
        from the input stream, allowing for a simplified version of
        cascaded parsing~\cite{cascaded-parsing}.

    \item [Matching rules to log lines is simple:]  The first matching rule
        wins: there is no backtracking to try alternate rules, no attempt
        is made to pick a \textit{best\/} rule; the first rule which
        matches the line is the rule which parses it and determines the
        action that will be invoked.

    \item [Transduction:]  The parser can be thought of as implementing
        transduction: it takes data in one form (log files) and transforms
        it to another form (a database); other output formats may be more
        suitable for other implementations.

    \item [Closer to Natural Language Processing than using a fixed
        grammar:] Unlike traditional parsers such as those used when
        compiling a programming language, this architecture does not have a
        fixed grammar specification which inputs must adhere to.  The
        architecture is capable of dealing with overlapping inputs, out of
        order inputs, and ambiguous inputs where heuristics must be applied
        --- all have arisen and been successfully dealt with in the Postfix
        log parser.

    \item [Line oriented:]  The architecture is line oriented at present:
        there is no facility for rules to consume more input or push unused
        input back onto the input stream.  This was not a deliberate
        decision, rather a consequence of the line oriented nature of the
        log files; more flexible approaches could be pursued.

\end{description}

\section{Results}

Parsing efficiency is an obvious concern when the parser routinely needs to
parse very large log files.  The server which generated the log files used
in testing this parser accepts approximately 10,000 mails for 700 users a
day.  Median log file size is 50MB, containing 285,000 log lines --- large
scale mail servers would have much larger log files.  When generating the
timing data used in this section, \numberOFlogFILES{} log files (totaling
10.08 GB, \numberOFlogLINEShuman{} log lines) were each parsed 10 times and
the parsing times averaged.  Saving results to the database was disabled
for the test runs, as that dominates the run time of the program, and the
tests are aimed at measuring the speed of the parser rather than the speed
of the database and the disks the database is stored on.  The computer used
for test runs was a Dell Optiplex 745 described in \tableref{Computer used
to generate statistics}.

\begin{table}[htbp]
    \caption{Computer used to generate statistics}
    \empty{}\label{Computer used to generate statistics}
    \begin{tabular}[]{ll}
        \tableline{}
        Component type  & Component in use                                  \\
        \tableline{}
        CPU             & One dual core 2.40GHz Intel\textregistered{}
                            Core\texttrademark{}2 CPU,                      \\
                        & with 32KB L1 cache and 4MB L2 cache.              \\
        RAM             & 2GB 667 MHz DDR RAM\@.                            \\
        Hard disk       & One Seagate Barracuda 7200 RPM 250GB SATA disk.   \\
        \tableline{}
    \end{tabular}
\end{table}

Parsing all \numberOFlogFILES{} log files in one run took
\input{build/include-timing-run-duration}.  The computer was dedicated to
the task of gathering statistics from test runs, and was not used for any
other purpose while test runs were ongoing; any services not necessary for
running the tests were disabled.

\subsection{Architecture Scalability: Input Size}

An important property of a parser is how parsing time scales relative to
input size: does it scale linearly, polynomially, or exponentially?
\Graphref{Parsing time, file size, and number of lines} shows the parsing
time in seconds, file size in MB, and tens of thousands of log lines for
each of the \numberOFlogFILES{} log files.  All three lines run roughly in
parallel, giving the impression that the algorithm scales linearly with
input size.  This impression is borne out by \graphref{parsing time vs file
size vs number lines factor} which plots both the ratio of file size vs
parsing time, and the ratio of number of log lines vs parsing time (higher
is better in both cases).  The ratios are quite tightly banded across the
graph, showing that the algorithm scales linearly.  The ratio increases
(i.e.\ improves) for log files 22 and 62--68 despite their larger than
average size (shown in \graphref{Parsing time, file size, and number of
lines}).  \showgraph{build/graph-input-size-vs-parsing-time}{Parsing time,
file size, and number of lines}{Parsing time, file size, and number of
lines} \showgraph{build/graph-input-size-vs-parsing-time-ratio}{Ratio of
file size and number of lines to parsing time}{parsing time vs file size vs
number lines factor}

\subsection{Rule Ordering}

\label{Rule ordering}
\label{rule ordering for efficiency}
\label{rule efficiency}

At the time of writing there are \numberOFrules{} different rules, with the
top 10\% matching the vast majority of the log lines, and the remaining log
lines split across the other 90\% of the rules in a Power Law distribution
(as shown in \graphref{rule hits graph}).  Assuming that the distribution
of log lines is reasonably steady over time, parser efficiency should
benefit from trying more frequently matching rules before those which match
less frequently.  To test this hypothesis three full test runs were
performed with different rule orderings:

\begin{description}

    \item [optimal]  Hypothetically the most optimal order: rules which
        match most often will be tried first.

    \item [shuffle] Random ordering, intended to represent an unsorted rule
        set.  The rules will be shuffled once before use and will retain
        that ordering for the entirety of the log file.  Note that the
        ordering will change every time the parser is executed, so 10
        different orderings will be generated for each log file in the test
        run.

    \item [reverse] Hypothetically the worst order: the most frequently
        matching rules will be tried last.

\end{description}

\Graphref{Parsing time relative to shuffled} shows the parsing times of
optimal and reversed orderings relative to shuffled ordering.  Overall this
optimisation provides a modest but worthwhile reduction in parsing time of
approximately 10\%.  \showgraph{build/graph-hits}{Hits per rule}{rule hits
graph} \showgraph{build/graph-optimal-and-reverse-vs-shuffle}{Parsing time
relative to shuffled ordering}{Parsing time relative to shuffled}

\subsection{Architecture Scalability: Number of Rules}

How any architecture scales as the number of rules increases is important,
but it is particularly important in this architecture because it is
expected that there will be a large number of rules.  There are
\numberOFrules{} rules in the full Postfix ruleset, whereas the minimum
number of rules required to parse the \numberOFlogFILES{} log files is
\numberOFrulesMINIMUM{}, \numberOFrulesMINIMUMpercentage{} of the total.  A
second set of statistics was generated using the minimum ruleset and the
results compared to the statistics generated using the full ruleset: the
percentage parsing time increase when using the full ruleset instead of the
minimal ruleset for optimal, shuffled and reversed orderings is shown in
\graphref{Percentage parsing time increase of maximum ruleset over minimum
ruleset}.
\showgraph{build/graph-full-ruleset-vs-minimum-ruleset}{Percentage parsing
time increase of maximum ruleset over minimum ruleset}{Percentage parsing
time increase of maximum ruleset over minimum ruleset}

It is clear from \graphref{Percentage parsing time increase of maximum
ruleset over minimum ruleset} that the increased number of rules has a
noticeable performance impact with reverse ordering, and to a lesser extent
with shuffled ordering.  The optimal ordering (where the most frequently
matching rules are tried first) shows a mean increase of
\input{build/include-full-ruleset-vs-minimum-ruleset} in parsing time for a
\numberOFrulesMAXIMUMpercentage{} increase in the number of rules.  These
results show that the architecture scales very well as the number of rules
increases, and that sorting the rules is an important optimisation enabling
this.

\subsection{Coverage}

\label{coverage}

The parser's coverage of Postfix log files is separated into two parts: log
line coverage and mail coverage.  The former is initially more important,
as the parser must successfully parse every line if it is to be complete,
but subsequently the latter takes precedence because reproducing the path a
mail takes through Postfix is the aim of the parser.  Improving the former
is less intrusive, as it just requires new rules to be written; improving
the latter is more intrusive as it requires changes to actions, and it can
also be much harder to notice a deficiency.

The framework issues warnings for any log lines which are not parsed; no
warnings are issued for unparsed log lines while parsing the
\numberOFlogFILES{} test log files, so it can be safely concluded that
there are zero false negatives.  False positives are harder to quantify:
manually verifying that the correct regex parsed each of the 60,721,709 log
lines is infeasible, making it impossible to quantify the false positive
rate; however a random sample of 6039 log lines (0.00994\% of the total)
was parsed and the results verified to ensure that the correct regex parsed
each log line.

Coverage of mails is much more difficult to determine accurately than
coverage of log lines.  The parser can dump its state tables in a human
readable form; examining these tables with reference to the log files is
the best way to detect mails which were not handled properly.  The parser
issues warnings when it detects any errors, some of which may alert the
user to a problem.  There should be few or no warnings when parsing, and
when finished parsing the state table should only contain entries for mails
which had yet to be delivered when the log files ended, or were accepted
before the log files began.  A sample of 6000 log lines was parsed with all
debugging options enabled, resulting in 167,448 lines of output.  All
167,448 lines were examined in conjunction with the input log file and a
dump of the resulting database, verifying that for each of the input lines
the parser used the correct rule and executed the correct action, which in
turn produced the correct result and inserted the correct data in the
database.  The log segment produced 4 warnings (about mails which had
started before the log file segment), 10 mails remaining in the state
tables, and 1625 connections correctly entered in the database.  Given the
evidence detailed above, the authors are confident that the false positive
rate when reconstructing a mail is zero.


\section{Conclusion}

The architecture's greatest strength is the ease with which it can be
adapted to deal with new requirements and inputs.  Parsing a variation of
an existing input is a trivial task: simply modify an existing rule or add
a new one with an appropriate regex and the task is complete; knowledge of
the framework is not required when writing rules.  Parsing a new category
of input is achieved by writing a new action and appropriate rules; quite
often the action will not need to interact directly with existing actions,
but when interaction is required the framework provides the necessary
facilities.  The decoupling of rules from actions allows different sets of
rules to be used with the same actions: e.g.\ a parser might have actions
to process versions one and two of a file format; the parser will then
parse either version one or version two depending on the ruleset selected.
The architecture makes it possible to apply commonly used programming
techniques such as object orientation, inheritance, composition, delegation
and roles when designing and implementing a parser, simplifying the process
of working within a team with shared responsibility or developing and
testing additional functionality.  This architecture is ideally suited to
parsing inputs where the input is not fully understood or does not follow a
fixed grammar: the architecture warns about unparsed inputs and other
errors, but continues on parsing as best it can, allowing the developer of
a new parser to decide which deficiencies are most important and require
attention first, rather than being forced to fix the first error which
arises.

The dataset gathered by the parser provides the foundation for the future
of this project: applying machine-learning algorithms to the data to
analyse and optimise the set of rules in use, followed by identifying
patterns in the data which could be used to write new filters to recognise
and reject spam rather than accepting it.  The parser provides the data in
a normalised form which is far easier to use as input to new or existing
implementations of the machine-learning algorithms than trying to adapt the
algorithms to extract data directly from the log files.  The current focus
is on clustering and decision trees to optimise the order in which rules
are applied; future efforts will involve using data gathered by the parser
to train and test new filters.  This task is very similar to optimising a
black box application based on its inputs and outputs, and this approach
could be applied to optimising the behaviour of any system given sufficient
logging to analyse.  An alternative approach to black box optimisation
which uses application profiling in conjunction with the application's
error messages to improve the error messages shown to users is described
in~\cite{black-box-error-reporting}; profiling data may be useful in
supplementing systems which fail to provide adequate log messages.

The Postfix log file parser based on this architecture provides a basis for
systems administrators to monitor the effectiveness of their anti-spam
measures and adapt their defences to combat the new techniques used by
those sending spam.  The parser is a fully usable application, built to
address a genuine need, rather than a proof of concept whose sole purpose
is to illustrate a new idea; it deals with the oddities and difficulties
which occur in the real world, rather than a clean, idealised scenario
developed to showcase the best features of a new approach.

\bibliographystyle{spmpsci}
\bibliography{logparser-bibliography.bib}

\end{document}
