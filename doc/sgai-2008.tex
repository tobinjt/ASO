% $Id$
\documentclass{svmult}
%\documentclass[draft]{svmult}

% Useful stuff for math mode.
\usepackage{amstext}
% Include images
\usepackage[final]{graphicx}
\usepackage{url}

\usepackage{lastpage}

% Extra footnote functionality, including references to earlier footnotes.
\usepackage[bottom]{footmisc}

% Tell Latex to use scalable fonts
\usepackage{type1cm}

% Extra packages recommended by Springer.
\usepackage{mathptmx}
\usepackage{helvet}
\usepackage{courier}
\usepackage{makeidx}
\usepackage{multicol}

% Embed SVN Id etc.
\usepackage{svn}

% Acronyms and glossary entries.
% global=true prevents acronyms being expanded twice
\usepackage[global=true,acronym=true]{glossary}
%\makeglossary{}
%\makeacronym{}

% Nicer citation lists - smaller spaces.
\usepackage{cite}
% Check for unused references.
% \usepackage{refcheck}
\usepackage{varioref}
% Warnings rather than errors when references cross page boundaries.
\vrefwarning{}
% Errors when loops are encountered
\vrefshowerrors{}

% \showgraph{filename}{caption}{label}
\newcommand{\showgraph}[3]{%
    \begin{figure}[btp]%
        \includegraphics{#1}%
        \caption{#2}\label{#3}%
    \end{figure}%
}

%\showtable{filename}{caption}{label}
\newcommand{\showtable}[3]{
    \begin{table}[btp]
        \caption{#2}\label{#3}
        \input{#1}
    \end{table}
}

\newcommand{\tabletopline}[0]{%
    \hline%
    \noalign{\smallskip}%
}

\newcommand{\tablebottomline}[0]{%
    \noalign{\smallskip}%
    \hline%
}

\newcommand{\tablemiddleline}[0]{%
    \noalign{\smallskip}%
    \hline%
    \noalign{\smallskip}%
}

% Replacement for \ref{}, adds the page number too.
\newcommand{\refwithpage}[1]{%
    \empty{}\vref{#1}%
}
% Shorten the text used for references with page numbers.
\renewcommand{\reftextfaraway}[1]{%
    [p.~\pageref{#1}]%
}
% section references, automatically add \textsection
\newcommand{\sectionref}[1]{%
    \textsection{}\vref*{#1}%
}

\newcommand{\refwithlabel}[2]{%
    #1~\vref{#2}%
}
% table references, for consistent formatting.
\newcommand{\graphref}[1]{%
    \refwithlabel{fig.}{#1}%
}
\newcommand{\Graphref}[1]{%
    \refwithlabel{Figure}{#1}%
}
\newcommand{\tableref}[1]{%
    \refwithlabel{table}{#1}%
}
\newcommand{\Tableref}[1]{%
    \refwithlabel{Table}{#1}%
}

% A command to format a Postfix daemon's name
\newcommand{\daemon}[1]{%
    \texttt{postfix/#1}%
}

\newcommand{\tab}[0]{%
    \hspace*{2em}%
}

\newcommand{\numberOFlogFILES}[0]{%
    93%
}

\newcommand{\numberOFrules}[0]{%
    169%
}

\newcommand{\numberOFruleINTERSECTIONS}[0]{%
    14196%
}

\newcommand{\numberOFrulesMINIMUM}[0]{%
    115%
}

\newcommand{\numberOFlogFILESall}[0]{%
    522%
}

% \numberOFrulesMINIMUM as percentage of \numberOFrules
\newcommand{\numberOFrulesMINIMUMpercentage}[0]{%
    68.04\%%
}

% \numberOFrules as percentage increase of \numberOFrulesMINIMUM
\newcommand{\numberOFrulesMAXIMUMpercentage}[0]{%
    46.95\%%
}

\newcommand{\numberOFlogLINES}[0]{%
    60,721,709%
}

\newcommand{\numberOFlogLINEShuman}[0]{%
    60.72 million%
}

\newcommand{\numberOFactions}[0]{%
    18%
}

\begin{document}

\title*{A User-Extensible and Adaptable Parser Architecture}
\author{John Tobin and Carl Vogel}
\institute{John Tobin \email{tobinjt@cs.tcd.ie}
            \and{} Carl Vogel \email{vogel@cs.tcd.ie}
            \at{} School of Computer Science and Statistics,
                    Trinity College, Dublin 2, Ireland.
                    \newline{}
                    Supported by Science Foundation Ireland RFP
                    05/RF/CMS002.
            }

\maketitle

\abstract{%
    Some parsers need to be very precise and strict when parsing, yet must
    allow users to easily adapt or extend the parser to parse new inputs,
    without requiring that the user have an in-depth knowledge and
    understanding of the parser's internal workings.  This paper presents a
    novel parsing architecture, designed for parsing Postfix log files,
    that aims to make the process of parsing new inputs as simple as
    possible, enabling users to trivially add new rules (to parse variants
    of existing inputs) and relatively easily add new actions (to process a
    previously unknown category of input).  The architecture scales
    linearly or better as the number of rules and size of input increases,
    making it suitable for parsing large corpora or months of accumulated
    data.
}

%\SVN$Id$
%\SVNId{}  \today{}

\newacronym{PDA}{Pushdown Automata}{}
\newacronym{FA}{Finite Automata}{}
\newacronym{CFL}{Context-Free Languages}{}
\newacronym{SMTP}{Simple Mail Transfer Protocol}{}
\newacronym{ATN}{Augmented Transition Networks}{}
\newacronym{DNSBL}{DNS Black List}{}

\section{Introduction}

\label{Introduction}

The architecture described herein was developed as part of a larger project
to improve anti-spam defenses by analysing the performance of the set of
filters currently in use, optimising the order and membership of the set
based on that analysis, and developing supplemental filters where
deficiencies are identified.  Most anti-spam techniques are content-based
(e.g.~\cite{a-plan-for-spam, word-stemming, relaxed-online-svms}) and
require the mail to be accepted before determining if it is spam, but
rejecting mail during the delivery attempt is preferable: senders of
non-spam mail that is mistakenly rejected will receive an immediate
non-delivery notice; resource usage is reduced on the accepting mail server
(allowing more intensive content-based techniques to be used on the mail
that is accepted); users have less spam mail to wade through.  Improving
the performance of anti-spam techniques that are applied when mail is being
transferred via \SMTP{}\footnotemark{} is the goal of this project, by
providing a platform for reasoning about anti-spam filters.  The approach
chosen to measure performance is to analyse the log files produced by the
\SMTP{} server in use, Postfix~\cite{postfix}, rather than modifying it to
generate statistics: this approach improves the chances of other sites
testing and using the software.  The need arose for a parser capable of
dealing with the great number and variety of log lines produced by Postfix:
the parser must be designed so that adding support for parsing new inputs
is a simple task, because the log lines to be parsed will change over time.
The variety in log lines occurs for several reasons:\footnotetext{Simple
Mail Transfer Protocol transfers mail across the Internet from the sender
to one or more recipients.  It is a simple, human readable, plain text
protocol, making it quite easy to test and debug problems with it.  The
original protocol definition is RFC~821~\cite{RFC821}, updated in
RFC~2821~\cite{RFC2821}.}

\begin{itemize}

    \item Log lines differ amongst versions of Postfix.
        
    \item The mail administrator can define custom rejection messages.

    \item External resources utilised by Postfix (e.g.\ \DNSBL{} or policy
        servers~\cite{policy-servers}) can change their messages without
        warning.

\end{itemize}

It was hoped to reuse an existing parser rather than writing one from
scratch, but the existing parsers considered were rejected for one or more
of the following reasons: they parsed too small a fraction of the log
files; their parsing was too inexact; they did not extract sufficient data.
The effort required to adapt and improve an existing parser was judged to
be greater than the effort to write a new one, because the techniques used
by the existing parsers severely limited their potential: some ignored the
majority of log lines, parsing specific log lines accurately, but without
any provision for parsing new or similar log lines; others sloppily parsed
the majority of log lines, but were incapable of distinguishing between log
lines of the same category, e.g.\ rejections.  The only prior published
work on the subject of parsing Postfix log files that the authors are aware
of is \textit{Log Mail Analyser: Architecture and Practical
Utilizations\/}~\cite{log-mail-analyser}, which aims to extract data from
log files, correlate it, and present it in a form suitable for a systems
administrator to search using the myriad of standard Unix text processing
utilities already available.  A full state of the art review is outside the
scope of this paper but will be included in the thesis resulting from this
work.

The solution developed is conceptually simple: provide a few generic
functions (\textit{actions\/}), each capable of dealing with an entire
category of inputs (e.g.\ rejecting a mail delivery attempt), accompanied by a multitude of
precise patterns (\textit{rules\/}), each of which matches all inputs of
a specific type and only that type (e.g.\ rejection by a specific
\DNSBL{}).  It is an accepted standard to separate the parsing procedure
from the declarative grammar it operates with; part of the novelty here is
in the way that the grammar is itself partially procedural (each action is
a separate procedure).  This architecture is ideally suited to parsing
inputs where the input is not fully understood or does not conform to a
fixed grammar: the architecture warns about unparsed inputs and other
errors, but continues on parsing as best it can, allowing the developer of
a new parser to decide which deficiencies are most important and require
attention first, rather than being forced to fix the first error that
arises.

\section{Architecture}

\label{Architecture}

The architecture is split into three sections: framework, actions and
rules.  Each will be discussed separately, but first an overview:

\begin{description}[Framework]

    \item [Framework]  The framework is the structure that actions and
        rules plug into.  It provides the parsing loop, shared data
        storage, loading and validation of rules, storage of results, and
        other support functions.

    \item [Actions]  Each action performs the work required to deal with a
        category of inputs, e.g.\ processing data from rejections.

    \item [Rules]  The rules are responsible for classifying
        inputs, specifying the action to invoke and the regex that matches
        the inputs and extracts data.

\end{description}

For each input the framework tries each rule in turn until it finds a rule
that matches the input, then invokes the action specified by that rule.

Decoupling the parsing rules from their associated actions allows new rules
to be written and tested without requiring modifications to the parser
source code, significantly lowering the barrier to entry for casual users
who need to parse new inputs, e.g.\ part-time systems administrators
attempting to combat and reduce spam; it also allows companies to develop
user-extensible parsers without divulging their source code.  Decoupling
the actions from the framework simplifies both framework and actions: the
framework provides services to the actions, but does not need to perform
any tasks specific to the input being parsed; actions benefit from having
services provided by the framework, freeing them to concentrate on the task
of accurately and correctly processing the information provided by rules.

Decoupling also creates a clear separation of functionality: rules handle
low level details of identifying inputs and extracting data; actions handle
the higher level tasks of assembling the required data, dealing with the
intricacies of the input being parsed, complications arising, etc.;\ the
framework provides services to actions and manages the parsing process.

Some similarity exists between this architecture and William Wood's
\ATN{}~\cite{atns, nlpip}, used in Computational Linguistics for creating
grammars to parse or generate sentences.  The resemblance between the two
(shown in \tableref{Similarities with ATN}) is accidental, but it is
obvious that the two approaches share a similar division of
responsibilities, despite having different semantics.

% Do Not Reformat!

\begin{table}[hbtp]
    \caption{Similarities with ATN}\label{Similarities with ATN}
    % The \smallskip{} below stop the text hitting the lines.
    \begin{tabular}[]{lll}
        \tabletopline{}%
        ATN           & Parser Architecture & Similarity                                \\
        \tablemiddleline{}%
        Networks      & Framework           & Determines the sequence of transitions or \\
                      &                     & actions that constitutes a valid input.   \\
        Transitions   & Actions             & Assembles data and imposes conditions the \\
                      &                     & input must meet to be accepted as valid.  \\
        Abbreviations & Rules               & Responsible for classifying input.        \\
        \tablebottomline{}%
    \end{tabular}
\end{table}

\subsection{Framework}

\label{Framework}

The framework takes care of miscellaneous support functions and low level
details of parsing, freeing the programmers writing actions to concentrate
on writing productive code.  It links actions and rules, allowing either to
be improved independently of the other.  It provides shared storage to pass
data between actions, loads and validates rules, manages parsing, invokes
actions, tracks how often each rule matches to optimise rule ordering
(\sectionref{Rule ordering}), and stores results in the database.  Most
parsers will require the same basic functionality from the framework, plus
some specialised support functions.  The framework is the core of the
architecture and is deliberately quite simple: the rules deal with the
variation in inputs, and the actions deal with the intricacies and
complications encountered when parsing.

The function that finds the rule matching the input and invokes the
requested action can be expressed in pseudo-code as:

% DO NOT REFORMAT!

\begin{verbatim}
for each input:
    for each rule defined by the user: 
        if this rule matches the input:
            perform the action specified by the rule
            skip the remaining rules
            process the next input
    warn the user that the input was not parsed
\end{verbatim}

\subsection{Actions}

\label{Actions}

Each action is a separate procedure written to deal with a particular
category of input, e.g.\ rejections.  The actions are parser-specific: each
parser author will need to write the required actions from scratch unless
extending an existing parser.  It is anticipated that parsers based on this
architecture will have a high ratio of rules to actions, with the aim of
having simpler rules and clearer distinctions between the inputs parsed by
different rules.  In the Postfix log parser developed for this project
there are \numberOFactions{} actions and \numberOFrules{} rules, with an
uneven distribution of rules to actions as shown in \graphref{Distribution
of rules per action}.
\showgraph{build/graph-action-distribution}{Distribution of rules per
action}{Distribution of rules per action} Unsurprisingly, the action with
the most associated rules is \texttt{DELIVERY\_REJECTED}, the action that
handles Postfix rejecting a mail delivery attempt; it is followed by
\texttt{SAVE\_DATA}, the action responsible for handling informative log
lines, supplementing the data gathered from other log lines.  The third
most common action is, perhaps surprisingly, \texttt{UNINTERESTING}: this
action does nothing when executed, allowing uninteresting log lines to be
parsed without causing any effects (it does not imply that the input is
ungrammatical or unparsed).  Generally rules specifying the
\texttt{UNINTERESTING} action parse log lines that are not associated with
a specific mail, e.g.\ notices about configuration files changing.  The
remaining actions have only one or two associated rules: some actions are
required to address a deficiency in the log files, or a complication that
arises during parsing;  other actions will only ever have one log line
variant, e.g.\ all log lines showing that a remote client has connected are
matched by a single rule and handled by the \texttt{CONNECT} action.

Using the \texttt{CONNECT} action as an example: it creates a new data
structure in memory for the new client connection, saving the data
extracted by the rule into it; this data will be entered into the database
when the mail delivery attempt is complete.  If a data structure already
exists for the new connection it is treated as a symptom of a bug, and the
action issues a warning containing the full contents of the existing data
structure, plus the log line that has just been parsed.

The ability to add special purpose actions to deal with difficulties and
new requirements that are discovered during parser development is one of
the strengths of this architecture.  Instead of writing a single monolithic
function that must be modified to support new behaviour, with all the
attendant risks of adversely affecting the existing parser, when a new
requirement arises an independent action can be written to satisfy it.
Sometimes the new action will require the cooperation of other actions,
e.g.\ to set or check a flag.  There is a possibility of introducing
failure when modifying existing actions in this way, but the modifications
will be smaller and occur less frequently than with a monolithic
architecture, thus failures will be less likely and will be easier to test
for and diagnose.  The architecture can be implemented in an object
oriented style, allowing sub-classes to extend or override actions in
addition to adding new actions; because each action is an independent
procedure, the sub-class need only modify the action it is overriding,
rather than reproducing large chunks of functionality.

During development of the Postfix log parser it became apparent that in
addition to the obvious variety in log lines there were many complications
to be overcome.  Some were the result of deficiencies in Postfix's logging
(some of which were rectified by later versions of Postfix); others were
due to the vagaries of process scheduling, client behaviour, and
administrative actions.  All were successfully accommodated in the Postfix
log parser: adding new actions was enough to overcome several of the
complications; others required modifications to a single existing action to
work around the difficulties; the remainder were resolved by adapting
existing actions to cooperate and exchange extra data, changing their
behaviour as appropriate.

Actions may return a modified input line that will be parsed as if read
from the input stream, allowing for a simplified version of cascaded
parsing~\cite{cascaded-parsing}.  This powerful facility allows several
rules and actions to parse a single input, potentially simplifying both
rules and actions.

\subsection{Rules}

\label{Rules}

Rules categorise inputs, specifying both the regex to match against each
input and the action to invoke when the match is successful.  The Postfix
log parser stores the rules it uses in the same SQL database the results
are stored in, removing any doubt about which set of rules was used to
produce a set of results; other implementations are free to store their
rules in whatever fashion suits their needs.  The framework warns about
every unparsed input, to alert the user that they need to alter or extend
their ruleset; the Postfix log parser successfully parses every log line in
the \numberOFlogFILESall{} log files it is currently being tested with.
The framework requires each rule to have \texttt{action} and \texttt{regex}
attributes; each implementation is free to add any additional attributes it
requires.  The Postfix log parser adds several attributes: optimising rule
ordering (\sectionref{Rule ordering}); restricting which log lines each
rule can be matched against; and describing each rule.  Data is
automatically extracted and saved from the input based on keywords in the
rule's regex, but the Postfix log parser also provides a mechanism to
specify extra data to be saved.

Parsing new inputs is generally achieved by creating a new rule that pairs
an existing action with a new regex.  The Postfix log parser supplies a
utility based on Simple Logfile Clustering Tool~\cite{slct-paper} to aid in
producing regexes from unparsed log lines.  Separating the rules from the
actions and framework enables other rule management approaches to be used,
e.g.\ instead of manually adding new rules, machine learning techniques
could be used to automatically generate new rules.  If this approach was
taken the choice of machine learning technique would be constrained by the
size of typical data sets (see \sectionref{Results}).  Techniques requiring
the full data set when training would be impractical; Instance Based
Learning~\cite{instance-based-learning} techniques that automatically
determine which inputs from the training set are valuable and which inputs
can be discarded might reduce the data required to a manageable size.  A
parser might also dynamically create new rules in response to certain
inputs, e.g.\ diagnostic messages indicating the source of the inputs has
read a new configuration file.  These avenues of research and development
has not been pursued by the authors, but could easily be undertaken
independently.

The architecture does not try to detect overlapping rules: that
responsibility is left to the author of the rules.  Unintentionally
overlapping rules lead to inconsistent parsing and data extraction because
the first matching rule wins, and the order in which rules are tried
against each input might change between parser invocations.  Overlapping
rules are frequently a requirement, allowing a more specific rule to match
some inputs and a more general rule to match the remainder, e.g.\
separating \SMTP{} delivery to specific sites from \SMTP{} delivery to the
rest of the world.  Allowing overlapping rules simplifies both the general
rule and the more specific rule; additionally rules from different sources
can be combined with a minimum of prior cooperation or modification
required.  Overlapping rules should have a priority attribute to specify
their relative ordering; negative priorities may be useful for catchall
rules.

Decoupling the rules from the actions allows external tools to be written
to detect overlapping rules.  Traditional regexes are equivalent in
computational power to \FA{} and can be converted to \FA{}, so regex
overlap can be detected by finding a non-empty intersection of two \FA{}\@.
The standard equation for \FA{} intersection (given for example
in~\cite{intersection-of-NFA-using-Z}) is: $FA1 \cap{} FA2 =
\overline{(\overline{FA1} \cup{} \overline{FA2})}$, which has considerable
computation complexity.  Perl 5.10 regexes are more powerful than
traditional regexes: it is possible to match correctly balanced brackets
nested to an arbitrary depth, e.g.\ \verb!z<123<pq<>rs>j<r>ml>s! is matched
by \verb!/^[^<>]*(<(?:(?>[^<>]+)|(?1))*>)[^<>]*$/! Perl 5.10
regexes can maintain an arbitrary state stack and are thus equivalent in
computational power to \PDA{} or \CFL{}, so detecting overlap may require
calculating the intersection of two \PDA{} or \CFL{}\@.  The intersection
of two \CFL{} is not closed, i.e.\ the resulting language cannot always be
parsed by a \CFL{}, so intersection may be intractable in some cases e.g.:
$a^{*}b^{n}c^{n}~\cap~a^{n}b^{n}c^{*}~\rightarrow~a^{n}b^{n}c^{n}$.

Detecting overlap amongst $n$ regexes requires calculating $n(n-1)/2$
intersections, resulting in $O(n^2x)$ complexity, where $O(x)$ is the
complexity of calculating intersection.  This is certainly not a task to be
performed every time the parser is used: detecting overlap amongst the Postfix log
parser's \numberOFrules{} rules would require calculating
\numberOFruleINTERSECTIONS{} intersections.

It is possible to define pathological regexes which fall into two main
categories: regexes that match every input, and regexes that consume
excessive amounts of CPU time during matching.  Defining a regex that
matches all inputs is trivial: \verb!/^/! matches the start of every input.
Usually excessive CPU time is consumed when a regex with a lot of
alteration and variable quantifiers fails to match; successful matching is
generally quite fast.  See~\cite{mastering-regular-expressions} for
in-depth discussion.

The example rule in \tableref{Example rule} matches the following sample
log line logged by Postfix when a remote client connects to deliver mail:
\newline{}\tab{}\verb!connect from client.example.com[192.0.2.3]!

% Do not reformat this!
\begin{table}[htbp]
    \caption{Example rule}
    \empty{}\label{Example rule}
    \begin{tabular}[]{ll}
        \tabletopline{}%
        Attribute                 & Value                                                           \\
        \tablemiddleline{}%
        regex                     & \verb!^connect from (__CLIENT_HOSTNAME__)\[(__CLIENT_IP__)\]$!  \\
        action                    & CONNECT \hspace{3em}(described in \sectionref{Actions})         \\
        \tablebottomline{}%
    \end{tabular}
\end{table}

\subsection{Architecture Characteristics}

\label{Architecture characteristics}

\begin{description}

    \item [Matching rules against inputs is simple:]  The first matching
        rule determines the action that will be invoked: there is no
        backtracking to try alternate rules, no attempt is made to pick a
        \textit{best\/} rule.

    \item [Line oriented:]  The architecture is line oriented at present:
        there is no facility for rules to consume more input or push unused
        input back onto the input stream.  This was not a deliberate design
        decision, rather a consequence of the line oriented nature of
        Postfix log files; more flexible approaches could be pursued.

    \item [Context-free rules:]  Rules can not take into account past or
        future inputs.  In context-free grammar terms the parser rules
        could be described as:
        \newline{}$\text{\textless{}input\textgreater{}} \mapsto
        \text{rule-1} | \text{rule-2} | \text{rule-3} | \dots |
        \text{rule-n}$

    \item [Context-aware actions:] Actions can consult the results (or lack
        of results) of previous actions during execution, providing some
        context sensitivity.  
        
    \item [Cascaded parsing:] Actions can not inspect past or future
        inputs, but actions can return a modified input to be parsed as if
        read from the input stream, allowing for a simplified version of
        cascaded parsing~\cite{cascaded-parsing}.

    \item [Transduction:]  The architecture can be thought of as
        implementing transduction: it takes data in one form (log files)
        and transforms it to another form (a database); other formats may
        be more suitable for other implementations.

    \item [Closer to Natural Language Processing than using a fixed
        grammar:] Unlike traditional parsers such as those used when
        compiling a programming language, this architecture does not
        require a fixed grammar specification that inputs must adhere to.
        The architecture is capable of dealing with interleaved inputs, out
        of order inputs, and ambiguous inputs where heuristics must be
        applied --- all have arisen and been successfully accommodated in
        the Postfix log parser.

\end{description}

\section{Results}

\label{Results}

Parsing efficiency is an obvious concern when the Postfix log parser
routinely needs to parse large log files.  The mail server which generated
the log files used in testing the Postfix log parser accepts approximately
10,000 mails for 700 users per day; median log file size is 50 MB,
containing 285,000 log lines --- large scale mail servers would have much
larger log files.  When generating the timing data used in this section,
\numberOFlogFILES{} log files (totaling 10.08 GB, \numberOFlogLINEShuman{}
log lines) were each parsed 10 times and the parsing times averaged.
Saving results to the database was disabled for the test runs, because the
tests are aimed at measuring the speed of the Postfix log parser rather
than the speed of the database.  The computer used for test runs is a Dell
Optiplex 745 described in \tableref{Computer used to generate statistics},
dedicated to the task of gathering statistics from test runs.  Parsing all
\numberOFlogFILES{} log files in one run took
\input{build/include-timing-run-duration}, averaging
\input{build/include-timing-run-throughput.tex}.

% DO NOT REFORMAT!

\begin{table}[htbp]
    \caption{Computer used to generate statistics}
    \empty{}\label{Computer used to generate statistics}
    \begin{tabular}[]{ll}
        \tabletopline{}%
        Component  & Component in use                                   \\
        \tablemiddleline{}%
        CPU        & 1 dual core 2.40GHz Intel\textregistered{}
                     Core\texttrademark{}2 CPU,                      
                     with 32KB L1 and 4MB L2 cache.                     \\
        RAM        & 2GB 667 MHz DDR RAM\@.                             \\
        Hard disk  & 1 Seagate Barracuda 7200 RPM 250GB SATA hard disk. \\
        \tablebottomline{}%
    \end{tabular}
\end{table}

\subsection{Architecture Scalability: Input Size}

An important property of a parser is how parsing time scales relative to
input size: linearly, polynomially, or exponentially?  \Graphref{Parsing
time, log file size, and number of log lines} shows the parsing time in
seconds, log file size in MB, and number of log lines in tens of thousands,
for each of the \numberOFlogFILES{} log files.  The lines on the graph run
roughly in parallel, giving the impression that the algorithm scales
linearly with input size.  This impression is borne out by
\graphref{parsing time vs log file size vs number lines factor}: the ratios
are tightly banded across the graph, showing that the algorithm scales
linearly.  The ratios increase (i.e.\ improve) for log files 22 and 62--68
despite their large size; that unusually large size is due to mail
forwarding loops resulting in a greatly increased number of mails delivered
and log lines generated.

\showgraph{build/graph-input-size-vs-parsing-time}{Parsing time, log file size,
and number of log lines}{Parsing time, log file size, and number of log lines}
\showgraph{build/graph-input-size-vs-parsing-time-ratio}{Ratio of number of
log lines and log file size to parsing time}{parsing time vs log file size vs
number lines factor}

\subsection{Rule Ordering}

\label{Rule ordering}

At the time of writing the Postfix log parser has \numberOFrules{}
different rules: as shown in \graphref{rule hits graph} 10\% of the rules
XXX PERCENTAGE match the vast majority of log lines, with the remaining log lines split
across the other 90\% of the rules, similar to a Power Law distribution.
Assuming that the distribution of log lines is reasonably consistent over
time, parser efficiency should benefit from trying more frequently matching
rules before those which match less frequently.  To test this hypothesis
three full test runs were performed with different rule orderings:

\begin{description}[shuffled]

    \item [optimal]  Hypothetically the best order: rules which match most
        often will be tried first.

    \item [shuffled] Random ordering, intended to represent an unsorted rule
        set.  Note that the ordering will change every time the parser is
        executed, so 10 different orderings will be generated for each log
        file in the test run.

    \item [reverse] Hypothetically the worst order: the most frequently
        matching rules will be tried last.

\end{description}

\Graphref{Parsing time relative to shuffled} shows the parsing times of
optimal and reverse orderings as a percentage of shuffled ordering parsing
time.  This optimisation provides a modest but worthwhile reduction in
parsing time of approximately 10\% with normal log files, less when a mail
loop occurs and the distribution of log lines is unusual.  Optimal rule
ordering has other benefits, described in \sectionref{Architecture
Scalability: Number of Rules}.  \showgraph{build/graph-hits}{Number of log
lines matched by each rule}{rule hits graph}
\showgraph{build/graph-optimal-and-reverse-vs-shuffle}{Parsing time
relative to shuffled ordering}{Parsing time relative to shuffled}

\subsection{Architecture Scalability: Number of Rules}

\label{Architecture Scalability: Number of Rules}

How any architecture scales as the number of rules increases is important,
but it is particularly important for this architecture because it is
expected that typical parsers will have a large number of rules.  There are
\numberOFrules{} rules in the full Postfix log parser ruleset (it is tested
with \numberOFlogFILESall{} log files), whereas the
minimum number of rules required to parse the \numberOFlogFILES{} log files
is \numberOFrulesMINIMUM{}, \numberOFrulesMINIMUMpercentage{} of the full
ruleset.  A second set of test runs was performed using the minimum
ruleset, and the parsing times compared to those generated using the full
ruleset: the percentage parsing time increase when using the full ruleset
instead of the minimal ruleset for optimal, shuffled and reverse orderings
is shown in \graphref{Percentage parsing time increase of maximum ruleset
over minimum ruleset}.  Clearly the increased number of rules has a
noticeable performance impact with reverse ordering, and a lesser impact
with shuffled ordering.  The optimal ordering shows a mean increase of
\input{build/include-full-ruleset-vs-minimum-ruleset} in parsing time for a
\numberOFrulesMAXIMUMpercentage{} increase in the number of rules.  These
results show that the architecture scales very well as the number of rules
increases, and that optimally ordering the rules is an important
optimisation enabling this.
\showgraph{build/graph-full-ruleset-vs-minimum-ruleset}{Percentage parsing
time increase of maximum ruleset over minimum ruleset}{Percentage parsing
time increase of maximum ruleset over minimum ruleset}

\subsection{Coverage}

\label{coverage}

The Postfix log parser has two different types of coverage to be measured:
log lines correctly parsed, and mail delivery attempts correctly understood
(the former is a requirement for the latter to be achieved).  Improving the
former is less difficult, as usually it just requires new rules to be
written; improving the latter is more difficult and intrusive as it
requires adding or changing actions, and it can be much harder to notice
that a deficiency exists.

Correct parsing of log lines must be measured first.  Warnings are issued
for any log lines that are not parsed; no such warnings are issued while
parsing the \numberOFlogFILES{} log files, therefore
there are zero false negatives.  False positives are harder to
quantify: manually verifying that the correct rule parsed each of the
\numberOFlogLINES{} log lines is infeasible.  A random sample of 6,039 log
lines (0.00994\% of \numberOFlogLINES{}) was parsed and the results
manually verified by inspection to ensure that the correct rule parsed each
log line.  The sample results contained zero false positives, and this
check has been automated to ensure continued accuracy.  The authors are
confident that zero false positives occur when parsing the
\numberOFlogFILES{} log files.

The proportion of mail delivery attempts correctly understood is much more
difficult to determine accurately than the proportion of log lines
correctly parsed.  The implementation can dump its state tables in a human
readable form; examining these tables with reference to the log files and
database is the best way to detect misunderstood mail delivery attempts.
The Postfix log parser issues warnings when it detects any errors or
discrepancies, alerting the user to the problem.  There should be few or no
warnings during parsing, and when parsing is finished the state table
should only contain entries for mail delivery attempts starting before or
ending after the log file.  A second sample of 6000 log lines was parsed
with all debugging options enabled, resulting in 167,448 lines of output.
All 167,448 lines were examined in conjunction with the log segment and a
dump of the resulting database, verifying that for each of the log lines
the Postfix log parser performed correctly.  The implementation produced 4
warnings about deficiencies in the log segment, 10 mails correctly
remaining in the state tables, and 1625 correct entries in the database: it
produced 0 false positives.  No error or warning messages were produced,
therefore there were no false negatives.  Given the evidence detailed
above, the authors are confident that zero false positives or negatives
occur when parsing the \numberOFlogFILES{} log files.

Experience implementing the Postfix log parser shows that full input
coverage is relatively easy to achieve with this architecture, and that
with enough time and effort full understanding of the input is possible.
Postfix log files would require substantial time and effort to correctly
parse regardless of the architecture used; this architecture enables an
iterative approach to be used (similar to Stepwise
Refinement~\cite{stepwise-refinement}), as is practiced in many other
software engineering disciplines.

\section{Conclusion}

This architecture's greatest strength is the ease with which it can be
adapted to deal with new requirements and inputs.  Parsing a variation of
an existing input is a trivial task: simply modify an existing rule or add
a new rule with an appropriate regex and the task is complete.  Parsing a
new category of input is achieved by writing a new action and appropriate
rules; quite often the new action will not need to interact with existing
actions, but when interaction is required the framework provides the
necessary facilities.  The decoupling of rules from actions allows
different sets of rules to be used with the same actions, e.g.\ a parser
might have actions to process versions one and two of a file format; by
choosing the appropriate ruleset the parser will parse version one, or
version two, or both versions.  Decoupling also allows other approaches to
rule management, as discussed in \sectionref{Rules}.  The architecture
makes it possible to apply commonly used programming techniques (such as
object orientation, inheritance, composition, delegation, roles,
modularisation, or closures) when designing and implementing a parser,
simplifying the process of working within a team or when developing and
testing additional functionality.  This architecture is ideally suited to
parsing inputs where the input is not fully understood or does not follow a
fixed grammar: the architecture warns about unparsed inputs and other
errors, but continues on parsing as best it can, allowing the developer of
a new parser to decide which deficiencies are most important and require
attention first, rather than being forced to fix the first error that
arises.

The data gathered by the Postfix log parser provides the foundation for the
future of this project: applying machine-learning algorithms to the data to
analyse and optimise the set of anti-spam measures in use, followed by
identifying patterns in the data that could be used to write new filters to
recognise and reject spam rather than accepting it.  The parser provides
the data in a normalised form that is far easier to use as input to new or
existing algorithm implementations than trying to adapt each algorithm to
extract data directly from the log files.  The current focus is on
clustering and decision trees to optimise the order in which rules are
applied; future efforts will involve using data gathered by the parser to
train and test new filters.  This task is similar to analysing a black
box application based on its inputs and outputs, and this approach could be
applied to analyse the behaviour of any system given sufficient log
messages to parse.  An alternate approach to black box optimisation that
uses application profiling in conjunction with the application's error
messages to improve the error messages shown to users is described
in~\cite{black-box-error-reporting}; profiling data may be useful in
supplementing systems that fail to provide adequate log messages.

The Postfix log file parser based on this architecture provides a basis for
systems administrators to monitor the effectiveness of their anti-spam
measures and adapt their defences to combat new techniques used by those
sending spam.  This parser is a fully usable application, built to address
a genuine need, rather than a proof of concept whose sole purpose is to
illustrate a new idea; it deals with the oddities and difficulties that
occur in the real world, rather than a clean, idealised scenario developed
to showcase the best features of a new approach.

\bibliographystyle{spmpsci}
\bibliography{logparser-bibliography.bib}

\end{document}
