% $Id$
% XXX REMOVE draft OPTION
\documentclass[draft]{svmult}

% Useful stuff for math mode.
\usepackage{amstext}
% Include images
\usepackage[final]{graphicx}
\usepackage{url}

\usepackage{lastpage}

% Extra footnote functionality, including references to earlier footnotes.
\usepackage[bottom]{footmisc}

% Tell Latex to use scalable fonts
\usepackage{type1cm}

% Extra packages recommended by Springer.
\usepackage{mathptmx}
\usepackage{helvet}
\usepackage{courier}
\usepackage{makeidx}
\usepackage{multicol}

% Embed SVN Id etc.
\usepackage{svn}

% XXX REMOVE THESE AND CHECK RESULTING LENGTH
%\setlength{\floatsep}{0pt}
%\setlength{\intextsep}{0pt}
%\setlength{\textfloatsep}{0pt}

% Acronyms and glossary entries.
% global=true prevents acronyms being expanded twice
\usepackage[global=true,acronym=true]{glossary}
%\makeglossary{}
%\makeacronym{}

% Nicer citation lists - smaller spaces.
\usepackage{cite}
% Check for unused references.
% \usepackage{refcheck}
\usepackage{varioref}
% Warnings rather than errors when references cross page boundaries.
\vrefwarning{}
% Errors when loops are encountered
\vrefshowerrors{}

% \showgraph{filename}{caption}{label}
\newcommand{\showgraph}[3]{%
    \begin{figure}[btp]%
        \caption{#2}\label{#3}%
        \includegraphics{#1}%
    \end{figure}%
}

%\showtable{filename}{caption}{label}
\newcommand{\showtable}[3]{
    \begin{table}[btp]
        \caption{#2}\label{#3}
        \input{#1}
    \end{table}
}

\newcommand{\tabletopline}[0]{%
    \hline%
    \noalign{\smallskip}%
}

\newcommand{\tablebottomline}[0]{%
    \noalign{\smallskip}%
    \hline%
}

\newcommand{\tablemiddleline}[0]{%
    \noalign{\smallskip}%
    \hline%
    \noalign{\smallskip}%
}

% Replacement for \ref{}, adds the page number too.
\newcommand{\refwithpage}[1]{%
    \empty{}\vref{#1}%
}
% Shorten the text used for references with page numbers.
\renewcommand{\reftextfaraway}[1]{%
    [p.~\pageref{#1}]%
}
% section references, automatically add \textsection
\newcommand{\sectionref}[1]{%
    \textsection{}\vref*{#1}%
}

\newcommand{\refwithlabel}[2]{%
    #1~\vref{#2}%
}
% table references, for consistent formatting.
\newcommand{\graphref}[1]{%
    \refwithlabel{fig.}{#1}%
}
\newcommand{\Graphref}[1]{%
    \refwithlabel{Figure}{#1}%
}
\newcommand{\tableref}[1]{%
    \refwithlabel{tbl.}{#1}%
}
\newcommand{\Tableref}[1]{%
    \refwithlabel{Table}{#1}%
}

% A command to format a Postfix daemon's name
\newcommand{\daemon}[1]{%
    \texttt{postfix/#1}%
}

\newcommand{\tab}[0]{%
    \hspace*{2em}%
}

\newcommand{\numberOFlogFILES}[0]{%
    93%
}

\newcommand{\numberOFrules}[0]{%
    169%
}

\newcommand{\numberOFruleINTERSECTIONS}[0]{%
    14196%
}

\newcommand{\numberOFrulesMINIMUM}[0]{%
    115%
}

\newcommand{\numberOFlogFILESall}[0]{%
    522%
}

% \numberOFrulesMINIMUM as percentage of \numberOFrules
\newcommand{\numberOFrulesMINIMUMpercentage}[0]{%
    68.04\%%
}

% \numberOFrules as percentage increase of \numberOFrulesMINIMUM
\newcommand{\numberOFrulesMAXIMUMpercentage}[0]{%
    46.95\%%
}

\newcommand{\numberOFlogLINES}[0]{%
    60,721,709%
}

\newcommand{\numberOFlogLINEShuman}[0]{%
    60.72 million%
}

\newcommand{\numberOFactions}[0]{%
    18%
}

\begin{document}

\title*{A user-extensible and adaptable parser architecture}
\author{John Tobin and Carl Vogel}
\institute{John Tobin \at{} School of Computer Science and Statistics,
Trinity College, Dublin 2, Ireland. \newline{} \email{tobinjt@cs.tcd.ie}
\and{} Carl Vogel \at{} School of Computer Science and Statistics,
Trinity College, Dublin 2, Ireland. \newline{} \email{vogel@cs.tcd.ie}}
\maketitle

\abstract{%
    Some parsers need to be very precise and strict in what they parse, yet
    must allow users to easily adapt or extend the parser to parse new
    inputs, without requiring that the user have an in-depth knowledge and
    understanding of the parser's internal workings.  This paper presents a
    novel parsing architecture designed for Postfix log files which aims to
    make the process of parsing new inputs as simple as possible, enabling
    users to trivially add new rules (to parse variants of existing inputs)
    and relatively easily add new actions (to process a previously unknown
    class of input).  The architecture scales linearly as the number of
    rules and/or size of input increases, making it suitable for parsing
    large corpora or months of accumulated data.
}

%\SVN$Id$
%\SVNId{}  \today{}

\newacronym{PDA}{Pushdown Automata}{}
\newacronym{FA}{Finite Automata}{}
\newacronym{CFL}{Context Free Languages}{}
\newacronym{SLCT}{Simple Logfile Clustering Tool}{}
\newacronym{SMTP}{Simple Mail Transfer Protocol}{}

\section{Introduction}

\label{Introduction}

The architecture described herein was developed as part of a larger project
to improve anti-spam techniques by analysing the performance of the set of
filters currently in use, optimising the order and membership of the set
based on that analysis, and developing supplemental filters where there is
a deficiency.  Most anti-spam techniques are
content-based~\cite{a-plan-for-spam, word-stemming, relaxed-online-svms},
which requires accepting the mail in the first place, but rejecting mail
during the delivery attempt is preferable: senders of non-spam mail that is
mistakenly rejected will receive an immediate non-delivery notice; load is
reduced on mail servers (allowing more intensive content-based techniques
to be used); users have less mail to wade through.  Improving the
performance of anti-spam techniques which are applied when mail is being
transferred via \SMTP{}\footnotemark{} is the goal of this project, by
providing a platform for reasoning about anti-spam filters.  The approach
chosen to measure performance was to analyse the log files produced by the
mail server, rather than modifying the mail server to generate performance
statistics: this approach improves the chances of other sites testing and
utilising the software.  The need arose for a parser capable of dealing
with the great number and variety of log lines produced by
Postfix~\cite{postfix}, the \SMTP{} server in use.  The parser must be
designed so that adding support for parsing new log lines is a simple task,
as the log lines to be parsed will change over time.  The variety in log
lines occurs for several reasons:\footnotetext{Simple Mail Transfer
Protocol transfers mail from the sender to one or more recipients across
the Internet.  It is a simple, human readable, plain text protocol, making
it quite easy to test and debug problems with it.  The original protocol
definition is RFC~821~\cite{RFC821}, updated in RFC~2821~\cite{RFC2821}.}

\begin{itemize}

    \item Log lines differ between versions of Postfix.
        
    \item The mail administrator can define custom rejection
        messages.

    \item External resources utilised by Postfix (e.g.\ policy
        servers~\cite{policy-servers} or DNSBLs) can also change their
        messages, sometimes without warning.

\end{itemize}

It was hoped to reuse an existing parser rather than starting from scratch:
several parsers were considered and rejected because their parsing coverage
was too low, their parsing too inexact, and/or they did not extract
sufficient data.  The effort required to adapt and improve an existing
parser was judged to be greater than the effort to write one, as the
techniques used by the existing parsers severely limited their potential:
some ignored the majority of log lines, parsing specific log lines with
great accuracy, but without any provision for readily parsing new or
similar log lines; others sloppily parsed the majority of log lines, but
were incapable of distinguishing between log lines of the same type, e.g.\
rejections, and extracted only a small portion of the data required.  The
only prior published work on the subject of parsing Postfix log files that
we are aware of is~\cite{log-mail-analyser}, which aims to present
correlated log data in a form suitable for a systems administrator to
search using the myriad of standard Unix text processing utilities already
available.  A full state of the art review is outside the scope of this
paper but will be included in the thesis resulting from this work.
The relationship to another procedural framework (ATN) is discussed below.

The solution developed is conceptually simple: provide a few generic
functions (\textit{actions\/}), each capable of dealing with an entire
category of log lines (e.g.\ rejecting a mail), accompanied by a multitude
of precise patterns (\textit{rules\/}), each of which matches all log lines
of a specific type, and only that type (e.g.\ rejection by a specific
DNSBL).  It is an accepted standard to separate the parsing procedure from
the declarative grammar it operates with.  Part of the novelty here is in
the way that the grammar is itself partially procedural (each action is a
separate procedure).  This architecture is ideally suited to parsing inputs
where the input is not fully understood or does not conform to a fixed
grammar: the architecture warns about unparsed inputs and other errors, but
continues on parsing as best it can, allowing the developer of a new parser
to decide which deficiencies are most important and require attention
first, rather than being forced to fix the first error which arises.

\section{Architecture}

\label{Architecture}

The architecture is split into three sections: framework, actions and
rules.  Each will be discussed separately, but first an overview:

\begin{description}

    \item [Framework]  The framework is the structure which actions and
        rules plug into.  It provides the parsing loop, loading and
        validation of rules, shared data storage, storage of results, and
        other support functions.

    \item [Actions]  Each action performs the work required to deal with a
        category of log lines, e.g.\ processing data from rejections.

    \item [Rules]  The rules are responsible for classifying or matching
        log lines, specifying the action to invoke and the data to be
        extracted from the log line.

\end{description}

For each input log line the framework tries each rule in turn until it
finds one which matches the input log line, then invokes the action
specified by that rule.

Decoupling the parsing rules from their associated actions and the
framework allows new rules to be written and tested without requiring
modifications to the parser source code, significantly lowering the barrier
to entry for casual users who need to parse new log lines, e.g.\ part-time
systems administrators attempting to reduce spam.  Decoupling the actions
from the framework simplifies both framework and actions: the framework
provides services to the actions, but does not need to perform any tasks
specific to the input being parsed; actions benefit from having services
provided by the framework, freeing them to concentrate on the task of
accurately and correctly processing the information provided by rules.

Decoupling also creates a clear separation of functionality: rules handle
low level details of identifying log lines and extracting data; actions
handle the higher level details of assembling the required data, dealing
with the intricacies of the input being parsed, complications arising,
etc.;\ the framework provides services to actions and manages the parsing
process.

There is some similarity (shown in \tableref{Similarities with ATN})
between the architecture and William Wood's Augmented Transition Networks
(ATN)~\cite{atns, nlpip}, a tool used in Computational Linguistics for
creating grammars to parse or generate sentences.  The resemblance between
ATN and the architecture is accidental, but it is interesting how two
different approaches have a similar division of responsibilities, while
having different semantics.

% Do Not Reformat!

\begin{table}[hbtp]
    \caption{Similarities with ATN}\label{Similarities with ATN}
    % The \smallskip{} below stop the text hitting the lines.
    \begin{tabular}[]{lll}
        \hline
        \noalign{\smallskip}
        ATN           & Architecture    & Similarity                          \\
        \noalign{\smallskip}
        \hline
        \noalign{\smallskip}
        Networks      & Framework       & Determines the sequence of
                                          transitions or actions which        \\
                      &                 & constitutes a valid input.          \\
        Transitions   & Actions         & Save data and impose conditions the
                                          input must meet to be               \\
                      &                 & considered valid.                   \\
        Abbreviations & Rules           & Responsible for classifying input.  \\
        \noalign{\smallskip}
        \hline
        \noalign{\smallskip}
    \end{tabular}
\end{table}

\subsection{Framework}

\label{Framework}

The framework takes care of miscellaneous support functions and low level
details of parsing, freeing the authors of actions to concentrate on
writing productive code.  It provides the link between actions and rules,
allowing either to be improved independently of the other.  It provides
shared storage to pass data between actions, loads and validates rules,
controls parsing of log files, invokes actions, tracks how often each rule
matches to optimise rule ordering (\sectionref{Rule ordering}), and stores
results in the database.  The framework is largely independent of the
parser being written: the same functionality will be required for most or
all parsers, though some specialised support functions may be provided.
The framework is the core of the architecture and is deliberately quite
simple: the rules deal with the variation in log lines, and the actions
deal with the intricacies and complications encountered when parsing.

The function which finds the rule matching the log line and invokes the
requested action can be expressed in pseudo-code as:

% DO NOT REFORMAT!

\begin{verbatim}
for each line in the input files: 
    for each rule defined by the user: 
        if this rule matches the input line:
            perform the action specified by the rule
            skip the remaining rules
            process the next input line
    warn the user that the input line was not parsed
\end{verbatim}

\subsection{Actions}

\label{Actions}

Each action contains the code required to deal with a particular type of
log lines, e.g.\ rejections.  The actions are parser-specific: each parser
author will need to write the actions required from scratch, unless
extending an existing parser.  In the Postfix log parser developed for this
project there are \numberOFactions{} actions and \numberOFrules{} rules,
with a very skewed distribution of rules to actions as shown in
\graphref{Distribution of rules per action}.  The distribution is skewed
for the same reason the architecture is so successful in enabling users to
parse new log lines: most new log lines can be parsed by a combination of a
new rule and an existing action.  It is expected that parsers utilising
this architecture will have a high ratio of rules to actions, hopefully
resulting in simpler rules and clearer distinctions between log lines.
\showgraph{build/graph-action-distribution}{Distribution of rules per
action}{Distribution of rules per action} Unsurprisingly the action with
the highest number of associated rules is \texttt{DELIVERY\_REJECTED}, the
action which handles Postfix rejecting a mail; it is followed by
\texttt{SAVE\_DATA}, the action responsible for handling informative
inputs: these are log lines that supplement the data gathered from other
log lines, but that do not by themselves represent interesting events.  The
third most common action is, perhaps surprisingly, \texttt{UNINTERESTING}:
this action does nothing when executed, allowing uninteresting log lines to
be parsed without causing any effects (it does not imply that the input is
ungrammatical or unparsed).  Generally rules specifying the
\texttt{UNINTERESTING} action are rules to parse log lines which are not
associated with a specific mail, e.g.\ warnings about configuration files
changing.  The framework warns about every unparsed log line, to alert the
user that they need to alter or extend their ruleset; the Postfix log
parser successfully parses every input line in the \numberOFlogFILESall{}
log files it is currently being tested with.  The remaining actions have
only one or two associated rules: in some cases this is because there will
only ever be one type of log line for that action, e.g.\ all log lines
showing that a remote client has connected are matched by a single rule and
handled by the \texttt{CONNECT} action; in other cases the action is
required to address a deficiency in the log files, or a complication which
arises during parsing.  Taking the \texttt{CONNECT} action as an example,
it creates a new data structure for each client connecting, and saves the
data extracted by the rule (in memory, subsequently to be entered into the
database).  If the data structure already exists it is treated as a symptom
of a bug, and the action issues a warning containing the full contents of
the data structure, plus the log line which has just been parsed.

The ability to add special purpose actions to deal with difficulties and
new requirements which are discovered during parser development is one of
the strengths of this architecture: instead of writing one monolithic
function which must be modified to support new behaviour, with all the
attendant risks of adversely affecting the existing parser, when a new
requirement arises an independent action can be written to satisfy it.  In
some cases the new action will require the cooperation of other actions,
e.g.\ to set or check a flag; there is a possibility of introducing failure
by modifying existing actions, but modifications will be smaller and occur
less frequently than with a monolithic architecture, thus failures should
be less likely, and will be easier to test for and diagnose.  The parser
can be written in an object oriented style, allowing sub-classes to extend
or override actions in addition to adding new actions; because each action
is an independent function, the sub-class need only modify the action it is
interested in, rather than reproducing large chunks of functionality.

During development of the Postfix log parser it became apparent that in
addition to the obvious variety in log lines, there were a large number of
complications to be overcome.  Some were the result of deficiencies in
Postfix's logging (some of which were improved on or resolved by later
versions of Postfix); others were due to the vagaries of process
scheduling, client behaviour, and administrative actions.  All were
successfully accommodated in the Postfix log parser: adding new actions was
enough to overcome several of the complications; others required
modifications to a single existing action to work around the difficulties;
the remainder were resolved by adapting actions to pass information to
subsequent actions (via the framework), which check for the extra
information and change their behaviour appropriately.

Actions may also return a modified input line which will be parsed as if
read from the input stream, allowing for a simplified version of cascaded
parsing~\cite{cascaded-parsing}.  This is a powerful facility allowing a
number of rules and actions to parse a single input line.

\subsection{Rules}

\label{Rules}

Rules categorise inputs, specifying both the regex to match against each
log line and the action to invoke when the match is successful.  The
Postfix log parser stores its rules in the same SQL database the results
are stored in, removing any doubt as to which set of rules was used to
produce a set of results; other implementations are free to store the rules
in whatever fashion suits their needs.  The framework requires each rule to
have \texttt{action} and \texttt{regex} attributes; each implementation is
free to add any additional attributes it requires.  The Postfix log parser
adds several attributes to optimise rule ordering (\sectionref{Rule
ordering}), to restrict which log lines rules can be matched against, and
to describe each rule.  Data is automatically extracted and saved from the
log line based on keywords in the rule's regex, but there is also a
facility to specify extra data to be saved.

Adding new rules is simple: define all the required attributes and append
the new rule to wherever the existing rules are stored.  The Postfix log
parser supplies a utility based on \SLCT{}~\cite{slct-paper} to aid in
producing regexes from previously unparsed inputs.  Storing the rules
separately allows other approaches to rule management to be used, e.g.\
instead of manually adding new rules when unparsed log lines are seen,
machine learning techniques could be used to automatically generate new
rules, based on the existing rules and input lines.  If this approach was
taken the choice of machine learning technique would be constrained by the
size of typical data sets: the \numberOFlogFILES{} log files used to
generate statistics contain \numberOFlogLINEShuman{} log lines.  Techniques
which require the full data set when training would be impractical;
Instance Based Learning~\cite{instance-based-learning} techniques which
automatically determine which inputs from the training set are valuable and
which inputs can be discarded would probably reduce the data required to a
manageable size.  This avenue of research and development has not been
pursued by the authors, but could easily be independently undertaken.

The architecture does not try to detect overlapping rules: that
responsibility is left to the author of the rules.  Unintentionally
overlapping rules lead to inconsistent parsing and data extraction because
the first matching rule wins, and the order in which rules are tried
against each line may change between log files.  Overlapping rules are
frequently a requirement, allowing a more specific rule to match some log
lines and a more general rule to match the majority, e.g.\ separating
\SMTP{} delivery to specific sites from \SMTP{} delivery to the rest of the
world.  This can simplify both the general rule and the more specific rule;
additionally rules from different sources can be combined with a minimum of
prior cooperation or modification required.  Overlapping rules should have
a priority attribute to specify the order in which they should be used;
negative priorities may be useful for catchall rules.

Storing the rules separately to the parser allows external tools to be
written to detect overlapping rules.  Traditional regexes are equivalent in
computational power to \FA{}; it is possible to convert between regexes and
\FA{}, so regex overlap could be detected by finding a non-empty
intersection of two \FA{}\@.  One formula for \FA{} intersection is given
in~\cite{intersection-of-NFA-using-Z}: $FA1 \cap{} FA2 =
\overline{(\overline{FA1} \cup{} \overline{FA2})}$.  Perl 5.10 regexes are
more powerful than traditional regexes, e.g.\ it is possible to match
correctly balanced brackets nested to an arbitrary depth:
\newline{}\tab{}\verb!/^[^<>]*(<(?:(?>[^<>]+)|(?1))*>)[^<>]*$/!\newline{}
Perl 5.10 regexes can maintain an arbitrary state stack and are thus
equivalent in computational power to \PDA{}, so overlap detection may
require calculating the intersection of two \PDA{}\@.  \CFL{} are
equivalent in computational power to \PDA{}, and intersection of two
\CFL{s} is not closed (i.e.\ intersection can produce a language which
cannot be parsed by a \CFL{}), e.g.\ $a^{*}b^{n}c^{n} \cap a^{n}b^{n}c^{*}
\rightarrow a^{n}b^{n}c^{n}$, so \PDA{} intersection may not be meaningful
in some cases.  Detecting overlap between $n$ regexes requires
$\frac{n(n-1)}{2}$ intersections to be calculated, giving $O(n^2x)$
complexity, where $O(x)$ is the complexity of calculating intersection.
This is certainly not a task to be performed on every parsing run:
detecting overlap amongst the Postfix log parser's \numberOFrules{} rules
would require calculating \numberOFruleINTERSECTIONS{} intersections.

It is possible to define pathological regexes, which fall into two main
categories: regexes which match every input line, and regexes which consume
large amounts of CPU time during matching.  Defining a regex which matches
every input line is trivial: \verb!/^/! matches the start of every input
line.  In general a regex will not consume a large amount of CPU time when
the match is successful; usually CPU time is consumed by a regex with a lot
of alteration and variable quantifiers failing to match.  This topic is
beyond the scope of this paper, see~\cite{mastering-regular-expressions}
for in-depth discussion.

The example rule in \tableref{Example rule} matches the following sample
log line logged by Postfix when a remote client connects to deliver mail:
\newline{}\tab{}\verb!connect from client.example.com[192.0.2.3]!

% Do not reformat this!
\begin{table}[htbp]
    \caption{Example rule}
    \empty{}\label{Example rule}
    \begin{tabular}[]{ll}
        \tabletopline{}%
        Attribute                 & Value                                            \\
        \tablemiddleline{}%
        %program                   & \daemon{smtpd}                                   \\
        regex                     & \verb!^connect from (__HOSTNAME__)\[(__IP__)\]$! \\
        action                    & CONNECT                                          \\
        %priority                  & 0                                                \\
        \tablebottomline{}%
    \end{tabular}
\end{table}


\subsection{Architecture Characteristics}

\label{Architecture characteristics}

\begin{description}

    \item [Context-free rules:]  Rules are context free: they do not take
        into account past or future inputs.  In context-free
        grammar terms the parser rules could be described as:
        $\text{\textless{}input\textgreater{}} \mapsto \text{rule-1} |
        \text{rule-2} | \text{rule-3} | \dots | \text{rule-n}$

    \item [Partially context-sensitive actions and cascaded parsing:]
        Actions can consult the results (or lack of results) of previous
        actions when executing, providing some context sensitivity.
        Actions can not inspect past or future inputs, however actions
        can return a modified input line that will be parsed as if read
        from the input stream, allowing for a simplified version of
        cascaded parsing~\cite{cascaded-parsing}.

    \item [Matching rules to log lines is simple:]  The first matching rule
        determines the action that will be invoked: there is no
        backtracking to try alternate rules, no attempt is made to pick a
        \textit{best\/} rule.

    \item [Transduction:]  The parser can be thought of as implementing
        transduction: it takes data in one form (log files) and transforms
        it to another form (a database); other output formats may be more
        suitable for other implementations.

    \item [Closer to Natural Language Processing than using a fixed
        grammar:] Unlike traditional parsers such as those used when
        compiling a programming language, this architecture does not have a
        fixed grammar specification which inputs must adhere to.  The
        architecture is capable of dealing with interleaved inputs, out of
        order inputs, and ambiguous inputs where heuristics must be applied
        --- all have arisen and been successfully dealt with in the Postfix
        log parser.

    \item [Line oriented:]  The architecture is line oriented at present:
        there is no facility for rules to consume more input or push unused
        input back onto the input stream (however actions may change input
        lines and cause the modified line to be re-parsed).  This was not a
        deliberate decision, rather a consequence of the line oriented
        nature of the log files; more flexible approaches could be pursued.

\end{description}

\section{Results}

Parsing efficiency is an obvious concern when the Postfix log parser
routinely needs to parse very large log files.  The mail server which
generated the log files used in testing the Postfix log parser accepts
approximately 10,000 mails for 700 users a day.  Median log file size is
50MB, containing 285,000 log lines --- large scale mail servers would have
much larger log files.  When generating the timing data used in this
section, \numberOFlogFILES{} log files (totaling 10.08 GB,
\numberOFlogLINEShuman{} log lines) were each parsed 10 times and the
parsing times averaged.  Saving results to the database was disabled for
the test runs, as that dominates the run time of the program, and the tests
are aimed at measuring the speed of the Postfix log parser rather than the
speed of the database and the disks the database is stored on.  The
computer used for test runs was a Dell Optiplex 745 described in
\tableref{Computer used to generate statistics}.  Parsing all
\numberOFlogFILES{} log files in one run took
\input{build/include-timing-run-duration}.  The computer was dedicated to
the task of gathering statistics from test runs, and was not used for any
other purpose while test runs were ongoing; any services not necessary for
running the tests were disabled.

% XXX MAYBE CHANGE POSITIONING
\begin{table}[htbp]
    \caption{Computer used to generate statistics}
    \empty{}\label{Computer used to generate statistics}
    \begin{tabular}[]{ll}
        \tabletopline{}
        Component  & Component used                                 \\
        \tablemiddleline{}
        CPU        & 1 dual core 2.40GHz Intel\textregistered{}
                     Core\texttrademark{}2 CPU,                      
                     with 32KB L1 and 4MB L2 cache.                 \\
        RAM        & 2GB 667 MHz DDR RAM\@.                         \\
        Hard disk  & 1 Seagate Barracuda 7200 RPM 250GB SATA disk.  \\
        \tablebottomline{}
    \end{tabular}
\end{table}

\subsection{Architecture Scalability: Input Size}

An important property of a parser is how parsing time scales relative to
input size: does it scale linearly, polynomially, or exponentially?
\Graphref{Parsing time, file size, and number of lines} shows the parsing
time in seconds, file size in MB, and number of log lines in tens of
thousands, for each of the \numberOFlogFILES{} log files.  All three lines
run roughly in parallel, giving the impression that the algorithm scales
linearly with input size.  This impression is borne out by
\graphref{parsing time vs file size vs number lines factor}: the ratios are
quite tightly banded across the graph, showing that the algorithm scales
linearly.  The ratio increases (i.e.\ improves) for log files 22 and 62--68
despite their larger than average size (due to a mail forwarding loop
resulting in a greatly increased number of mails delivered and log lines
generated).

\showgraph{build/graph-input-size-vs-parsing-time}{Parsing time, file size,
and number of lines}{Parsing time, file size, and number of lines}
\showgraph{build/graph-input-size-vs-parsing-time-ratio}{Ratio of number of
lines and file size to parsing time}{parsing time vs file size vs number
lines factor}

\subsection{Rule Ordering}

\label{Rule ordering}

At the time of writing there are \numberOFrules{} different rules, with the
top 10\% matching the vast majority of the log lines, and the remaining log
lines split across the other 90\% of the rules in a Power Law distribution
(as shown in \graphref{rule hits graph}).  Assuming that the distribution
of log lines is reasonably steady over time, parser efficiency should
benefit from trying more frequently matching rules before those which match
less frequently.  To test this hypothesis three full test runs were
performed with different rule orderings:

\begin{description}

    \item [optimal]  Hypothetically the most optimal order: rules which
        match most often will be tried first.

    \item [shuffle] Random ordering, intended to represent an unsorted rule
        set.  The rules will be shuffled once before use and will retain
        that ordering for the entirety of the log file.  Note that the
        ordering will change every time the parser is executed, so 10
        different orderings will be generated for each log file in the test
        run.

    \item [reverse] Hypothetically the worst order: the most frequently
        matching rules will be tried last.

\end{description}

\Graphref{Parsing time relative to shuffled} shows the parsing times of
optimal and reversed orderings as a percentage of the parsing time of
shuffled ordering.  Overall this optimisation provides a modest but
worthwhile reduction in parsing time of approximately 10\%.
\showgraph{build/graph-hits}{Hits per rule}{rule hits graph}
\showgraph{build/graph-optimal-and-reverse-vs-shuffle}{Parsing time
relative to shuffled ordering}{Parsing time relative to shuffled}

\subsection{Architecture Scalability: Number of Rules}

\newpage{} % XXX
How any architecture scales as the number of rules increases is important,
but it is particularly important in this architecture because it is
expected that there will be a large number of rules.  There are
\numberOFrules{} rules in the full Postfix log parser ruleset, whereas the
minimum number of rules required to parse the \numberOFlogFILES{} log files
is \numberOFrulesMINIMUM{}, \numberOFrulesMINIMUMpercentage{} of the full
ruleset.  A second set of statistics was generated using the minimum
ruleset and the parsing times compared to those generated using the full
ruleset: the percentage parsing time increase when using the full ruleset
instead of the minimal ruleset for optimal, shuffled and reversed orderings
is shown in \graphref{Percentage parsing time increase of maximum ruleset
over minimum ruleset}.  It is clear from \graphref{Percentage parsing time
increase of maximum ruleset over minimum ruleset} that the increased number
of rules has a noticeable performance impact with reverse ordering, and a
lesser impact with shuffled ordering.  The optimal ordering shows a mean
increase of \input{build/include-full-ruleset-vs-minimum-ruleset} in
parsing time for a \numberOFrulesMAXIMUMpercentage{} increase in the number
of rules.  These results show that the architecture scales very well as the
number of rules increases, and that sorting the rules is an important
optimisation enabling this.
\showgraph{build/graph-full-ruleset-vs-minimum-ruleset}{Percentage parsing
time increase of maximum ruleset over minimum ruleset}{Percentage parsing
time increase of maximum ruleset over minimum ruleset}

\subsection{Coverage}

\label{coverage}

The Postfix log parser's coverage of log files is separated into two parts:
log line coverage and mail coverage (the former is a requirement for the
latter to be achieved).  Improving the former is less intrusive, as it just
requires new rules to be written; improving the latter is more intrusive as
it requires changes to actions, and it can also be much harder to notice a
deficiency.

Warnings are issued for any log lines which are not parsed; no warnings are
issued for unparsed log lines while parsing the \numberOFlogFILES{} test
log files, so it can be safely concluded that there are zero false
negatives.  False positives are harder to quantify: manually verifying that
the correct regex parsed each of the \numberOFlogLINES{} log lines is
infeasible, making it impossible to quantify the false positive rate.  A
random sample of 6039 log lines (0.00994\% of the total) was parsed and the
results manually verified by inspection to ensure that the correct regex
parsed each log line.  The sample contained zero false positives, and this
check has been automated to ensure continued accuracy.

Coverage of mails is much more difficult to determine accurately than
coverage of log lines.  The implementation can dump its state tables in a
human readable form; examining these tables with reference to the log files
is the best way to detect mails which were not handled properly.  The
implementation issues warnings when it detects any errors, some of which
may alert the user to a problem.  There should be few or no warnings when
parsing, and when finished parsing the state table should only contain
entries for mails which had yet to be delivered when the log files ended,
or were accepted before the log files began.  A second sample of 6000 log
lines was parsed with all debugging options enabled, resulting in 167,448
lines of output.  All 167,448 lines were examined in conjunction with the
input log file and a dump of the resulting database, verifying that for
each of the input lines the implementation used the correct rule and
executed the correct action, which in turn produced the correct result and
inserted the correct data in the database.  The log segment produced 4
warnings (about mails which had started before the log file segment), 10
mails remaining in the state tables, and 1625 connections correctly entered
in the database.  No error messages were produced, therefore there were no
false negatives.  Given the evidence detailed above, the authors are
confident that the false positive rate when reconstructing a mail is zero.


\section{Conclusion}

The architecture's greatest strength is the ease with which it can be
adapted to deal with new requirements and inputs.  Parsing a variation of
an existing input is a trivial task: simply modify an existing rule or add
a new one with an appropriate regex and the task is complete; knowledge of
the framework is not required when writing rules.  Parsing a new category
of input is achieved by writing a new action and appropriate rules; quite
often the action will not need to interact directly with existing actions,
but when interaction is required the framework provides the necessary
facilities.  The decoupling of rules from actions allows different sets of
rules to be used with the same actions: e.g.\ a parser might have actions
to process versions one and two of a file format; the parser will then
parse either version one, version two, or both versions depending on the
ruleset selected.  The architecture makes it possible to apply commonly
used programming techniques such as object orientation, inheritance,
composition, delegation and roles when designing and implementing a parser,
simplifying the process of working within a team with shared
responsibility, or when developing and testing additional functionality.
This architecture is ideally suited to parsing inputs where the input is
not fully understood or does not follow a fixed grammar: the architecture
warns about unparsed inputs and other errors, but continues on parsing as
best it can, allowing the developer of a new parser to decide which
deficiencies are most important and require attention first, rather than
being forced to fix the first error which arises.

The dataset gathered by the Postfix log parser provides the foundation for
the future of this project: applying machine-learning algorithms to the
data to analyse and optimise the set of rules in use, followed by
identifying patterns in the data which could be used to write new filters
to recognise and reject spam rather than accepting it.  The parser provides
the data in a normalised form which is far easier to use as input to new or
existing implementations of machine-learning algorithms than trying to
adapt each algorithm to extract data directly from the log files.  The
current focus is on clustering and decision trees to optimise the order in
which rules are applied; future efforts will involve using data gathered by
the parser to train and test new filters.  This task is very similar to
optimising a black box application based on its inputs and outputs, and
this approach could be applied to optimising the behaviour of any system
given sufficient logging to analyse.  An alternative approach to black box
optimisation which uses application profiling in conjunction with the
application's error messages to improve the error messages shown to users
is described in~\cite{black-box-error-reporting}; profiling data may be
useful in supplementing systems which fail to provide adequate log
messages.

The Postfix log file parser based on this architecture provides a basis for
systems administrators to monitor the effectiveness of their anti-spam
measures and adapt their defences to combat the new techniques used by
those sending spam.  The parser is a fully usable application, built to
address a genuine need, rather than a proof of concept whose sole purpose
is to illustrate a new idea; it deals with the oddities and difficulties
which occur in the real world, rather than a clean, idealised scenario
developed to showcase the best features of a new approach.

\bibliographystyle{spmpsci}
\bibliography{logparser-bibliography.bib}

\end{document}
