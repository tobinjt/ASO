#!/usr/bin/env perl

# $Id$

# XXX UPDATE POD - OPTIONS ARE WRONG.

use strict;
use warnings;

use Time::HiRes qw(gettimeofday tv_interval);
my ($start_timestamp, $last_timestamp);
BEGIN {
    $start_timestamp = $last_timestamp = [gettimeofday()];
}

#use Devel::LeakTrace::Fast;
use Devel::Size qw(total_size);
use Data::Dumper;
#use Devel::Leak;
# Required for MProf
#local $Data::Dumper::Useperl = 1;
#use Devel::Cycle;
use lib q{..};
use ASO::Parser;
use Getopt::Long;
use IO::File;
use Pod::Usage;
use File::Temp qw(tempfile);
use Number::Format qw(format_bytes);
use File::stat;
use File::Spec::Functions qw(splitpath);
use feature qw(say);

# Required under Windows.
binmode STDOUT;
binmode STDERR;
$| = 1;

#my $devel_leak_handle; 
#my $devel_leak_count = Devel::Leak::NoteSV($devel_leak_handle);

my ($Options, $parser_options) = get_options();
if (not @ARGV) {
    die qq{Usage: $0 [options] logfile [logfiles]\n};
}
if ($Options->{q{memory-data}}) {
    $Options->{last_parser_size} = $Options->{last_process_size} = 0;
    $Options->{memory_in_fh} = IO::File->new(q{< /proc/self/statm})
        or die qq{$0: failed opening /proc/self/statm: $!\n};
    $Options->{memory_out_fh} = IO::File->new(q{> } . $Options->{q{memory-data}})
        or die qq{$0: failed opening } . $Options->{q{memory-data}} . qq{: $!\n};
    $Options->{memory_out_fh}->autoflush(1);
}

if ($Options->{q{timing-data}}) {
    $Options->{timing_file} = IO::File->new(q{> } . $Options->{q{timing-data}})
        or die qq{$0: failed opening } . $Options->{q{timing-data}} . qq{: $!\n};
    $Options->{timing_file}->autoflush(1);
}

# Save the startup times separately when profiling with Devel::Profile.
devel_profile_startup();
# Restrict the packages Devel::SmallProf will report about.
set_SmallProf_packages();

print_timing(q{Loaded modules and processed options}, q{});

warn qq{vim: set foldmethod=marker :\n};
my $parser = ASO::Parser->new($parser_options);
$parser->load_rules();
print_timing(q{Parser created}, q{});

if ($Options->{q{in-statefile}}) {
    $parser->load_state($Options->{q{in-statefile}});
}
print_timing(q{State loaded}, q{});

my $num_logs = scalar @ARGV;
# Start at one because that's what people expect.
my $num_logs_parsed = 1;
my %memory_usage;
foreach my $logfile (@ARGV) {
    my $stat = stat $logfile
        or die qq{$0: failed stating $logfile: $!\n};
    my $size = format_bytes($stat->size()) . q{B};
    print qq{Starting to read $logfile: $size, $num_logs_parsed/$num_logs\n};
    if (exists $Options->{memory_out_fh}) {
        print {$Options->{memory_out_fh}} qq{parsing $logfile\n};
    }
    $num_logs_parsed++;

    if ($Options->{q{in-rule-order}}) {
        my $rule_file = open_file(  $Options->{q{in-rule-order}},
                                    $logfile,
                                    q{individual-rule-order-files},
                                    q{<}
                                );
        $parser->load_rule_order_from($rule_file);
    }
    if ($Options->{q{out-rule-order}}) {
        my $rule_file = open_file(  $Options->{q{out-rule-order}},
                                    $logfile,
                                    q{individual-rule-order-files},
                                    q{>}
                                );
        $parser->save_rule_order_to($rule_file);
    }
    print_timing(qq{Rule order set up complete}, $logfile);

    print_timing(qq{Start parsing logfile}, $logfile);
    my $results = $parser->parse($logfile);
    if ($Options->{q{timing-data}}) {
        map { say {$Options->{timing_file}}
                qq{$_: $logfile: $results->{$_}} }
            sort grep /^num/, keys %{$results};
        map { say {$Options->{timing_file}}
                qq{$_ rules: $logfile: } . $results->{rules_by_program}->{$_}}
            sort keys %{$results->{rules_by_program}};
        map { say {$Options->{timing_file}}
                qq{$_ lines: $logfile: } . $results->{lines_by_program}->{$_}}
            sort keys %{$results->{lines_by_program}};
    }
    print_timing(qq{Parsed logfile}, $logfile);
    if (exists $Options->{memory_out_fh}) {
        $memory_usage{$logfile} = $Options->{last_process_size};
        local $Data::Dumper::Sortkeys = 1;
        print {$Options->{memory_out_fh}} Dumper(\%memory_usage);
    }

    $parser->post_parsing();
    print_timing(qq{Post parse}, $logfile);

    my $statefile = open_file(  $Options->{q{out-statefile}},
                                $logfile,
                                q{individual-state-files},
                                q{>}
                            );
    $parser->dump_state($statefile);
    print_timing(qq{Dumped state}, $logfile);

    print_timing(qq{Finished parsing}, $logfile);
}

$parser->update_check_order();
print_timing(qq{Updated hits}, q{});

# Some profiling modules dump their data to stdout, but we want to redirect
# that.  If the correct environment variable is set stdout will be redirected
# here and the data will go to the requested file.
redirect_stdout();
$last_timestamp = $start_timestamp;
print_timing(q{Total time}, q{});

#find_cycle($parser);
$Options = $parser = $parser_options = undef;
#my $devel_leak_new_count = Devel::Leak::CheckSV($devel_leak_handle);
#say qq{Devel::Leak says: }, $devel_leak_count, q{ -> }, $devel_leak_new_count;
exit 0;

sub devel_profile_startup {
    if (exists $ENV{PERL_PROFILE_SAVETIME}) {
        DB::reset();
        my $filename;
        if (exists $ENV{PERL_PROFILE_FILENAME}) {
            $filename = $ENV{PERL_PROFILE_FILENAME};
        } else {
            $filename = q{prof.out};
        }
        rename $filename, qq{$filename.startup}
            or warn qq{$0: failed renaming $filename: $!\n};
    }
}

sub set_SmallProf_packages {
    if (exists $INC{q{Devel/SmallProf.pm}}) {
        # Silence warnings about it only being used once.
        %DB::packages = ();
        %DB::packages = (
            q{main}                     => 1,
            q{ASO::DB}                  => 1,
            q{ASO::DB::Connection}      => 1,
            q{ASO::DB::Result}          => 1,
            q{ASO::DB::Rule}            => 1,
            q{ASO::Parser}              => 1,
            q{ASO::ProgressBar}         => 1,
            q{ASO::ProgressBar::Dummy}  => 1,
        );
    }
}

sub redirect_stdout {
    if (exists $ENV{REDIRECT_STDOUT}) {
        close STDOUT;
        open STDOUT, qq{> $ENV{REDIRECT_STDOUT}};
    }
}

sub get_options {
    # Start by extracting options from ASO::Parser.
    my @switches;
    my $parser_options = ASO::Parser::options_for_new();
    foreach my $option_type (keys %$parser_options) {
        if ($option_type =~ /toggle$/) {
            push @switches, map { qq{$_!} }
                keys %{$parser_options->{$option_type}};
        } elsif ($option_type =~ /argument$/) {
            push @switches, map { qq{$_=s} }
                keys %{$parser_options->{$option_type}};
        } else {
            die qq{$0: unknown option type $option_type\n};
        }
    }

    # Add logparser options.
    my %logparser_options = (
        q{in-statefile|i=s}                 => undef,
        q{out-statefile|o=s}                => q{state},
        q{individual-state-files}           => 0,
        q{out-rule-order=s}                 => undef,
        q{in-rule-order=s}                  => undef,
        q{individual-rule-order-files}      => 0,
        q{timing-data=s}                    => undef,
        q{memory-data=s}                    => undef,
        q{help|h}                           => 0,
        q{version|v}                        => 0,
    );
    push @switches, keys %logparser_options;

    # Now, default values.
    my %parser_defaults = (
        data_source => q{dbi:SQLite:dbname=../sql/db.sq3},
    );
    my %logparser_defaults = getopt_to_key(%logparser_options);

    # Finally process the command line.
    my %opts = (
        %parser_defaults,
        %logparser_defaults,
    );
    Getopt::Long::Configure qw(no_getopt_compat permute bundling);
    Getopt::Long::GetOptions(\%opts, @switches) or pod2usage(2);

    if ($opts{help}) {
        pod2usage(q{-exitval} => 0, q{-verbose} => 1);
    }
    if ($opts{version}) {
        my $version = q{$Id$};
        print qq{$0 version $version\n};
        exit 0;
    }

    if (exists $opts{q{perfect-rule-order}}
            and $opts{q{perfect-rule-order}} ne q{normal}
            and not defined $opts{q{in-rule-order}}) {
        die qq{$0: --perfect-rule-order requires --in-rule-order\n};
    }

    # Separate logparser and ASO::Parser options.
    my (%logparser, %parser);
    foreach my $option (keys %opts) {
        if (exists $logparser_defaults{$option}) {
            $logparser{$option} = $opts{$option};
        } else {
            $parser{$option} = $opts{$option};
        }
    }

    return (\%logparser, \%parser);
}

# Convert keys from Getopt::Long option specifiers to option names.
sub getopt_to_key {
    my (%opts) = @_;
    my %names;

    map {
        my ($key, $value) = ($_, $opts{$_});
        $key =~ s/[!|=].*//;
        $names{$key} = $value;
    } keys %opts;

    %names;
}

sub print_timing {
    my ($message, $filename) = @_;
    if ($Options->{q{timing-data}}) {
        my $now = [gettimeofday()];
        printf {$Options->{timing_file}} qq{$message: $filename: %.6f\n}, 
            tv_interval($last_timestamp, $now);
        $last_timestamp = $now;
    }
    print_memory($message, $filename);
}

sub print_memory {
    my ($message, $filename) = @_;
    if ($Options->{q{memory-data}}) {
        my $new_parser_size = total_size($parser);
        print_difference(qq{$message -  parser},
            $new_parser_size,
            $Options->{last_parser_size},
            $Options->{timing_file});
        $Options->{last_parser_size} = $new_parser_size;

        $Options->{memory_in_fh}->seek(0, 0);
        my $line = readline $Options->{memory_in_fh};
        $line =~ m/^(\d+)/;
        my $new_process_size = $1 * 1024;
        print_difference(qq{$message - process},
            $new_process_size,
            $Options->{last_process_size},
            $Options->{timing_file});
        $Options->{last_process_size} = $new_process_size;
    }
}

sub print_difference {
    my ($label, $new_size, $last_size, $filehandle) = @_;

    my $size_difference = $new_size - $last_size;
    my $sign = q{};
    if ($size_difference < 0) {
        $size_difference = 0 - $size_difference;
        $sign = q{-};
    }
    printf {$Options->{memory_out_fh}}
        qq{%46s: new size is %12d (%11s); increase is %12d (%11s)\n},
        $label,
        $new_size, format_bytes($new_size), 
        $sign . $size_difference, $sign . format_bytes($size_difference);
}

sub open_file {
    my ($filename, $logfile, $individualise, $mode) = @_;

    if ($Options->{$individualise}) {
        my ($volume, $dir, $will_be_suffix) = splitpath($logfile);
        $will_be_suffix =~ s/(.gz|.bz2|.zip|.lzo)$//;
        $filename = qq{$filename-$will_be_suffix};
    }

    my $fh = IO::File->new($mode . $filename)
        or die qq{Failed to open $filename: $!\n};
    return $fh;
}

=pod

=head1 NAME

logparser - parse Postfix log files

=head1 VERSION

Version $Id$

=head1 SYNOPSIS

    # Parse mail.log.1
    logparser mail.log.1
    # Parse mail.log.4, mail.log.3 and mail.log.2; save state to parser-state
    logparser --out-statefile parser-state mail.log.3 mail.log.2
    # Parse mail.log.1, loading state from the previous run
    logparser --in-statefile parser-state mail.log.1

    # Parse mail.log.1 without inserting results in the database to improve 
    # speed when testing new rules; also use a different database.
    logparser --skip_inserting_results --data_source 'dbi:SQLite:dbname=test-db.sq3' mail.log.1

=head1 DESCRIPTION

logparser is a thin wrapper around L<ASO::Parser> - it handles parsing of
multiple files, saving and loading state, plus any other housekeeping required
by L<ASO::Parser>.  It also performs various profiling related tasks, depending
on the profiling module, if any, in use.  L<ASO::Parser> utilises
L<IO::Uncompress::AnyUncompress> to read compressed files; see its documentation
for which compression formats it supports.

=head1 OPTIONS

Defaults are equivalent to:
    logparser --out-statefile 'state' \
              --data_source 'dbi:SQLite:dbname=../sql/db.sq3'

=over 4

=item --in-statefile FILE, -i FILE

Load state from a previous run from FILE.

=item --out-statefile FILE, -o FILE

Save state from this run to FILE.  If this option is not given state will be
saved to a file named 'state' in the current directory.

=item --individual-state-files

Instead of reusing the same state file when saving state, use one per logfile.
The directory component of the log file is stripped off, and the resulting
filename (plus a separating dash) is appended to the filename given to the
B<--out-statefile> option.  If the logfile is compressed, a suffix of .gz, .bz2,
.zip, or .lzo will be removed, to avoid confusion.

=item --timing-data FILE

Write timing data to FILE.

=item --data_source STRING

Specifies the database to use when loading rules and saving results.  Defaults
to 'dbi:SQLite:dbname=../sql/db.sq3'.  See L<DBI> for more information about the
format of this option.

=item --username USERNAME

The password to use when connecting to the database.

=item --password PASSWORD

The username to use when connecting to the database.

=item --year YEAR

When parsing log lines from previous years you must specify the year the log
lines are from.

Parse::Syslog will discard log lines which appear to come from the future.  If
today is 2008/01/01, and you're parsing log lines from 2007/06/01, the syslog
parser will assume the log line is from B<2008>/06/01 (because the year is not
included in the log line), decide it's from the future, and discard it.

=back



=head1 DEBUGGING OPTIONS

These options are not necessary for normal use, they are provided for debugging
new rules or the parser itself.

=over 4

=item --sort_rules optimal|shuffle|reverse

Whether to sort the rules for maximum efficiency (optimal), minimal efficiency
(reverse), or randomly (shuffle).  Run time increases by about 20% when using
reverse sorting, though it is highly data dependant.  This is useful for
detecting overlapping rules: you should get exactly the same results in the
database regardless of which ordering you use - if the results change it's an
indication you have overlapping rules.

=item --discard_compiled_regex

By default every rule's regex is compiled once and cached; this switch disables
that caching so each regex is compiled every time it's used.  This incurs
approximately a 450% increase in run time, though again it's data dependant.
The main use for this option is when generating data to show how much slower the
parser is without caching of compiled regexs, though it may be possible that a
sufficiently complicated regex may require re-compilation each time it is used
(if you can come up with such a regex please inform the author).

=item --skip_inserting_results

Inserting results into the database is slow because of the additional disk or
network IO required, causing a huge slowdown in execution.  When testing new
rules or parser features disabling insertion of results is essential for a quick
testing cycle.  Default is to inset results.

=item --parse_lines_only

Parse log lines, but don't run the rule's action.  Useful for testing new rules,
as no new data will be added to the database but warnings will still be issued
for unparsed lines.  Execution is also much faster.  Default is to run actions.

=item --print_matching_regex

Print the regex and the line on each successful match, in the format:

  REGEX !!!! LINE

This might be useful to verify that a line is being matched by the regex you
expect.  This option still takes effect when B<--parse_lines_only> is specified.

=item --debug_results

Save extra data in each result to aid in debugging.  The data will not be stored
in the database, but it will be shown when connections are dumped for any
reason.  There is extra memory overhead associated with saving this data, and
a slight run time increase.  Default is to save the minimum data required.

=item --dump_committed_connections

Dump each connection that is committed.  This might be useful for serious
debugging, or verifying the parser's behaviour after changing the parser or
rules.  This will take effect even if --skip_inserting_results has been
specified.

=item --out-rule-order FILE

Write the list of rules used to FILE.  This can be loaded when parsing the same
log file again to change parsing performance.  See also B<--in-rule-order>,
B<--perfect-rule-order>, and B<--individual-rule-order-files>

=item --in-rule-order FILE

Read the list of rules to use from FILE, to change parsing performance.  See
also B<--out-rule-order>, B<--perfect-rule-order>, and
B<--individual-rule-order-files>

=item --perfect-rule-order best|normal|worst

How to use the list of rules loaded by B<--in-rule-order>:

=over 8

=item best

Use the correct rule first, so only one rule will be tried per log line.

=item normal

Don't use the list of rules; this is the default behaviour.

=item worst

Use the correct rule last, so every other applicable rule will be tried per log
line.

=back

This option exists for performance tests: how does normal rule ordering compare
to a perfect random ordering, where the first or last rule randomly selected for
each log line is the correct rule.  Default is normal, i.e. ignore the list of
rules.  See also B<--in-rule-order>, B<--out-rule-order>, <--individual-rule-order-files>

=item --individual-rule-order-files

Instead of reusing the same file when saving or loading rule order, use one per
logfile.  The directory component of the log file is stripped off, and the
resulting filename (plus a separating dash) is appended to the filename given to
the B<--out-rule-order> option.  If the logfile is compressed, a suffix of .gz,
.bz2, .zip, or .lzo will be removed, to avoid confusion.  See also
B<--in-rule-order>, B<--out-rule-order>, and B<--perfect-rule-order>.

=item --timing-data FILE

Save timing data to FILE.

=item --memory-data FILE

Save information about memory usage to FILE.

=back

=head1 CONFIGURATION AND ENVIRONMENT

A database containing rules and results is required, created by running:

    perl setup-database create-tables-sqlite3-generated.sql populate-rules.sql

This will create an SQLite3 database containing the required tables and the
default rules.  You can obviously use a different database, but you'll need to
adapt the SQL appropriately.  If you do this please send me the new SQL and I'll
add it to the distribution.

=head1 DEPENDENCIES

Modules packaged with logparser: L<ASO::Parser> (which has its own
dependencies).

Standard Perl modules: L<Getopt::Long>, L<Pod::Usage>, L<File::Temp>,
L<Time::HiRes>, L<File::stat>, L<File::Spec::Functions>.

Additional Perl modules: L<IO::Uncompress::AnyUncompress>, L<Number::Format>.

=head1 SEE ALSO

L<IO::Uncompress::AnyUncompress>

=head1 INCOMPATIBILITIES

None known.

=head1 BUGS AND LIMITATIONS

None known.  Bug reports and optionally patches welcome.

=head1 AUTHOR

John Tobin <tobinjt@cs.tcd.ie>

=head1 LICENCE AND COPYRIGHT

Copyright (c) 2006-2007 John Tobin <tobinjt@cs.tcd.ie>.  All rights reserved.

This module is free software; you can redistribute it and/or
modify it under the same terms as Perl itself. See L<perlartistic>.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. 

=cut

