% $Id$
\documentclass[draft]{svmult}

% Useful stuff for math mode.
\usepackage{amstext}
% Include images
\usepackage[final]{graphicx}
\usepackage{url}

\usepackage[style=altlist,number=none,toc=true,global=true]{glossary}
\makeglossary{}

\usepackage{lastpage}

% Extra footnote functionality, including references to earlier footnotes.
\usepackage[bottom]{footmisc}

% Tell Latex to use scalable fonts
\usepackage{type1cm}

% Extra packages recommended by Springer.
\usepackage{mathptmx}
\usepackage{helvet}
\usepackage{courier}
\usepackage{makeidx}
\usepackage{multicol}

% Nicer citation lists - smaller spaces.
\usepackage{cite}
% Check for unused references.
% \usepackage{refcheck}
\usepackage{varioref}
% Warnings rather than errors when references cross page boundaries.
\vrefwarning{}
% Errors when loops are encountered
\vrefshowerrors{}

% \showgraph{filename}{caption}{label}
\newcommand{\showgraph}[3]{%
    \begin{figure}[hbt!]
        \caption{#2}\label{#3}
        \includegraphics{#1}
    \end{figure}
}

%\showtable{filename}{caption}{label}
\newcommand{\showtable}[3]{
    \begin{table}[ht]
        \caption{#2}\label{#3}
        \input{#1}
    \end{table}
}

% Replacement for \ref{}, adds the page number too.
\newcommand{\refwithpage}[1]{%
    \empty{}\vref{#1}%
}
% Shorten the text used for references with page numbers.
\renewcommand{\reftextfaraway}[1]{%
    [p.~\pageref{#1}]%
}
% section references, automatically add \textsection
\newcommand{\sectionref}[1]{%
    \textsection{}\vref*{#1}%
}

% A command to format a Postfix daemon's name
\newcommand{\daemon}[1]{%
    \texttt{postfix/#1}%
}

\newcommand{\tab}[0]{%
    \hspace*{2em}%
}

\newcommand{\numberOFlogFILES}[0]{%
    93%
}

\newcommand{\numberOFrules}[0]{%
    169%
}

\newcommand{\numberOFrulesMINIMUM}[0]{%
    115%
}

% XXX FIGURE OUT HOW TO DO THIS AUTOMATICALLY
% \numberOFrulesMINIMUM as percentage of \numberOFrules
\newcommand{\numberOFrulesMINIMUMpercentage}[0]{%
    68.04\%%
}

% \numberOFrules as percentage increase of \numberOFrulesMINIMUM
\newcommand{\numberOFrulesMAXIMUMpercentage}[0]{%
    46.95\%%
}

\newcommand{\numberOFlogLINES}[0]{%
    60,721,709%
}

\newcommand{\numberOFlogLINEShuman}[0]{%
    60.72 million%
}

\newcommand{\numberOFactions}[0]{%
    18%
}

\begin{document}

\title*{A user-extensible and adaptable parser architecture}
\author{John Tobin and Carl Vogel}
\institute{John Tobin \at{} School of Computer Science and Statistics,
Trinity College, Dublin 2, Ireland. \newline{} \email{tobinjt@cs.tcd.ie}
\and{} Carl Vogel \at{} School of Computer Science and Statistics,
Trinity College, Dublin 2, Ireland. \newline{} \email{vogel@cs.tcd.ie}}
\maketitle

\abstract{%
    Some parsers need to be very precise and strict in what they parse, yet
    must allow users to easily adapt or extend the parser to parse new
    inputs, without requiring that the user have an in-depth knowledge and
    understanding of the parser's internal workings.  This paper presents a
    novel parsing architecture which aims to make the process of parsing
    new inputs as simple as possible, enabling users to trivially add new
    rules (to parse variants of existing inputs) and relatively easily add
    new actions (to process a previously unknown class of input).  The
    architecture scales linearly as the number of rules and/or size of
    input increases, making it suitable for parsing large corpora or months
    of accumulated data.
}


\newacronym{ATN}{Augmented Transition Networks}{description={
    Augmented Transition Networks, originally described in~\cite{atns} and
    further in~\cite{nlpip}, are a tool used in Computational Linguistics
    for creating grammars to parse or generate sentences,
}}

\newacronym{DNSBL}{DNS Blacklist}{description={
    A DNS Blacklist is a simple collaborative anti-spam technique used to
    reject or penalise email sent from mail servers reported to have sent
    large volumes of spam.
}}

\newacronym{SMTP}{Simple Mail Transfer Protocol}{description={
    SMTP is the protocol which transfers mail from the sender to the
    recipient across the Internet.  It is a simple, human readable, plain
    text protocol, making it quite easy to test and debug problems with it.
    A detailed description of \SMTP{} is beyond the scope of this document:
    the original protocol definition is RFC~821~\cite{RFC821}, updated in
    RFC~2821~\cite{RFC2821}.
}}

\section{Introduction}

\label{Introduction}

The architecture described herein was developed as part of a larger project
to improve anti-spam techniques by analysing the performance of the set of
filters currently in use, optimising the order and membership of the set
based on that analysis, and developing new measures to supplement the
existing filters where there is a deficiency.  Most anti-spam techniques
are content-based~\cite{a-plan-for-spam, word-stemming,
relaxed-online-svms}, viz they examine the contents of each mail to
determine whether it is spam or not.  Examining content requires accepting
the mail in the first place, but rejecting rather than accepting mail is
preferable in many cases: senders of non-spam mail that is mistakenly
rejected will receive an immediate non-delivery notice; load is reduced on
mail servers (allowing more intensive content-based measures to be used on
mail that is accepted); users have less mail to wade through.  Improving
the performance of anti-spam measures which are applied when mail is being
transferred via \SMTP{} is the goal of this project, by providing a
platform for reasoning about anti-spam filters.  The approach chosen to
measure performance was to analyse the log files produced by the mail
server, rather than modifying the mail server to generate performance
statistics; this approach improves the chances of other sites testing and
utilising the software.  The need arose for a parser capable of dealing
with the great number and variety of log lines produced by
Postfix~\cite{postfix}, the \SMTP{} server in use.  The parser must be
designed so that adding support for parsing new log lines is a simple task,
as the log lines will continue to evolve over time.  The variety in log
lines occurs for several reasons:

\begin{itemize}

    \item Log lines differ between versions of Postfix.
        
    \item The mail administrator can define custom rejection
        messages, resulting in custom log lines.

    \item External resources utilised by Postfix (e.g.\ policy
        servers~\cite{policy-servers} or \DNSBL{} lookups) can also change
        their messages, sometimes without warning.

\end{itemize}

Existing parsers were considered, but their parsing coverage was too low,
their parsing too inexact, and/or they did not extract sufficient data for
the purposes of this project.  The effort required to adapt and improve an
existing parser was judged to be greater than the effort to write one from
scratch, as the techniques used by the existing parsers severely limited
their potential: some ignored the majority of log lines, parsing specific
log lines with great accuracy, but in an inextensible fashion; others
sloppily parsed the majority of log lines, but were incapable of
distinguishing between log lines of the same type, e.g.\ rejections, and
extracted only a portion of the data required for this project.  

The solution developed is conceptually simple: provide a few generic
functions (\textit{actions\/}), each capable of dealing with an entire
category of log lines (e.g.\ rejections), accompanied by a multitude of
precise patterns (\textit{rules\/}), each of which matches all log lines of
a specific type, and only that type (e.g.\ rejection by a specific
\DNSBL{}).  This architecture is ideally suited to parsing inputs where the
input is not fully understood or does not follow a fixed grammar: the
architecture warns about unparsed inputs and other errors, but continues on
parsing as best it can, allowing the developer of a new parser to decide
which deficiencies are most important and require attention first, rather
than being forced to fix the first error which arises.

\section{Architecture}

\label{Architecture}

The architecture is split into three sections: framework, actions and
rules.  Each will be discussed separately, but first an overview:

\begin{description}

    \item [Framework]  The framework is the structure which actions and
        rules fit into.  It provides the parsing loop, loading and
        validation of rules, shared data storage, storage of results, and
        other support functions.

    \item [Actions]  The actions perform the work required to deal with a
        category of log lines, e.g.\ processing data from rejections.

    \item [Rules]  The rules are responsible for classifying or matching
        log lines, specifying the action to invoke and the data to be
        extracted from the log line.

\end{description}

The framework tries each rule in turn until it finds one which matches the
input log line, then invokes the action specified by that rule.

Decoupling the parsing rules from the associated actions and framework
allows new rules to be written and tested without requiring modifications
to the parser source code, significantly lowering the barrier to entry for
casual users who need to parse new log lines, e.g.\ part-time systems
administrators attempting to reduce spam.  Decoupling the actions from the
framework simplifies both framework and actions: the framework provides
services to the actions, and doesn't need to perform any tasks specific to
the input being parsed; actions benefit from having services provided by
the framework, freeing them to concentrate on the task of accurately and
correctly processing the information provided by rules.

Decoupling also creates a clear separation of functionality: rules handle
low level details of identifying and extracting data from log lines;
actions handle the higher level details of assembling the required data,
dealing with the intricacies of the input being parsed, complications
arising, etc; the framework provides services to actions and manages the
parsing process.

There is some similarity between the parser's design and William Wood's
\ATN{}~\cite{atns, nlpip}, a tool used in Computational Linguistics for
creating grammars to parse or generate sentences.  The resemblance between
\ATN{} and the parser is accidental, but it is interesting how two
different approaches have a similar division of responsibilities, while
having completely different implementations and semantics.

% Do Not Reformat!

\begin{table}[ht]
    \caption{Similarities with ATN}\label{Similarities with ATN}
    % The \smallskip{} below stop the text hitting the lines.
    \begin{tabular}[]{lll}
        \hline
        \noalign{\smallskip}
        \ATN{}        & Parser    & Similarity                          \\
        \noalign{\smallskip}
        \hline
        \noalign{\smallskip}
        Networks      & Framework & Determines the sequence of
                                    transitions or actions which        \\
                      &           & constitutes a valid input.          \\
        Transitions   & Actions   & Save data and impose conditions the
                                    input must meet to be               \\
                      &           & considered valid.                   \\
        Abbreviations & Rules     & Responsible for classifying input.  \\
        \noalign{\smallskip}
        \hline
        \noalign{\smallskip}
    \end{tabular}
\end{table}

\subsection{Framework}

\label{Framework}

The framework provides utility and support functions for the actions and
rules.  It provides shared storage to pass data between actions, most
notably the data structures used to represent each connection and mail.  It
loads and validates rules, controls parsing of log files, invokes actions,
tracks how often each rule matches (to optimise rule ordering, see
\sectionref{Rule ordering}), and stores results in the database.  The
framework is independent of the parser being written: the same
functionality will be required for most or all parsers.

The function which finds the rule matching the log line and invokes the
requested action can be expressed in pseudo-code (with indentation denoting
flow of control):

% DO NOT REFORMAT!

\begin{verbatim}
for each line in the input files: 
    for each rule defined by the user: 
        if this rule matches the input line:
            perform the action specified by the rule
            skip the remaining rules
            process the next input line
    warn the user that the input line was not parsed
\end{verbatim}

The framework takes care of miscellaneous support functions and low level
details of parsing, freeing the authors of actions to concentrate on
writing productive code.  It provides the link between actions and rules,
allowing either to be improved independently of the other.  The framework
is the core of the architecture and is deliberately quite simple: the rules
deal with the varying log lines, and the actions deal with the intricacies
and complications encountered when parsing.

\subsection{Actions}

\label{Actions}

Each action contains the code required to deal with log lines from a
particular category, e.g.\ rejections.  The actions are parser-specific:
each parser will need to write the actions it requires from scratch, unless
it's extending an existing parser.  In the Postfix parser developed for
this project there are \numberOFactions{} actions and \numberOFrules{}
rules; the distribution of rules to actions is very skewed, as shown in
Fig.~\refwithpage{Distribution of rules per action}.
\showgraph{build/plot-action-distribution}{Distribution of rules per
action}{Distribution of rules per action} The distribution is skewed for
the same reason the architecture is so successful in enabling users to
parse new log lines: most new log lines can be parsed by a combination of a
new rule and an existing action.  Unsurprisingly the action with the
highest number of associated rules is \texttt{REJECTION}, the action which
handles rejections; it's followed by \texttt{SAVE\_BY\_QUEUEID}, the action
responsible for handling informative inputs: these are log lines that
supplement the data gathered from other log lines (such as rejections), but
that don't by themselves represent interesting events.  The third most
common action is, perhaps surprisingly, \texttt{IGNORE}: there are many log
lines which are not of interest to the parser, e.g.\ warning messages about
DNS mismatches.  The parser warns about every unparsed log line, to alert
the user that they need to alter or extend their rules; the \texttt{IGNORE}
action does nothing when executed, allowing uninteresting log lines to be
parsed without causing any effects.  The remaining actions have only one or
two associated rules: in some cases this is because there will only ever be
one type of log line for that action, e.g.\ all log lines showing that a
remote client has connected are matched by a single rule and handled by the
\texttt{CONNECT} action; in other cases the action is required to address a
deficiency in the log files, or a complication which arises during parsing.

The ability to add special purpose actions to deal with difficulties and
new requirements which are discovered during parser development is one of
the strengths of this architecture: instead of writing one monolithic
function which must be modified to support new behaviour or resolve
challenges, with all the attendant risks of adversely affecting the
existing, working parser, when a new requirement arises an independent
action can be written to satisfy it.  In some cases the new action will
require the cooperation of other actions, e.g.\ to set or check a flag;
there is a possibility of introducing failure by modifying existing
actions, but modifications will be smaller and occur less frequently than
with a monolithic architecture, and thus failures should be less likely and
easier to test for and diagnose.  The parser can be written in an object
oriented style, allowing sub-classes to extend or override actions in
addition to adding new actions; because each action is an independent
function, the sub-class need only modify the action it is interested in,
rather than reproducing large chunks of functionality.

\label{complications}

During development of the parser it became apparent that in addition to the
obvious variety in log lines, there were a large number of complications to
be overcome.  Some were the result of deficiencies in Postfix's logging,
(some of those deficiencies were improved or resolved by later versions of
Postfix); others were due to the vagaries of process scheduling, client
behaviour, and administrative actions.  All were successfully accommodated
in the parser: adding new actions was enough for several of the
complications; others required modifications to a single existing action to
work around the difficulties; the remainder were resolved by adapting
actions to pass information to subsequent actions (via the framework),
those subsequent actions then check for the extra information and change
their behaviour appropriately.

Actions may also return a modified input line which will be parsed as if
read from the input stream, allowing for a simplified version of cascaded
parsing~\cite{cascaded-parsing}.  This is a powerful facility allowing a
generic rule and action to parse part of the input, followed by more
specific rules and actions parsing the remainder.

\subsection{Rules}

\label{Rules}

Rules are responsible for categorising inputs, specifying both the data to
be extracted from each log line and the action that should be invoked.  The
Postfix log parser stores the rules in the same SQL database that the
results are stored in; this removes any doubt as to which set of rules was
used to produce a set of results.  Other implementations are free to store
the rules in whatever fashion suits their needs.  Each rule defines several
attributes:

\begin{description}

    \item [program] The Postfix program whose log lines the rule should be
        applied to.  This avoids needlessly trying rules that won't match
        the log line, or worse, might match unintentionally.  Rules whose
        program is \texttt{*} will be tried against any log lines not
        parsed by program specific rules, allowing generic rules to be
        written.

    \item [regex] The Regular Expression to match the log line against.

    \item [result\_cols] Specifies how the fields in the log line will be
        extracted.  The format is:
        \tab{} \texttt{smtp\_code = 1; recipient = 2, sender = 4;} \newline
        i.e.\ semi-colon or comma separated assignment statements, with the
        variable name on the left and the matching capture from the regex
        on the right.

    \item [result\_data] Sometimes rules need to supply a piece of data
        which isn't present in the log line: e.g.\ setting
        \texttt{smtp\_code} when mail is accepted.  The format is the same
        as for \texttt{result\_cols}, except that arbitrary data is
        permitted on the right hand side of the assignment.

    \item [action] The action that will be invoked when this rule matches a
        log line.

    \item [hits] This counter is used to optimise the order in which rules
        are tried against the input lines.  Assuming that the distribution
        of log lines is reasonably consistent between log files, rules
        matching more frequently will be tried before rules matching less
        frequently, lowering the parser's execution time.

    \item [priority] This is the user-configurable companion to hits:
        priority overrides hits when sorting rules, allowing more
        specific rules to take precedence over more general rules.

\end{description}

The rules used by the Postfix log file parser define several additional
attributes; each implementation is free to add any additional attributes it
requires.

Adding new rules is simple: simply define all the required attributes and
append the new rule to wherever the existing rules are stored.  The
software developed for this project includes a utility based on
SLCT~\cite{slct-paper} to aid in producing regexes to parse previously
unparsed inputs.

\label{overlapping rules}

%XXX DO I NEED TO DISCUSS OVERLAPPING RULES\@?

The parser does not try to detect overlapping rules; that responsibility is
left to the author of the rules.  Unintentionally overlapping rules lead to
inconsistent parsing and data extraction because the first matching rule
wins and the order in which rules are tried against each line may change
between log files.  Overlapping rules are frequently a requirement,
allowing a more specific rule to match some log lines and a more general
rule to match the majority, e.g.\ separating \SMTP{} delivery to specific
sites from \SMTP{} delivery to the rest of the world.  The algorithm
provides a facility for specifying the order of overlapping rules: the
priority field in each rule (defaults to zero).  Rules are sorted by
priority, highest first, and then rules with the same priority are sorted
by the number of successful matches when parsing the previous log file.
Negative priorities may be useful for catchall rules.



\subsection{Architecture characteristics}

\label{Architecture characteristics}

\begin{description}

    \item [Context-free rules:]  Rules are context free: they do not take
        into account inputs which have previously been seen, nor do they
        perform any kind of lookahead to future inputs.  In context-free
        grammar terms the parser rules could be described as:
        $\text{\textless{}input\textgreater{}} \mapsto \text{rule-1} |
        \text{rule-2} | \text{rule-3} | \dots | \text{rule-n}$

    \item [Partially context-sensitive actions and cascaded parsing:]
        Actions can consult the results (or lack of results) of previous
        actions when executing, providing some context sensitivity.
        Actions can not inspect past or upcoming inputs, however allowing
        actions can return a modified input line that will be parsed as if
        read from the input stream, allowing for a simplified version of
        cascaded parsing~\cite{cascaded-parsing}.

    \item [Matching rules to log lines is simple:]  The first matching rule
        wins: there is no backtracking to try alternate rules, no attempt
        is made to pick a \textit{best\/} rule; the first rule which
        matches the line is the rule which parses it and determines the
        action that will be invoked.  The priority attribute may be used to
        change the ordering of rules.

    \item [Transduction:]  The parser can be thought of as implementing
        transduction: it takes data in one form (log files) and transforms
        it to another form (a database); other output formats may be more
        suitable for other implementations.

    \item [Closer to Natural Language Processing than using a fixed
        grammar:] Unlike traditional parsers such as those used when
        compiling a programming language, this architecture does not have a
        fixed grammar specification which inputs must adhere to.  The
        architecture is capable of dealing with overlapping inputs, out of
        order inputs, and ambiguous inputs where heuristics must be applied
        --- all have arisen and been successfully dealt with in the Postfix
        log parser.

    \item [Line oriented:]  The architecture is line oriented at present:
        there is no facility for rules to consume more input or push unused
        input back onto the input stream.  This was not a deliberate
        decision, rather a consequence of the line oriented nature of the
        log files; other implementations could pursue a more flexible
        approach.

\end{description}

\section{Results}

Parsing efficiency is an obvious concern when the parser routinely needs to
deal with 75 MB log files containing 300,000 log lines (generated daily on
a mail server handling mail for approximately 700 users --- large scale
mail servers would produce much larger log files on a daily basis).  When
generating the data for the graphs included in this document,
\numberOFlogFILES{} log files (totaling 10.08 GB, \numberOFlogLINEShuman{}
log lines) were each parsed 10 times, the results from first run discarded,
and the execution time for the remaining 9 runs averaged.

\subsection{Architecture Scalability: Input Size}

An important property of a parser is how execution time scales relative to
input size: does it scale linearly, polynomially, or exponentially?
Figure~\refwithpage{Execution time, file size, and number of lines} shows
the execution time in seconds, file size in MB, and tens of thousands of
log lines per log file.  All three lines run roughly in parallel, giving
the impression that the algorithm scales linearly with input size.  This
impression is borne out by fig.~\refwithpage{execution time vs file size vs
number lines factor} which plots the ratios of file size vs execution time,
and number of log lines vs execution time (higher is better in both cases).
As the reader can see the ratios are quite tightly banded across the graph,
showing that the algorithm scales linearly: the much larger log files at
points 62--68 on the X axis in fig.~\refwithpage{Execution time, file size,
and number of lines} actually cause the ratio to increase (i.e.\ improve),
rather than decrease.  The strange behaviour where larger log files are
parsed more efficiently is explained in \sectionref{Why are there dips in
the graphs?}.  \showgraph{build/plot-normal-filesize-numlines}{Execution
time, file size, and number of lines}{Execution time, file size, and number
of lines} \showgraph{build/plot-normal-filesize-numlines-factor}{Ratio of
file size and number of lines to execution time}{execution time vs file
size vs number lines factor}

\subsection{Rule ordering}

\label{Rule ordering}
\label{rule ordering for efficiency}
\label{rule efficiency}

At the time of writing there are \numberOFrules{} different rules, with the
top 10\% matching the vast majority of the log lines, and the remaining log
lines split across the other 90\% of the rules in a Power Law distribution
(as shown in fig.~\refwithpage{rule hits graph}).
\showgraph{build/plot-hits}{Hits per rule}{rule hits graph} Assuming that
the distribution of log lines is reasonably steady over time, parser
efficiency should benefit from trying more frequently matching rules before
those which match less frequently.  To test this hypothesis three full test
runs were performed with different rule orderings:

\begin{description}

    \item [normal]  Hypothetically the most optimal order: rules which
        match most often will be tried first.

    \item [shuffle] Random ordering, intended to represent an unsorted rule
        set.  The rules will be shuffled once before use and will retain
        that ordering for the entirety of the log file.  Note that the
        ordering will change every time the parser is executed, so 10
        different orderings will be generated for each log file in the test
        run.

    \item [reverse] Hypothetically the worst order: the most frequently
        matching rules will be tried last.

\end{description}

Figure~\refwithpage{percentage increase of shuffled and reversed over
normal} shows the percentage increase of execution times for shuffled and
reversed rules.  Overall this optimisation provides a modest but worthwhile
performance increase of over 10\%, for a small investment in time and
programming.
\showgraph{build/plot-normal-shuffle-reverse-factor}{Percentage increase of
execution time for different rule orderings}{percentage increase of
shuffled and reversed over normal}

\noindent\textbf{Why are there dips in the graphs?}

\label{Why are there dips in the graphs?}

The dips at log files 22 and 62--68 in fig.~\refwithpage{percentage
increase of shuffled and reversed over normal} correspond to peaks in log
file size in fig.~\refwithpage{Execution time, file size, and number of
lines}, and peaks in fig.~\refwithpage{execution time vs file size vs
number lines factor} (where a peak means that performance is better).  The
explanation for this took some time to arrive at, but it turns out to be
reasonably simple.  The large log files in question were caused by a mail
forwarding loop, where the distribution of log lines is quite different to
normal, resulting in different performance characteristics.  The vast
majority of log lines when a mail loop occurs are from Postfix components
which have a small number of rules associated with them, whereas in general
the majority of log lines are from the Postfix component (\daemon{smtpd})
with the highest number of rules.  Furthermore in normal log files the
majority of log lines are distributed much more evenly across rules than
the majority of log lines during a mail loop.

These two characteristics combine to reduce the average number or rules
required to parse a log line when there is a mail loop, as shown by the
peaks in fig.~\refwithpage{execution time vs file size vs number lines
factor} and troughs in fig.~\refwithpage{percentage increase of shuffled
and reversed over normal}.  When the rule ordering is reversed the majority
of log lines generated by a mail loop will be parsed using very few rules,
because there are very few rules for the Postfix components producing the
log lines.  Without a mail loop the Postfix components producing the
majority of log lines have a large number of rules associated with them,
and reverse ordering deliberately starts with the least likely rules and
moves towards the most likely.  This leads to a noticeable drop in the
average time required to parse a log line during a mail loop, as shown in
fig.~\refwithpage{percentage increase of shuffled and reversed over
normal}.  The number of rules which need to be consulted when the ordering
is shuffled varies between the optimal and the pessimal, with the
performance varying proportionally.

\subsection{Architecture Scalability: Number of Rules}

How any architecture scales as the number of rules increases is important,
but it is particularly important in this architecture because it is
expected that there will be a large number of rules.  There are
\numberOFrules{} rules in the full ruleset, whereas the minimum number of
rules required to parse the \numberOFlogFILES{} log files is
\numberOFrulesMINIMUM{}, \numberOFrulesMINIMUMpercentage{} of the total.  A
second set of statistics was generated using the minimum ruleset and the
results compared to the statistics generated using the full ruleset: the
percentage execution increase when using the full ruleset instead of the
minimal ruleset for normal, shuffled and reversed orderings is shown in
fig.~\refwithpage{Percentage execution time increase of maximum ruleset
over minimum ruleset}.  \showgraph{build/plot-normal-vs-smaller}{Percentage
execution time increase of maximum ruleset over minimum ruleset}{Percentage
execution time increase of maximum ruleset over minimum ruleset}

It is clear from fig.~\refwithpage{Percentage execution time increase of
maximum ruleset over minimum ruleset} that the increased number of rules
has a noticeable performance impact with reverse ordering, and to a lesser
extent with shuffled ordering.  The normal ordering (where are the most
frequently matching rules are tried first) shows a
\input{build/normal-vs-smaller-percentage-increase.tex} increase in
execution time for a \numberOFrulesMAXIMUMpercentage{} increase in the
number of rules.  These results show that the architecture scales very well
as the number of rules increases, and that sorting the rules is an
important optimisation enabling this.

\subsection{Coverage}

\label{coverage}

The parser's coverage of Postfix log files is separated into two parts: log
line coverage and mail coverage.  The former is initially more important,
as the parser must successfully parse every line if it is to be complete,
but subsequently the latter takes precedence because reproducing the path a
mail takes through Postfix is the aim of the parser.  Improving the former
is less intrusive, as it just requires new rules to be written; improving
the latter is more intrusive as it requires changes to actions, and it can
also be much harder to notice a deficiency.

The framework issues warnings for any log lines which are not parsed; no
warnings are issued for unparsed log lines while parsing the
\numberOFlogFILES{} test log files, so it can be safely concluded that
there are zero false negatives.  False positives are harder to quantify:
manually verifying that the correct regex parsed each of the 60,721,709 log
lines is infeasible, making it impossible to quantify the false positive
rate; however a random sample of 6039 log lines (0.00994\% of the total)
was parsed and the results verified to ensure that the correct regex parsed
each log line.

Coverage of mails is much more difficult to determine accurately than
coverage of log lines.  The parser can dump its state tables in a human
readable form; examining these tables with reference to the log files is
the best way to detect mails which were not handled properly.  The parser
issues warnings when it detects any errors, some of which may alert the
user to a problem.  There should be few or no warnings when parsing, and
when finished parsing the state table should only contain entries for mails
which had yet to be delivered when the log files ended, or were accepted
before the log files began.  A sample of 6000 log lines was parsed with all
debugging options enabled, resulting in 167,448 lines of output.  All
167,448 lines were examined in conjunction with the input log file and a
dump of the resulting database, verifying that for each of the input lines
the parser used the correct rule and executed the correct action, which in
turn produced the correct result and inserted the correct data in the
database.  The log segment produced 4 warnings (about mails which had
started before the log file segment), 10 mails remaining in the state
tables, and 1625 connections correctly entered in the database.  Given the
evidence detailed above, the author is confident that the false positive
rate when reconstructing a mail is zero.


\section{Conclusion}

The architecture's greatest strength is the ease with which is can be
adapted to deal with new requirements and inputs.  Parsing a variation of
an existing input is a trivial task: simply modify an existing rule or add
a new one with an appropriate regex and the task is complete; knowledge of
the framework is not required when writing rules.  Parsing a new category
of input is achieved by writing a new action and appropriate rules; quite
often the action will not need to interact directly with existing actions,
but when interaction is required the framework provides the necessary
facilities.  The decoupling of rules from actions allows different sets of
rules to be used with the same actions: e.g.\ a parser might have actions
to process versions one and two of a file format; the parser will then
parse either version one or version two depending on the ruleset selected.
The architecture makes it possible to apply commonly used programming
techniques such as object orientation, inheritance, composition, delegation
and roles when designing and implementing a parser, simplifying the process
of working within a team with shared responsibility or developing and
testing additional functionality.  This architecture is ideally suited to
parsing inputs where the input is not fully understood or does not follow a
fixed grammar: the architecture warns about unparsed inputs and other
errors, but continues on parsing as best it can, allowing the developer of
a new parser to decide which deficiencies are most important and require
attention first, rather than being forced to fix the first error which
arises.

The dataset gathered by the parser provides the foundation for the future
of this project: applying machine-learning algorithms to the data to
analyse and optimise the set of rules in use, followed by identifying
patterns in the data which could be used to write new filters to recognise
and reject spam rather than accepting it.  The current focus is on
clustering and decision trees to optimise the order in which rules are
applied; future efforts will involve using gathered data to train and test
new filters.  This task is very similar to optimising a black box
application based on its inputs and outputs, and this approach could be
applied to optimising the behaviour of any system given sufficient logging
to analyse.  An alternative approach to black box optimisation which uses
application profiling in conjunction with the application's error messages
to improve the error messages shown to users is described
in~\cite{black-box-error-reporting}; profiling data may be useful in
supplementing systems which fail to provide adequate log messages.

The Postfix log file parser based on this architecture provides a basis for
systems administrators to monitor the effectiveness of their anti-spam
measures and adapt their defences to combat the new techniques used by
those sending spam.  The parser is a fully usable application, built to
address a genuine need, rather than a proof of concept whose sole purpose
is to illustrate a new idea; it deals with the oddities and difficulties
which occur in the real world, rather than a clean, idealised scenario
developed to showcase the best features of a new approach.

\bibliographystyle{spmpsci}
\bibliography{../common/bibliography}
\label{bibliography}

% Redefine the command used to produce the glossary title so that it
% doesn't produce a new section - this will save space.
\renewcommand{\glossarytitle}{\noindent{}\textbf{\large{Glossary}}}
\printglossary{}


\end{document}
