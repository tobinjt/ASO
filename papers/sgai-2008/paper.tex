% $Id$
\documentclass[draft]{svmult}

% Useful stuff for math mode.
\usepackage{amstext}
% The name of the program, so I only have to change it in one place.
\newcommand{\parsername}{\PLP{}}
\newcommand{\parsernames}{\PLP{}'s}
% Include images
\usepackage[final]{graphicx}
%\renewcommand{\refname}{Bibliography}
% Add the bibliography into the table of contents.
%\usepackage[section,numbib]{tocbibind}
% Provides commands to distinguish between pdf and dvi output.
\usepackage{ifpdf}
\usepackage{url}

\usepackage[style=altlist,number=none,toc=true,global=true]{glossary}
\makeglossary{}

% This is necessary for URLs in the bibliography; I dunno why the url
% package doesn't work; don't include it when generating \PDF{} output.
\ifpdf{}
\else{}
    \usepackage{breakurl}
\fi{}
\usepackage{lastpage}

% Extra footnote functionality, including references to earlier footnotes.
\usepackage[bottom]{footmisc}

% Tell Latex to use scalable fonts
\usepackage{type1cm}

% Extra packages recommended by Springer.
\usepackage{mathptmx}
\usepackage{helvet}
\usepackage{courier}
\usepackage{makeidx}
\usepackage{multicol}

% Nicer citation lists - smaller spaces.
\usepackage{cite}
% Adds page numbers to bibliography entries, showing where they were cited.
% Breaks when cite is used.
% \usepackage{citeref}
% Check for unused references.
% \usepackage{refcheck}
\usepackage{varioref}
% Warnings rather than errors when references cross page boundaries.
\vrefwarning{}
% Errors when loops are encountered
\vrefshowerrors{}
% Show all the files/packages loaded.
%\listfiles

% \showgraph{filename}{caption}{label}
\newcommand{\showgraph}[3]{%
    \begin{figure}[hbt!]
        \caption{#2}\label{#3}
        \includegraphics{#1}
    \end{figure}
}

%\showtable{filename}{caption}{label}
\newcommand{\showtable}[3]{
    \begin{table}[ht]
        \caption{#2}\label{#3}
        \input{#1}
    \end{table}
}

% Replacement for \ref{}, adds the page number too.
\newcommand{\refwithpage}[1]{%
    %\empty{}\ref{#1} [p.~\pageref{#1}]%
    \empty{}\vref{#1}%
}
% Shorten the text used for references with page numbers.
\renewcommand{\reftextfaraway}[1]{%
    [p.~\pageref{#1}]%
}
% section references, automatically add \textsection
\newcommand{\sectionref}[1]{%
    %\textsection{}\refwithpage{#1}%
    \textsection{}\vref*{#1}%
}

% A command to format a Postfix daemon's name
\newcommand{\daemon}[1]{%
    \texttt{postfix/#1}%
}

% This is ridiculous, but I can't put @ in glossary entries, so . . .
\newcommand{\at}[0]{%
    @%
}

\newcommand{\tab}[0]{%
    \hspace*{2em}%
}

\newcommand{\numberOFlogFILES}[0]{%
    93%
}

\newcommand{\numberOFrules}[0]{%
    169%
}

\newcommand{\numberOFlogLINES}[0]{%
    60,721,709%
}

\newcommand{\numberOFlogLINEShuman}[0]{%
    60.72 million%
}

\newcommand{\numberOFactions}[0]{%
    18%
}

\begin{document}

\title*{A user-extensible and adaptable parser architecture}
\author{John Tobin and Carl Vogel}
\institute{John Tobin \at{} School of Computer Science and Statistics,
Trinity College, Dublin 2, Ireland. \newline{} \email{tobinjt@cs.tcd.ie}
\and{} Carl Vogel \at{} School of Computer Science and Statistics,
Trinity College, Dublin 2, Ireland. \newline{} \email{vogel@cs.tcd.ie}}
\maketitle

\abstract{%
    Some parsers need to be very precise and strict in what they parse, yet
    must allow users to easily adapt or extend the parser to parse new
    inputs, without requiring that user have an in-depth
    knowledge and understanding of the parser's internal workings.  This
    paper presents a novel parsing architecture which aims to make the
    process of parsing new inputs as simple as possible, enabling users to
    trivially add new rules or adapt existing rules (to parse variants of
    existing inputs) and relatively easily add new actions (to process a
    previously unknown class of input).  The architecture scales linearly
    as the number of rules and/or size of input increases, making it
    suitable for parsing large corpora or months of accumulated data.
}

XXX DOES THE ABSTRACT NEED TO BE LONGER\@?  IS IT A GOOD TITLE\@?

XXX DUE MONDAY JUNE 2$^{ND}$ 2008\@.  PRESENTATION PAPERS\@: MAX 14
PAGES\@.  POSTER PAPERS\@: MAX 6 PAGES\@.  BLACK AND WHITE ONLY\@.

XXX DO I NEED TO SAY THAT THINGS WILL BE EXPANDED UPON IN MY THESIS\@?

XXX CHECK EVERY URL IN THE BIBLIOGRAPHY\@.

\input{../../doc/logparser-acronyms.tex}

\section{Introduction}

\label{Introduction}

The architecture described herein was developed as part of a larger project
to improve anti-spam techniques by analysing the performance of the set of
filters currently in use, optimising the order and membership of the set
based on that analysis, and developing new measures to supplement the
existing filters where there is a deficiency.  The approach chosen to
measure performance was to analyse the log files produced by the mail
server, rather than modifying the mail server to generate performance
statistics; this approach improves the chances of other sites testing and
utilising the software.  The need arose for a parser capable of dealing
with the great number and variety of log lines produced by
Postfix~\cite{postfix}, the \SMTP{} server in use.  The variety occurs for
several reasons:

\begin{itemize}

    \item Log lines differ between versions of Postfix.
        
    \item The mail administrator can define custom rejection messages,
        resulting in custom log lines~\cite{postfix-lookup-tables,
        smtpd_access_readme, smtpd_per_user_control}.

    \item External resources utilised by Postfix (e.g.\ policy
        servers~\cite{policy-servers} or \RBL{} lookups) can also change
        their messages, sometimes without warning.

\end{itemize}

The parser must be designed so that adding support for parsing new log
lines is a simple task, as the log lines will continue to evolve over time.

Existing parsers were considered, but their parsing coverage was too low,
their parsing too inexact, and/or they did not extract sufficient data for
the purposes of this project.\footnote{A review of the existing parsers
considered for this project will be included in the author's thesis.} The
effort required to adapt and improve an existing parser was judged to be
greater than the effort to write one from scratch, as the techniques used
by the existing parsers severely limited their potential: some ignored the
majority of log lines, parsing specific log lines with great accuracy, but
in an inextensible fashion; others sloppily parsed the majority of log
lines, but were incapable of distinguishing between log lines of the same
type, e.g.\ rejections, and extracted only a portion of the data required
for this project.

The solution developed is conceptually simple: provide a few generic
functions (\textit{actions\/}), each capable of dealing with an entire
category of log lines (e.g.\ rejections), accompanied by a multitude of
precise patterns (\textit{rules\/}), each of which matches all log lines of
a specific type, and only that type (e.g.\ rejection by a specific
\RBLshort{}).


\section{Architecture}

\label{Architecture}

The architecture is split into three sections: framework, actions and
rules.  Each will be discussed separately, but first an overview:

\begin{description}

    \item [Framework]  The framework is the structure which actions and
        rules fit into.  It provides the parsing loop, loading and
        validation of rules, shared data storage, storage of results, and
        other support functions.

    \item [Actions]  The actions perform the work required to deal with a
        category of log lines, e.g.\ creating a new data structure when a
        log line indicating a remote client connected is processed.

    \item [Rules]  The rules are responsible for matching log lines,
        specifying the action to invoke and the data to be extracted from
        the log line.

\end{description}

The framework tries each rule in turn until it finds one which matches the
input log line, then invokes the action specified by that rule.

Decoupling the parsing rules from the associated actions and framework
allows new rules to be written and tested without requiring modifications
to the parser source code (significantly lowering the barrier to entry for
new or casual users who need to parse new log lines), and greatly
simplifies the framework, actions and rules. Decoupling the actions from
the framework simplifies both framework and actions: the framework provides
services to the actions, and doesn't need to perform any tasks specific to
the input being parsed; actions benefit from having services provided by
the framework, freeing them to concentrate on the task of accurately and
correctly processing the information provided by rules.

Decoupling also creates a clear separation of functionality: rules handle
low level details of identifying and extracting data from log lines;
actions handle the higher level details of assembling the required data,
dealing with the intricacies of the input being parsed, complications
arising, etc; the framework provides services to actions and manages the
parsing process.

There is some similarity between the parser's design and William Wood's
\ATN{}~\cite{atns, nlpip}, a tool used in Computational Linguistics for
creating grammars to parse or generate sentences.  The resemblance between
\ATN{} and the parser is accidental, but it is interesting how two
different approaches have a similar division of responsibilities, while
having completely different implementations and semantics.

% Do Not Reformat!

\begin{table}[ht]
    \caption{Similarities with ATN}\label{Similarities with ATN}
    % The \smallskip{} below stop the text hitting the lines.
    \begin{tabular}[]{lll}
        \hline
        \noalign{\smallskip}
        \ATN{}        & Parser    & Similarity                          \\
        \noalign{\smallskip}
        \hline
        \noalign{\smallskip}
        Networks      & Framework & Determines the sequence of
                                    transitions or actions which        \\
                      &           & constitutes a valid input.          \\
        Transitions   & Actions   & Save data and impose conditions the
                                    input must meet to be               \\
                      &           & considered valid.                   \\
        Abbreviations & Rules     & Responsible for classifying input.  \\
        \noalign{\smallskip}
        \hline
        \noalign{\smallskip}
    \end{tabular}
\end{table}

\subsection{Framework}

\label{Framework}

The framework provides utility and support functions for the actions and
rules.  It provides shared storage to pass data between actions, most
notably the data structures used to represent each connection and mail.
It loads and validates rules, controls
parsing of log files, invokes actions, tracks how often each rule matches
(to optimise rule ordering, see \sectionref{Rule ordering}), and
stores results in the database on request.  The framework is independent of
the parser being written: the same functionality will be required from the
framework for most or all parsers.

The function which finds the rule matching the log line and invokes the
requested action can be expressed in pseudo-code (with indentation denoting
flow of control) as:

\begin{verbatim}

for each line in the input files:
    for each rule defined by the user:
        if this rule matches the input line:
            perform the action specified by the rule
            skip the remaining rules
            process the next input line
    warn the user that the input line was not parsed

\end{verbatim}

The framework takes care of miscellaneous support functions and low level
details of parsing, freeing the authors of actions to concentrate on
writing productive code.  It links actions and rules, allowing either to be
improved independently of the other.  The framework is the core of the
architecture and is deliberately quite simple: the rules deal with the
varying log lines, and the actions deal with the intricacies and
complications encountered when parsing.

\subsection{Actions}

\label{Actions}

Each action contains the code required to deal with log lines from a
particular category, e.g.\ rejections.  The actions are parser-specific:
each parser will need to write the actions it requires from scratch, unless
it's extending an existing parser.  In the Postfix parser developed for
this project there are \numberOFactions{} actions and \numberOFrules{}
rules; the distribution of rules to actions is very skewed, as shown in
Fig.~\refwithpage{Distribution of rules per action}.
\showgraph{build/plot-action-distribution}{Distribution of rules per
action}{Distribution of rules per action} The distribution is skewed for
the same reason the architecture is so successful in enabling users to
parse new log lines: most new log lines can be parsed by a combination of a
new rule and an existing action.  Unsurprisingly the action with the
highest number of associated rules is \texttt{REJECTION}, the action which
handles rejections; it's followed by \texttt{SAVE\_BY\_QUEUEID}, the action
responsible for handling informative inputs: these are log lines that
supplement the data gathered from other log lines (such as rejections), but
that don't by themselves represent interesting events.  The third most
common action is, perhaps surprisingly, \texttt{IGNORE}: there are many log
lines which are not of interest to the parser, e.g.\ warning messages about
DNS mismatches, or log lines lacking a way to identify which data structure
they should be associated with.\footnote{Postfix allocates a queueid
(explained in the glossary,~\sectionref{Glossary}) to each mail; most log
lines contain the queueid of the associated mail, but some log lines
(particularly with older versions of Postfix) lack a queueid, preventing
their association with a particular data structure.}  The parser warns
about every unparsed log line, to alert the user that they need to alter or
extend their rules; the \texttt{IGNORE} action does nothing when executed,
allowing uninteresting log lines to be parsed without causing any effects.
The remaining actions have only one or two associated rules: in some cases
this is because there will only ever be one type of log line for that
action, e.g.\ all log lines showing that a remote client has connected are
matched by a single rule and handled by the \texttt{CONNECT} action; in
other cases the action is required to address a deficiency in the log
files, or a complication which arises during
parsing.\footnote{\label{complications footnote}The complications which
arose during development of the parser will be documented in detail in the
author's thesis.}

The ability to add special purpose actions to deal with difficulties and
new requirements which are discovered during parser development is one of
the strengths of this architecture: instead of writing one monolithic
function which must be modified to support new behaviour or resolve
challenges, with all the attendant risks of adversely affecting the
existing, working parser, when a new requirement arises a new, independent
action can be written to satisfy it.  In some cases the new action will
require the cooperation of other actions, e.g.\ to set or check a flag;
there is a possibility of introducing failure by modifying existing
actions, but modifications will be smaller and occur less frequently than
with a monolithic architecture, and thus failures should be less likely and
easier to test for and diagnose.  The parser can be written in an Object
Oriented~\cite{Wikipedia-object-orientation} style, allowing sub-classes to
extend or override actions in addition to adding new actions; because each
action is an independent function, the sub-class need only modify the
action it is interested in, rather than reproducing large chunks of
functionality.

\label{complications}

During development of the parser it became apparent that in addition to the
obvious variety in log lines, there were a large number of complications to
be overcome.\footref{complications footnote}  Some were the result of
deficiencies in Postfix's logging, which were improved or resolved by later
versions of Postfix; others were due to the vagaries of process scheduling,
client behaviour, and administrative actions.  All were successfully
accommodated in the parser: adding new actions was enough for several of
the complications; others required modifications to a single existing
action to work around the difficulties; the remainder were resolved by
adapting actions to pass information to subsequent actions (via the
framework), which then check for the extra information and change their
behaviour appropriately.


\subsection{Rules}

\label{Rules}

Rules are responsible for categorising inputs, specifying both the data to
be extracted from each log line and the action that should be invoked.
This implementation stores the rules in the same \SQL{} database that the
results are stored in; this removes any doubt as to which set of rules was
used to produce a set of results.  Other implementations are free to store
the rules in whatever fashion suits their needs.  Each rule defines several
attributes:

\begin{description}

    \item [program] The Postfix program whose log lines the rule should be
        applied to.  This avoids needlessly trying rules that won't match
        the log line, or worse, might match unintentionally.  Rules whose
        program is \texttt{*} will be tried against any log lines not
        parsed by program specific rules, allowing generic rules to be
        written.

    \item [regex] The \regex{} to match the log line against.  For
        efficiency every rule's \regex{} is compiled before attempting to
        parse the log file --- otherwise each \regex{} would be recompiled
        each time it was used, resulting in a large, data dependent
        slowdown.\footnote{This will be discussed in more detail in the
        author's thesis.}  XXX MAYBE SCRAP THE SECOND SENTENCE\@?

    \item [result\_cols] Specifies how the fields in the
        log line will be extracted.  The format is:
        \tab{} \texttt{smtp\_code = 1; recipient = 2, sender = 4;}
        \newline i.e.\ semi-colon or comma separated assignment statements,
        with the variable name on the left and the matching capture from
        the \regex{} on the right.

    \item [result\_data] Sometimes rules need to supply a piece of data
        which isn't present in the log line: e.g.\ setting
        \texttt{smtp\_code} when mail is accepted.  The format is the same
        as for \texttt{result\_cols}, except that arbitrary data is
        permitted on the right hand side of the assignment.

    \item [action] The action that will be invoked when this rule matches a
        log line.

    \item [hits] This counter is used to optimise the order in which rules
        are tried against the input lines.  Assuming that the distribution
        of log lines is reasonably consistent between log files, rules
        matching more frequently will be tried before rules matching less
        frequently, lowering the parser's execution time.

    \item [priority] This is the user-configurable companion to hits:
        priority overrides hits when sorting rules.  This allows more
        specific rules to take precedence over more general rules
        (described in \sectionref{overlapping rules}).

\end{description}

The rules used by the Postfix log file parser define several additional
attributes; each implementation is free to add any additional attributes it
requires.

Adding new rules is simple: simply define all the required attributes and
append the new rule to wherever the existing rules are stored.  The
software developed for this project includes a utility based on
\SLCT{}~\cite{slct-paper} to aid in producing \regexes{} to parse
previously unparsed inputs.

\label{overlapping rules}

XXX DO I NEED TO DISCUSS OVERLAPPING RULES\@?

The parser does not try to detect overlapping rules; that responsibility is
left to the author of the rules.\footnote{It may be possible to parse each
rule's regex and determine if any overlap.  The author has not attempted to
do this: given the tremendous power and flexibility of regexes,
particularly Perl's regexes~\cite{perlre}, it would be an extremely
difficult task.} Unintentionally overlapping rules lead to inconsistent
parsing and data extraction because the first matching rule wins and the
order in which rules are tried against each line may change between log
files.  Overlapping rules are frequently a requirement, allowing a more
specific rule to match some log lines and a more general rule to match the
majority, e.g.\ separating \SMTP{} delivery to specific sites from \SMTP{}
delivery to the rest of the world.  The algorithm provides a facility for
specifying the order of overlapping rules: the priority field in each rule
(defaults to zero).  Rules are sorted by priority, highest first, and then
rules with the same priority are sorted by the number of successful matches
when parsing the previous log file.  Negative priorities may be useful for
catchall rules.

XXX MAYBE REMOVE THE SECTION ON DETECTING OVERLAPPING RULES\@?

Detecting overlapping rules is difficult, but the following approaches may
be helpful:

\begin{itemize}

    \item Sort by \regex{} and visually inspect the list, e.g.\ with \SQL{}
        similar to: \textbf{select regex from rules order by regex;}

    \item Compare the results of parsing using sorted, shuffled and
        reversed rules.\footnote{See \sectionref{rule efficiency} for more
        details of sorting the rules.}  Parse a number of log files using
        normal sorting, then dump a textual representation of the rules,
        connections and results tables.  Repeat with shuffled and reversed
        rules, starting with a fresh database.  If there are no overlapping
        rules the tables from each run will be identical; differences
        indicate overlapping rules.  Which rules overlap can be determined
        by examining the differences in the tables: each result contains a
        reference to the rule which created it, if the references differ
        between runs the two rules referenced in the differing records
        overlap.  Unfortunately this method cannot prove the absence of
        overlapping rules; it can detect overlapping rules, but only if
        there are log lines in the input files which match more than one
        rule.

\end{itemize}




\subsection{Architecture characteristics}

\label{Architecture characteristics}

\begin{description}

    \item [Context-free rules:]  Rules are context free: they do not take
        into account inputs which have previously been seen, nor do they
        perform any kind of lookahead to future inputs.  In context-free
        grammar terms the parser rules could be described as:
        $\text{\textless{}input\textgreater{}} \mapsto \text{rule-1} |
        \text{rule-2} | \text{rule-3} | \dots | \text{rule-n}$

    \item [Partially context-sensitive actions:]  XXX IS THIS THE CORRECT
        TERMINOLOGY\@?  Actions can consult the results (or lack of
        results) of previous actions when executing, providing some context
        sensitivity.  Actions can not inspect past or upcoming inputs.

    \item [Matching rules to log lines is simple:]  The first matching rule
        wins: there is no backtracking to try alternate rules, no attempt
        is made to pick a \textit{best\/} rule; the first rule which
        matches the line is the rule which parses it and determines the
        action that will be invoked.  The priority attribute may be used to
        change the ordering of rules.

    \item [Transduction:]  The parser can be thought of as implementing
        transduction: it takes data in one form (log files) and transforms
        it to another form (a database); other output formats may be more
        suitable for other implementations.

    \item [Closer to \NLP{} than fixed grammar:]  Unlike traditional
        parsers such as those used when compiling a programming language,
        this architecture does not have a fixed grammar specification which
        inputs must adhere to.  The architecture is capable of dealing with
        overlapping inputs, out of order inputs, and ambiguous inputs where
        heuristics must be applied --- all have arisen and been
        successfully dealt with in the Postfix log parser.

    \item [Line oriented:]  The architecture is line oriented at present:
        there is no facility for rules to consume more input or push
        unused input back onto the input stream.  This was not a deliberate
        decision, rather a consequence of the line oriented nature of the
        log files; other implementations could pursue a more flexible
        approach.

\end{description}

\section{Results}

Parsing efficiency is an obvious concern when the parser routinely needs to
deal with 75 MB log files containing 300,000 log lines (generated daily on
a mail server handling mail for approximately 700 users --- large scale
mail servers would produce much larger log files on a daily basis).  When
generating the data for the graphs included in this document,
\numberOFlogFILES{} log files (totaling 10.08 GB, \numberOFlogLINEShuman{}
log lines) were each parsed 10 times, the results from first run discarded,
and the execution time for the remaining 9 runs averaged.  XXX DO I NEED TO
EXPLAIN WHY\@?  The first run is discarded for two reasons:

\begin{enumerate}

    \item The execution time will be higher because the log file must be
        read from disk, whereas for subsequent runs the log file will be
        cached in memory by the operating system.

    \item The execution time will also be higher because the rule ordering
        will be sub-optimal compared to subsequent runs.

\end{enumerate}

Saving results to the database was disabled for the test runs, as that
dominates the run time of the program, and the tests are aimed at measuring
the speed of the parser rather than the speed of the database and the disks
the database is stored on.

\subsection{Architecture Scalability}

An important property of a parser is how execution time scales relative to
input size: does it scale linearly, polynomially, or exponentially?
Figure~\refwithpage{execution time vs file size vs number of lines graph}
shows the execution time in seconds, file size in MB, and tens of thousands
of log lines per log file.  All three lines run roughly in parallel, giving
a visual impression that the algorithm scales linearly with input size.
This impression is borne out by fig.~\refwithpage{execution time vs file
size vs number lines factor} which plots the ratio of file size vs
execution time, and ratio of number of log lines vs execution time (higher
is better in both cases).  As the reader can see the ratios are quite
tightly banded across the graph, showing that the algorithm scales
linearly: the much larger log files at points 62--68 on the X axis in
fig.~\refwithpage{execution time vs file size vs number of lines graph}
actually cause the ratio to increase (i.e.\ improve), rather than decrease.
The strange behaviour where larger log files are parsed more efficiently is
explained in \sectionref{Why are there dips in the graphs?}.  

\showgraph{build/plot-normal-filesize-numlines}{Execution time vs file
size vs number of lines}{execution time vs file size vs number of lines
graph}

\showgraph{build/plot-normal-filesize-numlines-factor}{Ratio of file
size and number of lines to execution time}{execution time vs file size vs
number lines factor}

XXX MINIMUM VERSUS MAXIMUM NUMBER OF RULES\@.

\subsection{Rule ordering}

\label{Rule ordering}
\label{rule ordering for efficiency}
\label{rule efficiency}

XXX MINIMUM VERSUS MAXIMUM NUMBER OF RULES\@.

At the time of writing there are \numberOFrules{} different rules, with the
top 10\% matching the vast majority of the log lines, and the remaining log
lines split across the other 90\% of the rules in a Power Law
distribution~\cite{powerlaw} (as shown in fig.~\refwithpage{rule hits
graph}).  \showgraph{build/plot-hits}{Hits per rule}{rule hits graph}
Assuming that the distribution of log lines is reasonably steady over time,
parser efficiency should benefit from trying more frequently matching rules
before those which match less frequently.  To test this hypothesis three
full test runs were performed with different rule orderings:

\begin{description}

    \item [normal]  Hypothetically the most optimal order: rules which
        match most often will be tried first.

    \item [shuffle] Random ordering, intended to represent an unsorted rule
        set.  The rules will be shuffled once before use and will retain
        that ordering for the entirety of the log file.  Note that the
        ordering will change every time the parser is executed, so 10
        different orderings will be generated for each log file in the test
        run.

    \item [reverse] Hypothetically the worst order: the most frequently
        matching rules will be tried last.

\end{description}

Figure~\refwithpage{percentage increase of shuffled and reversed over
normal} shows the percentage increase of execution times for shuffled and
reversed rules.  Overall this optimisation provides a modest but worthwhile
performance increase of over 10\%, for a small investment in time and
programming.
\showgraph{build/plot-normal-shuffle-reverse-factor}{Percentage increase of
execution time for different rule orderings}{percentage increase of
shuffled and reversed over normal}

\noindent\textbf{Why are there dips in the graphs?}

\label{Why are there dips in the graphs?}

The dips at log files 22 and 62--68 in fig.~\refwithpage{percentage
increase of shuffled and reversed over normal} correspond to peaks in log
file size in fig.~\refwithpage{execution time vs file size vs number of
lines graph}, and peaks in fig.~\refwithpage{execution time vs file size vs
number lines factor} (where a peak means that more lines are processed per
second, i.e.\ performance is better).  The explanation for this took some
time to arrive at, but it turns out to be reasonably simple.  The large log
files in question were caused by a mail forwarding loop, where the
distribution of log lines is quite different to normal, resulting in
different performance characteristics.\footnote{The cause of the mail
forwarding loop will be explained in the author's thesis.}

XXX REWRITE THIS NEXT PARAGRAPH SO IT DOESN'T MENTION postfix/smtpd.  The
vast majority of log lines when a mail loop occurs are from Postfix
components which have a small number of rules associated with them, whereas
in general \daemon{smtpd} adds the majority of log lines, and also has the
highest number of rules.  \daemon{smtpd} log lines are distributed across
rules much more evenly than the log lines which occur frequently during a
mail loop, so the average number of rules required to parse a
\daemon{smtpd} log line is much higher that the average number required to
parse other log lines.

These two characteristics combine to reduce the average number or rules
required to parse a log line when there is a mail loop, as shown by the
peaks in fig.~\refwithpage{execution time vs file size vs number lines
factor} and troughs in fig.~\refwithpage{percentage increase of shuffled
and reversed over normal}.  When the rule ordering is reversed the majority
of log lines generated by a mail loop will be parsed using very few rules,
because there are very few rules for the Postfix components producing the
log lines.  Without a mail loop the Postfix components producing the
majority of log lines have a large number of rules associated with them,
and reverse ordering deliberately starts with the least likely rules and
moves towards the most likely.  This leads to a noticeable drop in the
average time required to parse a log line during a mail loop, as shown in
fig.~\refwithpage{percentage increase of shuffled and reversed over
normal}.  The number of rules which need to be consulted when the ordering
is shuffled varies between the optimum and the nadir, and the performance
varies proportionally.



\subsection{Coverage}

\label{coverage}

XXX THIS NEEDS TO BE IMPROVED AND PROOFREAD\@.

The discussion of the parser's coverage of Postfix log files is separated
into two parts: log lines covered and mails covered.  The first is
important because the parser should handle all (relevant) log lines it's
given; the second is equally important because the parser must properly
deal with every mail if it is to be useful.  Improving the former is
less intrusive, as it just requires new rules to be written; improving the
latter is much more intrusive as it requires changes to the parser
algorithm, and it can also be much harder to notice a deficiency.

Warnings are issued for any log lines which are not parsed; no warnings are
issued for unparsed log lines while testing with the \numberOFlogFILES{}
test log files, so it can be safely concluded that there are zero false
negatives.  False positives are harder to quantify: short of examining each
of the 60,721,709 log lines and determining which \regex{} parsed it, there
is no way to be sure that every line was parsed by the correct \regex{},
making it impossible to quantify the false positive rate; however a random
sample of 6039 log lines was parsed and the results checked manually to
ensure that the correct \regex{} parsed each log line.

Coverage of mails is much more difficult to determine accurately than
coverage of log lines.  The parser can dump its state tables in a human
readable form; examining these tables with reference to the log files is
the best way to detect mails which were not handled properly.  The parser
issues warnings when it detects any errors, some of which may alert the
user to a problem.  There should be few or no warnings when parsing, and
when finished parsing the state table should only contain entries for mails
which had yet to be delivered when the log files ended, or were accepted
before the log files began.

Verifying by inspection that the parser correctly deals with all 60,721,709
lines in the test log files is infeasible, but verifying a subset of those
log files is a tractable, if extremely time consuming, task.  A sample of
log lines was parsed with all debugging options enabled, resulting in
167,448 lines of output.  All 167,448 lines were examined in conjunction
with the input log and a dump of the resulting database, verifying that for
each of the input lines the parser used the correct rule and executed the
correct action, which in turn produced the correct result and inserted the
correct data in the database.  The log segment produced 4 warnings, 10
mails remaining in the state tables, and 1625 connections correctly entered
in the database.

Given the evidence detailed above, the author is confident that the false
positive rate when reconstructing a mail is exceedingly low, if not
approaching zero.

Parser coverage is divided into two topics in this section: log lines
covered and mails covered.\footnote{There will be substantially more
coverage discussion in the author's thesis.}  The former is initially more
important, as the parser must successfully parse every line if it is to be
complete, but subsequently the latter takes precedence because reproducing
the path a mail takes through Postfix is the aim of the parser.  Increasing
the percentage of log lines parsed is relatively simple and non-intrusive:
adding new rules or modifying existing rules is simplified by the
separation of rules, actions and framework.  Improving the logical coverage
is harder, as the actions taken by Postfix must be reconstructed by the
author, and the new sequence of actions integrated into the existing model
without breaking the existing parsing.  Detecting a deficiency in the
parsing algorithm is also significantly harder than detecting unparsed log
lines, as the parser will warn about any unparsed line, whereas discovering
a flaw in the parser requires understanding of the warnings produced and
the mails remaining in the state table.  Rectifying a flaw in the parser
requires an understanding of both the parser and Postfix's log files, and
investigative work to determine the cause of the deficiency, followed by
further examination of the log files in developing a solution.

\section{Conclusion}

XXX WRITE THE CONCLUSION\@.

Makes adding new rules trivial, new actions tractable.  Knowledge of the
framework is rarely necessary.

The architecture's greatest strength is the ease with which new inputs
can be parsed and new actions written.

\bibliographystyle{spmpsci}
\bibliography{../common/bibliography}
\label{bibliography}

% Redefine the command used to produce the glossary title, because the
% default command produces an unnumbered section whereas I want a numbered
% section.
\renewcommand{\glossarytitle}{\section{Glossary}\label{Glossary}}
\printglossary{}

\glossary{name={queueid},description={
    Each mail in Postfix's queue is assigned a queueid to uniquely identify
    it.  Queueids are assigned from a limited pool, so although they are
    guaranteed to be unique for the lifetime of the mail, given sufficient
    time they will be reused.
}}


\end{document}

