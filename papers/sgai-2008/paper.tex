% $Id$
\documentclass[]{svmult}

% Useful stuff for math mode.
\usepackage{amstext}
% The name of the program, so I only have to change it in one place.
\newcommand{\parsername}{\PLP{}}
\newcommand{\parsernames}{\PLP{}'s}
% Include images
\usepackage[final]{graphicx}
\renewcommand{\refname}{Bibliography}
% Add the bibliography into the table of contents.
\usepackage[section,numbib]{tocbibind}
% Provides commands to distinguish between pdf and dvi output.
\usepackage{ifpdf}
\usepackage{url}

\usepackage[acronym=true,style=altlist,number=none,toc=true]{glossary}
\makeglossary{}
\makeacronym{}

% This is necessary for URLs in the bibliography; I dunno why the url
% package doesn't work; don't include it when generating \PDF{} output.
\ifpdf{}
\else{}
    \usepackage{breakurl}
\fi{}
\usepackage{lastpage}

% Extra footnote functionality, including references to earlier footnotes.
\usepackage[bottom]{footmisc}

% Extra packages recommended by Springer.
\usepackage{mathptmx}
\usepackage{helvet}
\usepackage{courier}
\usepackage{makeidx}
\usepackage{multicol}
\usepackage{cite}


% \showgraph{filename}{caption}{label}
\newcommand{\showgraph}[3]{
    \begin{figure}[hbt!]
        \caption{#2}\label{#3}
        \includegraphics{#1}
    \end{figure}
}

%\showtable{filename}{caption}{label}
\newcommand{\showtable}[3]{
    \begin{table}[ht]
        \caption{#2}\label{#3}
        \input{#1}
    \end{table}
}

% Replacement for \ref{}, adds the page number too.
\newcommand{\refwithpage}[1]{%
    \empty{}\ref{#1} [page~\pageref{#1}]%
}
% section references, automatically add \textsection
\newcommand{\sectionref}[1]{%
    \textsection{}\refwithpage{#1}%
}

% A command to format a Postfix daemon's name
\newcommand{\daemon}[1]{%
    \texttt{postfix/#1}%
}

% This is ridiculous, but I can't put @ in glossary entries, so . . .
\newcommand{\at}[0]{%
    @%
}

\newcommand{\tab}[0]{%
    \hspace*{2em}%
}

\begin{document}

\title*{A user-extensible and adaptable parser architecture}
\author{John Tobin and Carl Vogel}
\institute{John Tobin \at{} School of Computer Science and Statistics,
Trinity College, Dublin 2, Ireland. \newline{} \email{tobinjt@cs.tcd.ie} \and{}
Carl Vogel \at{} School of Computer Science and Statistics,
Trinity College, Dublin 2, Ireland. \newline{} \email{vogel@cs.tcd.ie}}
\maketitle

\abstract{%
    Some parsers need to be very precise and strict in what they parse, yet
    must allow users to easily adapt existing rules or add new rules to
    parse new inputs, without requiring the user to have an in-depth
    knowledge and understanding of the parser's internal workings.  This
    paper presents a novel parsing architecture which aims to make the
    process of parsing new inputs as simple as possible, enabling users to
    trivially add new rules or adapt existing rules (to parse variants of
    existing inputs) and relatively easily add new actions (to do something
    useful with a previously unknown class of input).  The architecture
    provides a framework which actions and rules fit into: this framework
    manages the parsing process, provides support functions for the
    actions, and automatically performs some optimisations which improve
    the speed of parsing.
}

XXX DUE MONDAY JUNE 2$^{ND}$ 2008\@.

PRESENTATION PAPERS\@: MAX 14 PAGES\@.

POSTER PAPERS\@: MAX 6 PAGES\@.

BLACK AND WHITE ONLY\@.

XXX FORMAT OF REFERENCES\@.

\input{../../doc/logparser-acronyms.tex}

\section{Introduction}

\label{Introduction}

The architecture described herein was developed as part of a larger project to
improve anti-spam techniques by analysing the performance of the set of filters
currently in use, optimising the order and membership of the set based on that
analysis, and developing new measures to supplement the existing filters where
there is a deficiency.  The approach chosen was to analyse the log files
produced by the mail server, rather than modifying the mail server to generate
performance statistics; this approach improves the chances of other sites
testing and utilising the software.  The need arose for a parser capable of
dealing with the huge variety of log lines produced by Postfix~\cite{postfix},
the \MTA{} in use: 

\begin{itemize}

    \item Log lines differ between versions of Postfix.
        
    \item The mail administrator can define custom rejection messages,
        resulting in custom log lines.

    \item External resources utilised by Postfix (e.g.\ policy
        servers~\cite{policy-servers} or \RBL{} lookups) can also change
        their messages, without warning.

\end{itemize}

Existing parsers were considered, but their parsing coverage was too low,
their parsing too inexact, and/or they did not extract sufficient data for
the purposes of this project.\footnote{A review of the existing parsers
considered for this project will be included in the thesis.} The effort
required to adapt and improve an existing parser was judged to be greater
than the effort to write one from scratch, as the techniques used by the
existing parsers severely limited their potential: some ignored the
majority of log lines, parsing specific log lines with great accuracy, but
in an inextensible fashion; others sloppily parsed the majority of log
lines, but were incapable of distinguishing between log lines of the same
type, e.g.\ rejections, and the data they extracted was constrained to the
\LCD{}.

The solution developed is conceptually simple: provide a few generic functions,
each capable of dealing with an entire \textit{category\/} of log lines (e.g.\
rejections), accompanied by a multitude of precise patterns, each of which
matches all log lines of a specific type, and only that type (e.g.\ rejection
by a specific \RBL{}).




XXX WHERE SHOULD I DISCUSS COMPLICATIONS\@?

During development of the parser it became apparent that in addition to the
obvious variety in log lines, there were a large number of complications to
be overcome

\section{Architecture}

\label{Architecture}

Explain adding new rules and actions.  Cut down rule attributes to the
minimum possible.

Thoughts from Carl: 

\begin{itemize}

    \item context-free rules

    \item partially context-aware actions, though not really

    \item transduction: input log lines $\rightarrow$ database

    \item Closer to NLP than fixed grammar.

\end{itemize}

XXX BEGIN\@:

The architecture is split into three sections: framework, actions and
rules.  Each will be discussed separately, but first an overview:

\begin{description}

    \item [Framework]  The framework is the structure which actions and
        rules fit into.  The framework provides the parsing loop, loading
        and validation of rules, shared data storage, storage of results,
        and other support functions.

    \item [Actions]  The actions perform the work required to deal with a
        category of log lines, e.g.\ creating a new data structure when a
        log line indicating a remote client connected is processed.

    \item [Rules]  The rules are responsible for matching log lines,
        specifying the action to invoke and the data to be extracted from
        the log line.

\end{description}

The framework tries each rule in turn until it finds one which matches the
input line, then invokes the action specified by that rule.

Decoupling the parsing rules from the associated actions and framework allows
new rules to be written and tested without requiring modifications to the
parser source code (significantly lowering the barrier to entry for new or
casual users who need to parse new log lines), and greatly simplifies
framework, actions and rules. Decoupling the actions from the
framework simplifies both framework and actions: the framework provides
services to the actions, and doesn't need to deal with the complications which
arise, or the task of reconstructing a mail's journey through Postfix; actions
benefit from having services provided by the framework, freeing them to
concentrate on the task of accurately reconstructing each mail's journey
through Postfix and dealing with the complications arising (XXX REFERENCE).

Decoupling also creates a clear separation of functionality: rules handle low
level details of identifying log lines and extracting data from a log line;
actions handle the higher level details of following the path a mail takes
through Postfix, assembling the required data, dealing with complications
arising, etc; the framework provides services to actions and stores data.  

There is some similarity between the parser's design and William Wood's
\ATN{}~\cite{atns, nlpip}, a tool used in Computational Linguistics for
creating grammars to parse or generate sentences.  The resemblance between
\ATN{} and the parser is accidental, but it is interesting how two
different approaches have a similar division of responsibilities, while
having completely different implementations and semantics.

% Do Not Reformat!

\begin{table}[ht]
    \caption{Similarities with ATN}\label{Similarities with ATN}
    % The \smallskip{} below stop the text hitting the lines.
    \begin{tabular}[]{lll}
        \hline
        \noalign{\smallskip}
        \ATN{}        & Parser    & Similarity                          \\
        \noalign{\smallskip}
        \hline
        \noalign{\smallskip}
        Networks      & Framework & Determines the sequence of
                                    transitions or actions which        \\
                      &           & constitutes a valid input.          \\
        Transitions   & Actions   & Save data and impose conditions the
                                    input must meet to be               \\
                      &           & considered valid.                   \\
        Abbreviations & Rules     & Responsible for classifying input.  \\
        \noalign{\smallskip}
        \hline
        \noalign{\smallskip}
    \end{tabular}
\end{table}

\subsection{Framework}

\label{Framework}

The framework provides utility and support functions for the actions and
rules.  It provides shared storage to pass data between actions, most
notably the data structures used to represent each connection and mail,
though there are some others.  It loads and validates rules, controls
parsing of log files, invoking actions and tracking performance data, and
stores results on request.

The function which finds the rule matching the input line and invokes the
requested action can be expressed in pseudo-code (with indentation denoting
flow-of-control) as:

\begin{verbatim}

for each line in the input files:
    for each rule defined by the user:
        if this rule matches the input line:
            perform the action specified by the rule
            skip the remaining rules
            process the next input line
    warn the user that the input line was not parsed

\end{verbatim}

The function tracks the number of times each rule matches; this data is
used during subsequent parsing to optimise the order the rules are tried
in.  Rules specify the Postfix component which produces the log lines they
parse; this avoids needlessly trying rules which won't match the log line,
or worse, might match unintentionally.

\subsection{Actions}

\label{Actions}

Actions contain the code required to deal with log lines from a particular
category, e.g.\ rejections.  


\subsection{Rules}

\label{Rules}

Rules are responsible for categorising input lines, specifying both the
data to be extracted from each line and the action that should be invoked.
This implementation stores the rules in the same \SQL{} database that the
results are stored in; this removes any doubt as to which set of rules was
used to produce a set of results.  Other implementations are free to store
the rules in whatever fashion suits their needs.  Each rule defines several
attributes:

\begin{description}

    \item [program] The Postfix program whose log lines the rule applies
        to.  This avoids needlessly trying rules which won't match the log
        line, or worse, might match unintentionally.  Rules whose program
        is \texttt{*} will be tried against any log lines which aren't
        parsed by program specific rules.

    \item [regex] The \regex{} to match the log line against.  The \regex{}
        will first have several keywords expanded: this simplifies reading
        and writing rules; avoids needless repetition of complex \regex{}
        components; allows the components to be corrected and/or improved
        in one location; and makes each \regex{} largely self-documenting.

        For efficiency the keywords are expanded and every rule's \regex{}
        is compiled before attempting to parse the log file --- otherwise
        each \regex{} would be recompiled each time it was used, resulting
        in a large, data dependent slowdown.  Rule efficiency concerns are
        discussed in \sectionref{rule efficiency}, with the impact of
        compiling and caching \regexes{} covered in \sectionref{Caching
        each regex}.

    \item [result\_cols] Specifies how the fields in the
        log line will be extracted.  The format is:
        \newline \tab{} \texttt{smtp\_code = 1; recipient = 2, sender = 4;}
        \newline i.e.\ semi-colon or comma separated assignment statements,
        with the variable name on the left and the matching capture from
        the \regex{} on the right hand side.

    \item [result\_data] Sometimes rules need to supply a piece of data
        which isn't present in the log line: e.g.\ setting
        \texttt{smtp\_code} when mail is accepted.  The format and allowed
        variables are the same as for \texttt{result\_cols}, except that
        arbitrary data\footnote{Commas and semi-colons cannot be escaped
        and thus cannot be used.  This is intended for use with small
        amounts of data rather than large amounts in any one rule, so
        dealing with escape sequences was deemed unnecessary.} is permitted
        on the right hand side of the assignment.

    \item [action] The action that will be invoked when this rule matches a
        log line; actions are describe in \sectionref{Actions}.

    \item [hits] This counter starts at zero and is incremented each time
        the rule successfully matches.  At the start of each run the parser
        sorts the rules in descending order of hits, and at the end of the
        run updates every rule's hits.  Assuming that the distribution of
        log lines is reasonably consistent between log files, rules
        matching more commonly occurring log lines will be tried before
        rules matching less commonly occurring log lines, lowering the
        program's execution time.  Rule ordering for efficiency is
        discussed in \sectionref{rule ordering for efficiency}.

    \item [priority] This is the user-configurable companion to hits: rules
        will be tried in order of priority, overriding hits.  This allows
        more specific rules to take precedence over more general rules
        (described in \sectionref{overlapping rules}).

\end{description}

The rules used by the Postfix log file parser define several additional
attributes; each implementation is free to add any additional attributes it
requires.

\subsubsection{Overlapping rules}

\label{overlapping rules}

The parser does not try to detect overlapping rules; that responsibility is
left to the author of the rules.\footnote{It may be possible to parse
each rule's regex and determine if any overlap.  The author has not
attempted to do this: given the tremendous power and flexibility of
regex, particularly Perl's (XXX REFERENCE FOR PERL\@; GLOSSARY ENTRY?)
regex, it would be an extremely difficult task.} Unintentionally
overlapping rules lead to inconsistent parsing and data extraction because
the order in which rules are tried against each line may change between log
files, and the first matching rule wins.  Overlapping rules are frequently
a requirement, allowing a more specific rule to match some log lines and a
more general rule to match the majority, e.g.\ separating \SMTP{} delivery
to specific sites from \SMTP{} delivery to the rest of the world.  The
algorithm provides a facility for ordering overlapping rules: the priority
field in each rule (defaults to zero).  Rules are sorted by priority,
highest first, and then rules with the same priority are sorted by the
number of successful matches when parsing the previous log file.  Negative
priorities may be useful for catchall rules.

Detecting overlapping rules is difficult, but the following approaches may
be helpful:

\begin{itemize}

    \item Sort by \regex{} and visually inspect the list, e.g.\ with \SQL{}
        similar to: \textbf{select regex from rules order by regex;}

    \item Compare the results of parsing using sorted, shuffled and
        reversed rules.\footnote{See \sectionref{rule efficiency} for more
        details of sorting the rules.}  Parse a number of log files using
        normal sorting, then dump a textual representation of the rules,
        connections and results tables.  Repeat with shuffled and reversed
        rules, starting with a fresh database.  If there are no overlapping
        rules the tables from each run will be identical; differences
        indicate overlapping rules.  Which rules overlap can be determined
        by examining the differences in the tables: each result contains a
        reference to the rule which created it, if the references differ
        between runs the two rules referenced in the differing records
        overlap.  Unfortunately this method cannot prove the absence of
        overlapping rules; it can detect overlapping rules, but only if
        there are log lines in the input files which match more than one
        rule.

\end{itemize}


\section{Results}

Scalability, ordering, caching regexes.  Interesting to run with all rules
versus running with the minimum number necessary to parse the test log
files.  Then try that again with different orderings?  Coverage.

\section{Conclusion}

Makes adding new rules trivial, new actions tractable.  Knowledge of the
framework is rarely necessary.

\bibliographystyle{../common/bibliography-style}
\bibliography{../common/bibliography}
\label{bibliography}

% Redefine the command used to produce the glossary title, because the
% default command produces an unnumbered section whereas I want a numbered
% section.
\renewcommand{\glossarytitle}{\section{Glossary}\label{Glossary}}
\printglossary{}
% Redefine the command a second time to produce acronyms instead of a
% glossary.
\renewcommand{\glossarytitle}{\section{Acronyms}\label{Acronyms}}
\printacronym{}
\end{document}
